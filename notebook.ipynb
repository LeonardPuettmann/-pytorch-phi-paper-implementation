{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "PhiForCausalLM                                          --\n",
       "├─PhiModel: 1-1                                         --\n",
       "│    └─Embedding: 2-1                                   104,857,600\n",
       "│    └─Dropout: 2-2                                     --\n",
       "│    └─ModuleList: 2-3                                  --\n",
       "│    │    └─PhiDecoderLayer: 3-1                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-2                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-3                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-4                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-5                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-6                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-7                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-8                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-9                        50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-10                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-11                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-12                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-13                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-14                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-15                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-16                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-17                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-18                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-19                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-20                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-21                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-22                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-23                       50,354,176\n",
       "│    │    └─PhiDecoderLayer: 3-24                       50,354,176\n",
       "│    └─LayerNorm: 2-4                                   4,096\n",
       "├─Linear: 1-2                                           104,908,800\n",
       "================================================================================\n",
       "Total params: 1,418,270,720\n",
       "Trainable params: 1,418,270,720\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  198, 15496,   616,  1438,   318, 50275,   198]], device='cuda:0')}\n",
      "\n",
      "Hello my name is             \n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following regex:\n",
      "^[a-zA-Z0-9\\s]+$\n",
      "\n",
      "This will match any string that contains only letters, numbers, spaces, and the beginning of the string.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use this regex:\n",
      "^[a-zA-Z0-9\\s]+$\n",
      "\n",
      "A:\n",
      "\n",
      "You can use this regex:\n",
      "^[a-zA-Z0-9\\s]+$\n",
      "\n",
      "A:\n",
      "\n",
      "You can use this regex:\n",
      "^[a-zA-Z0-9\\s]+$\n",
      "\n",
      "A:\n",
      "\n",
      "You can use this regex:\n",
      "^[a-zA-Z0-9\\s]+$\n",
      "\n",
      "A:\n",
      "\n",
      "You can use this regex:\n",
      "^[a-zA-Z0-9\\s]+$\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('''\n",
    "Hello my name is             \n",
    "''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "print(inputs)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating the textbook dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the original authors, they use three different source datasets:\n",
    "- A filtered code-language dataset, which is a subset of The Stack and StackOverflow, obtained by\n",
    "using a language model-based classifier (consisting of about 6B tokens).\n",
    "- A synthetic textbook dataset consisting of <1B tokens of GPT-3.5 generated Python textbooks.\n",
    "- A small synthetic exercises dataset consisting of ∼180M tokens of Python exercises and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\n",
      "# -*- coding: utf-8 -*-\n",
      "# #*** <License> ************************************************************#\n",
      "# This module is part of the repository CNDB.\n",
      "#\n",
      "# This module is licensed under the terms of the BSD 3-Clause License\n",
      "# <http://www.c-tanzer.at/license/bsd_3c.html>.\n",
      "# #*** </License> ***********************************************************#\n",
      "\n",
      "from   _TFL.pyk           import pyk\n",
      "\n",
      "from   rsclib.HTML_Parse  import tag, Page_Tree\n",
      "from   rsclib.autosuper   import autosuper\n",
      "from   spider.common      import Interface, Inet4, Inet6, unroutable\n",
      "from   spider.common      import WLAN_Config\n",
      "from   spider.luci        import Version_Mixin\n",
      "\n",
      "class Status (Page_Tree, Version_Mixin) :\n",
      "    url          = 'cgi-bin/luci/freifunk/status/status'\n",
      "    retries      = 2\n",
      "    timeout      = 10\n",
      "    html_charset = 'utf-8' # force utf-8 encoding\n",
      "\n",
      "    wl_names = dict \\\n",
      "        ( ssid    = 'ssid'\n",
      "        , _bsiid  = 'bssid'\n",
      "        , channel = 'channel'\n",
      "        , mode    = 'mode'\n",
      "        )\n",
      "\n",
      "    def parse (self) :\n",
      "        root  = self.tree.getroot ()\n",
      "        self.wlans  = []\n",
      "        self.routes = {}\n",
      "        for div in root.findall (\".//%s\" % tag (\"div\")) :\n",
      "            id = div.get ('id')\n",
      "            if id == 'cbi-wireless' :\n",
      "                wlan_div = div\n",
      "            elif id == 'cbi-routes' :\n",
      "                route_div = div\n",
      "            self.try_get_version (div)\n",
      "        for d in self.tbl_iter (wlan_div) :\n",
      "            for k, newkey in pyk.iteritems (self.wl_names) :\n",
      "                if k in d :\n",
      "                    d [newkey] = d [k]\n",
      "            wl = WLAN_Config (** d)\n",
      "            self.wlans.append (wl)\n",
      "        for d in self.tbl_iter (route_div) :\n",
      "            iface = d.get ('iface')\n",
      "            gw    = d.get ('gateway')\n",
      "            if iface and gw :\n",
      "                self.routes [iface] = gw\n",
      "        self.set_version (root)\n",
      "    # end def parse\n",
      "\n",
      "    def tbl_iter (self, div) :\n",
      "        tbl = div.find (\".//%s\" % tag (\"table\"))\n",
      "        assert tbl.get ('class') == 'cbi-section-table'\n",
      "        d = {}\n",
      "        for tr in tbl :\n",
      "            if 'cbi-section-table-row' not in tr.get ('class').split () :\n",
      "                continue\n",
      "            for input in tr.findall (\".//%s\" % tag ('input')) :\n",
      "                name = input.get ('id').split ('.') [-1]\n",
      "                val  = input.get ('value')\n",
      "                d [name] = val\n",
      "            if not d :\n",
      "                continue\n",
      "            yield d\n",
      "    # end def tbl_iter\n",
      "\n",
      "# end class Status\n",
      "\n",
      "class Table_Iter (Page_Tree) :\n",
      "\n",
      "    def table_iter (self) :\n",
      "        root  = self.tree.getroot ()\n",
      "        for div in root.findall (\".//%s\" % tag (\"div\")) :\n",
      "            if div.get ('id') == 'maincontent' :\n",
      "                break\n",
      "        tbl = div.find (\".//%s\" % tag (\"table\"))\n",
      "        if tbl is None :\n",
      "            return\n",
      "        for tr in tbl :\n",
      "            if tr [0].tag == tag ('th') :\n",
      "                continue\n",
      "            yield (self.tree.get_text (x) for x in tr)\n",
      "    # end def table_iter\n",
      "\n",
      "# end class Table_Iter\n",
      "\n",
      "class OLSR_Connections (Table_Iter) :\n",
      "    url          = 'cgi-bin/luci/freifunk/olsr/'\n",
      "    retries      = 2\n",
      "    timeout      = 10\n",
      "    html_charset = 'utf-8' # force utf-8 encoding\n",
      "\n",
      "    def parse (self) :\n",
      "        self.neighbors = {}\n",
      "        for l in self.table_iter () :\n",
      "            neighbor, ip, lq, nlq, etx = l\n",
      "            lq, nlq, etx = (float (x) for x in (lq, nlq, etx))\n",
      "            self.neighbors [neighbor] = [ip, lq, nlq, etx]\n",
      "    # end def parse\n",
      "\n",
      "# end class OLSR_Connections\n",
      "\n",
      "class OLSR_Routes (Table_Iter) :\n",
      "    url          = 'cgi-bin/luci/freifunk/olsr/routes'\n",
      "    retries      = 2\n",
      "    timeout      = 10\n",
      "    html_charset = 'utf-8' # force utf-8 encoding\n",
      "\n",
      "    def parse (self) :\n",
      "        self.iface_by_gw = {}\n",
      "        for l in self.table_iter () :\n",
      "            announced, gw, iface, metric, etx = l\n",
      "            if gw in self.iface_by_gw :\n",
      "                assert iface == self.iface_by_gw [gw]\n",
      "            else :\n",
      "                self.iface_by_gw [gw] = iface\n",
      "    # end def parse\n",
      "\n",
      "# end class OLSR_Routes\n",
      "\n",
      "class OpenWRT (autosuper) :\n",
      "\n",
      "    def __init__ (self, site, request) :\n",
      "        self.site    = site\n",
      "        self.request = request\n",
      "        if 'interfaces' in self.request or 'ips' in self.request :\n",
      "            st    = Status           (site = site)\n",
      "            conn  = OLSR_Connections (site = site)\n",
      "            route = OLSR_Routes      (site = site)\n",
      "            self.version = st.version\n",
      "            assert len (st.wlans) <= 1\n",
      "            interfaces   = {}\n",
      "            ips          = {}\n",
      "            count = 0\n",
      "            for gw, ifname in pyk.iteritems (route.iface_by_gw) :\n",
      "                ip, lq, nlq, etx  = conn.neighbors [gw]\n",
      "                i4 = Inet4 (ip, None, None, iface = ifname)\n",
      "                ips [i4] = 1\n",
      "                is_wlan = True\n",
      "                if lq == nlq == etx == 1.0 :\n",
      "                    is_wlan = False\n",
      "                if ifname in interfaces :\n",
      "                    iface = interfaces [ifname]\n",
      "                    if not iface.is_wlan and is_wlan :\n",
      "                        iface.is_wlan   = True\n",
      "                        iface.wlan_info = st.wlans [0]\n",
      "                else :\n",
      "                    iface = Interface (count, ifname, None)\n",
      "                    iface.is_wlan = is_wlan\n",
      "                    if is_wlan :\n",
      "                        iface.wlan_info = st.wlans [0]\n",
      "                    count += 1\n",
      "                    interfaces [ifname] = iface\n",
      "                if i4 not in iface.inet4 :\n",
      "                    iface.append_inet4 (i4)\n",
      "            wl_if = None\n",
      "            for iface in pyk.itervalues (interfaces) :\n",
      "                if iface.is_wlan :\n",
      "                    if wl_if :\n",
      "                        m = \"Duplicate wlan: %s/%s\" % (iface.name, wl_if.name)\n",
      "                        raise ValueError (m)\n",
      "                    wl_if = iface\n",
      "            # check own ip\n",
      "            n  = 'unknown'\n",
      "            i4 = Inet4 (self.request ['ip'], None, None, iface = n)\n",
      "            if i4 not in ips :\n",
      "                assert n not in interfaces\n",
      "                iface = interfaces [n] = Interface (count, n, None)\n",
      "                iface.append_inet4 (i4)\n",
      "                iface.is_wlan = False\n",
      "                if not wl_if and st.wlans :\n",
      "                    iface.is_wlan   = True\n",
      "                    iface.wlan_info = st.wlans [0]\n",
      "                ips [i4] = True\n",
      "\n",
      "            self.request ['ips']        = ips\n",
      "            self.request ['interfaces'] = interfaces\n",
      "            self.request ['version']    = st.version\n",
      "    # end def __init__\n",
      "\n",
      "# end class OpenWRT\n",
      "\n",
      "# UCF Senior Design 2017-18\n",
      "# Group 38\n",
      "\n",
      "from PIL import Image\n",
      "import cv2\n",
      "import imagehash\n",
      "import math\n",
      "import numpy as np\n",
      "\n",
      "DIFF_THRES = 20\n",
      "LIMIT = 2\n",
      "RESIZE = 1000\n",
      "\n",
      "\n",
      "def calc_hash(img):\n",
      "    \"\"\"\n",
      "    Calculate the wavelet hash of the image\n",
      "        img: (ndarray) image file\n",
      "    \"\"\"\n",
      "    # resize image if height > 1000\n",
      "    img = resize(img)\n",
      "    return imagehash.whash(Image.fromarray(img))\n",
      "\n",
      "\n",
      "def compare(hash1, hash2):\n",
      "    \"\"\"\n",
      "    Calculate the difference between two images\n",
      "        hash1: (array) first wavelet hash\n",
      "        hash2: (array) second wavelet hash\n",
      "    \"\"\"\n",
      "    return hash1 - hash2\n",
      "\n",
      "\n",
      "def limit(img, std_hash, count):\n",
      "    \"\"\"\n",
      "    Determine whether image should be removed from image dictionary in main.py\n",
      "        img: (ndarray) image file\n",
      "        std_hash: (array) wavelet hash of comparison standard\n",
      "        count: (int) global count of images similar to comparison standard\n",
      "    \"\"\"\n",
      "    # calculate hash for given image\n",
      "    cmp_hash = calc_hash(img)\n",
      "\n",
      "    # compare to standard\n",
      "    diff = compare(std_hash, cmp_hash)\n",
      "\n",
      "    # image is similar to standard\n",
      "    if diff <= DIFF_THRES:\n",
      "        # if there are 3 similar images already, remove image\n",
      "        if count >= LIMIT:\n",
      "            return 'remove'\n",
      "\n",
      "    # non-similar image found\n",
      "    else:\n",
      "        # update comparison standard\n",
      "        return 'update_std'\n",
      "\n",
      "    # else continue reading images with same standard\n",
      "    return 'continue'\n",
      "\n",
      "\n",
      "def resize(img):\n",
      "    \"\"\"\n",
      "    Resize an image\n",
      "        img: (ndarray) RGB color image\n",
      "    \"\"\"\n",
      "    # get dimensions of image\n",
      "    width = np.shape(img)[1]\n",
      "    height = np.shape(img)[0]\n",
      "\n",
      "    # if height of image is greater than 1000, resize it to 1000\n",
      "    if width > RESIZE:\n",
      "        # keep resize proportional\n",
      "        scale = RESIZE / width\n",
      "        resized_img = cv2.resize(\n",
      "            img, (RESIZE, math.floor(height / scale)), cv2.INTER_AREA)\n",
      "        # return resized image\n",
      "        return resized_img\n",
      "\n",
      "    # if height of image is less than 1000, return image unresized\n",
      "    return img\n",
      "\n",
      "\n",
      "def set_standard(images, filename):\n",
      "    \"\"\"\n",
      "    Set new comparison standard and update information\n",
      "        images: (dictionary) dictionary containing all the image data\n",
      "        filename: (String) name of the image file\n",
      "    \"\"\"\n",
      "    return filename, calc_hash(images[filename]), 0\n",
      "\n",
      "# Copyright 2018 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#      http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "from .cli.cli import main\n",
      "\n",
      "# TODO(hongyes): add more commands:\n",
      "# kfp compile (migrate from dsl-compile)\n",
      "# kfp experiment (manage experiments)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "import time\n",
      "\n",
      "from PyQt5 import QtGui, QtCore\n",
      "\n",
      "from ui.room_item import Ui_Form\n",
      "from PyQt5.QtWidgets import QWidget\n",
      "\n",
      "class Room_Item(QWidget,Ui_Form):\n",
      "    def __init__(self,parent=None,room_data=None):\n",
      "        super(Room_Item,self).__init__(parent)\n",
      "        self.setupUi(self)\n",
      "        self.data = room_data\n",
      "        self.setRoomInfo()\n",
      "\n",
      "    def setRoomInfo(self):\n",
      "        self.room_name.setText('{}({})'.format(self.data['naturalName'], self.data['roomName']))\n",
      "        self.description.setText(\"<a style='color:#BCBCBC'>{}</a>\".format(self.data['description']))\n",
      "        timeStamp = int(self.data['creationDate']) / 1000\n",
      "        timeArray = time.localtime(timeStamp)\n",
      "        otherStyleTime = time.strftime(\"%Y-%m-%d\", timeArray)\n",
      "        self.create_time.setText(\"<a style='color:#BCBCBC'>{}</a>\".format(otherStyleTime))\n",
      "        members = len(self.data['owners']) + len(self.data['admins']) + len(self.data['members'])\n",
      "        memberCounter = \"<a style='color:#BCBCBC'>{}/{}</a>\".format(members, ('∞' if self.data['maxUsers']==0 else self.data['maxUsers']))\n",
      "        self.member.setText(memberCounter)\n",
      "import asyncio\n",
      "import re\n",
      "import sys\n",
      "import traceback\n",
      "\n",
      "import toga\n",
      "from toga import Key\n",
      "from .keys import toga_to_winforms_key\n",
      "\n",
      "from .libs import Threading, WinForms, shcore, user32, win_version\n",
      "from .libs.proactor import WinformsProactorEventLoop\n",
      "from .window import Window\n",
      "\n",
      "\n",
      "class MainWindow(Window):\n",
      "    def winforms_FormClosing(self, sender, event):\n",
      "        if not self.interface.app._impl._is_exiting:\n",
      "            event.Cancel = not self.interface.app.exit()\n",
      "\n",
      "\n",
      "class App:\n",
      "    _MAIN_WINDOW_CLASS = MainWindow\n",
      "\n",
      "    def __init__(self, interface):\n",
      "        self.interface = interface\n",
      "        self.interface._impl = self\n",
      "\n",
      "        # Winforms app exit is tightly bound to the close of the MainWindow.\n",
      "        # The FormClosing message on MainWindow calls app.exit(), which\n",
      "        # will then trigger the \"on_exit\" handler (which might abort the\n",
      "        # close). However, if app.exit() succeeds, it will request the\n",
      "        # Main Window to close... which calls app.exit().\n",
      "        # So - we have a flag that is only ever sent once a request has been\n",
      "        # made to exit the native app. This flag can be used to shortcut any\n",
      "        # window-level close handling.\n",
      "        self._is_exiting = False\n",
      "\n",
      "        self.loop = WinformsProactorEventLoop()\n",
      "        asyncio.set_event_loop(self.loop)\n",
      "\n",
      "    def create(self):\n",
      "        self.native = WinForms.Application\n",
      "        self.app_context = WinForms.ApplicationContext()\n",
      "\n",
      "        # Check the version of windows and make sure we are setting the DPI mode\n",
      "        # with the most up to date API\n",
      "        # Windows Versioning Check Sources : https://www.lifewire.com/windows-version-numbers-2625171\n",
      "        # and https://docs.microsoft.com/en-us/windows/release-information/\n",
      "        if win_version.Major >= 6:  # Checks for Windows Vista or later\n",
      "            # Represents Windows 8.1 up to Windows 10 before Build 1703 which should use\n",
      "            # SetProcessDpiAwareness(True)\n",
      "            if ((win_version.Major == 6 and win_version.Minor == 3) or\n",
      "                    (win_version.Major == 10 and win_version.Build < 15063)):\n",
      "                shcore.SetProcessDpiAwareness(True)\n",
      "            # Represents Windows 10 Build 1703 and beyond which should use\n",
      "            # SetProcessDpiAwarenessContext(-2)\n",
      "            elif win_version.Major == 10 and win_version.Build >= 15063:\n",
      "                user32.SetProcessDpiAwarenessContext(-2)\n",
      "            # Any other version of windows should use SetProcessDPIAware()\n",
      "            else:\n",
      "                user32.SetProcessDPIAware()\n",
      "\n",
      "        self.native.EnableVisualStyles()\n",
      "        self.native.SetCompatibleTextRenderingDefault(False)\n",
      "\n",
      "        self.interface.commands.add(\n",
      "            toga.Command(\n",
      "                lambda _: self.interface.about(),\n",
      "                'About {}'.format(self.interface.name),\n",
      "                group=toga.Group.HELP\n",
      "            ),\n",
      "            toga.Command(None, 'Preferences', group=toga.Group.FILE),\n",
      "            # Quit should always be the last item, in a section on it's own\n",
      "            toga.Command(\n",
      "                lambda _: self.interface.exit(),\n",
      "                'Exit ' + self.interface.name,\n",
      "                shortcut=Key.MOD_1 + 'q',\n",
      "                group=toga.Group.FILE,\n",
      "                section=sys.maxsize\n",
      "            ),\n",
      "            toga.Command(\n",
      "                lambda _: self.interface.visit_homepage(),\n",
      "                'Visit homepage',\n",
      "                enabled=self.interface.home_page is not None,\n",
      "                group=toga.Group.HELP\n",
      "            )\n",
      "        )\n",
      "        self._create_app_commands()\n",
      "\n",
      "        # Call user code to populate the main window\n",
      "        self.interface.startup()\n",
      "        self.create_menus()\n",
      "        self.interface.icon.bind(self.interface.factory)\n",
      "        self.interface.main_window._impl.set_app(self)\n",
      "\n",
      "    def create_menus(self):\n",
      "        self._menu_items = {}\n",
      "        self._menu_groups = {}\n",
      "\n",
      "        toga.Group.FILE.order = 0\n",
      "        menubar = WinForms.MenuStrip()\n",
      "        submenu = None\n",
      "        for cmd in self.interface.commands:\n",
      "            if cmd == toga.GROUP_BREAK:\n",
      "                submenu = None\n",
      "            elif cmd == toga.SECTION_BREAK:\n",
      "                submenu.DropDownItems.Add('-')\n",
      "            else:\n",
      "                submenu = self._submenu(cmd.group, menubar)\n",
      "\n",
      "                item = WinForms.ToolStripMenuItem(cmd.label)\n",
      "\n",
      "                if cmd.action:\n",
      "                    item.Click += cmd._impl.as_handler()\n",
      "                item.Enabled = cmd.enabled\n",
      "\n",
      "                if cmd.shortcut is not None:\n",
      "                    shortcut_keys = toga_to_winforms_key(cmd.shortcut)\n",
      "                    item.ShortcutKeys = shortcut_keys\n",
      "                    item.ShowShortcutKeys = True\n",
      "\n",
      "                cmd._impl.native.append(item)\n",
      "\n",
      "                self._menu_items[item] = cmd\n",
      "                submenu.DropDownItems.Add(item)\n",
      "\n",
      "        self.interface.main_window._impl.native.Controls.Add(menubar)\n",
      "        self.interface.main_window._impl.native.MainMenuStrip = menubar\n",
      "        self.interface.main_window.content.refresh()\n",
      "\n",
      "    def _submenu(self, group, menubar):\n",
      "        try:\n",
      "            return self._menu_groups[group]\n",
      "        except KeyError:\n",
      "            if group is None:\n",
      "                submenu = menubar\n",
      "            else:\n",
      "                parent_menu = self._submenu(group.parent, menubar)\n",
      "\n",
      "                submenu = WinForms.ToolStripMenuItem(group.label)\n",
      "\n",
      "                # Top level menus are added in a different way to submenus\n",
      "                if group.parent is None:\n",
      "                    parent_menu.Items.Add(submenu)\n",
      "                else:\n",
      "                    parent_menu.DropDownItems.Add(submenu)\n",
      "\n",
      "            self._menu_groups[group] = submenu\n",
      "        return submenu\n",
      "\n",
      "    def _create_app_commands(self):\n",
      "        # No extra menus\n",
      "        pass\n",
      "\n",
      "    def open_document(self, fileURL):\n",
      "        '''Add a new document to this app.'''\n",
      "        print(\"STUB: If you want to handle opening documents, implement App.open_document(fileURL)\")\n",
      "\n",
      "    def winforms_thread_exception(self, sender, winforms_exc):\n",
      "        # The PythonException returned by Winforms doesn't give us\n",
      "        # easy access to the underlying Python stacktrace; so we\n",
      "        # reconstruct it from the string message.\n",
      "        # The Python message is helpfully included in square brackets,\n",
      "        # as the context for the first line in the .net stack trace.\n",
      "        # So, look for the closing bracket and the start of the Python.net\n",
      "        # stack trace. Then, reconstruct the line breaks internal to the\n",
      "        # remaining string.\n",
      "        print(\"Traceback (most recent call last):\")\n",
      "        py_exc = winforms_exc.get_Exception()\n",
      "        full_stack_trace = py_exc.StackTrace\n",
      "        regex = re.compile(\n",
      "            r\"^\\[(?:'(.*?)', )*(?:'(.*?)')\\]   (?:.*?) Python\\.Runtime\",\n",
      "            re.DOTALL | re.UNICODE\n",
      "        )\n",
      "\n",
      "        stacktrace_relevant_lines = regex.findall(full_stack_trace)\n",
      "        if len(stacktrace_relevant_lines) == 0:\n",
      "            self.print_stack_trace(full_stack_trace)\n",
      "        else:\n",
      "            for lines in stacktrace_relevant_lines:\n",
      "                for line in lines:\n",
      "                    self.print_stack_trace(line)\n",
      "        print(py_exc.Message)\n",
      "\n",
      "    @classmethod\n",
      "    def print_stack_trace(cls, stack_trace_line):\n",
      "        for level in stack_trace_line.split(\"', '\"):\n",
      "            for line in level.split(\"\\\\n\"):\n",
      "                if line:\n",
      "                    print(line)\n",
      "\n",
      "    def run_app(self):\n",
      "        try:\n",
      "            self.create()\n",
      "\n",
      "            self.native.ThreadException += self.winforms_thread_exception\n",
      "\n",
      "            self.loop.run_forever(self.app_context)\n",
      "        except:  # NOQA\n",
      "            traceback.print_exc()\n",
      "\n",
      "    def main_loop(self):\n",
      "        thread = Threading.Thread(Threading.ThreadStart(self.run_app))\n",
      "        thread.SetApartmentState(Threading.ApartmentState.STA)\n",
      "        thread.Start()\n",
      "        thread.Join()\n",
      "\n",
      "    def show_about_dialog(self):\n",
      "        message_parts = []\n",
      "        if self.interface.name is not None:\n",
      "            if self.interface.version is not None:\n",
      "                message_parts.append(\n",
      "                    \"{name} v{version}\".format(\n",
      "                        name=self.interface.name,\n",
      "                        version=self.interface.version,\n",
      "                    )\n",
      "                )\n",
      "            else:\n",
      "                message_parts.append(\n",
      "                    \"{name}\".format(name=self.interface.name)\n",
      "                )\n",
      "        elif self.interface.version is not None:\n",
      "            message_parts.append(\n",
      "                \"v{version}\".format(version=self.interface.version)\n",
      "            )\n",
      "\n",
      "        if self.interface.author is not None:\n",
      "            message_parts.append(\n",
      "                \"Author: {author}\".format(author=self.interface.author)\n",
      "            )\n",
      "        if self.interface.description is not None:\n",
      "            message_parts.append(\n",
      "                \"\\n{description}\".format(\n",
      "                    description=self.interface.description\n",
      "                )\n",
      "            )\n",
      "        self.interface.main_window.info_dialog(\n",
      "            'About {}'.format(self.interface.name), \"\\n\".join(message_parts)\n",
      "        )\n",
      "\n",
      "    def exit(self):\n",
      "        self._is_exiting = True\n",
      "        self.native.Exit()\n",
      "\n",
      "    def set_main_window(self, window):\n",
      "        self.app_context.MainForm = window._impl.native\n",
      "\n",
      "    def set_on_exit(self, value):\n",
      "        pass\n",
      "\n",
      "    def current_window(self):\n",
      "        self.interface.factory.not_implemented('App.current_window()')\n",
      "\n",
      "    def enter_full_screen(self, windows):\n",
      "        self.interface.factory.not_implemented('App.enter_full_screen()')\n",
      "\n",
      "    def exit_full_screen(self, windows):\n",
      "        self.interface.factory.not_implemented('App.exit_full_screen()')\n",
      "\n",
      "    def set_cursor(self, value):\n",
      "        self.interface.factory.not_implemented('App.set_cursor()')\n",
      "\n",
      "    def show_cursor(self):\n",
      "        self.interface.factory.not_implemented('App.show_cursor()')\n",
      "\n",
      "    def hide_cursor(self):\n",
      "        self.interface.factory.not_implemented('App.hide_cursor()')\n",
      "\n",
      "    def add_background_task(self, handler):\n",
      "        self.loop.call_soon(handler, self)\n",
      "\n",
      "\n",
      "class DocumentApp(App):\n",
      "    def _create_app_commands(self):\n",
      "        self.interface.commands.add(\n",
      "            toga.Command(\n",
      "                lambda w: self.open_file,\n",
      "                label='Open...',\n",
      "                shortcut=Key.MOD_1 + 'o',\n",
      "                group=toga.Group.FILE,\n",
      "                section=0\n",
      "            ),\n",
      "        )\n",
      "\n",
      "    def open_document(self, fileURL):\n",
      "        \"\"\"Open a new document in this app.\n",
      "\n",
      "        Args:\n",
      "            fileURL (str): The URL/path to the file to add as a document.\n",
      "        \"\"\"\n",
      "        self.interface.factory.not_implemented('DocumentApp.open_document()')\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "/***************************************************************************\n",
      " SimplePhotogrammetryRoutePlanner\n",
      "                                 A QGIS plugin\n",
      " A imple photogrammetry route planner.\n",
      " Generated by Plugin Builder: http://g-sherman.github.io/Qgis-Plugin-Builder/\n",
      "                             -------------------\n",
      "        begin                : 2021-04-24\n",
      "        copyright            : (C) 2021 by Xiangyong Luo\n",
      "        email                : solo_lxy@126.com\n",
      "        git sha              : $Format:%H$\n",
      " ***************************************************************************/\n",
      "\n",
      "/***************************************************************************\n",
      " *                                                                         *\n",
      " *   This program is free software; you can redistribute it and/or modify  *\n",
      " *   it under the terms of the GNU General Public License as published by  *\n",
      " *   the Free Software Foundation; either version 2 of the License, or     *\n",
      " *   (at your option) any later version.                                   *\n",
      " *                                                                         *\n",
      " ***************************************************************************/\n",
      " This script initializes the plugin, making it known to QGIS.\n",
      "\"\"\"\n",
      "__version__ = \"0.4.0\"\n",
      "\n",
      "# noinspection PyPep8Naming\n",
      "def classFactory(iface):  # pylint: disable=invalid-name\n",
      "    \"\"\"Load SimplePhotogrammetryRoutePlanner class from file SimplePhotogrammetryRoutePlanner.\n",
      "\n",
      "    :param iface: A QGIS interface instance.\n",
      "    :type iface: QgsInterface\n",
      "    \"\"\"\n",
      "    #\n",
      "    from .SimplePhotogrammetryRoutePlanner import SimplePhotogrammetryRoutePlanner\n",
      "    return SimplePhotogrammetryRoutePlanner(iface)\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "Created on Tue Jul 24 14:38:20 2018\n",
      "dimension reduction with VarianceThreshold using sklearn.\n",
      "Feature selector that removes all low-variance features.\n",
      "@author: lenovo\n",
      "\"\"\"\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "import numpy as np\n",
      "#\n",
      "np.random.seed(1)\n",
      "X = np.random.randn(100, 10)\n",
      "X = np.hstack([X, np.zeros([100, 5])])\n",
      "#\n",
      "\n",
      "\n",
      "def featureSelection_variance(X, thrd):\n",
      "    sel = VarianceThreshold(threshold=thrd)\n",
      "    X_selected = sel.fit_transform(X)\n",
      "    mask = sel.get_support()\n",
      "    return X_selected, mask\n",
      "\n",
      "\n",
      "X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
      "selector = VarianceThreshold()\n",
      "selector.fit_transform(X)\n",
      "selector.variances_\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# coding=utf-8\n",
      "\n",
      "from my_multi_main3 import main\n",
      "import numpy as np\n",
      "import argparse\n",
      "import time\n",
      "\n",
      "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
      "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
      "                    help='input batch size for training (default: 64)')\n",
      "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
      "                    help='input batch size for testing (default: 1000)')\n",
      "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
      "                    help='number of epochs to train (default: 10)')\n",
      "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
      "                    help='learning rate (default: 0.01)')\n",
      "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
      "                    help='SGD momentum (default: 0.5)')\n",
      "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
      "                    help='disables CUDA training')\n",
      "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
      "                    help='random seed (default: 1)')\n",
      "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
      "                    help='how many batches to wait before logging training status')\n",
      "parser.add_argument('--save-model', action='store_true', default=False,\n",
      "                    help='For Saving the current Model')\n",
      "parser.add_argument('--norm-flag', type=bool, default=False,\n",
      "                    help='Triggering the Layer Normalization flag for attention scores')\n",
      "parser.add_argument('--gamma', type=float, default=None,\n",
      "                    help='Controlling the sparisty of gfusedmax/sparsemax, the smaller, the more sparse')\n",
      "parser.add_argument('--lam', type=float, default=1.0,\n",
      "                    help='Lambda: Controlling the smoothness of gfusedmax, the larger, the smoother')\n",
      "parser.add_argument('--max-type', type=str, default='softmax',choices=['softmax','sparsemax','gfusedmax'],\n",
      "                    help='mapping function in attention')\n",
      "parser.add_argument('--optim-type', type=str, default='SGD',choices=['SGD','Adam'],\n",
      "                    help='mapping function in attention')\n",
      "parser.add_argument('--head-cnt', type=int, default=2, metavar='S', choices=[1,2,4,5,10],\n",
      "                    help='Number of heads for attention (default: 1)')\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "hyperparameter_choices = {\n",
      "    'lr':list(10**np.arange(-4,-1,0.5)),\n",
      "    'norm_flag': [True,False],\n",
      "    'gamma':list(10**np.arange(-1,3,0.5))+[None,],\n",
      "    'lam':list(10**np.arange(-2,2,0.5)),\n",
      "    'max_type':['softmax','sparsemax','gfusedmax'],\n",
      "    # 'max_type':['sparsemax'],\n",
      "    'optim_type':['SGD','Adam'],\n",
      "    'head_cnt':[1,2,4,5,10,20]\n",
      "}\n",
      "\n",
      "param_num = 25\n",
      "record = np.zeros([param_num,len(hyperparameter_choices)+1])\n",
      "record_name = 'record3_multi_%s.csv'%time.strftime('%Y-%m-%d_%H-%M-%S',time.localtime())\n",
      "for n in range(param_num):\n",
      "    for param_index,(k,v) in enumerate(hyperparameter_choices.items()):\n",
      "        print(param_index,k)\n",
      "        value_index = np.random.choice(len(v))\n",
      "        if isinstance(v[value_index],str) or isinstance(v[value_index],bool) or v[value_index] is None:\n",
      "            record[n,param_index] = value_index\n",
      "        else:\n",
      "            record[n,param_index] = v[value_index]\n",
      "        setattr(args,k,v[value_index])\n",
      "    record[n,-1] = main(args)\n",
      "    np.savetxt(record_name, record, delimiter=',')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"\"\"HDF5 related files.\n",
      "\n",
      "This file contains a set of functions that related to read and write\n",
      "HDF5 files.\n",
      "\n",
      "Author: Yuhuang Hu\n",
      "Email : duguyue100@gmail.com\n",
      "\"\"\"\n",
      "from __future__ import print_function, absolute_import\n",
      "\n",
      "import h5py\n",
      "\n",
      "from spiker import log\n",
      "\n",
      "logger = log.get_logger(\"data-hdf5\", log.DEBUG)\n",
      "\n",
      "\n",
      "def init_hdf5(file_path, mode=\"w\", cam_type=\"davis\"):\n",
      "    \"\"\"Init HDF5 file object.\n",
      "\n",
      "    # Parameters\n",
      "    file_path : str\n",
      "        absolute path for the HDF5 file.\n",
      "    mode : str\n",
      "        w : for writing\n",
      "        r : for reading\n",
      "    cam_type : str\n",
      "        davis : for DAVIS camera\n",
      "        dvs   : for DVS camera\n",
      "\n",
      "    # Returns\n",
      "    dataset : h5py.File\n",
      "        The file object of the given dataset\n",
      "    \"\"\"\n",
      "    if mode == \"w\":\n",
      "        dataset = h5py.File(file_path, mode=mode)\n",
      "        dataset.create_group(\"dvs\")\n",
      "        dataset.create_group(\"extra\")\n",
      "        if cam_type == \"davis\":\n",
      "            dataset.create_group(\"aps\")\n",
      "            dataset.create_group(\"imu\")\n",
      "    elif mode == \"r\":\n",
      "        dataset = h5py.File(file_path, mode=mode)\n",
      "\n",
      "    return dataset\n",
      "\n",
      "# automatically generated by the FlatBuffers compiler, do not modify\n",
      "\n",
      "# namespace: flatbuf\n",
      "\n",
      "import flatbuffers\n",
      "\n",
      "class FloatingPoint(object):\n",
      "    __slots__ = ['_tab']\n",
      "\n",
      "    @classmethod\n",
      "    def GetRootAsFloatingPoint(cls, buf, offset):\n",
      "        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\n",
      "        x = FloatingPoint()\n",
      "        x.Init(buf, n + offset)\n",
      "        return x\n",
      "\n",
      "    # FloatingPoint\n",
      "    def Init(self, buf, pos):\n",
      "        self._tab = flatbuffers.table.Table(buf, pos)\n",
      "\n",
      "    # FloatingPoint\n",
      "    def Precision(self):\n",
      "        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))\n",
      "        if o != 0:\n",
      "            return self._tab.Get(flatbuffers.number_types.Int16Flags, o + self._tab.Pos)\n",
      "        return 0\n",
      "\n",
      "def FloatingPointStart(builder): builder.StartObject(1)\n",
      "def FloatingPointAddPrecision(builder, precision): builder.PrependInt16Slot(0, precision, 0)\n",
      "def FloatingPointEnd(builder): return builder.EndObject()\n",
      "\n",
      "\"\"\"[Scynced Lights]\n",
      "Class attributes are \"shared\"\n",
      "Instance attributes are not shared.\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "def sub(x, y):\n",
      "    f\n",
      "\n",
      "\n",
      "class Light:\n",
      "    pass\n",
      "\n",
      "a = Light()\n",
      "b = Ligth()\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# -*- encoding: utf-8 -*-\n",
      "# vim: set et sw=4 ts=4 sts=4 ff=unix fenc=utf8:\n",
      "# Author: Binux<i@binux.me>\n",
      "#         http://binux.me\n",
      "# Created on 2012-11-14 17:09:50\n",
      "\n",
      "from __future__ import unicode_literals, division, absolute_import\n",
      "\n",
      "import time\n",
      "import logging\n",
      "from collections import deque\n",
      "try:\n",
      "    from UserDict import DictMixin\n",
      "except ImportError:\n",
      "    from collections import Mapping as DictMixin\n",
      "\n",
      "import six\n",
      "from six import iteritems\n",
      "from six.moves import cPickle\n",
      "\n",
      "\n",
      "class BaseCounter(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def event(self, value=1):\n",
      "        \"\"\"Fire a event.\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def value(self, value):\n",
      "        \"\"\"Set counter value.\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    @property\n",
      "    def avg(self):\n",
      "        \"\"\"Get average value\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    @property\n",
      "    def sum(self):\n",
      "        \"\"\"Get sum of counter\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "    def empty(self):\n",
      "        \"\"\"Clear counter\"\"\"\n",
      "        raise NotImplementedError\n",
      "\n",
      "\n",
      "class TotalCounter(BaseCounter):\n",
      "    \"\"\"Total counter\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.cnt = 0\n",
      "\n",
      "    def event(self, value=1):\n",
      "        self.cnt += value\n",
      "\n",
      "    def value(self, value):\n",
      "        self.cnt = value\n",
      "\n",
      "    @property\n",
      "    def avg(self):\n",
      "        return self.cnt\n",
      "\n",
      "    @property\n",
      "    def sum(self):\n",
      "        return self.cnt\n",
      "\n",
      "    def empty(self):\n",
      "        return self.cnt == 0\n",
      "\n",
      "\n",
      "class AverageWindowCounter(BaseCounter):\n",
      "    \"\"\"\n",
      "    Record last N(window) value\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, window_size=300):\n",
      "        self.window_size = window_size\n",
      "        self.values = deque(maxlen=window_size)\n",
      "\n",
      "    def event(self, value=1):\n",
      "        self.values.append(value)\n",
      "\n",
      "    value = event\n",
      "\n",
      "    @property\n",
      "    def avg(self):\n",
      "        return self.sum / len(self.values)\n",
      "\n",
      "    @property\n",
      "    def sum(self):\n",
      "        return sum(self.values)\n",
      "\n",
      "    def empty(self):\n",
      "        if not self.values:\n",
      "            return True\n",
      "\n",
      "\n",
      "class TimebaseAverageWindowCounter(BaseCounter):\n",
      "    \"\"\"\n",
      "    Record last window_size * window_interval seconds values.\n",
      "\n",
      "    records will trim evert window_interval seconds\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, window_size=30, window_interval=10):\n",
      "        self.max_window_size = window_size\n",
      "        self.window_size = 0\n",
      "        self.window_interval = window_interval\n",
      "        self.values = deque(maxlen=window_size)\n",
      "        self.times = deque(maxlen=window_size)\n",
      "\n",
      "        self.cache_value = 0\n",
      "        self.cache_start = None\n",
      "        self._first_data_time = None\n",
      "\n",
      "    def event(self, value=1):\n",
      "        now = time.time()\n",
      "        if self._first_data_time is None:\n",
      "            self._first_data_time = now\n",
      "\n",
      "        if self.cache_start is None:\n",
      "            self.cache_value = value\n",
      "            self.cache_start = now\n",
      "        elif now - self.cache_start > self.window_interval:\n",
      "            self.values.append(self.cache_value)\n",
      "            self.times.append(self.cache_start)\n",
      "            self.on_append(self.cache_value, self.cache_start)\n",
      "            self.cache_value = value\n",
      "            self.cache_start = now\n",
      "        else:\n",
      "            self.cache_value += value\n",
      "        return self\n",
      "\n",
      "    def value(self, value):\n",
      "        self.cache_value = value\n",
      "\n",
      "    def _trim_window(self):\n",
      "        now = time.time()\n",
      "        if self.cache_start and now - self.cache_start > self.window_interval:\n",
      "            self.values.append(self.cache_value)\n",
      "            self.times.append(self.cache_start)\n",
      "            self.on_append(self.cache_value, self.cache_start)\n",
      "            self.cache_value = 0\n",
      "            self.cache_start = None\n",
      "\n",
      "        if self.window_size != self.max_window_size and self._first_data_time is not None:\n",
      "            time_passed = now - self._first_data_time\n",
      "            self.window_size = min(self.max_window_size, time_passed / self.window_interval)\n",
      "        window_limit = now - self.window_size * self.window_interval\n",
      "        while self.times and self.times[0] < window_limit:\n",
      "            self.times.popleft()\n",
      "            self.values.popleft()\n",
      "\n",
      "    @property\n",
      "    def avg(self):\n",
      "        sum = float(self.sum)\n",
      "        if not self.window_size:\n",
      "            return 0\n",
      "        return sum / self.window_size / self.window_interval\n",
      "\n",
      "    @property\n",
      "    def sum(self):\n",
      "        self._trim_window()\n",
      "        return sum(self.values) + self.cache_value\n",
      "\n",
      "    def empty(self):\n",
      "        self._trim_window()\n",
      "        if not self.values and not self.cache_start:\n",
      "            return True\n",
      "\n",
      "    def on_append(self, value, time):\n",
      "        pass\n",
      "\n",
      "\n",
      "class CounterValue(DictMixin):\n",
      "    \"\"\"\n",
      "    A dict like value item for CounterManager.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, manager, keys):\n",
      "        self.manager = manager\n",
      "        self._keys = keys\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        if key == '__value__':\n",
      "            key = self._keys\n",
      "            return self.manager.counters[key]\n",
      "        else:\n",
      "            key = self._keys + (key, )\n",
      "\n",
      "        available_keys = []\n",
      "        for _key in self.manager.counters:\n",
      "            if _key[:len(key)] == key:\n",
      "                available_keys.append(_key)\n",
      "\n",
      "        if len(available_keys) == 0:\n",
      "            raise KeyError\n",
      "        elif len(available_keys) == 1:\n",
      "            if available_keys[0] == key:\n",
      "                return self.manager.counters[key]\n",
      "            else:\n",
      "                return CounterValue(self.manager, key)\n",
      "        else:\n",
      "            return CounterValue(self.manager, key)\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.keys())\n",
      "\n",
      "    def __iter__(self):\n",
      "        return iter(self.keys())\n",
      "\n",
      "    def __contains__(self, key):\n",
      "        return key in self.keys()\n",
      "\n",
      "    def keys(self):\n",
      "        result = set()\n",
      "        for key in self.manager.counters:\n",
      "            if key[:len(self._keys)] == self._keys:\n",
      "                key = key[len(self._keys):]\n",
      "                result.add(key[0] if key else '__value__')\n",
      "        return result\n",
      "\n",
      "    def to_dict(self, get_value=None):\n",
      "        \"\"\"Dump counters as a dict\"\"\"\n",
      "        result = {}\n",
      "        for key, value in iteritems(self):\n",
      "            if isinstance(value, BaseCounter):\n",
      "                if get_value is not None:\n",
      "                    value = getattr(value, get_value)\n",
      "                result[key] = value\n",
      "            else:\n",
      "                result[key] = value.to_dict(get_value)\n",
      "        return result\n",
      "\n",
      "\n",
      "class CounterManager(DictMixin):\n",
      "    \"\"\"\n",
      "    A dict like counter manager.\n",
      "\n",
      "    When using a tuple as event key, say: ('foo', 'bar'), You can visite counter\n",
      "    with manager['foo']['bar'].  Or get all counters which first element is 'foo'\n",
      "    by manager['foo'].\n",
      "\n",
      "    It's useful for a group of counters.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, cls=TimebaseAverageWindowCounter):\n",
      "        \"\"\"init manager with Counter cls\"\"\"\n",
      "        self.cls = cls\n",
      "        self.counters = {}\n",
      "\n",
      "    def event(self, key, value=1):\n",
      "        \"\"\"Fire a event of a counter by counter key\"\"\"\n",
      "        if isinstance(key, six.string_types):\n",
      "            key = (key, )\n",
      "        assert isinstance(key, tuple), \"event key type error\"\n",
      "        if key not in self.counters:\n",
      "            self.counters[key] = self.cls()\n",
      "        self.counters[key].event(value)\n",
      "        return self\n",
      "\n",
      "    def value(self, key, value=1):\n",
      "        \"\"\"Set value of a counter by counter key\"\"\"\n",
      "        if isinstance(key, six.string_types):\n",
      "            key = (key, )\n",
      "        assert isinstance(key, tuple), \"event key type error\"\n",
      "        if key not in self.counters:\n",
      "            self.counters[key] = self.cls()\n",
      "        self.counters[key].value(value)\n",
      "        return self\n",
      "\n",
      "    def trim(self):\n",
      "        \"\"\"Clear not used counters\"\"\"\n",
      "        for key, value in list(iteritems(self.counters)):\n",
      "            if value.empty():\n",
      "                del self.counters[key]\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        key = (key, )\n",
      "        available_keys = []\n",
      "        for _key in self.counters:\n",
      "            if _key[:len(key)] == key:\n",
      "                available_keys.append(_key)\n",
      "\n",
      "        if len(available_keys) == 0:\n",
      "            raise KeyError\n",
      "        elif len(available_keys) == 1:\n",
      "            if available_keys[0] == key:\n",
      "                return self.counters[key]\n",
      "            else:\n",
      "                return CounterValue(self, key)\n",
      "        else:\n",
      "            return CounterValue(self, key)\n",
      "\n",
      "    def __iter__(self):\n",
      "        return iter(self.keys())\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.keys())\n",
      "\n",
      "    def keys(self):\n",
      "        result = set()\n",
      "        for key in self.counters:\n",
      "            result.add(key[0] if key else ())\n",
      "        return result\n",
      "\n",
      "    def to_dict(self, get_value=None):\n",
      "        \"\"\"Dump counters as a dict\"\"\"\n",
      "        self.trim()\n",
      "        result = {}\n",
      "        for key, value in iteritems(self):\n",
      "            if isinstance(value, BaseCounter):\n",
      "                if get_value is not None:\n",
      "                    value = getattr(value, get_value)\n",
      "                result[key] = value\n",
      "            else:\n",
      "                result[key] = value.to_dict(get_value)\n",
      "        return result\n",
      "\n",
      "    def dump(self, filename):\n",
      "        \"\"\"Dump counters to file\"\"\"\n",
      "        try:\n",
      "            with open(filename, 'wb') as fp:\n",
      "                cPickle.dump(self.counters, fp)\n",
      "        except:\n",
      "            logging.error(\"can't dump counter to file: %s\" % filename)\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    def load(self, filename):\n",
      "        \"\"\"Load counters to file\"\"\"\n",
      "        try:\n",
      "            with open(filename) as fp:\n",
      "                self.counters = cPickle.load(fp)\n",
      "        except:\n",
      "            logging.debug(\"can't load counter from file: %s\" % filename)\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "# coding=utf-8\n",
      "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\"\"\"Feature extractor class for ViT.\"\"\"\n",
      "\n",
      "from typing import List, Optional, Union\n",
      "\n",
      "import numpy as np\n",
      "from PIL import Image\n",
      "\n",
      "from ...feature_extraction_utils import BatchFeature, FeatureExtractionMixin\n",
      "from ...file_utils import TensorType\n",
      "from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageFeatureExtractionMixin, is_torch_tensor\n",
      "from ...utils import logging\n",
      "\n",
      "\n",
      "logger = logging.get_logger(__name__)\n",
      "\n",
      "\n",
      "class ViTFeatureExtractor(FeatureExtractionMixin, ImageFeatureExtractionMixin):\n",
      "    r\"\"\"\n",
      "    Constructs a ViT feature extractor.\n",
      "\n",
      "    This feature extractor inherits from :class:`~transformers.FeatureExtractionMixin` which contains most of the main\n",
      "    methods. Users should refer to this superclass for more information regarding those methods.\n",
      "\n",
      "    Args:\n",
      "        do_resize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether to resize the input to a certain :obj:`size`.\n",
      "        size (:obj:`int` or :obj:`Tuple(int)`, `optional`, defaults to 224):\n",
      "            Resize the input to the given size. If a tuple is provided, it should be (width, height). If only an\n",
      "            integer is provided, then the input will be resized to (size, size). Only has an effect if :obj:`do_resize`\n",
      "            is set to :obj:`True`.\n",
      "        resample (:obj:`int`, `optional`, defaults to :obj:`PIL.Image.BILINEAR`):\n",
      "            An optional resampling filter. This can be one of :obj:`PIL.Image.NEAREST`, :obj:`PIL.Image.BOX`,\n",
      "            :obj:`PIL.Image.BILINEAR`, :obj:`PIL.Image.HAMMING`, :obj:`PIL.Image.BICUBIC` or :obj:`PIL.Image.LANCZOS`.\n",
      "            Only has an effect if :obj:`do_resize` is set to :obj:`True`.\n",
      "        do_normalize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not to normalize the input with mean and standard deviation.\n",
      "        image_mean (:obj:`List[int]`, defaults to :obj:`[0.5, 0.5, 0.5]`):\n",
      "            The sequence of means for each channel, to be used when normalizing images.\n",
      "        image_std (:obj:`List[int]`, defaults to :obj:`[0.5, 0.5, 0.5]`):\n",
      "            The sequence of standard deviations for each channel, to be used when normalizing images.\n",
      "    \"\"\"\n",
      "\n",
      "    model_input_names = [\"pixel_values\"]\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        do_resize=True,\n",
      "        size=224,\n",
      "        resample=Image.BILINEAR,\n",
      "        do_normalize=True,\n",
      "        image_mean=None,\n",
      "        image_std=None,\n",
      "        **kwargs\n",
      "    ):\n",
      "        super().__init__(**kwargs)\n",
      "        self.do_resize = do_resize\n",
      "        self.size = size\n",
      "        self.resample = resample\n",
      "        self.do_normalize = do_normalize\n",
      "        self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n",
      "        self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n",
      "\n",
      "    def __call__(\n",
      "        self,\n",
      "        images: Union[\n",
      "            Image.Image, np.ndarray, \"torch.Tensor\", List[Image.Image], List[np.ndarray], List[\"torch.Tensor\"]  # noqa\n",
      "        ],\n",
      "        return_tensors: Optional[Union[str, TensorType]] = None,\n",
      "        **kwargs\n",
      "    ) -> BatchFeature:\n",
      "        \"\"\"\n",
      "        Main method to prepare for the model one or several image(s).\n",
      "\n",
      "        .. warning::\n",
      "\n",
      "           NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass\n",
      "           PIL images.\n",
      "\n",
      "        Args:\n",
      "            images (:obj:`PIL.Image.Image`, :obj:`np.ndarray`, :obj:`torch.Tensor`, :obj:`List[PIL.Image.Image]`, :obj:`List[np.ndarray]`, :obj:`List[torch.Tensor]`):\n",
      "                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n",
      "                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n",
      "                number of channels, H and W are image height and width.\n",
      "\n",
      "            return_tensors (:obj:`str` or :class:`~transformers.file_utils.TensorType`, `optional`, defaults to :obj:`'np'`):\n",
      "                If set, will return tensors of a particular framework. Acceptable values are:\n",
      "\n",
      "                * :obj:`'tf'`: Return TensorFlow :obj:`tf.constant` objects.\n",
      "                * :obj:`'pt'`: Return PyTorch :obj:`torch.Tensor` objects.\n",
      "                * :obj:`'np'`: Return NumPy :obj:`np.ndarray` objects.\n",
      "                * :obj:`'jax'`: Return JAX :obj:`jnp.ndarray` objects.\n",
      "\n",
      "        Returns:\n",
      "            :class:`~transformers.BatchFeature`: A :class:`~transformers.BatchFeature` with the following fields:\n",
      "\n",
      "            - **pixel_values** -- Pixel values to be fed to a model, of shape (batch_size, num_channels, height,\n",
      "              width).\n",
      "        \"\"\"\n",
      "        # Input type checking for clearer error\n",
      "        valid_images = False\n",
      "\n",
      "        # Check that images has a valid type\n",
      "        if isinstance(images, (Image.Image, np.ndarray)) or is_torch_tensor(images):\n",
      "            valid_images = True\n",
      "        elif isinstance(images, (list, tuple)):\n",
      "            if len(images) == 0 or isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0]):\n",
      "                valid_images = True\n",
      "\n",
      "        if not valid_images:\n",
      "            raise ValueError(\n",
      "                \"Images must of type `PIL.Image.Image`, `np.ndarray` or `torch.Tensor` (single example),\"\n",
      "                \"`List[PIL.Image.Image]`, `List[np.ndarray]` or `List[torch.Tensor]` (batch of examples).\"\n",
      "            )\n",
      "\n",
      "        is_batched = bool(\n",
      "            isinstance(images, (list, tuple))\n",
      "            and (isinstance(images[0], (Image.Image, np.ndarray)) or is_torch_tensor(images[0]))\n",
      "        )\n",
      "\n",
      "        if not is_batched:\n",
      "            images = [images]\n",
      "\n",
      "        # transformations (resizing + normalization)\n",
      "        if self.do_resize and self.size is not None:\n",
      "            images = [self.resize(image=image, size=self.size, resample=self.resample) for image in images]\n",
      "        if self.do_normalize:\n",
      "            images = [self.normalize(image=image, mean=self.image_mean, std=self.image_std) for image in images]\n",
      "\n",
      "        # return as BatchFeature\n",
      "        data = {\"pixel_values\": images}\n",
      "        encoded_inputs = BatchFeature(data=data, tensor_type=return_tensors)\n",
      "\n",
      "        return encoded_inputs\n",
      "\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "UNKNOWN = -1\n",
      "\n",
      "def read_val():\n",
      "    return int(input())\n",
      "\n",
      "def read_row():\n",
      "    return list(map(int, input().split()))\n",
      "\n",
      "def read_grid():\n",
      "    return [read_row() for _ in range(read_val())]\n",
      "\n",
      "def make_blank_row(i):\n",
      "    return [UNKNOWN] * i\n",
      "\n",
      "def make_blank_grid(n):\n",
      "    return [make_blank_row(i) for i in range(1, n + 1)]\n",
      "\n",
      "def compute_max_path_sum(grid):\n",
      "    memo = make_blank_grid(len(grid))\n",
      "    \n",
      "    def dfs(i, j):\n",
      "        if i == len(grid):\n",
      "            return 0\n",
      "        \n",
      "        if memo[i][j] == UNKNOWN:\n",
      "            memo[i][j] = grid[i][j] + max(dfs(i + 1, j), dfs(i + 1, j + 1))\n",
      "        \n",
      "        return memo[i][j]\n",
      "    \n",
      "    return dfs(0, 0)\n",
      "\n",
      "for t in range(read_val()):\n",
      "    print(compute_max_path_sum(read_grid()))\n",
      "\n",
      "import platform\n",
      "\n",
      "\n",
      "# print(platform.system())\n",
      "operating_system = platform.system().lower()\n",
      "if operating_system == 'darwin':\n",
      "    from .blender_utils_macos import get_installed_blender_versions\n",
      "    operating_system_name = 'macos'\n",
      "elif operating_system == 'linux':\n",
      "    from .blender_utils_linux import get_installed_blender_versions\n",
      "    operating_system_name = 'linux'\n",
      "elif operating_system == 'windows':\n",
      "    from .blender_utils_windows import get_installed_blender_versions\n",
      "    operating_system_name = 'windows'\n",
      "else:\n",
      "    raise Exception(\"Unimplemented for OS {}\".format(operating_system))\n",
      "\n",
      "from .blender_utils_web import get_blender_version_download_links\n",
      "\n",
      "\n",
      "def find_blender(version):\n",
      "    # TODO: add fuzzy version matching, ie. '>=2.80', '~2.80', '<2.80', etc.\n",
      "    installed_versions = get_installed_blender_versions()\n",
      "    if version in installed_versions:\n",
      "        return installed_versions[version]\n",
      "    else:\n",
      "        print(\"blender version '{}' not found; found {} version(s):\".format(version, len(installed_versions)))\n",
      "        for v, path in installed_versions.items():\n",
      "            print(\"    {}: {}\".format(v, path))\n",
      "        print(\"searching web archive...\")\n",
      "        versions = get_blender_version_download_links(version, operating_system_name)\n",
      "        print(\"found {} download(s) for blender version '{}', platform '{}':\".format(len(versions), version, operating_system_name))\n",
      "        for url in versions:\n",
      "            print(\"    {}\".format(url))\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    for version, exec_path in get_installed_blender_versions().items():\n",
      "        print(\"found blender {version}: {path}\".format(version=version,\n",
      "                                                       path=exec_path))\n",
      "    blender = find_blender('2.80')\n",
      "    if blender:\n",
      "        print(\"Found blender: '{}'\".format(blender))\n",
      "    else:\n",
      "        print(\"No matching blender version installed :(\")\n",
      "\n",
      "import functools\n",
      "import random\n",
      "from math import cos, pi\n",
      "\n",
      "import cv2\n",
      "import kornia\n",
      "import numpy as np\n",
      "import torch\n",
      "from kornia.augmentation import ColorJitter\n",
      "\n",
      "from data.util import read_img\n",
      "from PIL import Image\n",
      "from io import BytesIO\n",
      "\n",
      "\n",
      "# Get a rough visualization of the above distribution. (Y-axis is meaningless, just spreads data)\n",
      "from utils.util import opt_get\n",
      "\n",
      "'''\n",
      "if __name__ == '__main__':\n",
      "    import numpy as np\n",
      "    import matplotlib.pyplot as plt\n",
      "    data = np.asarray([get_rand() for _ in range(5000)])\n",
      "    plt.plot(data, np.random.uniform(size=(5000,)), 'x')\n",
      "    plt.show()\n",
      "'''\n",
      "\n",
      "\n",
      "def kornia_color_jitter_numpy(img, setting):\n",
      "    if setting * 255 > 1:\n",
      "        # I'm using Kornia's ColorJitter, which requires pytorch arrays in b,c,h,w format.\n",
      "        img = torch.from_numpy(img).permute(2,0,1).unsqueeze(0)\n",
      "        img = ColorJitter(setting, setting, setting, setting)(img)\n",
      "        img = img.squeeze(0).permute(1,2,0).numpy()\n",
      "    return img\n",
      "\n",
      "\n",
      "# Performs image corruption on a list of images from a configurable set of corruption\n",
      "# options.\n",
      "class ImageCorruptor:\n",
      "    def __init__(self, opt):\n",
      "        self.opt = opt\n",
      "        self.reset_random()\n",
      "        self.blur_scale = opt['corruption_blur_scale'] if 'corruption_blur_scale' in opt.keys() else 1\n",
      "        self.fixed_corruptions = opt['fixed_corruptions'] if 'fixed_corruptions' in opt.keys() else []\n",
      "        self.num_corrupts = opt['num_corrupts_per_image'] if 'num_corrupts_per_image' in opt.keys() else 0\n",
      "        self.cosine_bias = opt_get(opt, ['cosine_bias'], True)\n",
      "        if self.num_corrupts == 0:\n",
      "            return\n",
      "        else:\n",
      "            self.random_corruptions = opt['random_corruptions'] if 'random_corruptions' in opt.keys() else []\n",
      "\n",
      "    def reset_random(self):\n",
      "        if 'random_seed' in self.opt.keys():\n",
      "            self.rand = random.Random(self.opt['random_seed'])\n",
      "        else:\n",
      "            self.rand = random.Random()\n",
      "\n",
      "    # Feeds a random uniform through a cosine distribution to slightly bias corruptions towards \"uncorrupted\".\n",
      "    # Return is on [0,1] with a bias towards 0.\n",
      "    def get_rand(self):\n",
      "        r = self.rand.random()\n",
      "        if self.cosine_bias:\n",
      "            return 1 - cos(r * pi / 2)\n",
      "        else:\n",
      "            return r\n",
      "\n",
      "    def corrupt_images(self, imgs, return_entropy=False):\n",
      "        if self.num_corrupts == 0 and not self.fixed_corruptions:\n",
      "            if return_entropy:\n",
      "                return imgs, []\n",
      "            else:\n",
      "                return imgs\n",
      "\n",
      "        if self.num_corrupts == 0:\n",
      "            augmentations = []\n",
      "        else:\n",
      "            augmentations = random.choices(self.random_corruptions, k=self.num_corrupts)\n",
      "\n",
      "        # Sources of entropy\n",
      "        corrupted_imgs = []\n",
      "        entropy = []\n",
      "        undo_fns = []\n",
      "        applied_augs = augmentations + self.fixed_corruptions\n",
      "        for img in imgs:\n",
      "            for aug in augmentations:\n",
      "                r = self.get_rand()\n",
      "                img, undo_fn = self.apply_corruption(img, aug, r, applied_augs)\n",
      "                if undo_fn is not None:\n",
      "                    undo_fns.append(undo_fn)\n",
      "            for aug in self.fixed_corruptions:\n",
      "                r = self.get_rand()\n",
      "                img, undo_fn = self.apply_corruption(img, aug, r, applied_augs)\n",
      "                entropy.append(r)\n",
      "                if undo_fn is not None:\n",
      "                    undo_fns.append(undo_fn)\n",
      "            # Apply undo_fns after all corruptions are finished, in same order.\n",
      "            for ufn in undo_fns:\n",
      "                img = ufn(img)\n",
      "            corrupted_imgs.append(img)\n",
      "\n",
      "\n",
      "        if return_entropy:\n",
      "            return corrupted_imgs, entropy\n",
      "        else:\n",
      "            return corrupted_imgs\n",
      "\n",
      "    def apply_corruption(self, img, aug, rand_val, applied_augmentations):\n",
      "        undo_fn = None\n",
      "        if 'color_quantization' in aug:\n",
      "            # Color quantization\n",
      "            quant_div = 2 ** (int(rand_val * 10 / 3) + 2)\n",
      "            img = img * 255\n",
      "            img = (img // quant_div) * quant_div\n",
      "            img = img / 255\n",
      "        elif 'color_jitter' in aug:\n",
      "            lo_end = 0\n",
      "            hi_end = .2\n",
      "            setting = rand_val * (hi_end - lo_end) + lo_end\n",
      "            img = kornia_color_jitter_numpy(img, setting)\n",
      "        elif 'gaussian_blur' in aug:\n",
      "            img = cv2.GaussianBlur(img, (0,0), self.blur_scale*rand_val*1.5)\n",
      "        elif 'motion_blur' in aug:\n",
      "            # Motion blur\n",
      "            intensity = self.blur_scale*rand_val * 3 + 1\n",
      "            angle = random.randint(0,360)\n",
      "            k = np.zeros((intensity, intensity), dtype=np.float32)\n",
      "            k[(intensity - 1) // 2, :] = np.ones(intensity, dtype=np.float32)\n",
      "            k = cv2.warpAffine(k, cv2.getRotationMatrix2D((intensity / 2 - 0.5, intensity / 2 - 0.5), angle, 1.0),\n",
      "                               (intensity, intensity))\n",
      "            k = k * (1.0 / np.sum(k))\n",
      "            img = cv2.filter2D(img, -1, k)\n",
      "        elif 'block_noise' in aug:\n",
      "            # Large distortion blocks in part of an img, such as is used to mask out a face.\n",
      "            pass\n",
      "        elif 'lq_resampling' in aug:\n",
      "            # Random mode interpolation HR->LR->HR\n",
      "            if 'lq_resampling4x' == aug:\n",
      "                scale = 4\n",
      "            else:\n",
      "                if rand_val < .3:\n",
      "                    scale = 1\n",
      "                elif rand_val < .7:\n",
      "                    scale = 2\n",
      "                else:\n",
      "                    scale = 4\n",
      "            if scale > 1:\n",
      "                interpolation_modes = [cv2.INTER_NEAREST, cv2.INTER_CUBIC, cv2.INTER_LINEAR, cv2.INTER_LANCZOS4]\n",
      "                mode = random.randint(0,4) % len(interpolation_modes)\n",
      "                # Downsample first, then upsample using the random mode.\n",
      "                img = cv2.resize(img, dsize=(img.shape[1]//scale, img.shape[0]//scale), interpolation=mode)\n",
      "                def lq_resampling_undo_fn(scale, img):\n",
      "                    return cv2.resize(img, dsize=(img.shape[1]*scale, img.shape[0]*scale), interpolation=cv2.INTER_LINEAR)\n",
      "                undo_fn = functools.partial(lq_resampling_undo_fn, scale)\n",
      "        elif 'color_shift' in aug:\n",
      "            # Color shift\n",
      "            pass\n",
      "        elif 'interlacing' in aug:\n",
      "            # Interlacing distortion\n",
      "            pass\n",
      "        elif 'chromatic_aberration' in aug:\n",
      "            # Chromatic aberration\n",
      "            pass\n",
      "        elif 'noise' in aug:\n",
      "            # Random noise\n",
      "            if 'noise-5' == aug:\n",
      "                noise_intensity = 5 / 255.0\n",
      "            else:\n",
      "                noise_intensity = (rand_val*6) / 255.0\n",
      "            img += np.random.rand(*img.shape) * noise_intensity\n",
      "        elif 'jpeg' in aug:\n",
      "            if 'noise' not in applied_augmentations and 'noise-5' not in applied_augmentations:\n",
      "                if aug == 'jpeg':\n",
      "                    lo=10\n",
      "                    range=20\n",
      "                elif aug == 'jpeg-low':\n",
      "                    lo=15\n",
      "                    range=10\n",
      "                elif aug == 'jpeg-medium':\n",
      "                    lo=23\n",
      "                    range=25\n",
      "                elif aug == 'jpeg-broad':\n",
      "                    lo=15\n",
      "                    range=60\n",
      "                elif aug == 'jpeg-normal':\n",
      "                    lo=47\n",
      "                    range=35\n",
      "                else:\n",
      "                    raise NotImplementedError(\"specified jpeg corruption doesn't exist\")\n",
      "                # JPEG compression\n",
      "                qf = (int((1-rand_val)*range) + lo)\n",
      "                # Use PIL to perform a mock compression to a data buffer, then swap back to cv2.\n",
      "                img = (img * 255).astype(np.uint8)\n",
      "                img = Image.fromarray(img)\n",
      "                buffer = BytesIO()\n",
      "                img.save(buffer, \"JPEG\", quality=qf, optimize=True)\n",
      "                buffer.seek(0)\n",
      "                jpeg_img_bytes = np.asarray(bytearray(buffer.read()), dtype=\"uint8\")\n",
      "                img = read_img(\"buffer\", jpeg_img_bytes, rgb=True)\n",
      "        elif 'saturation' in aug:\n",
      "            # Lightening / saturation\n",
      "            saturation = rand_val * .3\n",
      "            img = np.clip(img + saturation, a_max=1, a_min=0)\n",
      "        elif 'greyscale' in aug:\n",
      "            img = np.tile(np.mean(img, axis=2, keepdims=True), [1,1,3])\n",
      "        elif 'none' not in aug:\n",
      "            raise NotImplementedError(\"Augmentation doesn't exist\")\n",
      "\n",
      "        return img, undo_fn\n",
      "\n",
      "# This test requires CPython3.5\n",
      "print(b\"%%\" % ())\n",
      "print(b\"=%d=\" % 1)\n",
      "print(b\"=%d=%d=\" % (1, 2))\n",
      "\n",
      "print(b\"=%s=\" % b\"str\")\n",
      "print(b\"=%r=\" % b\"str\")\n",
      "\n",
      "print(\"PASS\")\n",
      "#\n",
      "# test_JpegCompression.py\n",
      "#\n",
      "\n",
      "import pytest\n",
      "import albumentations as A\n",
      "from .context import TfDataAugmentation as Tfda\n",
      "from . import test_utils\n",
      "from .test_utils import TestResult\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"quality_lower, quality_upper, expected, message\", [\n",
      "        # quality_lower\n",
      "        (-1, 100, TestResult.Error,\n",
      "         \"quality_lower < min => Error\"),\n",
      "        (0, 100, TestResult.OK,\n",
      "         \"quality_lower == min => OK\"),\n",
      "        (100, 100, TestResult.OK,\n",
      "         \"quality_lower == max => OK\"),\n",
      "        (101, 100, TestResult.Error,\n",
      "         \"quality_lower >= max => Error\"),\n",
      "\n",
      "        # quality_upper\n",
      "        (0, -1, TestResult.Error,\n",
      "         \"quality_upper < min => Error\"),\n",
      "        (0, 0, TestResult.OK,\n",
      "         \"quality_upper == min => OK\"),\n",
      "        (0, 100, TestResult.OK,\n",
      "         \"quality_upper == max => OK\"),\n",
      "        (0, 101, TestResult.Error,\n",
      "         \"quality_upper > max => Error\"),\n",
      "\n",
      "        # Relation\n",
      "        (50, 50, TestResult.OK,\n",
      "         \"quality_lower == quality_upper => OK\"),\n",
      "        (51, 50, TestResult.Error,\n",
      "         \"quality_lower > quality_upper => Error\"),\n",
      "    ])\n",
      "def test_hue_shift_limit_value(\n",
      "        quality_lower, quality_upper, expected, message):\n",
      "    try:\n",
      "        Tfda.JpegCompression(\n",
      "            quality_lower=quality_lower,\n",
      "            quality_upper=quality_upper)\n",
      "        actual = TestResult.OK\n",
      "    except ValueError:\n",
      "        actual = TestResult.Error\n",
      "    assert expected == actual, message\n",
      "\n",
      "\n",
      "def test_call():\n",
      "    quality_lower = 50\n",
      "    quality_upper = 100\n",
      "    tgt_jpeg = Tfda.JpegCompression(\n",
      "        quality_lower=quality_lower,\n",
      "        quality_upper=quality_upper,\n",
      "        p=1.0)\n",
      "    tgt_transform = \\\n",
      "        test_utils.make_tgt_transform(tgt_jpeg)\n",
      "    image = test_utils.make_test_image()\n",
      "\n",
      "    tgt_result = tgt_transform(image=image)\n",
      "    actual_image = tgt_result['image']\n",
      "\n",
      "    image_np = image.numpy()\n",
      "    quality = float(tgt_jpeg.get_param('quality'))\n",
      "    expected_image = A.image_compression(\n",
      "        image_np, quality, image_type='.jpg')\n",
      "\n",
      "    test_utils.partial_assert_array(\n",
      "        expected_image, actual_image, 0.6, \"image\", eps=0.1)\n",
      "\n",
      "import os\n",
      "\n",
      "from torch.utils.data import DataLoader\n",
      "from continuum.datasets import CIFAR10, InMemoryDataset\n",
      "from continuum.datasets import MNIST\n",
      "import torchvision\n",
      "from continuum.scenarios import TransformationIncremental\n",
      "import pytest\n",
      "import numpy as np\n",
      "\n",
      "from continuum.transforms.bg_swap import BackgroundSwap\n",
      "\n",
      "DATA_PATH = os.environ.get(\"CONTINUUM_DATA_PATH\")\n",
      "\n",
      "# Uncomment for debugging via image output\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "def test_bg_swap_fast():\n",
      "    \"\"\"\n",
      "    Fast test for background swap.\n",
      "    \"\"\"\n",
      "    bg_x = np.ones(shape=[2, 5, 5, 3]) * -1\n",
      "    bg_y = np.random.rand(2)\n",
      "\n",
      "    fg = np.random.normal(loc=.5, scale=.1, size=[5, 5])\n",
      "    bg = InMemoryDataset(bg_x, bg_y)\n",
      "\n",
      "    bg_swap = BackgroundSwap(bg, input_dim=(5, 5), normalize_bg=None)\n",
      "\n",
      "    spliced_1_channel = bg_swap(fg)[:, :, 0]\n",
      "\n",
      "    assert np.array_equal((spliced_1_channel <= -1), (fg <= .5))\n",
      "\n",
      "\n",
      "@pytest.mark.slow\n",
      "def test_background_swap_numpy():\n",
      "    \"\"\"\n",
      "    Test background swap on a single ndarray input.\n",
      "    \"\"\"\n",
      "    mnist = MNIST(DATA_PATH, download=True, train=True)\n",
      "    cifar = CIFAR10(DATA_PATH, download=True, train=True)\n",
      "\n",
      "    bg_swap = BackgroundSwap(cifar, input_dim=(28, 28))\n",
      "\n",
      "    im = mnist.get_data()[0][0]\n",
      "    im = bg_swap(im)\n",
      "\n",
      "    # Uncomment for debugging\n",
      "    # plt.imshow(im, interpolation='nearest')\n",
      "    # plt.show()\n",
      "\n",
      "\n",
      "@pytest.mark.slow\n",
      "def test_background_swap_torch():\n",
      "    \"\"\"\n",
      "    Test background swap on a single tensor input.\n",
      "    \"\"\"\n",
      "    cifar = CIFAR10(DATA_PATH, download=True, train=True)\n",
      "\n",
      "    mnist = torchvision.datasets.MNIST(DATA_PATH, train=True, download=True,\n",
      "                                       transform=torchvision.transforms.Compose([\n",
      "                                           torchvision.transforms.ToTensor()\n",
      "                                       ]))\n",
      "\n",
      "    bg_swap = BackgroundSwap(cifar, input_dim=(28, 28))\n",
      "    im = mnist[0][0]\n",
      "\n",
      "    im = bg_swap(im)\n",
      "\n",
      "    # Uncomment for debugging\n",
      "    # plt.imshow(im.permute(1, 2, 0), interpolation='nearest')\n",
      "    # plt.show()\n",
      "\n",
      "\n",
      "@pytest.mark.slow\n",
      "def test_background_tranformation():\n",
      "    \"\"\"\n",
      "    Example code using TransformationIncremental to create a setting with 3 tasks.\n",
      "    \"\"\"\n",
      "    cifar = CIFAR10(DATA_PATH, train=True)\n",
      "    mnist = MNIST(DATA_PATH, download=False, train=True)\n",
      "    nb_task = 3\n",
      "    list_trsf = []\n",
      "    for i in range(nb_task):\n",
      "        list_trsf.append([torchvision.transforms.ToTensor(), BackgroundSwap(cifar, bg_label=i, input_dim=(28, 28)),\n",
      "                          torchvision.transforms.ToPILImage()])\n",
      "    scenario = TransformationIncremental(mnist, base_transformations=[torchvision.transforms.ToTensor()],\n",
      "                                         incremental_transformations=list_trsf)\n",
      "    folder = \"tests/samples/background_trsf/\"\n",
      "    if not os.path.exists(folder):\n",
      "        os.makedirs(folder)\n",
      "    for task_id, task_data in enumerate(scenario):\n",
      "        task_data.plot(path=folder, title=f\"background_{task_id}.jpg\", nb_samples=100, shape=[28, 28, 3])\n",
      "        loader = DataLoader(task_data)\n",
      "        _, _, _ = next(iter(loader))\n",
      "\n",
      "# =========================================================================================\n",
      "#  Copyright 2015 Community Information Online Consortium (CIOC) and KCL Software Solutions\n",
      "#\n",
      "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "#  you may not use this file except in compliance with the License.\n",
      "#  You may obtain a copy of the License at\n",
      "#\n",
      "#      http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "#  Unless required by applicable law or agreed to in writing, software\n",
      "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "#  See the License for the specific language governing permissions and\n",
      "#  limitations under the License.\n",
      "# =========================================================================================\n",
      "\n",
      "\n",
      "# std lib\n",
      "import os\n",
      "\n",
      "# jQuery and jQueryUI versions\n",
      "JQUERY_VERSION = \"1.6.2\"\n",
      "JQUERY_UI_VERSION = \"1.8.16\"\n",
      "\n",
      "# formatting constants\n",
      "DATE_TEXT_SIZE = 25\n",
      "TEXT_SIZE = 85\n",
      "TEXTAREA_COLS = 85\n",
      "TEXTAREA_ROWS_SHORT = 2\n",
      "TEXTAREA_ROWS_LONG = 4\n",
      "TEXTAREA_ROWS_XLONG = 10\n",
      "MAX_LENGTH_CHECKLIST_NOTES = 255\n",
      "EMAIL_LENGTH = 60\n",
      "\n",
      "# application running constants\n",
      "_app_path = None\n",
      "_config_file = None\n",
      "_app_name = None\n",
      "session_lock_dir = None\n",
      "publish_dir = None\n",
      "\n",
      "\n",
      "def update_cache_values():\n",
      "    # called from application init at startup\n",
      "    global _app_path, _config_file, _app_name, session_lock_dir, publish_dir\n",
      "\n",
      "    if _app_path is None:\n",
      "        _app_path = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))\n",
      "        _app_name = os.path.split(_app_path)[1]\n",
      "        _config_file = os.path.join(_app_path, '..', '..', 'config', _app_name + '.ini')\n",
      "        session_lock_dir = os.path.join(_app_path, 'python', 'session_lock')\n",
      "        publish_dir = os.path.join(_app_path, 'python', 'published_files')\n",
      "\n",
      "        try:\n",
      "            os.makedirs(session_lock_dir)\n",
      "        except os.error:\n",
      "            pass\n",
      "\n",
      "        try:\n",
      "            os.makedirs(publish_dir)\n",
      "        except os.error:\n",
      "            pass\n",
      "\n",
      "# coding=utf-8\n",
      "# --------------------------------------------------------------------------\n",
      "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
      "# Licensed under the MIT License. See License.txt in the project root for license information.\n",
      "# Code generated by Microsoft (R) AutoRest Code Generator.\n",
      "# Changes may cause incorrect behavior and will be lost if the code is regenerated.\n",
      "# --------------------------------------------------------------------------\n",
      "\n",
      "from typing import Any, TYPE_CHECKING\n",
      "\n",
      "from azure.core.configuration import Configuration\n",
      "from azure.core.pipeline import policies\n",
      "from azure.mgmt.core.policies import ARMHttpLoggingPolicy\n",
      "\n",
      "from .._version import VERSION\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    # pylint: disable=unused-import,ungrouped-imports\n",
      "    from azure.core.credentials_async import AsyncTokenCredential\n",
      "\n",
      "\n",
      "class WebSiteManagementClientConfiguration(Configuration):\n",
      "    \"\"\"Configuration for WebSiteManagementClient.\n",
      "\n",
      "    Note that all parameters used to create this instance are saved as instance\n",
      "    attributes.\n",
      "\n",
      "    :param credential: Credential needed for the client to connect to Azure.\n",
      "    :type credential: ~azure.core.credentials_async.AsyncTokenCredential\n",
      "    :param subscription_id: Your Azure subscription ID. This is a GUID-formatted string (e.g. 00000000-0000-0000-0000-000000000000).\n",
      "    :type subscription_id: str\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        credential: \"AsyncTokenCredential\",\n",
      "        subscription_id: str,\n",
      "        **kwargs: Any\n",
      "    ) -> None:\n",
      "        if credential is None:\n",
      "            raise ValueError(\"Parameter 'credential' must not be None.\")\n",
      "        if subscription_id is None:\n",
      "            raise ValueError(\"Parameter 'subscription_id' must not be None.\")\n",
      "        super(WebSiteManagementClientConfiguration, self).__init__(**kwargs)\n",
      "\n",
      "        self.credential = credential\n",
      "        self.subscription_id = subscription_id\n",
      "        self.api_version = \"2015-08-01\"\n",
      "        self.credential_scopes = kwargs.pop('credential_scopes', ['https://management.azure.com/.default'])\n",
      "        kwargs.setdefault('sdk_moniker', 'mgmt-web/{}'.format(VERSION))\n",
      "        self._configure(**kwargs)\n",
      "\n",
      "    def _configure(\n",
      "        self,\n",
      "        **kwargs: Any\n",
      "    ) -> None:\n",
      "        self.user_agent_policy = kwargs.get('user_agent_policy') or policies.UserAgentPolicy(**kwargs)\n",
      "        self.headers_policy = kwargs.get('headers_policy') or policies.HeadersPolicy(**kwargs)\n",
      "        self.proxy_policy = kwargs.get('proxy_policy') or policies.ProxyPolicy(**kwargs)\n",
      "        self.logging_policy = kwargs.get('logging_policy') or policies.NetworkTraceLoggingPolicy(**kwargs)\n",
      "        self.http_logging_policy = kwargs.get('http_logging_policy') or ARMHttpLoggingPolicy(**kwargs)\n",
      "        self.retry_policy = kwargs.get('retry_policy') or policies.AsyncRetryPolicy(**kwargs)\n",
      "        self.custom_hook_policy = kwargs.get('custom_hook_policy') or policies.CustomHookPolicy(**kwargs)\n",
      "        self.redirect_policy = kwargs.get('redirect_policy') or policies.AsyncRedirectPolicy(**kwargs)\n",
      "        self.authentication_policy = kwargs.get('authentication_policy')\n",
      "        if self.credential and not self.authentication_policy:\n",
      "            self.authentication_policy = policies.AsyncBearerTokenCredentialPolicy(self.credential, *self.credential_scopes, **kwargs)\n",
      "\n",
      "import django.http\n",
      "\n",
      "import unittest.mock\n",
      "\n",
      "from .. import middleware\n",
      "\n",
      "\n",
      "def get_response(req):\n",
      "    # dummy get_response, just return an empty response\n",
      "    return django.http.HttpResponse()\n",
      "\n",
      "\n",
      "def test_leaves_remote_addr_alone_if_no_real_ip():\n",
      "    remote_addr = object()\n",
      "    request = unittest.mock.MagicMock()\n",
      "    request.META = {\"REMOTE_ADDR\": remote_addr}\n",
      "\n",
      "    middleware.XRealIPMiddleware(get_response)(request)\n",
      "\n",
      "    assert request.META[\"REMOTE_ADDR\"] is remote_addr\n",
      "\n",
      "\n",
      "def test_switches_out_x_real_ip_if_available():\n",
      "    remote_addr = object()\n",
      "    x_real_ip = object()\n",
      "\n",
      "    request = unittest.mock.MagicMock()\n",
      "    request.META = {\"REMOTE_ADDR\": remote_addr, \"HTTP_X_REAL_IP\": x_real_ip}\n",
      "\n",
      "    middleware.XRealIPMiddleware(get_response)(request)\n",
      "\n",
      "    assert request.META[\"REMOTE_ADDR\"] is x_real_ip\n",
      "    assert request.META[\"HTTP_X_REAL_IP\"] is x_real_ip\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import time\n",
      "\n",
      "import RPi.GPIO as GPIO\n",
      "\n",
      "\n",
      "GPIO.setmode(GPIO.BCM)\n",
      "GPIO.setup(21, GPIO.OUT)\n",
      "GPIO.output(21, GPIO.LOW)\n",
      "\n",
      "time.sleep(3.00)\n",
      "\n",
      "GPIO.output(21, GPIO.HIGH)\n",
      "GPIO.cleanup()\n",
      "\n",
      "\n",
      "from direct.directnotify.DirectNotifyGlobal import directNotify\n",
      "\n",
      "\n",
      "class Notifier:\n",
      "    def __init__(self, name):\n",
      "        \"\"\"\n",
      "        @param name: The name of the notifier. Be sure to add it to your config/Config.prc!\n",
      "        @type name: str\n",
      "        \"\"\"\n",
      "        self.notify = directNotify.newCategory(name)\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def train_ml_squarer() -> None:\n",
      "    print(\"Training!\")\n",
      "\n",
      "\n",
      "def square() -> int:\n",
      "    \"\"\"Square a number...maybe\"\"\"\n",
      "    return np.random.randint(1, 100)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    train_ml_squarer()\n",
      "\"\"\"\n",
      "Platformer Game\n",
      "\"\"\"\n",
      "import arcade\n",
      "\n",
      "# Constants\n",
      "SCREEN_WIDTH = 1000\n",
      "SCREEN_HEIGHT = 650\n",
      "SCREEN_TITLE = \"Platformer\"\n",
      "\n",
      "# Constants used to scale our sprites from their original size\n",
      "CHARACTER_SCALING = 1\n",
      "TILE_SCALING = 0.5\n",
      "COIN_SCALING = 0.5\n",
      "SPRITE_PIXEL_SIZE = 128\n",
      "GRID_PIXEL_SIZE = SPRITE_PIXEL_SIZE * TILE_SCALING\n",
      "\n",
      "# Movement speed of player, in pixels per frame\n",
      "PLAYER_MOVEMENT_SPEED = 10\n",
      "GRAVITY = 1\n",
      "PLAYER_JUMP_SPEED = 20\n",
      "\n",
      "\n",
      "class MyGame(arcade.Window):\n",
      "    \"\"\"\n",
      "    Main application class.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        # Call the parent class and set up the window\n",
      "        super().__init__(SCREEN_WIDTH, SCREEN_HEIGHT, SCREEN_TITLE)\n",
      "\n",
      "        # Our TileMap Object\n",
      "        self.tile_map = None\n",
      "\n",
      "        # Our Scene Object\n",
      "        self.scene = None\n",
      "\n",
      "        # Separate variable that holds the player sprite\n",
      "        self.player_sprite = None\n",
      "\n",
      "        # Our physics engine\n",
      "        self.physics_engine = None\n",
      "\n",
      "        # A Camera that can be used for scrolling the screen\n",
      "        self.camera = None\n",
      "\n",
      "        # A Camera that can be used to draw GUI elements\n",
      "        self.gui_camera = None\n",
      "\n",
      "        # Keep track of the score\n",
      "        self.score = 0\n",
      "\n",
      "        # Load sounds\n",
      "        self.collect_coin_sound = arcade.load_sound(\":resources:sounds/coin1.wav\")\n",
      "        self.jump_sound = arcade.load_sound(\":resources:sounds/jump1.wav\")\n",
      "\n",
      "        arcade.set_background_color(arcade.csscolor.CORNFLOWER_BLUE)\n",
      "\n",
      "    def setup(self):\n",
      "        \"\"\"Set up the game here. Call this function to restart the game.\"\"\"\n",
      "\n",
      "        # Setup the Cameras\n",
      "        self.camera = arcade.Camera(self.width, self.height)\n",
      "        self.gui_camera = arcade.Camera(self.width, self.height)\n",
      "\n",
      "        # Name of map file to load\n",
      "        map_name = \":resources:tiled_maps/map.json\"\n",
      "\n",
      "        # Layer specific options are defined based on Layer names in a dictionary\n",
      "        # Doing this will make the SpriteList for the platforms layer\n",
      "        # use spatial hashing for detection.\n",
      "        layer_options = {\n",
      "            \"Platforms\": {\n",
      "                \"use_spatial_hash\": True,\n",
      "            },\n",
      "        }\n",
      "\n",
      "        # Read in the tiled map\n",
      "        self.tile_map = arcade.load_tilemap(map_name, TILE_SCALING, layer_options)\n",
      "\n",
      "        # Initialize Scene with our TileMap, this will automatically add all layers\n",
      "        # from the map as SpriteLists in the scene in the proper order.\n",
      "        self.scene = arcade.Scene.from_tilemap(self.tile_map)\n",
      "\n",
      "        # Keep track of the score\n",
      "        self.score = 0\n",
      "\n",
      "        # Set up the player, specifically placing it at these coordinates.\n",
      "        image_source = \":resources:images/animated_characters/female_adventurer/femaleAdventurer_idle.png\"\n",
      "        self.player_sprite = arcade.Sprite(image_source, CHARACTER_SCALING)\n",
      "        self.player_sprite.center_x = 128\n",
      "        self.player_sprite.center_y = 128\n",
      "        self.scene.add_sprite(\"Player\", self.player_sprite)\n",
      "\n",
      "        # --- Other stuff\n",
      "        # Set the background color\n",
      "        if self.tile_map.background_color:\n",
      "            arcade.set_background_color(self.tile_map.background_color)\n",
      "\n",
      "        # Create the 'physics engine'\n",
      "        self.physics_engine = arcade.PhysicsEnginePlatformer(\n",
      "            self.player_sprite, gravity_constant=GRAVITY, walls=self.scene[\"Platforms\"]\n",
      "        )\n",
      "\n",
      "    def on_draw(self):\n",
      "        \"\"\"Render the screen.\"\"\"\n",
      "\n",
      "        # Clear the screen to the background color\n",
      "        arcade.start_render()\n",
      "\n",
      "        # Activate the game camera\n",
      "        self.camera.use()\n",
      "\n",
      "        # Draw our Scene\n",
      "        self.scene.draw()\n",
      "\n",
      "        # Activate the GUI camera before drawing GUI elements\n",
      "        self.gui_camera.use()\n",
      "\n",
      "        # Draw our score on the screen, scrolling it with the viewport\n",
      "        score_text = f\"Score: {self.score}\"\n",
      "        arcade.draw_text(\n",
      "            score_text,\n",
      "            10,\n",
      "            10,\n",
      "            arcade.csscolor.WHITE,\n",
      "            18,\n",
      "        )\n",
      "\n",
      "    def on_key_press(self, key, modifiers):\n",
      "        \"\"\"Called whenever a key is pressed.\"\"\"\n",
      "\n",
      "        if key == arcade.key.UP or key == arcade.key.W:\n",
      "            if self.physics_engine.can_jump():\n",
      "                self.player_sprite.change_y = PLAYER_JUMP_SPEED\n",
      "                arcade.play_sound(self.jump_sound)\n",
      "        elif key == arcade.key.LEFT or key == arcade.key.A:\n",
      "            self.player_sprite.change_x = -PLAYER_MOVEMENT_SPEED\n",
      "        elif key == arcade.key.RIGHT or key == arcade.key.D:\n",
      "            self.player_sprite.change_x = PLAYER_MOVEMENT_SPEED\n",
      "\n",
      "    def on_key_release(self, key, modifiers):\n",
      "        \"\"\"Called when the user releases a key.\"\"\"\n",
      "\n",
      "        if key == arcade.key.LEFT or key == arcade.key.A:\n",
      "            self.player_sprite.change_x = 0\n",
      "        elif key == arcade.key.RIGHT or key == arcade.key.D:\n",
      "            self.player_sprite.change_x = 0\n",
      "\n",
      "    def center_camera_to_player(self):\n",
      "        screen_center_x = self.player_sprite.center_x - (self.camera.viewport_width / 2)\n",
      "        screen_center_y = self.player_sprite.center_y - (\n",
      "            self.camera.viewport_height / 2\n",
      "        )\n",
      "        if screen_center_x < 0:\n",
      "            screen_center_x = 0\n",
      "        if screen_center_y < 0:\n",
      "            screen_center_y = 0\n",
      "        player_centered = screen_center_x, screen_center_y\n",
      "\n",
      "        self.camera.move_to(player_centered)\n",
      "\n",
      "    def on_update(self, delta_time):\n",
      "        \"\"\"Movement and game logic\"\"\"\n",
      "\n",
      "        # Move the player with the physics engine\n",
      "        self.physics_engine.update()\n",
      "\n",
      "        # See if we hit any coins\n",
      "        coin_hit_list = arcade.check_for_collision_with_list(\n",
      "            self.player_sprite, self.scene[\"Coins\"]\n",
      "        )\n",
      "\n",
      "        # Loop through each coin we hit (if any) and remove it\n",
      "        for coin in coin_hit_list:\n",
      "            # Remove the coin\n",
      "            coin.remove_from_sprite_lists()\n",
      "            # Play a sound\n",
      "            arcade.play_sound(self.collect_coin_sound)\n",
      "            # Add one to the score\n",
      "            self.score += 1\n",
      "\n",
      "        # Position the camera\n",
      "        self.center_camera_to_player()\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function\"\"\"\n",
      "    window = MyGame()\n",
      "    window.setup()\n",
      "    arcade.run()\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "'''\n",
      "lib/ycmd/start.py\n",
      "Server bootstrap logic. Includes a utility class for normalizing parameters and\n",
      "calculating default ones. Also includes a helper to set up the temporary\n",
      "options file.\n",
      "'''\n",
      "\n",
      "import logging\n",
      "import os\n",
      "import tempfile\n",
      "\n",
      "from ..process import (\n",
      "    FileHandles,\n",
      "    Process,\n",
      ")\n",
      "from ..util.fs import (\n",
      "    default_python_binary_path,\n",
      "    save_json_file,\n",
      ")\n",
      "from ..ycmd.constants import (\n",
      "    YCMD_LOG_SPOOL_OUTPUT,\n",
      "    YCMD_LOG_SPOOL_SIZE,\n",
      "    YCMD_DEFAULT_SERVER_CHECK_INTERVAL_SECONDS,\n",
      "    YCMD_DEFAULT_SERVER_IDLE_SUICIDE_SECONDS,\n",
      ")\n",
      "from ..ycmd.settings import (\n",
      "    get_default_settings_path,\n",
      "    generate_settings_data,\n",
      ")\n",
      "\n",
      "logger = logging.getLogger('sublime-ycmd.' + __name__)\n",
      "\n",
      "\n",
      "class StartupParameters(object):\n",
      "    '''\n",
      "    Startup parameters for a ycmd server instance.\n",
      "    Should include all the necessary configuration for creating the ycmd\n",
      "    server process. Also calculates defaults for certain parameters.\n",
      "    '''\n",
      "\n",
      "    def __init__(self, ycmd_root_directory=None,\n",
      "                 ycmd_settings_path=None,\n",
      "                 working_directory=None,\n",
      "                 python_binary_path=None,\n",
      "                 server_idle_suicide_seconds=None,\n",
      "                 server_check_interval_seconds=None):\n",
      "        self._ycmd_root_directory = None\n",
      "        self._ycmd_settings_path = None\n",
      "\n",
      "        self._working_directory = None\n",
      "        self._python_binary_path = None\n",
      "        self._server_idle_suicide_seconds = None\n",
      "        self._server_check_interval_seconds = None\n",
      "\n",
      "        # additional attributes, can be set via the properties\n",
      "        self._log_level = None\n",
      "        self._stdout_log_path = None\n",
      "        self._stderr_log_path = None\n",
      "        self._keep_logs = None\n",
      "\n",
      "        self.ycmd_root_directory = ycmd_root_directory\n",
      "        self.ycmd_settings_path = ycmd_settings_path\n",
      "        self.working_directory = working_directory\n",
      "        self.python_binary_path = python_binary_path\n",
      "        self.server_idle_suicide_seconds = server_idle_suicide_seconds\n",
      "        self.server_check_interval_seconds = server_check_interval_seconds\n",
      "\n",
      "    @property\n",
      "    def ycmd_root_directory(self):\n",
      "        if self._ycmd_root_directory is None:\n",
      "            logger.warning('no ycmd root directory has been set')\n",
      "        return self._ycmd_root_directory\n",
      "\n",
      "    @ycmd_root_directory.setter\n",
      "    def ycmd_root_directory(self, ycmd_root_directory):\n",
      "        if ycmd_root_directory is not None and \\\n",
      "                not isinstance(ycmd_root_directory, str):\n",
      "            raise TypeError(ycmd_root_directory,)\n",
      "        self._ycmd_root_directory = ycmd_root_directory\n",
      "\n",
      "    @property\n",
      "    def ycmd_settings_path(self):\n",
      "        if self._ycmd_settings_path is None:\n",
      "            if self._ycmd_root_directory is not None:\n",
      "                return get_default_settings_path(self._ycmd_root_directory)\n",
      "            logger.warning('no ycmd root directory has been set')\n",
      "\n",
      "        return self._ycmd_settings_path\n",
      "\n",
      "    @ycmd_settings_path.setter\n",
      "    def ycmd_settings_path(self, ycmd_settings_path):\n",
      "        if ycmd_settings_path is not None and \\\n",
      "                not isinstance(ycmd_settings_path, str):\n",
      "            raise TypeError(ycmd_settings_path,)\n",
      "        self._ycmd_settings_path = ycmd_settings_path\n",
      "\n",
      "    @property\n",
      "    def working_directory(self):\n",
      "        if self._working_directory is None:\n",
      "            return os.getcwd()\n",
      "        return self._working_directory\n",
      "\n",
      "    @working_directory.setter\n",
      "    def working_directory(self, working_directory):\n",
      "        if working_directory is not None and \\\n",
      "                not isinstance(working_directory, str):\n",
      "            raise TypeError(working_directory,)\n",
      "        self._working_directory = working_directory\n",
      "\n",
      "    @property\n",
      "    def python_binary_path(self):\n",
      "        if self._python_binary_path is None:\n",
      "            return default_python_binary_path()\n",
      "        return self._python_binary_path\n",
      "\n",
      "    @python_binary_path.setter\n",
      "    def python_binary_path(self, python_binary_path):\n",
      "        if python_binary_path is not None and \\\n",
      "                not isinstance(python_binary_path, str):\n",
      "            raise TypeError(python_binary_path,)\n",
      "        self._python_binary_path = python_binary_path\n",
      "\n",
      "    @property\n",
      "    def server_idle_suicide_seconds(self):\n",
      "        if self._server_idle_suicide_seconds is None:\n",
      "            return YCMD_DEFAULT_SERVER_IDLE_SUICIDE_SECONDS\n",
      "        return self._server_idle_suicide_seconds\n",
      "\n",
      "    @server_idle_suicide_seconds.setter\n",
      "    def server_idle_suicide_seconds(self, server_idle_suicide_seconds):\n",
      "        if server_idle_suicide_seconds is not None and \\\n",
      "                not isinstance(server_idle_suicide_seconds, int):\n",
      "            raise TypeError(server_idle_suicide_seconds,)\n",
      "        self._server_idle_suicide_seconds = server_idle_suicide_seconds\n",
      "\n",
      "    @property\n",
      "    def server_check_interval_seconds(self):\n",
      "        if self._server_check_interval_seconds is None:\n",
      "            return YCMD_DEFAULT_SERVER_CHECK_INTERVAL_SECONDS\n",
      "        return self._server_check_interval_seconds\n",
      "\n",
      "    @server_check_interval_seconds.setter\n",
      "    def server_check_interval_seconds(self, server_check_interval_seconds):\n",
      "        if server_check_interval_seconds is not None and \\\n",
      "                not isinstance(server_check_interval_seconds, int):\n",
      "            raise TypeError(server_check_interval_seconds,)\n",
      "        self._server_check_interval_seconds = server_check_interval_seconds\n",
      "\n",
      "    @property\n",
      "    def log_level(self):\n",
      "        return self._log_level\n",
      "\n",
      "    @log_level.setter\n",
      "    def log_level(self, log_level):\n",
      "        if log_level is not None and not isinstance(log_level, str):\n",
      "            raise TypeError('log level must be a str: %r' % (log_level))\n",
      "\n",
      "        if log_level is not None and not _is_valid_log_level(log_level):\n",
      "            logger.warning('log level unrecognized: %r', log_level)\n",
      "            # but fall through and do it anyway\n",
      "\n",
      "        self._log_level = log_level\n",
      "\n",
      "    @property\n",
      "    def stdout_log_path(self):\n",
      "        return self._stdout_log_path\n",
      "\n",
      "    @stdout_log_path.setter\n",
      "    def stdout_log_path(self, stdout_log_path):\n",
      "        if stdout_log_path is not None and \\\n",
      "                not isinstance(stdout_log_path, str):\n",
      "            raise TypeError(\n",
      "                'stdout log path must be a str: %r' % (stdout_log_path)\n",
      "            )\n",
      "        self._stdout_log_path = stdout_log_path\n",
      "\n",
      "    @property\n",
      "    def stderr_log_path(self):\n",
      "        return self._stderr_log_path\n",
      "\n",
      "    @stderr_log_path.setter\n",
      "    def stderr_log_path(self, stderr_log_path):\n",
      "        if stderr_log_path is not None and \\\n",
      "                not isinstance(stderr_log_path, str):\n",
      "            raise TypeError(\n",
      "                'stderr_log_path must be a str: %r' % (stderr_log_path)\n",
      "            )\n",
      "        self._stderr_log_path = stderr_log_path\n",
      "\n",
      "    @property\n",
      "    def keep_logs(self):\n",
      "        if self._keep_logs is None:\n",
      "            return False\n",
      "        return self._keep_logs\n",
      "\n",
      "    @keep_logs.setter\n",
      "    def keep_logs(self, keep_logs):\n",
      "        if keep_logs is not None and not isinstance(keep_logs, bool):\n",
      "            raise TypeError('keep-logs must be a bool: %r' % (keep_logs))\n",
      "        self._keep_logs = keep_logs\n",
      "\n",
      "    @property\n",
      "    def ycmd_module_directory(self):\n",
      "        if self._ycmd_root_directory is None:\n",
      "            logger.error('no ycmd root directory set')\n",
      "            raise AttributeError\n",
      "        return os.path.join(self._ycmd_root_directory, 'ycmd')\n",
      "\n",
      "    def copy(self):\n",
      "        '''\n",
      "        Creates a shallow-copy of the startup parameters.\n",
      "        '''\n",
      "        raw_attrs = [\n",
      "            '_ycmd_root_directory',\n",
      "            '_ycmd_settings_path',\n",
      "            '_working_directory',\n",
      "            '_python_binary_path',\n",
      "            '_server_idle_suicide_seconds',\n",
      "            '_server_check_interval_seconds',\n",
      "            '_log_level',\n",
      "            '_stdout_log_path',\n",
      "            '_stderr_log_path',\n",
      "            '_keep_logs',\n",
      "        ]\n",
      "        result = StartupParameters()\n",
      "\n",
      "        for attr in raw_attrs:\n",
      "            attr_value = getattr(self, attr)\n",
      "            setattr(result, attr, attr_value)\n",
      "\n",
      "        return result\n",
      "\n",
      "    def __iter__(self):\n",
      "        ''' Dictionary-compatible iterator. '''\n",
      "        return iter((\n",
      "            ('ycmd_root_directory', self.ycmd_root_directory),\n",
      "            ('ycmd_settings_path', self.ycmd_settings_path),\n",
      "            ('working_directory', self.working_directory),\n",
      "            ('python_binary_path', self.python_binary_path),\n",
      "            ('server_idle_suicide_seconds', self.server_idle_suicide_seconds),\n",
      "            (\n",
      "                'server_check_interval_seconds',\n",
      "                self.server_check_interval_seconds,\n",
      "            ),\n",
      "            ('ycmd_module_directory', self.ycmd_module_directory),\n",
      "            ('log_level', self.log_level),\n",
      "            ('stdout_log_path', self.stdout_log_path),\n",
      "            ('stderr_log_path', self.stderr_log_path),\n",
      "            ('keep_logs', self.keep_logs),\n",
      "        ))\n",
      "\n",
      "    def __str__(self):\n",
      "        return (\n",
      "            'ycmd path, default settings path, '\n",
      "            'python binary path, working directory: '\n",
      "            '%(ycmd_root_directory)s, %(ycmd_settings_path)s, '\n",
      "            '%(python_binary_path)s, %(working_directory)s' %\n",
      "            (dict(self))\n",
      "        )\n",
      "\n",
      "    def __repr__(self):\n",
      "        return '%s(%r)' % (StartupParameters, dict(self))\n",
      "\n",
      "\n",
      "def to_startup_parameters(ycmd_root_directory,\n",
      "                          ycmd_settings_path=None,\n",
      "                          working_directory=None,\n",
      "                          python_binary_path=None,\n",
      "                          server_idle_suicide_seconds=None,\n",
      "                          server_check_interval_seconds=None):\n",
      "    '''\n",
      "    Internal convenience function. Receives the raw arguments to starting a\n",
      "    ycmd server and returns a `StartupParameters` instance from it.\n",
      "\n",
      "    If the first argument is already `StartupParameters`, it is returned as-is,\n",
      "    and the remaining parameters are ignored.\n",
      "\n",
      "    Otherwise, a `StartupParameters` instance is constructed with all the given\n",
      "    parameters and returned.\n",
      "    '''\n",
      "    if isinstance(ycmd_root_directory, StartupParameters):\n",
      "        # great, already in the desired state\n",
      "        # check if other params are provided and issue a warning\n",
      "        # (they get ignored in that case)\n",
      "        if ycmd_settings_path is not None:\n",
      "            logger.warning(\n",
      "                'ycmd settings path will be ignored: %s', ycmd_settings_path,\n",
      "            )\n",
      "        if working_directory is not None:\n",
      "            logger.warning(\n",
      "                'working directory will be ignored: %s', working_directory,\n",
      "            )\n",
      "        if python_binary_path is not None:\n",
      "            logger.warning(\n",
      "                'python binary path will be ignored: %s', python_binary_path,\n",
      "            )\n",
      "        if server_idle_suicide_seconds is not None:\n",
      "            logger.warning(\n",
      "                'server idle suicide seconds will be ignored: %s',\n",
      "                server_idle_suicide_seconds,\n",
      "            )\n",
      "        if server_check_interval_seconds is not None:\n",
      "            logger.warning(\n",
      "                'server check interval seconds will be ignored: %s',\n",
      "                server_check_interval_seconds,\n",
      "            )\n",
      "\n",
      "        return ycmd_root_directory\n",
      "\n",
      "    # else, generate them\n",
      "    logger.warning('[DEPRECATED] to startup parameters', stack_info=True)\n",
      "    logger.debug(\n",
      "        'generating startup parameters with root: %s', ycmd_root_directory,\n",
      "    )\n",
      "\n",
      "    return StartupParameters(\n",
      "        ycmd_root_directory,\n",
      "        ycmd_settings_path=ycmd_settings_path,\n",
      "        working_directory=working_directory,\n",
      "        python_binary_path=python_binary_path,\n",
      "        server_idle_suicide_seconds=server_idle_suicide_seconds,\n",
      "        server_check_interval_seconds=server_check_interval_seconds,\n",
      "    )\n",
      "\n",
      "\n",
      "def check_startup_parameters(startup_parameters):\n",
      "    '''\n",
      "    Performs quick, non-blocking validation on startup parameters to catch type\n",
      "    mismatches or empty configurations. Raises an exception or returns `None`.\n",
      "\n",
      "    This is meant to be run on the main thread to catch common startup errors\n",
      "    before initializing the server off-thread. It isn't strictly necessary, but\n",
      "    produces nicer error messages when the plugin is not configured correctly.\n",
      "\n",
      "    NOTE : This does not check the file system for things like missing files,\n",
      "           as that can be a blocking operation.\n",
      "    '''\n",
      "    if not isinstance(startup_parameters, StartupParameters):\n",
      "        raise TypeError(\n",
      "            'startup parameters must be StartupParameters: %r' %\n",
      "            (startup_parameters)\n",
      "        )\n",
      "\n",
      "    ycmd_root_directory = startup_parameters.ycmd_root_directory\n",
      "    if not ycmd_root_directory:\n",
      "        raise RuntimeError('no ycmd root directory has been set')\n",
      "\n",
      "    ycmd_settings_path = startup_parameters.ycmd_settings_path\n",
      "    if not ycmd_settings_path:\n",
      "        raise RuntimeError('no ycmd default settings path has been set')\n",
      "\n",
      "    logger.debug(\n",
      "        'startup parameters seem to be filled in, '\n",
      "        'ready to attempt startup: %r', startup_parameters,\n",
      "    )\n",
      "\n",
      "\n",
      "def write_ycmd_settings_file(ycmd_settings_path, ycmd_hmac_secret, out=None):\n",
      "    '''\n",
      "    Writes out a ycmd server settings file based on the template file\n",
      "    `ycmd_settings_path`. A uniquely-generated `ycmd_hmac_secret` must also be\n",
      "    supplied, as it needs to be written into this file.\n",
      "    The return value is the path to the settings file, as a `str`.\n",
      "    If `out` is omitted, a secure temporary file is created, and the returned\n",
      "    path should be passed via the options flag to ycmd.\n",
      "    If `out` is provided, it should be a path to an output file (`str`), or a\n",
      "    file-like handle (must support `.write`). This is not recommended for use\n",
      "    with ycmd, as it may be insecure.\n",
      "    '''\n",
      "    ycmd_settings_data = generate_settings_data(\n",
      "        ycmd_settings_path, ycmd_hmac_secret,\n",
      "    )\n",
      "\n",
      "    out_path = None\n",
      "\n",
      "    if out is None:\n",
      "        # no point using `with` for this, since we also use `delete=False`\n",
      "        temp_file_object = tempfile.NamedTemporaryFile(\n",
      "            prefix='ycmd_settings_', suffix='.json', delete=False,\n",
      "        )\n",
      "        temp_file_name = temp_file_object.name\n",
      "        temp_file_handle = temp_file_object.file    # type: io.TextIOWrapper\n",
      "\n",
      "        out = temp_file_handle\n",
      "        out_path = temp_file_name\n",
      "\n",
      "        def flush():\n",
      "            temp_file_handle.flush()\n",
      "\n",
      "        def close():\n",
      "            temp_file_object.close()\n",
      "    else:\n",
      "        raise NotImplementedError('unimplemented: output to specific file')\n",
      "\n",
      "    if out_path is None and out is not None:\n",
      "        logger.error('failed to get path for output file: %r', out)\n",
      "        # fall through and write it out anyway\n",
      "\n",
      "    save_json_file(out, ycmd_settings_data)\n",
      "\n",
      "    flush()\n",
      "    close()\n",
      "\n",
      "    logger.debug('successfully wrote file: %s', out_path)\n",
      "    return out_path\n",
      "\n",
      "\n",
      "def prepare_ycmd_process(startup_parameters, ycmd_settings_tempfile_path,\n",
      "                         ycmd_server_hostname, ycmd_server_port):\n",
      "    '''\n",
      "    Initializes and returns a `Process` handle, correctly configured to launch\n",
      "    a ycmd server process. It does not automatically start it though.\n",
      "    The `ycmd_settings_tempfile_path` should be created by (return value of)\n",
      "    `write_ycmd_settings_file`. The ycmd server process will read that file on\n",
      "    startup and then immediately delete it.\n",
      "    The `ycmd_server_hostname` and `ycmd_server_port` must also be provided to\n",
      "    instruct the server to listen on the given address.\n",
      "    '''\n",
      "    assert isinstance(startup_parameters, StartupParameters), \\\n",
      "        'startup parameters must be StartupParameters: %r' % \\\n",
      "        (startup_parameters)\n",
      "    assert isinstance(ycmd_settings_tempfile_path, str), \\\n",
      "        'ycmd settings temporary file path must be a str: %r' % \\\n",
      "        (ycmd_settings_tempfile_path)\n",
      "\n",
      "    # this may throw:\n",
      "    check_startup_parameters(startup_parameters)\n",
      "\n",
      "    working_directory = startup_parameters.working_directory\n",
      "    python_binary_path = startup_parameters.python_binary_path\n",
      "    server_idle_suicide_seconds = \\\n",
      "        startup_parameters.server_idle_suicide_seconds\n",
      "    server_check_interval_seconds = \\\n",
      "        startup_parameters.server_check_interval_seconds\n",
      "    ycmd_module_directory = startup_parameters.ycmd_module_directory\n",
      "\n",
      "    if YCMD_LOG_SPOOL_OUTPUT:\n",
      "        stdout_log_spool = \\\n",
      "            tempfile.SpooledTemporaryFile(max_size=YCMD_LOG_SPOOL_SIZE)\n",
      "        stderr_log_spool = \\\n",
      "            tempfile.SpooledTemporaryFile(max_size=YCMD_LOG_SPOOL_SIZE)\n",
      "\n",
      "        logger.debug(\n",
      "            'using temporary spools for stdout, stderr: %r, %r',\n",
      "            stdout_log_spool, stderr_log_spool,\n",
      "        )\n",
      "\n",
      "        stdout_handle = stdout_log_spool\n",
      "        stderr_handle = stderr_log_spool\n",
      "    else:\n",
      "        # explicitly close handles - don't inherit from this process\n",
      "        stdout_handle = FileHandles.DEVNULL\n",
      "        stderr_handle = FileHandles.DEVNULL\n",
      "\n",
      "    ycmd_process_handle = Process()\n",
      "\n",
      "    ycmd_process_handle.binary = python_binary_path\n",
      "    ycmd_process_handle.args.extend([\n",
      "        ycmd_module_directory,\n",
      "        '--host=%s' % (ycmd_server_hostname),\n",
      "        '--port=%s' % (ycmd_server_port),\n",
      "        '--idle_suicide_seconds=%s' % (server_idle_suicide_seconds),\n",
      "        '--check_interval_seconds=%s' % (server_check_interval_seconds),\n",
      "        '--options_file=%s' % (ycmd_settings_tempfile_path),\n",
      "    ])\n",
      "\n",
      "    ycmd_process_handle.cwd = working_directory\n",
      "    ycmd_process_handle.filehandles.stdout = stdout_handle\n",
      "    ycmd_process_handle.filehandles.stderr = stderr_handle\n",
      "\n",
      "    if startup_parameters.log_level is not None:\n",
      "        add_ycmd_debug_args(\n",
      "            ycmd_process_handle,\n",
      "            log_level=startup_parameters.log_level,\n",
      "            stdout_file_name=startup_parameters.stdout_log_path,\n",
      "            stderr_file_name=startup_parameters.stderr_log_path,\n",
      "            keep_logfiles=startup_parameters.keep_logs,\n",
      "        )\n",
      "\n",
      "    return ycmd_process_handle\n",
      "\n",
      "\n",
      "def add_ycmd_debug_args(ycmd_process_handle, log_level='info',\n",
      "                        stdout_file_name=None, stderr_file_name=None,\n",
      "                        keep_logfiles=False):\n",
      "    '''\n",
      "    Adds startup flags to `ycmd_process_handle` to enable logging output.\n",
      "\n",
      "    The `ycmd_process_handle` should be an instance of `Process`.\n",
      "\n",
      "    The `log_level` should be one of 'debug', 'info', 'warning', 'error', or\n",
      "    'critical'. Any `str` is accepted, this routine does not actually check it.\n",
      "\n",
      "    If `stdout_file_name` and `stderr_file_name` are provided, the server will\n",
      "    write log messages to the given files. The bulk of the logs will be on\n",
      "    stderr, with only a few startup messages appearing on stdout.\n",
      "\n",
      "    If `keep_logfiles` is `True`, then the server won't delete the log files\n",
      "    when it exits. Otherwise, the log files will be deleted when it shuts down.\n",
      "    '''\n",
      "    if not isinstance(ycmd_process_handle, Process):\n",
      "        raise TypeError(\n",
      "            'ycmd process handle must be a Process: %r' % (ycmd_process_handle)\n",
      "        )\n",
      "    assert isinstance(ycmd_process_handle, Process)\n",
      "    if ycmd_process_handle.alive():\n",
      "        raise ValueError(\n",
      "            'ycmd process is already started, cannot modify it: %r' %\n",
      "            (ycmd_process_handle)\n",
      "        )\n",
      "\n",
      "    if not _is_valid_log_level(log_level):\n",
      "        logger.warning('log level unrecognized: %r', log_level)\n",
      "        # but fall through and do it anyway\n",
      "\n",
      "    ycmd_debug_args = [\n",
      "        '--log=%s' % (log_level),\n",
      "    ]\n",
      "    if stdout_file_name and stderr_file_name:\n",
      "        ycmd_debug_args.extend([\n",
      "            '--stdout=%s' % (stdout_file_name),\n",
      "            '--stderr=%s' % (stderr_file_name),\n",
      "        ])\n",
      "\n",
      "        if keep_logfiles:\n",
      "            ycmd_debug_args.append(\n",
      "                '--keep_logfiles',\n",
      "            )\n",
      "\n",
      "    logger.debug('adding ycmd debug args: %r', ycmd_debug_args)\n",
      "    ycmd_process_handle.args.extend(ycmd_debug_args)\n",
      "\n",
      "\n",
      "def _is_valid_log_level(log_level):\n",
      "    if not isinstance(log_level, str):\n",
      "        raise TypeError('log level must be a str: %r' % (log_level))\n",
      "\n",
      "    # these can be found by running `python /path/to/ycmd/ycmd --help`\n",
      "    recognized_log_levels = [\n",
      "        'debug',\n",
      "        'info',\n",
      "        'warning',\n",
      "        'error',\n",
      "        'critical',\n",
      "    ]\n",
      "    return log_level in recognized_log_levels\n",
      "\n",
      "#!/usr/bin/env python\n",
      "import serial\n",
      "import sys\n",
      "import struct\n",
      "import pprint\n",
      "import argparse\n",
      "import code\n",
      "\n",
      "pp = pprint.PrettyPrinter()\n",
      "\n",
      "\n",
      "class ConsoleUI:\n",
      "    def opStart(self, name):\n",
      "        sys.stdout.write(name.ljust(40))\n",
      "\n",
      "    def opProgress(self, progress, total=-1):\n",
      "        if (total >= 0):\n",
      "            prstr = \"0x%04x / 0x%04x\" % (progress, total)\n",
      "        else:\n",
      "            prstr = \"0x%04x\" % (progress)\n",
      "\n",
      "        sys.stdout.write(prstr.ljust(20))\n",
      "        sys.stdout.write('\\x08' * 20)\n",
      "        sys.stdout.flush()\n",
      "\n",
      "    def opEnd(self, result):\n",
      "        sys.stdout.write(result.ljust(20))\n",
      "        sys.stdout.write(\"\\n\")\n",
      "\n",
      "\n",
      "class XFlash:\n",
      "    def __init__(self, serialport):\n",
      "        self.serial = serial.Serial(serialport, baudrate=115200)\n",
      "\n",
      "    def __del__(self):\n",
      "        try:\n",
      "            self.serial.close()\n",
      "            del self.serial\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "    def cmd(self, cmd, argA=0, argB=0):\n",
      "        buffer = struct.pack(\"<LL\", argA, argB)\n",
      "\n",
      "        self.serial.write(bytes([cmd]))\n",
      "        self.serial.write(buffer)\n",
      "        self.serial.flush()\n",
      "\n",
      "    def flashPowerOn(self):\n",
      "        self.cmd(0x10)\n",
      "\n",
      "    def flashShutdown(self):\n",
      "        self.cmd(0x11)\n",
      "\n",
      "    def update(self):\n",
      "        try:\n",
      "            self.cmd(0xF0)\n",
      "        except:\n",
      "            pass\n",
      "\n",
      "    def flashInit(self):\n",
      "        self.cmd(0x03)\n",
      "\n",
      "        buffer = self.serial.read(4)\n",
      "        return struct.unpack(\"<L\", buffer)[0]\n",
      "\n",
      "    def flashDeInit(self):\n",
      "        self.cmd(0x04)\n",
      "\n",
      "    def flashStatus(self):\n",
      "        self.cmd(0x05)\n",
      "\n",
      "        buffer = self.serial.read(2)\n",
      "        return struct.unpack(\"<H\", buffer)[0]\n",
      "\n",
      "    def flashErase(self, block):\n",
      "        self.cmd(0x06, block)\n",
      "        # return self.flashStatus()\n",
      "\n",
      "    def flashReadBlock(self, block):\n",
      "        self.cmd(0x01, block, 528 * 32)\n",
      "\n",
      "        # for i in range(0, 32):\n",
      "        buffer = self.serial.read(528 * 32)\n",
      "\n",
      "        status = self.flashStatus()\n",
      "        return (status, buffer)\n",
      "\n",
      "    def flashWriteBlock(self, block, buffer):\n",
      "        self.cmd(0x02, block, len(buffer))\n",
      "\n",
      "        self.serial.write(buffer)\n",
      "\n",
      "        return self.flashStatus()\n",
      "\n",
      "    # def calcecc(data):\n",
      "\n",
      "\n",
      "#   assert len(data) == 0x210\n",
      "#   val = 0\n",
      "#   for i in range(0x1066):\n",
      "#     if not i & 31:\n",
      "#       v = ~struct.unpack(\"<L\", data[i/8:i/8+4])[0]\n",
      "#     val ^= v & 1\n",
      "#     v >>= 1\n",
      "#     if val & 1:\n",
      "#       val ^= 0x6954559\n",
      "#     val >>= 1\n",
      "#\n",
      "#   val = ~val\n",
      "#   return data[:-4] + struct.pack(\"<L\", (val << 6) & 0xFFFFFFFF)\n",
      "#\n",
      "# def addecc(data, block = 0, off_8 = \"\\x00\" * 4):\n",
      "#   res = \"\"\n",
      "#   while len(data):\n",
      "#     d = (data[:0x200] + \"\\x00\" * 0x200)[:0x200]\n",
      "#     data = data[0x200:]\n",
      "#\n",
      "#     d += struct.pack(\"<L4B4s4s\", block / 32, 0, 0xFF, 0, 0, off_8, \"\\0\\0\\0\\0\")\n",
      "#     d = calcecc(d)\n",
      "#     block += 1\n",
      "#     res += d\n",
      "#   return res\n",
      "\n",
      "\n",
      "def main(argv):\n",
      "    parser = argparse.ArgumentParser(description='XBox 360 NAND Flasher')\n",
      "    parser.add_argument('port', metavar='port', type=str,\n",
      "                        help='serial port for comms (e.g. COM5 or /dev/ttyUSB0)')\n",
      "\n",
      "    subparsers = parser.add_subparsers(title='Operations', dest='action')\n",
      "\n",
      "    parser_read = subparsers.add_parser('read', help='Dumps an image from the NAND')\n",
      "    parser_read.add_argument('file', nargs=1, type=argparse.FileType('wb'), help='The file to dump the NAND to')\n",
      "    parser_read.add_argument('start', nargs='?', metavar='start', action='store', type=int, default=0,\n",
      "                             help='The block to start the action from')\n",
      "    parser_read.add_argument('end', nargs='?', metavar='end', action='store', type=int, default=0x400,\n",
      "                             help='The count of blocks to perform the action to')\n",
      "\n",
      "    parser_write = subparsers.add_parser('write', help='Writes an image into the NAND')\n",
      "    parser_write.add_argument('file', nargs=1, type=argparse.FileType('rb'), help='The image file to write to the NAND')\n",
      "    parser_write.add_argument('start', nargs='?', metavar='start', action='store', type=int, default=0,\n",
      "                              help='The block to start the action from')\n",
      "    parser_write.add_argument('end', nargs='?', metavar='end', action='store', type=int, default=0x400,\n",
      "                              help='The count of blocks to perform the action to')\n",
      "\n",
      "    # parser_erase = subparsers.add_parser('erase', help='Erases blocks in the NAND')\n",
      "    # parser_erase.add_argument('start', nargs='?', metavar='start', action='store', type=int, default=0,\n",
      "    #                           help='The block to start the action from')\n",
      "    # parser_erase.add_argument('end', nargs='?', metavar='end', action='store', type=int, default=0x400,\n",
      "    #                           help='The count of blocks to perform the action to')\n",
      "    #\n",
      "    # parser_update = subparsers.add_parser('update',\n",
      "    #                                       help='Jumps into the bootloader of the NAND Flashing device for updating the firmware')\n",
      "    # parser_shutdown = subparsers.add_parser('shutdown', help='Shuts down the attached XBox 360')\n",
      "    # parser_poweron = subparsers.add_parser('powerup', help='Powers up the attached XBox 360')\n",
      "\n",
      "    arguments = parser.parse_args(argv[1:])\n",
      "\n",
      "    ui = ConsoleUI()\n",
      "\n",
      "    xf = XFlash(arguments.port)\n",
      "\n",
      "    if arguments.action in ('erase', 'write', 'read'):\n",
      "        try:\n",
      "            flash_config = xf.flashInit()\n",
      "            print(\"FlashConfig: 0x%08x\" % (flash_config))\n",
      "            if flash_config <= 0:\n",
      "                raise Exception(\"FlashConfig invalid!\")\n",
      "        except Exception as e:\n",
      "            print(\"Error!\", e)\n",
      "            xf.flashDeInit()\n",
      "            return 1\n",
      "\n",
      "    try:\n",
      "        if arguments.action == 'erase':\n",
      "            # start = 0\n",
      "            # end = (options.flashsize * 1024) / 16\n",
      "            start = arguments.start\n",
      "            end = arguments.end\n",
      "\n",
      "            ui.opStart('Erase')\n",
      "\n",
      "            ui.opProgress(0, end)\n",
      "            for b in range(start, end):\n",
      "                status = xf.flashErase(b)\n",
      "                ui.opProgress(b + 1, end)\n",
      "\n",
      "            ui.opEnd('0x%04x blocks OK' % (end))\n",
      "\n",
      "        if arguments.action == 'read':\n",
      "            # start = 0\n",
      "            # end = (options.flashsize * 1024) / 16\n",
      "            start = arguments.start\n",
      "            end = arguments.end\n",
      "\n",
      "            ui.opStart('Read')\n",
      "\n",
      "            ui.opProgress(0, end)\n",
      "            for b in range(start, end):\n",
      "                (status, buffer) = xf.flashReadBlock(b)\n",
      "                ui.opProgress(b + 1, end)\n",
      "                arguments.file[0].write(buffer)\n",
      "\n",
      "        if arguments.action == 'write':\n",
      "            # start = 0\n",
      "            # end = (options.flashsize * 1024) / 16\n",
      "\n",
      "            start = arguments.start\n",
      "            end = arguments.end\n",
      "            blocksize = 528 * 32\n",
      "\n",
      "            ui.opStart('Write')\n",
      "\n",
      "            ui.opProgress(0, end)\n",
      "            for b in range(start, end):\n",
      "                buffer = arguments.file[0].read(blocksize)\n",
      "\n",
      "                if len(buffer) < blocksize:\n",
      "                    buffer += ('\\xFF' * (blocksize - len(buffer)))\n",
      "\n",
      "                status = xf.flashWriteBlock(b, buffer)\n",
      "                ui.opProgress(b + 1, end)\n",
      "        #\n",
      "        # if arguments.action == 'update':\n",
      "        #     xf.update()\n",
      "        #\n",
      "        # if arguments.action == 'powerup':\n",
      "        #     xf.flashPowerOn()\n",
      "        #\n",
      "        # if arguments.action == 'shutdown':\n",
      "        #     xf.flashShutdown()\n",
      "    except Exception as e:\n",
      "        raise e\n",
      "    finally:\n",
      "        xf.flashDeInit()\n",
      "        return 0\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    sys.exit(main(sys.argv))\n",
      "\n",
      "\n",
      "# Copyright (C) 2010-2011 Richard Lincoln\n",
      "#\n",
      "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
      "# of this software and associated documentation files (the \"Software\"), to\n",
      "# deal in the Software without restriction, including without limitation the\n",
      "# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or\n",
      "# sell copies of the Software, and to permit persons to whom the Software is\n",
      "# furnished to do so, subject to the following conditions:\n",
      "#\n",
      "# The above copyright notice and this permission notice shall be included in\n",
      "# all copies or substantial portions of the Software.\n",
      "#\n",
      "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
      "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
      "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
      "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
      "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
      "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\n",
      "# IN THE SOFTWARE.\n",
      "\n",
      "from CIM16.IEC61968.Common.ActivityRecord import ActivityRecord\n",
      "\n",
      "class ComplianceEvent(ActivityRecord):\n",
      "    \"\"\"Compliance events are used for reporting regulatory or contract compliance issues and/or variances. These might be created as a consequence of local business processes and associated rules. It is anticipated that this class will be customised extensively to meet local implementation needs. Use inherited 'category' to indicate that, for example, expected performance will not be met or reported as mandated.Compliance events are used for reporting regulatory or contract compliance issues and/or variances. These might be created as a consequence of local business processes and associated rules. It is anticipated that this class will be customised extensively to meet local implementation needs. Use inherited 'category' to indicate that, for example, expected performance will not be met or reported as mandated.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, deadline='', *args, **kw_args):\n",
      "        \"\"\"Initialises a new 'ComplianceEvent' instance.\n",
      "\n",
      "        @param deadline: The deadline for compliance. \n",
      "        \"\"\"\n",
      "        #: The deadline for compliance.\n",
      "        self.deadline = deadline\n",
      "\n",
      "        super(ComplianceEvent, self).__init__(*args, **kw_args)\n",
      "\n",
      "    _attrs = [\"deadline\"]\n",
      "    _attr_types = {\"deadline\": str}\n",
      "    _defaults = {\"deadline\": ''}\n",
      "    _enums = {}\n",
      "    _refs = []\n",
      "    _many_refs = []\n",
      "\n",
      "\n",
      "import logging\n",
      "\n",
      "from django.db.models.query_utils import Q\n",
      "from django.shortcuts import get_object_or_404\n",
      "from django.utils.decorators import method_decorator\n",
      "from django_filters.rest_framework import DjangoFilterBackend\n",
      "from drf_yasg import openapi\n",
      "from drf_yasg.openapi import Parameter\n",
      "from drf_yasg.utils import no_body, swagger_auto_schema\n",
      "from notifications.signals import notify\n",
      "from rest_framework import mixins, status, viewsets\n",
      "from rest_framework.decorators import action\n",
      "from rest_framework.decorators import parser_classes as dparser_classes\n",
      "from rest_framework.parsers import FormParser, JSONParser, MultiPartParser\n",
      "from rest_framework.permissions import IsAuthenticated\n",
      "from rest_framework.response import Response\n",
      "from rest_framework_extensions.mixins import DetailSerializerMixin, NestedViewSetMixin\n",
      "\n",
      "from looking_for_group.mixins import AutoPermissionViewSetMixin, ParentObjectAutoPermissionViewSetMixin\n",
      "\n",
      "from . import models, serializers\n",
      "from .signals import player_kicked, player_left\n",
      "\n",
      "logger = logging.getLogger(\"api\")\n",
      "\n",
      "parent_lookup_game__slug = Parameter(\n",
      "    name=\"parent_lookup_game__slug\",\n",
      "    in_=\"path\",\n",
      "    type=\"string\",\n",
      "    format=openapi.FORMAT_SLUG,\n",
      "    description=\"Slug of related game object.\",\n",
      ")\n",
      "parent_lookup_session__slug = Parameter(\n",
      "    name=\"parent_lookup_session__slug\",\n",
      "    in_=\"path\",\n",
      "    type=\"string\",\n",
      "    format=openapi.FORMAT_SLUG,\n",
      "    description=\"Slug of related session object.\",\n",
      ")\n",
      "parent_lookup_session__game__slug = Parameter(\n",
      "    name=\"parent_lookup_session__game__slug\",\n",
      "    in_=\"path\",\n",
      "    type=\"string\",\n",
      "    format=openapi.FORMAT_SLUG,\n",
      "    description=\"Slug of related game object.\",\n",
      ")\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"list\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"List Games\",\n",
      "        operation_description=\"Fetch a list of game records. **NOTE**: You will probably want to filter by status at least.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"create\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Create\",\n",
      "        operation_description=\"Create a new game posting.\",\n",
      "        request_body=serializers.GameDataSerializer,\n",
      "        responses={201: serializers.GameDataSerializer},\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Details\",\n",
      "        operation_description=\"Fetch the details for the given game. **NOTE**: If you are not a member of the game, only a subset of the available information will be displayed.\",\n",
      "        responses={\n",
      "            200: serializers.GameDataSerializer,\n",
      "            403: \"You are not authorized to view this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Update\",\n",
      "        operation_description=\"Update the details of this game. (Only available to GM)\",\n",
      "        request_body=serializers.GameDataSerializer,\n",
      "        responses={\n",
      "            200: serializers.GameDataSerializer,\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"partial_update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Update\",\n",
      "        operation_description=\"Update the details of this game. (Only available to GM)\",\n",
      "        request_body=serializers.GameDataSerializer,\n",
      "        responses={\n",
      "            200: serializers.GameDataSerializer,\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"destroy\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Delete\",\n",
      "        operation_description=\"Delete the given game. (Only available to GM.)\",\n",
      "        request_body=no_body,\n",
      "        responses={204: \"Game was deleted.\", 403: \"You are not the GM of this game.\"},\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"leave\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Leave\",\n",
      "        operation_description=\"Leave the current game. (Players only.)\",\n",
      "        request_body=no_body,\n",
      "        reponses={\n",
      "            204: \"You have successfully left the game.\",\n",
      "            400: \"You are not a member of this game.\",\n",
      "            403: \"You are the GM and cannot leave.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"apply\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Apply\",\n",
      "        operation_description=\"Apply to join this game.\",\n",
      "        request_body=serializers.GameApplicationSerializer,\n",
      "        responses={\n",
      "            201: serializers.GameApplicationSerializer,\n",
      "            400: \"You are already a member of this game.\",\n",
      "            403: \"You are not permitted to apply to this game either due to your access rights or the game's status.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "class GamePostingViewSet(\n",
      "    AutoPermissionViewSetMixin,\n",
      "    DetailSerializerMixin,\n",
      "    NestedViewSetMixin,\n",
      "    viewsets.ModelViewSet,\n",
      "):\n",
      "    \"\"\"\n",
      "    A view set that allows the retrieval and manipulation of posted game data.\n",
      "    \"\"\"\n",
      "\n",
      "    permission_classes = (IsAuthenticated,)\n",
      "    parser_classes = [FormParser, MultiPartParser]\n",
      "    model = models.GamePosting\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    serializer_class = serializers.GameDataListSerializer\n",
      "    serializer_detail_class = serializers.GameDataSerializer\n",
      "    filter_backends = [DjangoFilterBackend]\n",
      "    filterset_fields = [\n",
      "        \"published_game\",\n",
      "        \"game_system\",\n",
      "        \"published_module\",\n",
      "        \"status\",\n",
      "        \"game_type\",\n",
      "        \"game_mode\",\n",
      "    ]\n",
      "    permission_type_map = {\n",
      "        **AutoPermissionViewSetMixin.permission_type_map,\n",
      "        \"apply\": \"apply\",\n",
      "        \"leave\": \"leave\",\n",
      "    }\n",
      "\n",
      "    def get_queryset(self):\n",
      "        gamer = self.request.user.gamerprofile\n",
      "        friends = gamer.friends.all()\n",
      "        communities = [f.id for f in gamer.communities.all()]\n",
      "        game_player_ids = [\n",
      "            obj.game.id\n",
      "            for obj in models.Player.objects.filter(gamer=gamer).select_related(\"game\")\n",
      "        ]\n",
      "        q_gm = Q(gm=gamer)\n",
      "        q_gm_is_friend = Q(gm__in=friends) & Q(privacy_level=\"community\")\n",
      "        q_isplayer = Q(id__in=game_player_ids)\n",
      "        q_community = Q(communities__id__in=communities) & Q(privacy_level=\"community\")\n",
      "        q_public = Q(privacy_level=\"public\")\n",
      "        qs = models.GamePosting.objects.filter(\n",
      "            q_gm | q_public | q_gm_is_friend | q_isplayer | q_community\n",
      "        ).distinct()\n",
      "        return qs\n",
      "\n",
      "    def create(self, request, *args, **kwargs):\n",
      "        self.serializer_class = serializers.GameDataSerializer\n",
      "        return super().create(request, *args, **kwargs)\n",
      "\n",
      "    def retrieve(self, request, *args, **kwargs):\n",
      "        if not request.user.has_perm(\"game.is_member\", self.get_object()):\n",
      "            logger.debug(\n",
      "                \"User is not a member of game, swtiching serializer to list view mode.\"\n",
      "            )\n",
      "            self.serializer_detail_class = serializers.GameDataListSerializer\n",
      "        return super().retrieve(request, *args, **kwargs)\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def apply(self, request, *args, **kwargs):\n",
      "        obj = self.get_object()\n",
      "        logger.debug(\"Retrieved game object of {}\".format(obj))\n",
      "        if request.user.has_perm(\"game.is_member\", obj):\n",
      "            return Response(\n",
      "                data={\"errors\": \"You are already in this game...\"},\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "        new_application = serializers.GameApplicationSerializer(\n",
      "            data=request.data, context={\"request\": request}\n",
      "        )\n",
      "        if not new_application.is_valid():\n",
      "            return Response(\n",
      "                data=new_application.errors, status=status.HTTP_400_BAD_REQUEST\n",
      "            )\n",
      "        app = models.GamePostingApplication.objects.create(\n",
      "            game=obj,\n",
      "            gamer=request.user.gamerprofile,\n",
      "            message=new_application.validated_data[\"message\"],\n",
      "            status=\"pending\",\n",
      "        )\n",
      "        notify.send(\n",
      "            request.user.gamerprofile,\n",
      "            recipient=obj.gm.user,\n",
      "            verb=\"submitted application\",\n",
      "            action_object=app,\n",
      "            target=obj,\n",
      "        )\n",
      "        return Response(\n",
      "            data=serializers.GameApplicationSerializer(\n",
      "                app, context={\"request\": request}\n",
      "            ).data,\n",
      "            status=status.HTTP_201_CREATED,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def leave(self, request, *args, **kwargs):\n",
      "        obj = self.get_object()\n",
      "        if request.user == obj.gm.user:\n",
      "            return Response(\n",
      "                data={\"errors\": \"The GM cannot leave the game.\"},\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "        player = models.Player.objects.get(gamer=request.user.gamerprofile, game=obj)\n",
      "        player_left.send(models.Player, player=player)\n",
      "        player.delete()\n",
      "        return Response(status=status.HTTP_204_NO_CONTENT)\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"list\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: List Sessions\",\n",
      "        operation_description=\"List the sessions for the given game.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Details\",\n",
      "        operation_description=\"Get the details for the given session. **NOTE**: If the user is just a player, the GM notes and player details will not be included.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            403: \"You are not a member of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Update\",\n",
      "        operation_description=\"Update details of the game session.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=serializers.GameSessionGMSerializer,\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"partial_update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Update\",\n",
      "        operation_description=\"Update details of the game session.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=serializers.GameSessionGMSerializer,\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"destroy\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Delete\",\n",
      "        operation_description=\"Delete the game session.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=serializers.GameSessionGMSerializer,\n",
      "        responses={\n",
      "            204: \"Session was deleted.\",\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"cancel\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Cancel\",\n",
      "        operation_description=\"Cancel the game session.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            400: \"This session is already canceled or complete.\",\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"uncancel\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Uncancel\",\n",
      "        operation_description=\"Uncancel the game session.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            400: \"This session is not canceled.\",\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"complete\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Mark Complete\",\n",
      "        operation_description=\"Mark the game session as complete.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            400: \"This session is already canceled or complete.\",\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"uncomplete\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Uncomplete\",\n",
      "        operation_description=\"Undo the completion status of the session.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            400: \"This session isn't marked as complete.\",\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"reschedule\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Reschedule\",\n",
      "        operation_description=\"Reschedule the game session to another date/time.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=serializers.ScheduleSerializer,\n",
      "        responses={\n",
      "            200: serializers.GameSessionGMSerializer,\n",
      "            400: \"Your date and time were invalid or the session is already marked as complete or canceled.\",\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"addlog\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Session: Add Adventure Log\",\n",
      "        operation_description=\"Add an adventure log to this session.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=serializers.AdventureLogSerializer,\n",
      "        responses={\n",
      "            201: serializers.AdventureLogSerializer,\n",
      "            400: \"This session already has an adventure log. You should update that instead.\",\n",
      "            403: \"You don't have permission to add an adventure log.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "class GameSessionViewSet(\n",
      "    ParentObjectAutoPermissionViewSetMixin,\n",
      "    NestedViewSetMixin,\n",
      "    mixins.ListModelMixin,\n",
      "    mixins.RetrieveModelMixin,\n",
      "    mixins.UpdateModelMixin,\n",
      "    mixins.DestroyModelMixin,\n",
      "    viewsets.GenericViewSet,\n",
      "):\n",
      "    \"\"\"\n",
      "    Views for seeing game session data.\n",
      "    \"\"\"\n",
      "\n",
      "    model = models.GameSession\n",
      "    serializer_class = serializers.GameSessionSerializer\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    parent_dependent_actions = [\n",
      "        \"create\",\n",
      "        \"retrieve\",\n",
      "        \"update\",\n",
      "        \"partial_update\",\n",
      "        \"list\",\n",
      "        \"destroy\",\n",
      "        \"reschedule\",\n",
      "        \"cancel\",\n",
      "        \"uncancel\",\n",
      "        \"addlog\",\n",
      "        \"complete\",\n",
      "        \"uncomplete\",\n",
      "    ]\n",
      "    parent_lookup_field = \"game\"\n",
      "    parent_object_model = models.GamePosting\n",
      "    parent_object_lookup_field = \"slug\"\n",
      "    parent_object_url_kwarg = \"parent_lookup_game__slug\"\n",
      "    permission_type_map = {\n",
      "        **ParentObjectAutoPermissionViewSetMixin.permission_type_map,\n",
      "        \"addlog\": \"view\",\n",
      "        \"reschedule\": \"change\",\n",
      "        \"cancel\": \"change\",\n",
      "        \"uncancel\": \"change\",\n",
      "        \"complete\": \"change\",\n",
      "        \"uncomplete\": \"change\",\n",
      "    }\n",
      "    permission_type_map[\"list\"] = \"view\"\n",
      "\n",
      "    def get_parent_game(self):\n",
      "        return get_object_or_404(\n",
      "            models.GamePosting, slug=self.kwargs[\"parent_lookup_game__slug\"]\n",
      "        )\n",
      "\n",
      "    def get_queryset(self):\n",
      "        return self.model.objects.filter(\n",
      "            game__slug=self.kwargs[\"parent_lookup_game__slug\"]\n",
      "        ).order_by(\"-scheduled_time\")\n",
      "\n",
      "    def dispatch(self, request, *args, **kwargs):\n",
      "        if (\n",
      "            request.user.is_authenticated\n",
      "            and request.user.gamerprofile == self.get_parent_game().gm\n",
      "        ):\n",
      "            self.serializer_class = serializers.GameSessionGMSerializer\n",
      "        return super().dispatch(request, *args, **kwargs)\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def reschedule(self, request, *args, **kwargs):\n",
      "        date_serializer = serializers.ScheduleSerializer(data=request.data)\n",
      "        if not date_serializer.is_valid():\n",
      "            return Response(\n",
      "                data=date_serializer.errors, status=status.HTTP_400_BAD_REQUEST\n",
      "            )\n",
      "        obj = self.get_object()\n",
      "        if obj.status in [\"complete\", \"cancel\"]:\n",
      "            return Response(\n",
      "                data={\n",
      "                    \"errors\": \"This session is already marked as {} and cannot be rescheduled.\".format(\n",
      "                        obj.get_status_display()\n",
      "                    )\n",
      "                },\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "        obj.move(date_serializer.validated_data[\"new_scheduled_time\"])\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def complete(self, request, *args, **kwargs):\n",
      "        obj = self.get_object()\n",
      "        if obj.status in [\"complete\", \"cancel\"]:\n",
      "            return Response(\n",
      "                data={\n",
      "                    \"errors\": \"This object is either already completed or canceled and cannot be toggled to complete.\"\n",
      "                },\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "        obj.status = \"complete\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def uncomplete(self, request, *args, **kwargs):\n",
      "        obj = self.get_object()\n",
      "        if obj.status != \"complete\":\n",
      "            return Response(\n",
      "                data={\n",
      "                    \"errors\": \"This object is not completed and so completion cannot be undone.\"\n",
      "                },\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "        obj.status = \"pending\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def cancel(self, request, *args, **kwargs):\n",
      "        obj = self.get_object()\n",
      "        if obj.status in [\"complete\", \"cancel\"]:\n",
      "            return Response(\n",
      "                data={\"errors\": \"This session is already completed or canceled.\"},\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "        obj.cancel()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def uncancel(self, request, *args, **kwargs):\n",
      "        obj = self.get_object()\n",
      "        if obj.status != \"cancel\":\n",
      "            return Response(\n",
      "                data={\n",
      "                    \"errors\": \"This session is not canceled and can't be changed this way.\"\n",
      "                },\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "        obj.uncancel()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def addlog(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Create the adventure log for this session.\n",
      "        \"\"\"\n",
      "        session = self.get_object()\n",
      "        if hasattr(session, \"adventurelog\"):\n",
      "            return Response(\n",
      "                data={\"errors\": \"This session already has an adventure log.\"},\n",
      "                status=status.HTTP_400_BAD_REQUEST,\n",
      "            )\n",
      "\n",
      "        log_serializer = serializers.AdventureLogSerializer(\n",
      "            session=session, data=request.data, context={\"request\": request}\n",
      "        )\n",
      "        if not log_serializer.is_valid():\n",
      "            return Response(\n",
      "                data=log_serializer.errors, status=status.HTTP_400_BAD_REQUEST\n",
      "            )\n",
      "        new_log = log_serializer.save()\n",
      "        return Response(\n",
      "            data=serializers.AdventureLogSerializer(\n",
      "                new_log, context={\"request\": request}\n",
      "            ).data,\n",
      "            status=status.HTTP_201_CREATED,\n",
      "        )\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Adventure Log: Details\",\n",
      "        operation_description=\"Fetch the details for a given adventure log.\",\n",
      "        manual_parameters=[\n",
      "            parent_lookup_session__game__slug,\n",
      "            parent_lookup_session__slug,\n",
      "        ],\n",
      "        responses={\n",
      "            200: serializers.AdventureLogSerializer,\n",
      "            403: \"You are not a member of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Adventure Log: Update\",\n",
      "        operation_description=\"Update the details for a given adventure log.\",\n",
      "        manual_parameters=[\n",
      "            parent_lookup_session__game__slug,\n",
      "            parent_lookup_session__slug,\n",
      "        ],\n",
      "        request_body=serializers.AdventureLogSerializer,\n",
      "        responses={\n",
      "            200: serializers.AdventureLogSerializer,\n",
      "            403: \"You don't have permissions to edit this adventure log.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"partial_update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Adventure Log: Update\",\n",
      "        operation_description=\"Update the details for a given adventure log.\",\n",
      "        manual_parameters=[\n",
      "            parent_lookup_session__game__slug,\n",
      "            parent_lookup_session__slug,\n",
      "        ],\n",
      "        request_body=serializers.AdventureLogSerializer,\n",
      "        responses={\n",
      "            200: serializers.AdventureLogSerializer,\n",
      "            403: \"You don't have permissions to edit this adventure log.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"destroy\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Adventure Log: Delete\",\n",
      "        operation_description=\"Delete a given adventure log.\",\n",
      "        manual_parameters=[\n",
      "            parent_lookup_session__game__slug,\n",
      "            parent_lookup_session__slug,\n",
      "        ],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            204: \"The adventure log was successfully deleted.\",\n",
      "            403: \"You don't have permissions to edit this adventure log.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "class AdventureLogViewSet(\n",
      "    ParentObjectAutoPermissionViewSetMixin,\n",
      "    NestedViewSetMixin,\n",
      "    mixins.RetrieveModelMixin,\n",
      "    mixins.UpdateModelMixin,\n",
      "    mixins.DestroyModelMixin,\n",
      "    viewsets.GenericViewSet,\n",
      "):\n",
      "    \"\"\"\n",
      "    Allows the manipulation of view sets.\n",
      "    \"\"\"\n",
      "\n",
      "    model = models.AdventureLog\n",
      "    parent_lookup_field = \"session__game\"\n",
      "    parent_object_model = models.GamePosting\n",
      "    parent_object_lookup_field = \"slug\"\n",
      "    parent_object_url_kwarg = \"parent_lookup_session__game__slug\"\n",
      "    serializer_class = serializers.AdventureLogSerializer\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    permission_required = \"game.is_member\"\n",
      "    permission_type_map = {**ParentObjectAutoPermissionViewSetMixin.permission_type_map}\n",
      "    permission_type_map[\"list\"] = \"add\"\n",
      "    parent_dependent_actions = [\n",
      "        \"create\",\n",
      "        \"retrieve\",\n",
      "        \"update\",\n",
      "        \"partial_update\",\n",
      "        \"destroy\",\n",
      "    ]\n",
      "\n",
      "    def get_queryset(self):\n",
      "        return models.AdventureLog.objects.filter(\n",
      "            session__slug=self.kwargs[\"parent_lookup_session__slug\"]\n",
      "        )\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"list\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"List Your Game Applications\",\n",
      "        operation_description=\"Fetch a list of all your game applications.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Game Application: Details\",\n",
      "        operation_description=\"Fetch the details of your game application.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Game Application: Update\",\n",
      "        operation_description=\"Update the details of your game application.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"partial_update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Game Application: Update\",\n",
      "        operation_description=\"Update the details of your game application.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"destroy\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Game Application: Withdraw\",\n",
      "        operation_description=\"Withdraw your game application by deleting the record.\",\n",
      "    ),\n",
      ")\n",
      "class GameApplicationViewSet(\n",
      "    AutoPermissionViewSetMixin,\n",
      "    mixins.ListModelMixin,\n",
      "    mixins.RetrieveModelMixin,\n",
      "    mixins.UpdateModelMixin,\n",
      "    mixins.DestroyModelMixin,\n",
      "    viewsets.GenericViewSet,\n",
      "):\n",
      "    \"\"\"\n",
      "    View for an applicant to review, create, update, and delete their applications to games.\n",
      "    \"\"\"\n",
      "\n",
      "    permission_classes = (IsAuthenticated,)\n",
      "    serializer_class = serializers.GameApplicationSerializer\n",
      "    filter_backends = [DjangoFilterBackend]\n",
      "    filterset_fields = [\"status\"]\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    permission_type_map = {**AutoPermissionViewSetMixin.permission_type_map}\n",
      "\n",
      "    def get_queryset(self):\n",
      "        logger.debug(\"Fetching gamerprofile from request...\")\n",
      "        gamer = self.request.user.gamerprofile\n",
      "        logger.debug(\"Fetching game applications for gamer {}\".format(gamer))\n",
      "        qs = models.GamePostingApplication.objects.filter(\n",
      "            gamer=self.request.user.gamerprofile\n",
      "        ).order_by(\"-modified\", \"-created\", \"status\")\n",
      "        logger.debug(\n",
      "            \"Retrieved queryset of length {} for gamer {}\".format(\n",
      "                qs.count(), self.request.user.gamerprofile\n",
      "            )\n",
      "        )\n",
      "        return qs\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"list\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"List Applicants for Game\",\n",
      "        operation_description=\"List the applicants for the current game. (GM Only)\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Applicant: Details\",\n",
      "        operation_description=\"Fetch details for a given game application. (GM Only)\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        reponses={\n",
      "            200: serializers.GameApplicationGMSerializer,\n",
      "            403: \"You are not the GM for this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"approve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Applicant: Approve\",\n",
      "        operation_description=\"Approve the game applicant and add as a player to game.\",\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            201: serializers.PlayerSerializer,\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"reject\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game Applicant: Reject\",\n",
      "        operation_description=\"Reject the game applicant.\",\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.GameApplicationGMSerializer,\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "class GMGameApplicationViewSet(\n",
      "    ParentObjectAutoPermissionViewSetMixin,\n",
      "    NestedViewSetMixin,\n",
      "    mixins.ListModelMixin,\n",
      "    mixins.RetrieveModelMixin,\n",
      "    viewsets.GenericViewSet,\n",
      "):\n",
      "    \"\"\"\n",
      "    View for a GM to review and approve applicants.\n",
      "    \"\"\"\n",
      "\n",
      "    permission_classes = (IsAuthenticated,)\n",
      "    serializer_class = serializers.GameApplicationGMSerializer\n",
      "    filter_backends = [DjangoFilterBackend]\n",
      "    filterset_fields = [\"status\"]\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    parent_lookup_field = \"game\"\n",
      "    parent_object_lookup_field = \"slug\"\n",
      "    parent_object_model = models.GamePosting\n",
      "    parent_object_url_kwarg = \"parent_lookup_game__slug\"\n",
      "    parent_dependent_actions = [\"list\", \"retrieve\", \"approve\", \"reject\"]\n",
      "    permission_type_map = {\n",
      "        **ParentObjectAutoPermissionViewSetMixin.permission_type_map,\n",
      "        \"approve\": \"approve\",\n",
      "        \"reject\": \"approve\",\n",
      "    }\n",
      "    permission_type_map[\"retrieve\"] = \"approve\"\n",
      "    permission_type_map[\"list\"] = \"approve\"\n",
      "\n",
      "    def get_queryset(self):\n",
      "        return models.GamePostingApplication.objects.filter(\n",
      "            game__slug=self.kwargs[\"parent_lookup_game__slug\"]\n",
      "        ).exclude(status=\"new\")\n",
      "\n",
      "    def get_parent_game(self):\n",
      "        return get_object_or_404(\n",
      "            models.GamePosting, slug=self.kwargs[\"parent_lookup_game__slug\"]\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def approve(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Approves the game application.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"approve\"\n",
      "        player = models.Player.objects.create(game=obj.game, gamer=obj.gamer)\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=serializers.PlayerSerializer(\n",
      "                player, context={\"request\", request}\n",
      "            ).data,\n",
      "            status=status.HTTP_201_CREATED,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def reject(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Rejects the game application.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"deny\"\n",
      "        obj.save()\n",
      "        notify.send(\n",
      "            obj,\n",
      "            recipient=obj.gamer.user,\n",
      "            verb=\"Your player application was not accepted\",\n",
      "            action_object=obj,\n",
      "            target=obj.game,\n",
      "        )\n",
      "        return Response(\n",
      "            data=serializers.GameApplicationSerializer(\n",
      "                obj, context={\"request\": request}\n",
      "            ).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"list\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Player List\",\n",
      "        operation_description=\"List players for a given game\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Player: Details\",\n",
      "        operation_description=\"Details for a player record in a given game.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        responses={\n",
      "            200: serializers.PlayerSerializer,\n",
      "            403: \"You are not a member of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"kick\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Player: Kick from game\",\n",
      "        operation_description=\"Kick the player out of the game.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            204: \"Player was removed from the game.\",\n",
      "            403: \"You are not the GM of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "class PlayerViewSet(\n",
      "    ParentObjectAutoPermissionViewSetMixin,\n",
      "    NestedViewSetMixin,\n",
      "    mixins.ListModelMixin,\n",
      "    mixins.RetrieveModelMixin,\n",
      "    viewsets.GenericViewSet,\n",
      "):\n",
      "    \"\"\"\n",
      "    Provides views for players in a given game.\n",
      "    \"\"\"\n",
      "\n",
      "    permission_classes = (IsAuthenticated,)\n",
      "    serializer_class = serializers.PlayerSerializer\n",
      "    permission_required = \"game.is_member\"\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    parent_lookup_field = \"game\"\n",
      "    parent_object_model = models.GamePosting\n",
      "    parent_object_lookup_field = \"slug\"\n",
      "    parent_object_url_kwarg = \"parent_lookup_game__slug\"\n",
      "    parent_dependent_actions = [\"list\", \"retrieve\"]\n",
      "    permission_type_map = {**ParentObjectAutoPermissionViewSetMixin.permission_type_map}\n",
      "    permission_type_map[\"list\"] = \"view\"\n",
      "\n",
      "    def get_parent_game(self):\n",
      "        return get_object_or_404(\n",
      "            models.GamePosting, slug=self.kwargs[\"parent_lookup_game__slug\"]\n",
      "        )\n",
      "\n",
      "    def get_queryset(self):\n",
      "        return models.Player.objects.filter(game=self.get_parent_game())\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True)\n",
      "    def kick(self, request, *args, **kwargs):\n",
      "        obj = self.get_object()\n",
      "        player_kicked.send(request.user, player=obj)\n",
      "        obj.delete()\n",
      "        return Response(status=status.HTTP_204_NO_CONTENT)\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"list\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: List Characters\",\n",
      "        operation_description=\"Fetch the list of characters for a given game.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Character Details\",\n",
      "        operation_description=\"Fetch the details of a character for a given game.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        responses={\n",
      "            200: serializers.CharacterSerializer,\n",
      "            403: \"You are not a member of this game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Update Character Details\",\n",
      "        operation_description=\"Update the character for the given game.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=serializers.CharacterSerializer,\n",
      "        responses={\n",
      "            200: serializers.CharacterSerializer,\n",
      "            403: \"You are not the owner of this character or the GM of the game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"partial_update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Update Character Details\",\n",
      "        operation_description=\"Update the character for the given game.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=serializers.CharacterSerializer,\n",
      "        responses={\n",
      "            200: serializers.CharacterSerializer,\n",
      "            403: \"You are not the owner of this character or the GM of the game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"deactivate\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Deactivate Character\",\n",
      "        operation_description=\"Mark the character as inactive.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.CharacterSerializer,\n",
      "            400: \"This character is already inactive.\",\n",
      "            403: \"You are not the owner of this character or the GM of the game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"reactivate\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Reactivate Character\",\n",
      "        operation_description=\"Mark the character as active.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.CharacterSerializer,\n",
      "            400: \"This character is already active.\",\n",
      "            403: \"You are not the owner of this character or the GM of the game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"destroy\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Delete Character\",\n",
      "        operation_description=\"Delete the character.\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            204: \"Character was deleted.\",\n",
      "            403: \"You are not the owner of this character.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"approve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Approve Character\",\n",
      "        operation_description=\"Mark the character as approved (GM Only).\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.CharacterSerializer,\n",
      "            400: \"This character is already approved.\",\n",
      "            403: \"You are not the GM of the game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"reject\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Game: Reject Character\",\n",
      "        operation_description=\"Mark the character as rejected (GM Only).\",\n",
      "        manual_parameters=[parent_lookup_game__slug],\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: serializers.CharacterSerializer,\n",
      "            400: \"This character is already rejected.\",\n",
      "            403: \"You are not the GM of the game.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "class CharacterViewSet(\n",
      "    ParentObjectAutoPermissionViewSetMixin, NestedViewSetMixin, viewsets.ModelViewSet\n",
      "):\n",
      "    \"\"\"\n",
      "    Provides views for the characters in a game.\n",
      "    \"\"\"\n",
      "\n",
      "    permission_classes = (IsAuthenticated,)\n",
      "    parser_classes = [FormParser, MultiPartParser]\n",
      "    parent_object_lookup_field = \"slug\"\n",
      "    parent_object_url_kwarg = \"parent_lookup_game__slug\"\n",
      "    parent_lookup_field = \"game\"\n",
      "    parent_object_model = models.GamePosting\n",
      "    parent_dependent_actions = [\"create\", \"list\", \"retrieve\"]\n",
      "    serializer_class = serializers.CharacterSerializer\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    filter_backends = [DjangoFilterBackend]\n",
      "    filterset_fields = [\"status\"]\n",
      "    parent_game = None\n",
      "    permission_type_map = {\n",
      "        **ParentObjectAutoPermissionViewSetMixin.permission_type_map,\n",
      "        \"approve\": \"approve\",\n",
      "        \"reject\": \"approve\",\n",
      "        \"deactivate\": \"delete\",\n",
      "        \"reactivate\": \"delete\",\n",
      "    }\n",
      "    permission_type_map[\"list\"] = \"gamelist\"\n",
      "\n",
      "    def get_parent_game(self):\n",
      "        if not self.parent_game:\n",
      "            self.parent_game = get_object_or_404(\n",
      "                models.GamePosting, slug=self.kwargs[\"parent_lookup_game__slug\"]\n",
      "            )\n",
      "        return self.parent_game\n",
      "\n",
      "    def get_queryset(self):\n",
      "        return models.Character.objects.filter(game=self.get_parent_game())\n",
      "\n",
      "    def create(self, request, *args, **kwargs):\n",
      "        if request.user.gamerprofile == self.get_parent_game().gm:\n",
      "            return Response(\n",
      "                data={\"errors\": \"Only a player can create a character.\"},\n",
      "                status=status.HTTP_403_FORBIDDEN,\n",
      "            )\n",
      "        char_ser = serializers.CharacterSerializer(\n",
      "            data=request.data,\n",
      "            context={\"request\": request, \"game\": self.get_parent_game()},\n",
      "        )\n",
      "        if not char_ser.is_valid():\n",
      "            return Response(data=char_ser.errors, status=status.HTTP_400_BAD_REQUEST)\n",
      "        char_ser.save()\n",
      "        return Response(data=char_ser.data, status=status.HTTP_201_CREATED)\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def approve(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Approves the proposed character.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"approved\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def reject(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Rejects the proposed character.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"rejected\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def deactivate(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Make a character inactive.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"inactive\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def reactivate(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Reactivate an inactive character.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"pending\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "\n",
      "@method_decorator(\n",
      "    name=\"list\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"List Your Characters\",\n",
      "        operation_description=\"Fetch a list of all of your characters.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"retrieve\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Character: Details\",\n",
      "        operation_description=\"Fetch the details of your character.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Character: Update\",\n",
      "        operation_description=\"Update the details of your character.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"partial_update\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Character: Update\",\n",
      "        operation_description=\"Update the details of your character.\",\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"destroy\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Character: Delete\",\n",
      "        operation_description=\"Delete your character.\",\n",
      "        request_body=no_body,\n",
      "        responses={204: \"Character was deleted.\"},\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"deactivate\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Character: Deactivate\",\n",
      "        operation_description=\"Mark your character as inactive.\",\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: \"Character was marked as inactive.\",\n",
      "            400: \"Character was already inactive.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "@method_decorator(\n",
      "    name=\"reactivate\",\n",
      "    decorator=swagger_auto_schema(\n",
      "        operation_summary=\"Your Character: Reactivate\",\n",
      "        operation_description=\"Mark your character as active.\",\n",
      "        request_body=no_body,\n",
      "        responses={\n",
      "            200: \"Character was marked as active.\",\n",
      "            400: \"Character was already active.\",\n",
      "        },\n",
      "    ),\n",
      ")\n",
      "class MyCharacterViewSet(\n",
      "    AutoPermissionViewSetMixin,\n",
      "    NestedViewSetMixin,\n",
      "    mixins.ListModelMixin,\n",
      "    mixins.RetrieveModelMixin,\n",
      "    mixins.UpdateModelMixin,\n",
      "    mixins.DestroyModelMixin,\n",
      "    viewsets.GenericViewSet,\n",
      "):\n",
      "    \"\"\"\n",
      "    Provides a vew so that players can view all their characters in one place.\n",
      "    \"\"\"\n",
      "\n",
      "    serializer_class = serializers.CharacterSerializer\n",
      "    permission_classes = (IsAuthenticated,)\n",
      "    lookup_field = \"slug\"\n",
      "    lookup_url_kwarg = \"slug\"\n",
      "    filter_backends = [DjangoFilterBackend]\n",
      "    filterset_fields = [\"status\"]\n",
      "    permission_type_map = {\n",
      "        **AutoPermissionViewSetMixin.permission_type_map,\n",
      "        \"deactivate\": \"delete\",\n",
      "        \"reactivate\": \"delete\",\n",
      "    }\n",
      "    permission_type_map[\"retrieve\"] = \"delete\"\n",
      "    parser_classes = [FormParser, MultiPartParser]\n",
      "\n",
      "    def get_queryset(self):\n",
      "        return models.Character.objects.filter(\n",
      "            player__gamer=self.request.user.gamerprofile\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def deactivate(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Make a character inactive.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"inactive\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @action(methods=[\"post\"], detail=True, parser_classes=[FormParser, JSONParser])\n",
      "    def reactivate(self, request, *args, **kwargs):\n",
      "        \"\"\"\n",
      "        Reactivate an inactive character.\n",
      "        \"\"\"\n",
      "        obj = self.get_object()\n",
      "        obj.status = \"pending\"\n",
      "        obj.save()\n",
      "        return Response(\n",
      "            data=self.serializer_class(obj, context={\"request\": request}).data,\n",
      "            status=status.HTTP_200_OK,\n",
      "        )\n",
      "\n",
      "    @dparser_classes([FormParser, JSONParser])\n",
      "    def destroy(self, request, *args, **kwargs):\n",
      "        self.parser_classes = [FormParser, JSONParser]\n",
      "        return super().destroy(request, *args, **kwargs)\n",
      "\n",
      "\"\"\"Mypy style test cases for SQLAlchemy stubs and plugin.\"\"\"\n",
      "\n",
      "import os\n",
      "import os.path\n",
      "import sys\n",
      "\n",
      "import pytest  # type: ignore  # no pytest in typeshed\n",
      "\n",
      "from mypy.test.config import test_temp_dir\n",
      "from mypy.test.data import DataDrivenTestCase, DataSuite\n",
      "from mypy.test.helpers import assert_string_arrays_equal\n",
      "from mypy.util import try_find_python2_interpreter\n",
      "from mypy import api\n",
      "\n",
      "this_file_dir = os.path.dirname(os.path.realpath(__file__))\n",
      "prefix = os.path.dirname(this_file_dir)\n",
      "inipath = os.path.abspath(os.path.join(prefix, 'test'))\n",
      "\n",
      "# Locations of test data files such as test case descriptions (.test).\n",
      "test_data_prefix = os.path.join(prefix, 'test', 'test-data')\n",
      "\n",
      "\n",
      "class SQLDataSuite(DataSuite):\n",
      "    files = ['sqlalchemy-basics.test',\n",
      "             'sqlalchemy-sql-elements.test',\n",
      "             'sqlalchemy-sql-sqltypes.test',\n",
      "             'sqlalchemy-sql-selectable.test',\n",
      "             'sqlalchemy-sql-schema.test',\n",
      "             'sqlalchemy-plugin-features.test',\n",
      "             'sqlalchemy-plugin-query.test']\n",
      "    data_prefix = test_data_prefix\n",
      "\n",
      "    def run_case(self, testcase: DataDrivenTestCase) -> None:\n",
      "\n",
      "        assert testcase.old_cwd is not None, \"test was not properly set up\"\n",
      "        mypy_cmdline = [\n",
      "            '--show-traceback',\n",
      "            '--no-silence-site-packages',\n",
      "            '--config-file={}/sqlalchemy.ini'.format(inipath),\n",
      "        ]\n",
      "        py2 = testcase.name.lower().endswith('python2')\n",
      "        if py2:\n",
      "            if try_find_python2_interpreter() is None:\n",
      "                pytest.skip()\n",
      "                return\n",
      "            mypy_cmdline.append('--py2')\n",
      "        else:\n",
      "            mypy_cmdline.append('--python-version={}'.format('.'.join(map(str,\n",
      "                                                                          sys.version_info[:2]))))\n",
      "\n",
      "        # Write the program to a file.\n",
      "        program_path = os.path.join(test_temp_dir, 'main.py')\n",
      "        mypy_cmdline.append(program_path)\n",
      "        with open(program_path, 'w') as file:\n",
      "            for s in testcase.input:\n",
      "                file.write('{}\\n'.format(s))\n",
      "        output = []\n",
      "        # Type check the program.\n",
      "        out, err, returncode = api.run(mypy_cmdline)\n",
      "        # split lines, remove newlines, and remove directory of test case\n",
      "        for line in (out + err).splitlines():\n",
      "            if line.startswith(test_temp_dir + os.sep):\n",
      "                output.append(line[len(test_temp_dir + os.sep):].rstrip(\"\\r\\n\").replace('.py',\n",
      "                                                                                        ''))\n",
      "            else:\n",
      "                output.append(line.rstrip(\"\\r\\n\"))\n",
      "        # Remove temp file.\n",
      "        os.remove(program_path)\n",
      "        assert_string_arrays_equal(testcase.output, output,\n",
      "                                   'Invalid output ({}, line {})'.format(\n",
      "                                   testcase.file, testcase.line))\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# Copyright (C) 2017 The Android Open Source Project\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "# Generates the text to paste into the email for announcing a new\n",
      "# release of Gerrit. The text is generated based on a template that\n",
      "# is filled with values either passed to the script or calculated\n",
      "# at runtime.\n",
      "#\n",
      "# The script outputs a plain text file with the announcement text:\n",
      "#\n",
      "#   release-announcement-gerrit-X.Y.txt\n",
      "#\n",
      "# and, if GPG is available, the announcement text wrapped with a\n",
      "# signature:\n",
      "#\n",
      "#   release-announcement-gerrit-X.Y.txt.asc\n",
      "#\n",
      "# Usage:\n",
      "#\n",
      "#   ./tools/release-announcement.py -v 2.14.2 -p 2.14.1 \\\n",
      "#      -s \"This release fixes several bugs since 2.14.1\"\n",
      "#\n",
      "# Parameters:\n",
      "#\n",
      "#   --version (-v): The version of Gerrit being released.\n",
      "#\n",
      "#   --previous (-p): The previous version of Gerrit.  Optional. If\n",
      "#   specified, the generated text includes a link to the gitiles\n",
      "#   log of commits between the previous and new versions.\n",
      "#\n",
      "#   --summary (-s): Short summary of the release. Optional. When\n",
      "#   specified, the summary is inserted in the introductory sentence\n",
      "#   of the generated text.\n",
      "#\n",
      "# Prerequisites:\n",
      "#\n",
      "# - The Jinja2 python library [1] must be installed.\n",
      "#\n",
      "# - For GPG signing to work, the python-gnupg library [2] must be\n",
      "#   installed, and the ~/.gnupg folder must exist.\n",
      "#\n",
      "# - The war file must have been installed to the local Maven repository\n",
      "#   using the `./tools/mvn/api.sh war_install` command.\n",
      "#\n",
      "# [1] http://jinja.pocoo.org/\n",
      "# [2] http://pythonhosted.org/gnupg/\n",
      "\n",
      "\n",
      "from __future__ import print_function\n",
      "import argparse\n",
      "import hashlib\n",
      "import os\n",
      "import sys\n",
      "from gnupg import GPG\n",
      "from jinja2 import Template\n",
      "\n",
      "\n",
      "class Version:\n",
      "    def __init__(self, version):\n",
      "        self.version = version\n",
      "        parts = version.split('.')\n",
      "        if len(parts) > 2:\n",
      "            self.major = \".\".join(parts[:2])\n",
      "            self.patch = version\n",
      "        else:\n",
      "            self.major = version\n",
      "            self.patch = None\n",
      "\n",
      "    def __str__(self):\n",
      "        return self.version\n",
      "\n",
      "\n",
      "def _main():\n",
      "    descr = 'Generate Gerrit release announcement email text'\n",
      "    parser = argparse.ArgumentParser(\n",
      "        description=descr,\n",
      "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
      "    parser.add_argument('-v', '--version', dest='version',\n",
      "                        required=True,\n",
      "                        help='gerrit version to release')\n",
      "    parser.add_argument('-p', '--previous', dest='previous',\n",
      "                        help='previous gerrit version (optional)')\n",
      "    parser.add_argument('-s', '--summary', dest='summary',\n",
      "                        help='summary of the release content (optional)')\n",
      "    options = parser.parse_args()\n",
      "\n",
      "    summary = options.summary\n",
      "    if summary and not summary.endswith(\".\"):\n",
      "        summary = summary + \".\"\n",
      "\n",
      "    data = {\n",
      "         \"version\": Version(options.version),\n",
      "         \"previous\": options.previous,\n",
      "         \"summary\": summary\n",
      "    }\n",
      "\n",
      "    war = os.path.join(\n",
      "        os.path.expanduser(\"~/.m2/repository/com/google/gerrit/gerrit-war/\"),\n",
      "        \"%(version)s/gerrit-war-%(version)s.war\" % data)\n",
      "    if not os.path.isfile(war):\n",
      "        print(\"Could not find war file for Gerrit %s in local Maven repository\"\n",
      "              % data[\"version\"], file=sys.stderr)\n",
      "        sys.exit(1)\n",
      "\n",
      "    md5 = hashlib.md5()\n",
      "    sha1 = hashlib.sha1()\n",
      "    sha256 = hashlib.sha256()\n",
      "    BUF_SIZE = 65536  # Read data in 64kb chunks\n",
      "    with open(war, 'rb') as f:\n",
      "        while True:\n",
      "            d = f.read(BUF_SIZE)\n",
      "            if not d:\n",
      "                break\n",
      "            md5.update(d)\n",
      "            sha1.update(d)\n",
      "            sha256.update(d)\n",
      "\n",
      "    data[\"sha1\"] = sha1.hexdigest()\n",
      "    data[\"sha256\"] = sha256.hexdigest()\n",
      "    data[\"md5\"] = md5.hexdigest()\n",
      "\n",
      "    template = Template(open(\"tools/release-announcement-template.txt\").read())\n",
      "    output = template.render(data=data)\n",
      "\n",
      "    filename = \"release-announcement-gerrit-%s.txt\" % data[\"version\"]\n",
      "    with open(filename, \"w\") as f:\n",
      "        f.write(output)\n",
      "\n",
      "    gpghome = os.path.abspath(os.path.expanduser(\"~/.gnupg\"))\n",
      "    if not os.path.isdir(gpghome):\n",
      "        print(\"Skipping signing due to missing gnupg home folder\")\n",
      "    else:\n",
      "        try:\n",
      "            gpg = GPG(homedir=gpghome)\n",
      "        except TypeError:\n",
      "            gpg = GPG(gnupghome=gpghome)\n",
      "        signed = gpg.sign(output)\n",
      "        filename = filename + \".asc\"\n",
      "        with open(filename, \"w\") as f:\n",
      "            f.write(str(signed))\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    _main()\n",
      "\n",
      "# from flask import Flask, Blueprint\n",
      "# from flask_sqlalchemy import SQLAlchemy\n",
      "# from flask_login import LoginManager\n",
      "# import os\n",
      "\n",
      "from flask import Flask, jsonify, request, make_response, redirect, url_for\n",
      "import jwt\n",
      "import datetime\n",
      "import os\n",
      "from functools import wraps\n",
      "from flask_sqlalchemy import SQLAlchemy\n",
      "import uuid\n",
      "from werkzeug.security import generate_password_hash, check_password_hash\n",
      "from werkzeug.utils import secure_filename\n",
      "from sqlalchemy import select\n",
      "from flask_migrate import Migrate, migrate\n",
      "from flask_cors import CORS\n",
      "from sqlalchemy import inspect\n",
      "from sqlalchemy import Table, Column, MetaData, Integer, Computed\n",
      "from numpy import array\n",
      "\n",
      "app = Flask(__name__)\n",
      "app.config['SECRET_KEY'] = 'secretollave'\n",
      "app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///todo.db'\n",
      "ABSOLUTE_PATH_TO_YOUR_FOLDER ='/home/dani/flask/static/fotosPerfil'\n",
      "ABSOLUTE_PATH_TO_YOUR_PDF_FOLDER ='/home/dani/flask/static/pdf'\n",
      "CORS(app)\n",
      "db  = SQLAlchemy(app)\n",
      "migrate = Migrate(app, db)\n",
      "\n",
      "\n",
      "# Models\n",
      "class Usuario(db.Model):\n",
      "    nick = db.Column(db.String(20), primary_key=True)\n",
      "    Nombre_de_usuario = db.Column(db.String(50))\n",
      "    password = db.Column(db.String(50))\n",
      "    e_mail = db.Column(db.String(50), unique=True, nullable=False)\n",
      "    descripcion  = db.Column(db.String(1000))\n",
      "    link  = db.Column(db.String(200))\n",
      "    foto_de_perfil = db.Column(db.String(400))\n",
      "\n",
      "class Sigue(db.Model):\n",
      "    #id  = db.Column(db.Integer, primary_key=True )\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "    Usuario_Nickb = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "\n",
      "class Chat(db.Model):\n",
      "\n",
      "    #Column('timestamp', TIMESTAMP(timezone=False), nullable=False, default=datetime.now())\n",
      "    timestamp = db.Column(db.TIMESTAMP, nullable=False,\n",
      "                  server_default=db.func.now(),\n",
      "                  onupdate=db.func.now())\n",
      "\n",
      "    mensaje  = db.Column(db.String(1000))\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "    Usuario_Nickb = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "\n",
      "\n",
      "class Publicacion(db.Model):\n",
      "\n",
      "    id  = db.Column(Integer,primary_key=True)\n",
      "    #id = db.Sequence('id', start=1, increment=1)\n",
      "    descripcion  = db.Column(db.String(1000))\n",
      "    #Column('timestamp', TIMESTAMP(timezone=False), nullable=False, default=datetime.now())\n",
      "    timestamp = db.Column(db.TIMESTAMP, nullable=False,\n",
      "                  server_default=db.func.now(),\n",
      "                  onupdate=db.func.now())\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'))\n",
      "\n",
      "class Propia(db.Model):\n",
      "\n",
      "    pdf = db.Column(db.String(400))\n",
      "    id = db.Column(db.String(20), db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "\n",
      "\n",
      "class Recomendacion(db.Model):\n",
      "\n",
      "    link  = db.Column(db.String(200),nullable=False)\n",
      "    titulo = db.Column(db.String(200),nullable=False)\n",
      "    autor  = db.Column(db.String(200),nullable=False)\n",
      "    id = db.Column(db.String(20), db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "\n",
      "class Tematica(db.Model):\n",
      "\n",
      "    tema  = db.Column(db.String(50), primary_key=True )\n",
      "\n",
      "\n",
      "class Notificaciones(db.Model):\n",
      "\n",
      "    id  = db.Column(db.Integer, primary_key=True )\n",
      "    fecha  = db.Column(db.Date)\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "\n",
      "\n",
      "class Prefiere(db.Model):\n",
      "\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "    tema = db.Column(db.String(50), db.ForeignKey('tematica.tema'),primary_key=True)\n",
      "\n",
      "\n",
      "class Trata_pub_del_tema(db.Model):\n",
      "\n",
      "    id = db.Column(db.Integer, db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "    tema = db.Column(db.String(50), db.ForeignKey('tematica.tema'),primary_key=True)\n",
      "\n",
      "class Gusta(db.Model):\n",
      "\n",
      "    id = db.Column(db.Integer, db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "\n",
      "\n",
      "class Comenta(db.Model):\n",
      "\n",
      "    id = db.Column(db.Integer, db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "    comentario  = db.Column(db.String(1000))\n",
      "\n",
      "class Guarda(db.Model):\n",
      "\n",
      "    id = db.Column(db.Integer, db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "\n",
      "class Trata(db.Model):\n",
      "\n",
      "    id_publi = db.Column(db.Integer, db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "    id_notif = db.Column(db.String(20), db.ForeignKey('notificaciones.id'),primary_key=True)\n",
      "\n",
      "\n",
      "class Genera(db.Model):\n",
      "\n",
      "    id = db.Column(db.Integer, db.ForeignKey('publicacion.id'),primary_key=True)\n",
      "    Usuario_Nicka = db.Column(db.String(20), db.ForeignKey('usuario.nick'),primary_key=True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "def token_required(f):\n",
      "    @wraps(f)\n",
      "    def decorated(*args, **kwargs):\n",
      "        #token = request.args.get('token') #http://127.0.0.1:5000/route?token=djsnvidnoffofn\n",
      "        #data = request.get_json()\n",
      "        token = request.headers['token']\n",
      "        #token = data['token']\n",
      "        if not token:\n",
      "            return jsonify({'error': 'Token no existe'}), 403\n",
      "\n",
      "        try:\n",
      "            data = jwt.decode(token, app.config['SECRET_KEY'])\n",
      "            current_user = Usuario.query.filter_by(nick=data['nick']).first()\n",
      "            current_user = data['nick']\n",
      "        except:\n",
      "            return jsonify({'error': 'Token no valido'}), 403\n",
      "\n",
      "        return f(current_user,*args, **kwargs)\n",
      "    return decorated\n",
      "\n",
      "def token_required_id(f):\n",
      "    @wraps(f)\n",
      "    def decorated(*args, **kwargs):\n",
      "        #token = request.args.get('token') #http://127.0.0.1:5000/route?token=djsnvidnoffofn\n",
      "        #data = request.get_json()\n",
      "        token = request.headers['token']\n",
      "        #token = data['token']\n",
      "        if not token:\n",
      "            return jsonify({'error': 'Token no existe'}), 403\n",
      "\n",
      "        try:\n",
      "            data = jwt.decode(token, app.config['SECRET_KEY'])\n",
      "            current_user = Usuario.query.filter_by(nick=data['nick']).first()\n",
      "            current_user = data['nick']\n",
      "            current_id = Publicacion.query.filter_by(id=data['id']).first()\n",
      "            _id = data['id']\n",
      "        except:\n",
      "            return jsonify({'error': 'Token no valido'}), 403\n",
      "\n",
      "        return f(current_user,_id,*args, **kwargs)\n",
      "    return decorated\n",
      "\n",
      "\n",
      "@app.route('/unprotected')\n",
      "def unprotected():\n",
      "    return jsonify({'message': 'Puede entrar tol mundo'})\n",
      "\n",
      "@app.route('/protected')\n",
      "@token_required\n",
      "def protected(current_user):\n",
      "    print(current_user)\n",
      "    return jsonify({'message': 'Puedes entrar si puedes'})\n",
      "\n",
      "# Ruta para el login\n",
      "\n",
      "\n",
      "\n",
      "@app.route('/register', methods=['POST'])\n",
      "def add_data():\n",
      "    data= request.get_json()\n",
      "    #nick = request.form.get(\"nick\")\n",
      "    #password = request.form.get(\"password\")\n",
      "    #e_mail = request.form.get(\"e_mail\")\n",
      "\n",
      "\n",
      "    user = Usuario.query.filter_by(e_mail=data['e_mail']).first()\n",
      "    nick = Usuario.query.filter_by(nick=data['nick']).first()\n",
      "    if user: # si esto devuelve algo entonces el email existe\n",
      "        return jsonify({'error': 'Existe correo'}) #json diciendo error existe email\n",
      "    if nick:\n",
      "        return jsonify({'error': 'Existe nick'})\n",
      "    #if (check_email(e_mail) == True and check_password(data['password']) == True ):\n",
      "    register = Usuario(nick=data['nick'],password=generate_password_hash(data['password']), e_mail=data['e_mail'],foto_de_perfil=\"platon.jpg\")\n",
      "    db.session.add(register)\n",
      "    db.session.commit()\n",
      "\n",
      "\n",
      "    token = jwt.encode({'nick' : data['nick'], 'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=30)}, app.config['SECRET_KEY'])\n",
      "    return jsonify({'token' : token.decode('UTF-8')})\n",
      "\n",
      "\n",
      "\n",
      "@app.route('/login', methods=['POST'])\n",
      "def login():\n",
      "    # auth = request.authorization #new ESTO SI LO HACES CON AUTH\n",
      "\n",
      "    data= request.get_json()\n",
      "\n",
      "    if '@' in data['nickOcorreo']:\n",
      "        user = Usuario.query.filter_by(e_mail=data['nickOcorreo']).first()\n",
      "    else:\n",
      "        user = Usuario.query.filter_by(nick=data['nickOcorreo']).first()\n",
      "\n",
      "    if not user:\n",
      "        return jsonify({'error': 'No existe ese usuario'})#error mal user\n",
      "    if not check_password_hash(user.password, data['password']):\n",
      "        return jsonify({'error': 'Mal contraseña'}) #error mala contraseña\n",
      "\n",
      "\n",
      "    token = jwt.encode({'nick' : data['nickOcorreo'], 'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=9999999)}, app.config['SECRET_KEY'])\n",
      "    return jsonify({'token' : token.decode('UTF-8')})\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@app.route('/editarPerfil', methods=['GET'])\n",
      "@token_required\n",
      "def editarPerfilget(current_user):\n",
      "    s = select([Usuario.Nombre_de_usuario,  Usuario.descripcion,Usuario.link, Usuario.foto_de_perfil]).where((Usuario.nick == current_user))\n",
      "    result = db.session.execute(s)\n",
      "\n",
      "    seguidos= db.session.query(Sigue).filter(Sigue.Usuario_Nicka == current_user ).count()\n",
      "    seguidores= db.session.query(Sigue).filter(Sigue.Usuario_Nickb == current_user ).count()\n",
      "    nposts= db.session.query(Publicacion).filter(Publicacion.Usuario_Nicka == current_user ).count()\n",
      "\n",
      "    tema = select([Prefiere.tema]).where((Prefiere.Usuario_Nicka == current_user))\n",
      "    temas = db.session.execute(tema)\n",
      "    vector = []\n",
      "    for row in temas:\n",
      "        vector += row\n",
      "    for row in result:\n",
      "        fila = {\n",
      "            \"nick\": current_user,\n",
      "            \"nombre_de_usuario\":row[0],\n",
      "            \"descripcion\":row[1],\n",
      "            \"link\":row[2],\n",
      "            \"foto_de_perfil\": 'http://51.255.50.207:5000/display/' + row[3],\n",
      "            \"nsiguiendo\": seguidos,\n",
      "            \"nseguidores\": seguidores,\n",
      "            \"nposts\": nposts,\n",
      "            \"tematicas\": vector\n",
      "            #\"foto_de_perfil\" :url_for('static', filename='fotosPerfil/' + row[3])\n",
      "        }\n",
      "    return fila\n",
      "\n",
      "@app.route('/display/<filename>')\n",
      "def foto(filename):\n",
      "    return redirect(url_for('static', filename='fotosPerfil/' + filename),code = 301)\n",
      "\n",
      "\n",
      "@app.route('/editarPerfil', methods=['POST'])\n",
      "@token_required\n",
      "def editarPerfilpost(current_user):\n",
      "\n",
      "    data= request.get_json()\n",
      "    user = Usuario.query.filter_by(nick=current_user).first()\n",
      "    user.Nombre_de_usuario = data['nombre_de_usuario']\n",
      "    print(data['nombre_de_usuario'])\n",
      "    print(data['descripcion'])\n",
      "    print(data['link'])\n",
      "    print(data['tematicas'])\n",
      "    user.descripcion = data['descripcion']\n",
      "    user.link = data['link']\n",
      "    tematicas = data['tematicas']\n",
      "    for temas in tematicas:\n",
      "        tema = Prefiere.query.filter_by(tema=temas).first()\n",
      "        if not tema:\n",
      "            tema = Prefiere(Usuario_Nicka=current_user, tema = temas)\n",
      "            db.session.add(tema)\n",
      "        #db.session.commit()\n",
      "    #cambia_foto\n",
      "\n",
      "    db.session.commit()\n",
      "\n",
      "    token = jwt.encode({'nick' : current_user, 'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=30)}, app.config['SECRET_KEY'])\n",
      "    return jsonify({'token' : token.decode('UTF-8')})\n",
      "\n",
      "\n",
      "@app.route('/actualizarImagen', methods=['POST'])\n",
      "@token_required\n",
      "def actualizarImagen(current_user):\n",
      "    user = Usuario.query.filter_by(nick=current_user).first()\n",
      "\n",
      "    if request.files['nueva_foto'] is not None: #data['cambia_foto']:\n",
      "\n",
      "    \tfile = request.files['nueva_foto']\n",
      "    \tprint(request.files['nueva_foto'])\n",
      "    \tfilename = secure_filename(file.filename)\n",
      "    \tfile.save(os.path.join(ABSOLUTE_PATH_TO_YOUR_FOLDER, filename))\n",
      "    \tuser.foto_de_perfil = filename\n",
      "    \tdb.session.commit()\n",
      "\n",
      "    token = jwt.encode({'nick' : current_user, 'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=30)}, app.config['SECRET_KEY'])\n",
      "    return jsonify({'token' : token.decode('UTF-8')})\n",
      "\n",
      "@app.route('/subirPost', methods=['POST'])\n",
      "@token_required\n",
      "def subirPost(current_user):\n",
      "\n",
      "    data= request.get_json()\n",
      "\n",
      "    publicacion = Publicacion(descripcion=data['descripcion'],Usuario_Nicka=current_user) #coger id\n",
      "    db.session.add(publicacion)\n",
      "    db.session.commit()\n",
      "\n",
      "    tematicas = data['tematicas']\n",
      "    for temas in tematicas:\n",
      "        temita = Tematica.query.filter_by(tema=temas).first()\n",
      "        if temita:\n",
      "            nuevo = Trata_pub_del_tema(id=publicacion.id, tema = temita.tema)\n",
      "            db.session.add(nuevo)\n",
      "    db.session.commit()\n",
      "    if (data['tipo']==\"1\"): # articulo\n",
      "        return jsonify({'id' : publicacion.id})\n",
      "        #guardarPDF(request.files['pdf'], publicacion.id)\n",
      "    elif(data['tipo']==\"2\"): # recomendacion\n",
      "        recomendacion = Recomendacion(link=data['link'],titulo=data['titulo'], autor = data['autor'], id = publicacion.id)\n",
      "        db.session.add(recomendacion)\n",
      "        \n",
      "    \n",
      "    db.session.commit()\n",
      "    token = jwt.encode({'nick' : current_user, 'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=30)}, app.config['SECRET_KEY'])\n",
      "    return jsonify({'token' : token.decode('UTF-8')})\n",
      "\n",
      "@app.route('/subirPdf', methods=['POST'])\n",
      "@token_required\n",
      "def guardarPDF(current_user):\n",
      "\n",
      "    _id=request.headers['id']\n",
      "\n",
      "    propia = Propia( id = _id)\n",
      "    db.session.add(propia)\n",
      "    db.session.commit()\n",
      "    propia = Propia.query.filter_by(id=_id).first()\n",
      "    if request.files['pdf'] is not None:\n",
      "        file = request.files['pdf']\n",
      "        #print(pdf)\n",
      "        filename = secure_filename(file.filename)\n",
      "        file.save(os.path.join(ABSOLUTE_PATH_TO_YOUR_PDF_FOLDER, filename))\n",
      "        propia.pdf = filename\n",
      "        db.session.add(propia)\n",
      "        db.session.commit()\n",
      "    else:\n",
      "        print(\"pdf nulisimo\")\n",
      "\n",
      "    token = jwt.encode({'nick' : current_user, 'exp': datetime.datetime.utcnow() + datetime.timedelta(minutes=30)}, app.config['SECRET_KEY'])\n",
      "    return jsonify({'token' : token.decode('UTF-8')})\n",
      "\n",
      "\n",
      "@app.route('/getPostsPropios', methods=['GET'])\n",
      "@token_required\n",
      "def getPostsPropios(current_user):\n",
      "\n",
      "       data= request.get_json()\n",
      "\n",
      "    x = select([Usuario.Nombre_de_usuario]).where((Usuario.nick == current_user))\n",
      "    resultb = db.session.execute(x)\n",
      "    Nombre_de_usuario = \"\"\n",
      "    for b in resultb: \n",
      "        Nombre_de_usuario=b.Nombre_de_usuario\n",
      "    \n",
      "    id = select([Publicacion.id]).where(Publicacion.Usuario_Nicka == current_user).order_by(Publicacion.id.desc())\n",
      "    descripcion = select( [Publicacion.descripcion]).where(Publicacion.Usuario_Nicka == current_user).order_by(Publicacion.id.desc())\n",
      "    timestamp = select([Publicacion.timestamp]).where(Publicacion.Usuario_Nicka == current_user).order_by(Publicacion.id.desc())\n",
      "\n",
      "    results = db.session.execute(id)\n",
      "    resultss = db.session.execute(descripcion)\n",
      "    resultsss = db.session.execute(timestamp)\n",
      "\n",
      "\n",
      "    vector0 = []\n",
      "    vector1 = []\n",
      "    vector2 = []\n",
      "    Gustas = []\n",
      "    Comentarios= []\n",
      "    Guardados= []\n",
      "    for r in results:\n",
      "        #print(str(r))\n",
      "        vector0 += r\n",
      "        Gustas += str(db.session.query(Gusta).filter(Gusta.Usuario_Nicka == current_user, Gusta.id == 'r' ).count())\n",
      "        Comentarios += str(db.session.query(Comenta).filter(Comenta.Usuario_Nicka == current_user, Comenta.id == 'r' ).count())\n",
      "        Guardados += str(db.session.query(Guarda).filter(Guarda.Usuario_Nicka == current_user, Guarda.id == 'r').count())\n",
      "    for r in resultss:\n",
      "        vector1 += r\n",
      "    \n",
      "    for r in resultsss:\n",
      "        vector2 += r\n",
      "    \n",
      "\n",
      "    vector3 = []\n",
      "    vector4 = []\n",
      "    vector5 = []\n",
      "    for r in vector0:\n",
      "        link = select([Recomendacion.link]).where((Recomendacion.id == r))\n",
      "        titulo = select([Recomendacion.titulo]).where((Recomendacion.id == r))\n",
      "        autor = select([Recomendacion.autor]).where((Recomendacion.id == r))\n",
      "        resulta = db.session.execute(link)\n",
      "        resultaa = db.session.execute(titulo)\n",
      "        resultaaa = db.session.execute(autor)\n",
      "\n",
      "        for a in resulta:\n",
      "            vector3 +=a\n",
      "        for a in resultaa:\n",
      "            vector4 +=a\n",
      "        for a in resultaaa:\n",
      "            vector5 +=a  \n",
      "\n",
      "        fila = {\n",
      "            \"id\": r.id,\n",
      "            \"nick\": current_user,\n",
      "            \"descripcion\":r.descripcion,\n",
      "            \"timestamp\":r.timestamp,\n",
      "            \"pdf\": 'http://51.255.50.207:5000/display2/' + a.pdf,\n",
      "            \"nlikes\": Gustas,\n",
      "            \"ncomentarios\": Comentarios,\n",
      "            \"nguardados\": Guardados,\n",
      "            \"usuario\": resulta.nombre_de_usuario\n",
      "        }              \n",
      "    fila = {\n",
      "                \"id\": vector0,\n",
      "                \"link\": vector3,\n",
      "                \"titulo\": vector4,\n",
      "                \"autor\": vector5,\n",
      "                \"nick\": current_user,\n",
      "                \"descripcion\": vector1,\n",
      "                \"timestamp\": vector2,\n",
      "                \"nlikes\": Gustas,\n",
      "                \"ncomentarios\": Comentarios,\n",
      "                \"nguardados\": Guardados,\n",
      "                \"usuario\": Nombre_de_usuario,\n",
      "                #\"likemio\",\n",
      "                #\"guardadomio\"\n",
      "                }\n",
      "        \n",
      "    return fila\n",
      "            \n",
      "\n",
      "\n",
      "@app.route('/display2/<filename>')\n",
      "def pdf(filename):\n",
      "    return redirect(url_for('static', filename='pdf/' + filename),code = 301)\n",
      "\n",
      "@app.route('/getPostsRecomendados', methods=['GET'])\n",
      "@token_required\n",
      "def getPostsRecomendados(current_user):\n",
      "\n",
      "    data= request.get_json()\n",
      "\n",
      "    x = select([Usuario.Nombre_de_usuario]).where((Usuario.nick == current_user))\n",
      "    resultb = db.session.execute(x)\n",
      "    Nombre_de_usuario = \"\"\n",
      "    for b in resultb: \n",
      "        Nombre_de_usuario=b.Nombre_de_usuario\n",
      "    \n",
      "    id = select([Publicacion.id]).where(Publicacion.Usuario_Nicka == current_user).order_by(Publicacion.id.desc())\n",
      "    descripcion = select( [Publicacion.descripcion]).where(Publicacion.Usuario_Nicka == current_user).order_by(Publicacion.id.desc())\n",
      "    timestamp = select([Publicacion.timestamp]).where(Publicacion.Usuario_Nicka == current_user).order_by(Publicacion.id.desc())\n",
      "\n",
      "    results = db.session.execute(id)\n",
      "    resultss = db.session.execute(descripcion)\n",
      "    resultsss = db.session.execute(timestamp)\n",
      "\n",
      "\n",
      "    vector0 = []\n",
      "    vector1 = []\n",
      "    vector2 = []\n",
      "    Gustas = []\n",
      "    Comentarios= []\n",
      "    Guardados= []\n",
      "    for r in results:\n",
      "        #print(str(r))\n",
      "        vector0 += r\n",
      "        Gustas += str(db.session.query(Gusta).filter(Gusta.Usuario_Nicka == current_user, Gusta.id == 'r' ).count())\n",
      "        Comentarios += str(db.session.query(Comenta).filter(Comenta.Usuario_Nicka == current_user, Comenta.id == 'r' ).count())\n",
      "        Guardados += str(db.session.query(Guarda).filter(Guarda.Usuario_Nicka == current_user, Guarda.id == 'r').count())\n",
      "    for r in resultss:\n",
      "        vector1 += r\n",
      "    \n",
      "    for r in resultsss:\n",
      "        vector2 += r\n",
      "    \n",
      "\n",
      "    vector3 = []\n",
      "    vector4 = []\n",
      "    vector5 = []\n",
      "    for r in vector0:\n",
      "        link = select([Recomendacion.link]).where((Recomendacion.id == r))\n",
      "        titulo = select([Recomendacion.titulo]).where((Recomendacion.id == r))\n",
      "        autor = select([Recomendacion.autor]).where((Recomendacion.id == r))\n",
      "        resulta = db.session.execute(link)\n",
      "        resultaa = db.session.execute(titulo)\n",
      "        resultaaa = db.session.execute(autor)\n",
      "\n",
      "        for a in resulta:\n",
      "            vector3 +=a\n",
      "        for a in resultaa:\n",
      "            vector4 +=a\n",
      "        for a in resultaaa:\n",
      "            vector5 +=a                \n",
      "    fila = {\n",
      "                \"id\": vector0,\n",
      "                \"link\": vector3,\n",
      "                \"titulo\": vector4,\n",
      "                \"autor\": vector5,\n",
      "                \"nick\": current_user,\n",
      "                \"descripcion\": vector1,\n",
      "                \"timestamp\": vector2,\n",
      "                \"nlikes\": Gustas,\n",
      "                \"ncomentarios\": Comentarios,\n",
      "                \"nguardados\": Guardados,\n",
      "                \"usuario\": Nombre_de_usuario,\n",
      "                #\"likemio\",\n",
      "                #\"guardadomio\"\n",
      "                }\n",
      "        \n",
      "    return fila\n",
      "\n",
      "def check_email(email):\n",
      "\n",
      "    regex = '^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w{2,3}$'\n",
      "\n",
      "    if(re.search(regex,email)):\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "# Contraseñas de entre 8 y 32 carácteres.\n",
      "\n",
      "def check_password(password):\n",
      "\n",
      "    regex = '^(?=.*[0-9])(?=.*[a-z])(?=.*[A-Z])(?=.*[*.!@$%^&(){}[]:;<>,.?/~_+-=|\\]).{8,32}$'\n",
      "\n",
      "    if(re.search(regex,password)):\n",
      "        return True\n",
      "    else:\n",
      "        return False\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# coding: utf-8\n",
      "\n",
      "\"\"\"\n",
      "    An API to insert and retrieve metadata on cloud artifacts.\n",
      "\n",
      "    No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)  # noqa: E501\n",
      "\n",
      "    OpenAPI spec version: v1alpha1\n",
      "    \n",
      "    Generated by: https://github.com/swagger-api/swagger-codegen.git\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "import pprint\n",
      "import re  # noqa: F401\n",
      "\n",
      "import six\n",
      "\n",
      "\n",
      "class ApiArtifact(object):\n",
      "    \"\"\"NOTE: This class is auto generated by the swagger code generator program.\n",
      "\n",
      "    Do not edit the class manually.\n",
      "    \"\"\"\n",
      "\n",
      "    \"\"\"\n",
      "    Attributes:\n",
      "      swagger_types (dict): The key is attribute name\n",
      "                            and the value is attribute type.\n",
      "      attribute_map (dict): The key is attribute name\n",
      "                            and the value is json key in definition.\n",
      "    \"\"\"\n",
      "    swagger_types = {\n",
      "        'name': 'str',\n",
      "        'checksum': 'str',\n",
      "        'id': 'str',\n",
      "        'names': 'list[str]'\n",
      "    }\n",
      "\n",
      "    attribute_map = {\n",
      "        'name': 'name',\n",
      "        'checksum': 'checksum',\n",
      "        'id': 'id',\n",
      "        'names': 'names'\n",
      "    }\n",
      "\n",
      "    def __init__(self, name=None, checksum=None, id=None, names=None):  # noqa: E501\n",
      "        \"\"\"ApiArtifact - a model defined in Swagger\"\"\"  # noqa: E501\n",
      "\n",
      "        self._name = None\n",
      "        self._checksum = None\n",
      "        self._id = None\n",
      "        self._names = None\n",
      "        self.discriminator = None\n",
      "\n",
      "        if name is not None:\n",
      "            self.name = name\n",
      "        if checksum is not None:\n",
      "            self.checksum = checksum\n",
      "        if id is not None:\n",
      "            self.id = id\n",
      "        if names is not None:\n",
      "            self.names = names\n",
      "\n",
      "    @property\n",
      "    def name(self):\n",
      "        \"\"\"Gets the name of this ApiArtifact.  # noqa: E501\n",
      "\n",
      "        Name of the artifact. This may be the path to a binary or jar file, or in the case of a container build, the name used to push the container image to Google Container Registry, as presented to `docker push`.  This field is deprecated in favor of the plural `names` field; it continues to exist here to allow existing BuildProvenance serialized to json in google.devtools.containeranalysis.v1alpha1.BuildDetails.provenance_bytes to deserialize back into proto.  # noqa: E501\n",
      "\n",
      "        :return: The name of this ApiArtifact.  # noqa: E501\n",
      "        :rtype: str\n",
      "        \"\"\"\n",
      "        return self._name\n",
      "\n",
      "    @name.setter\n",
      "    def name(self, name):\n",
      "        \"\"\"Sets the name of this ApiArtifact.\n",
      "\n",
      "        Name of the artifact. This may be the path to a binary or jar file, or in the case of a container build, the name used to push the container image to Google Container Registry, as presented to `docker push`.  This field is deprecated in favor of the plural `names` field; it continues to exist here to allow existing BuildProvenance serialized to json in google.devtools.containeranalysis.v1alpha1.BuildDetails.provenance_bytes to deserialize back into proto.  # noqa: E501\n",
      "\n",
      "        :param name: The name of this ApiArtifact.  # noqa: E501\n",
      "        :type: str\n",
      "        \"\"\"\n",
      "\n",
      "        self._name = name\n",
      "\n",
      "    @property\n",
      "    def checksum(self):\n",
      "        \"\"\"Gets the checksum of this ApiArtifact.  # noqa: E501\n",
      "\n",
      "        Hash or checksum value of a binary, or Docker Registry 2.0 digest of a container.  # noqa: E501\n",
      "\n",
      "        :return: The checksum of this ApiArtifact.  # noqa: E501\n",
      "        :rtype: str\n",
      "        \"\"\"\n",
      "        return self._checksum\n",
      "\n",
      "    @checksum.setter\n",
      "    def checksum(self, checksum):\n",
      "        \"\"\"Sets the checksum of this ApiArtifact.\n",
      "\n",
      "        Hash or checksum value of a binary, or Docker Registry 2.0 digest of a container.  # noqa: E501\n",
      "\n",
      "        :param checksum: The checksum of this ApiArtifact.  # noqa: E501\n",
      "        :type: str\n",
      "        \"\"\"\n",
      "\n",
      "        self._checksum = checksum\n",
      "\n",
      "    @property\n",
      "    def id(self):\n",
      "        \"\"\"Gets the id of this ApiArtifact.  # noqa: E501\n",
      "\n",
      "\n",
      "        :return: The id of this ApiArtifact.  # noqa: E501\n",
      "        :rtype: str\n",
      "        \"\"\"\n",
      "        return self._id\n",
      "\n",
      "    @id.setter\n",
      "    def id(self, id):\n",
      "        \"\"\"Sets the id of this ApiArtifact.\n",
      "\n",
      "\n",
      "        :param id: The id of this ApiArtifact.  # noqa: E501\n",
      "        :type: str\n",
      "        \"\"\"\n",
      "\n",
      "        self._id = id\n",
      "\n",
      "    @property\n",
      "    def names(self):\n",
      "        \"\"\"Gets the names of this ApiArtifact.  # noqa: E501\n",
      "\n",
      "        Related artifact names. This may be the path to a binary or jar file, or in the case of a container build, the name used to push the container image to Google Container Registry, as presented to `docker push`. Note that a single Artifact ID can have multiple names, for example if two tags are applied to one image.  # noqa: E501\n",
      "\n",
      "        :return: The names of this ApiArtifact.  # noqa: E501\n",
      "        :rtype: list[str]\n",
      "        \"\"\"\n",
      "        return self._names\n",
      "\n",
      "    @names.setter\n",
      "    def names(self, names):\n",
      "        \"\"\"Sets the names of this ApiArtifact.\n",
      "\n",
      "        Related artifact names. This may be the path to a binary or jar file, or in the case of a container build, the name used to push the container image to Google Container Registry, as presented to `docker push`. Note that a single Artifact ID can have multiple names, for example if two tags are applied to one image.  # noqa: E501\n",
      "\n",
      "        :param names: The names of this ApiArtifact.  # noqa: E501\n",
      "        :type: list[str]\n",
      "        \"\"\"\n",
      "\n",
      "        self._names = names\n",
      "\n",
      "    def to_dict(self):\n",
      "        \"\"\"Returns the model properties as a dict\"\"\"\n",
      "        result = {}\n",
      "\n",
      "        for attr, _ in six.iteritems(self.swagger_types):\n",
      "            value = getattr(self, attr)\n",
      "            if isinstance(value, list):\n",
      "                result[attr] = list(map(\n",
      "                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n",
      "                    value\n",
      "                ))\n",
      "            elif hasattr(value, \"to_dict\"):\n",
      "                result[attr] = value.to_dict()\n",
      "            elif isinstance(value, dict):\n",
      "                result[attr] = dict(map(\n",
      "                    lambda item: (item[0], item[1].to_dict())\n",
      "                    if hasattr(item[1], \"to_dict\") else item,\n",
      "                    value.items()\n",
      "                ))\n",
      "            else:\n",
      "                result[attr] = value\n",
      "        if issubclass(ApiArtifact, dict):\n",
      "            for key, value in self.items():\n",
      "                result[key] = value\n",
      "\n",
      "        return result\n",
      "\n",
      "    def to_str(self):\n",
      "        \"\"\"Returns the string representation of the model\"\"\"\n",
      "        return pprint.pformat(self.to_dict())\n",
      "\n",
      "    def __repr__(self):\n",
      "        \"\"\"For `print` and `pprint`\"\"\"\n",
      "        return self.to_str()\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        \"\"\"Returns true if both objects are equal\"\"\"\n",
      "        if not isinstance(other, ApiArtifact):\n",
      "            return False\n",
      "\n",
      "        return self.__dict__ == other.__dict__\n",
      "\n",
      "    def __ne__(self, other):\n",
      "        \"\"\"Returns true if both objects are not equal\"\"\"\n",
      "        return not self == other\n",
      "\n",
      "# Licensed under a 3-clause BSD style license - see LICENSE.rst\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "from __future__ import absolute_import, division, unicode_literals, print_function\n",
      "\n",
      "import re\n",
      "import os\n",
      "import tempfile\n",
      "\n",
      "import six\n",
      "\n",
      "from .. import environment\n",
      "from ..console import log\n",
      "from .. import util\n",
      "\n",
      "\n",
      "WIN = (os.name == \"nt\")\n",
      "\n",
      "\n",
      "def _find_conda():\n",
      "    \"\"\"Find the conda executable robustly across conda versions.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    conda : str\n",
      "        Path to the conda executable.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    IOError\n",
      "        If the executable cannot be found in either the CONDA_EXE environment\n",
      "        variable or in the PATH.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    In POSIX platforms in conda >= 4.4, conda can be set up as a bash function\n",
      "    rather than an executable. (This is to enable the syntax\n",
      "    ``conda activate env-name``.) In this case, the environment variable\n",
      "    ``CONDA_EXE`` contains the path to the conda executable. In other cases,\n",
      "    we use standard search for the appropriate name in the PATH.\n",
      "\n",
      "    See https://github.com/airspeed-velocity/asv/issues/645 for more details.\n",
      "    \"\"\"\n",
      "    if 'CONDA_EXE' in os.environ:\n",
      "        conda = os.environ['CONDA_EXE']\n",
      "    else:\n",
      "        conda = util.which('conda')\n",
      "    return conda\n",
      "\n",
      "\n",
      "class Conda(environment.Environment):\n",
      "    \"\"\"\n",
      "    Manage an environment using conda.\n",
      "\n",
      "    Dependencies are installed using ``conda``.  The benchmarked\n",
      "    project is installed using ``pip`` (since ``conda`` doesn't have a\n",
      "    method to install from an arbitrary ``setup.py``).\n",
      "    \"\"\"\n",
      "    tool_name = \"conda\"\n",
      "    _matches_cache = {}\n",
      "\n",
      "    def __init__(self, conf, python, requirements):\n",
      "        \"\"\"\n",
      "        Parameters\n",
      "        ----------\n",
      "        conf : Config instance\n",
      "\n",
      "        python : str\n",
      "            Version of Python.  Must be of the form \"MAJOR.MINOR\".\n",
      "\n",
      "        requirements : dict\n",
      "            Dictionary mapping a PyPI package name to a version\n",
      "            identifier string.\n",
      "        \"\"\"\n",
      "        self._python = python\n",
      "        self._requirements = requirements\n",
      "        self._conda_channels = conf.conda_channels\n",
      "        super(Conda, self).__init__(conf, python, requirements)\n",
      "\n",
      "    @classmethod\n",
      "    def matches(cls, python):\n",
      "        # Calling conda can take a long time, so remember the result\n",
      "        if python not in cls._matches_cache:\n",
      "            cls._matches_cache[python] = cls._matches(python)\n",
      "        return cls._matches_cache[python]\n",
      "\n",
      "    @classmethod\n",
      "    def _matches(cls, python):\n",
      "        if not re.match(r'^[0-9].*$', python):\n",
      "            # The python name should be a version number\n",
      "            return False\n",
      "\n",
      "        try:\n",
      "            conda = _find_conda()\n",
      "        except IOError:\n",
      "            return False\n",
      "        else:\n",
      "            # This directory never gets created, since we're just\n",
      "            # doing a dry run below.  All it needs to be is something\n",
      "            # that doesn't already exist.\n",
      "            path = os.path.join(tempfile.gettempdir(), 'check')\n",
      "\n",
      "            # Check that the version number is valid\n",
      "            try:\n",
      "                util.check_call([\n",
      "                    conda,\n",
      "                    'create',\n",
      "                    '--yes',\n",
      "                    '-p',\n",
      "                    path,\n",
      "                    'python={0}'.format(python),\n",
      "                    '--dry-run'], display_error=False, dots=False)\n",
      "            except util.ProcessError:\n",
      "                return False\n",
      "            else:\n",
      "                return True\n",
      "\n",
      "    def _setup(self):\n",
      "        try:\n",
      "            conda = _find_conda()\n",
      "        except IOError as e:\n",
      "            raise util.UserError(str(e))\n",
      "\n",
      "        log.info(\"Creating conda environment for {0}\".format(self.name))\n",
      "\n",
      "        # create a temporary environment.yml file\n",
      "        # and use that to generate the env for benchmarking\n",
      "        env_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".yml\")\n",
      "        try:\n",
      "            env_file.write('name: {0}\\n'\n",
      "                           'channels:\\n'.format(self.name))\n",
      "            env_file.writelines(('   - %s\\n' % ch for ch in self._conda_channels))\n",
      "            env_file.write('dependencies:\\n'\n",
      "                           '   - python={0}\\n'\n",
      "                           '   - wheel\\n'\n",
      "                           '   - pip\\n'.format(self._python))\n",
      "\n",
      "            # categorize & write dependencies based on pip vs. conda\n",
      "            conda_args, pip_args = self._get_requirements(conda)\n",
      "            env_file.writelines(('   - %s\\n' % s for s in conda_args))\n",
      "            if pip_args:\n",
      "                # and now specify the packages that are to be installed in\n",
      "                # the pip subsection\n",
      "                env_file.write('   - pip:\\n')\n",
      "                env_file.writelines(('     - %s\\n' % s for s in pip_args))\n",
      "\n",
      "            env_file.close()\n",
      "\n",
      "            util.check_output([conda] + ['env', 'create', '-f', env_file.name,\n",
      "                                         '-p', self._path, '--force'])\n",
      "        except Exception as exc:\n",
      "            if os.path.isfile(env_file.name):\n",
      "                with open(env_file.name, 'r') as f:\n",
      "                    text = f.read()\n",
      "                log.info(\"conda env create failed: in {} with:\\n{}\".format(self._path, text))\n",
      "            raise\n",
      "        finally:\n",
      "            os.unlink(env_file.name)\n",
      "\n",
      "    def _get_requirements(self, conda):\n",
      "        if self._requirements:\n",
      "            # retrieve and return all conda / pip dependencies\n",
      "            conda_args = []\n",
      "            pip_args = []\n",
      "\n",
      "            for key, val in six.iteritems(self._requirements):\n",
      "                if key.startswith('pip+'):\n",
      "                    if val:\n",
      "                        pip_args.append(\"{0}=={1}\".format(key[4:], val))\n",
      "                    else:\n",
      "                        pip_args.append(key[4:])\n",
      "                else:\n",
      "                    if val:\n",
      "                        conda_args.append(\"{0}={1}\".format(key, val))\n",
      "                    else:\n",
      "                        conda_args.append(key)\n",
      "\n",
      "            return conda_args, pip_args\n",
      "        else:\n",
      "            return [], []\n",
      "\n",
      "    def run(self, args, **kwargs):\n",
      "        log.debug(\"Running '{0}' in {1}\".format(' '.join(args), self.name))\n",
      "        return self.run_executable('python', args, **kwargs)\n",
      "\n",
      "    def run_executable(self, executable, args, **kwargs):\n",
      "        # Conda doesn't guarantee that user site directories are excluded\n",
      "        kwargs[\"env\"] = dict(kwargs.pop(\"env\", os.environ),\n",
      "                             PYTHONNOUSERSITE=str(\"True\"))\n",
      "        return super(Conda, self).run_executable(executable, args, **kwargs)\n",
      "\n",
      "\"\"\"\n",
      "Function objects.\n",
      "\n",
      "In PyPy there is no difference between built-in and user-defined function\n",
      "objects; the difference lies in the code object found in their func_code\n",
      "attribute.\n",
      "\"\"\"\n",
      "\n",
      "from pypy.rlib.unroll import unrolling_iterable\n",
      "from pypy.interpreter.error import OperationError\n",
      "from pypy.interpreter.baseobjspace import Wrappable\n",
      "from pypy.interpreter.eval import Code\n",
      "from pypy.interpreter.argument import Arguments, ArgumentsFromValuestack\n",
      "\n",
      "funccallunrolling = unrolling_iterable(range(4))\n",
      "\n",
      "class Function(Wrappable):\n",
      "    \"\"\"A function is a code object captured with some environment:\n",
      "    an object space, a dictionary of globals, default arguments,\n",
      "    and an arbitrary 'closure' passed to the code object.\"\"\"\n",
      "\n",
      "    def __init__(self, space, code, w_globals=None, defs_w=[], closure=None, forcename=None, creator_nametoken=None):\n",
      "        self.space = space\n",
      "        self.name = forcename or code.co_name\n",
      "        self.w_doc = None   # lazily read from code.getdocstring()\n",
      "        self.code = code       # Code instance\n",
      "        self.w_func_globals = w_globals  # the globals dictionary\n",
      "        self.closure   = closure    # normally, list of Cell instances or None\n",
      "        self.defs_w    = defs_w     # list of w_default's\n",
      "        self.w_func_dict = None # filled out below if needed\n",
      "        self.w_module = None\n",
      "        self.creator_nametoken = creator_nametoken\n",
      "\n",
      "    def __repr__(self):\n",
      "        # return \"function %s.%s\" % (self.space, self.name)\n",
      "        # maybe we want this shorter:\n",
      "        name = getattr(self, 'name', '?')\n",
      "        return \"<%s %s>\" % (self.__class__.__name__, name)\n",
      "\n",
      "    def call_args(self, args):\n",
      "        # delegate activation to code        \n",
      "        return self.code.funcrun(self, args)\n",
      "\n",
      "    def call_obj_args(self, w_obj, args):\n",
      "        # delegate activation to code\n",
      "        return self.code.funcrun_obj(self, w_obj, args)\n",
      "\n",
      "    def getcode(self):\n",
      "        return self.code\n",
      "    \n",
      "    def funccall(self, *args_w): # speed hack\n",
      "        from pypy.interpreter import gateway\n",
      "        from pypy.interpreter.pycode import PyCode\n",
      "        \n",
      "        code = self.getcode() # hook for the jit\n",
      "        nargs = len(args_w)\n",
      "        fast_natural_arity = code.fast_natural_arity\n",
      "        if nargs == fast_natural_arity:\n",
      "            if nargs == 0:\n",
      "                assert isinstance(code, gateway.BuiltinCode0)                \n",
      "                return code.fastcall_0(self.space, self)\n",
      "            elif nargs == 1:\n",
      "                assert isinstance(code, gateway.BuiltinCode1)\n",
      "                return code.fastcall_1(self.space, self, args_w[0])\n",
      "            elif nargs == 2:\n",
      "                assert isinstance(code, gateway.BuiltinCode2)\n",
      "                return code.fastcall_2(self.space, self, args_w[0], args_w[1])\n",
      "            elif nargs == 3:\n",
      "                assert isinstance(code, gateway.BuiltinCode3)                \n",
      "                return code.fastcall_3(self.space, self, args_w[0],\n",
      "                                       args_w[1], args_w[2])\n",
      "            elif nargs == 4:\n",
      "                assert isinstance(code, gateway.BuiltinCode4)                \n",
      "                return code.fastcall_4(self.space, self, args_w[0],\n",
      "                                       args_w[1], args_w[2], args_w[3])\n",
      "        elif (nargs|PyCode.FLATPYCALL) == fast_natural_arity:\n",
      "            assert isinstance(code, PyCode)            \n",
      "            if nargs < 5:\n",
      "                new_frame = self.space.createframe(code, self.w_func_globals,\n",
      "                                                   self.closure)\n",
      "                for i in funccallunrolling:\n",
      "                    if i < nargs:\n",
      "                        new_frame.fastlocals_w[i] = args_w[i]\n",
      "                return new_frame.run()                                    \n",
      "        elif nargs >= 1 and fast_natural_arity == -1:\n",
      "            assert isinstance(code, gateway.BuiltinCodePassThroughArguments1)\n",
      "            return code.funcrun_obj(self, args_w[0],\n",
      "                                    Arguments(self.space,\n",
      "                                              list(args_w[1:])))\n",
      "        return self.call_args(Arguments(self.space, list(args_w)))\n",
      "\n",
      "    def funccall_valuestack(self, nargs, frame): # speed hack\n",
      "        from pypy.interpreter import gateway\n",
      "        from pypy.interpreter.pycode import PyCode\n",
      "            \n",
      "        code = self.getcode() # hook for the jit\n",
      "        fast_natural_arity = code.fast_natural_arity        \n",
      "        if nargs == fast_natural_arity:\n",
      "            if nargs == 0:\n",
      "                assert isinstance(code, gateway.BuiltinCode0)\n",
      "                return code.fastcall_0(self.space, self)\n",
      "            elif nargs == 1:\n",
      "                assert isinstance(code, gateway.BuiltinCode1)\n",
      "                return code.fastcall_1(self.space, self, frame.peekvalue(0))\n",
      "            elif nargs == 2:\n",
      "                assert isinstance(code, gateway.BuiltinCode2)\n",
      "                return code.fastcall_2(self.space, self, frame.peekvalue(1),\n",
      "                                       frame.peekvalue(0))\n",
      "            elif nargs == 3:\n",
      "                assert isinstance(code, gateway.BuiltinCode3)\n",
      "                return code.fastcall_3(self.space, self, frame.peekvalue(2),\n",
      "                                       frame.peekvalue(1), frame.peekvalue(0))\n",
      "            elif nargs == 4:\n",
      "                assert isinstance(code, gateway.BuiltinCode4)\n",
      "                return code.fastcall_4(self.space, self, frame.peekvalue(3),\n",
      "                                       frame.peekvalue(2), frame.peekvalue(1),\n",
      "                                        frame.peekvalue(0))\n",
      "        elif (nargs|PyCode.FLATPYCALL) == fast_natural_arity:\n",
      "            assert isinstance(code, PyCode)\n",
      "            return self._flat_pycall(code, nargs, frame)\n",
      "        elif fast_natural_arity == -1 and nargs >= 1:\n",
      "            assert isinstance(code, gateway.BuiltinCodePassThroughArguments1)\n",
      "            w_obj = frame.peekvalue(nargs-1)\n",
      "            args = frame.make_arguments(nargs-1)\n",
      "            try:\n",
      "                return code.funcrun_obj(self, w_obj, args)\n",
      "            finally:\n",
      "                if isinstance(args, ArgumentsFromValuestack):\n",
      "                    args.frame = None\n",
      "                    \n",
      "        args = frame.make_arguments(nargs)\n",
      "        try:\n",
      "            return self.call_args(args)\n",
      "        finally:\n",
      "            if isinstance(args, ArgumentsFromValuestack):\n",
      "                args.frame = None\n",
      "\n",
      "    def _flat_pycall(self, code, nargs, frame):\n",
      "        # code is a PyCode\n",
      "        new_frame = self.space.createframe(code, self.w_func_globals,\n",
      "                                                   self.closure)\n",
      "        for i in xrange(nargs):\n",
      "            w_arg = frame.peekvalue(nargs-1-i)\n",
      "            new_frame.fastlocals_w[i] = w_arg\n",
      "        return new_frame.run()                        \n",
      "\n",
      "    def getdict(self):\n",
      "        if self.w_func_dict is None:\n",
      "            self.w_func_dict = self.space.newdict()\n",
      "        return self.w_func_dict\n",
      "\n",
      "    def setdict(self, space, w_dict):\n",
      "        if not space.is_true(space.isinstance( w_dict, space.w_dict )):\n",
      "            raise OperationError( space.w_TypeError, space.wrap(\"setting function's dictionary to a non-dict\") )\n",
      "        self.w_func_dict = w_dict\n",
      "\n",
      "    # unwrapping is done through unwrap_specs in typedef.py\n",
      "\n",
      "    def descr_function__new__(space, w_subtype, w_code, w_globals, \n",
      "                            w_name=None, w_argdefs=None, w_closure=None):\n",
      "        code = space.interp_w(Code, w_code)\n",
      "        if not space.is_true(space.isinstance(w_globals, space.w_dict)):\n",
      "            raise OperationError(space.w_TypeError, space.wrap(\"expected dict\"))\n",
      "        if not space.is_w(w_name, space.w_None):\n",
      "            name = space.str_w(w_name)\n",
      "        else:\n",
      "            name = None\n",
      "        if not space.is_w(w_argdefs, space.w_None):\n",
      "            defs_w = space.unpackiterable(w_argdefs)\n",
      "        else:\n",
      "            defs_w = []\n",
      "        nfreevars = 0\n",
      "        from pypy.interpreter.pycode import PyCode\n",
      "        if isinstance(code, PyCode):\n",
      "            nfreevars = len(code.co_freevars)\n",
      "        if space.is_w(w_closure, space.w_None) and nfreevars == 0:\n",
      "            closure = None\n",
      "        elif not space.is_w(space.type(w_closure), space.w_tuple):\n",
      "            raise OperationError(space.w_TypeError, space.wrap(\"invalid closure\"))\n",
      "        else:\n",
      "            from pypy.interpreter.nestedscope import Cell\n",
      "            closure_w = space.unpackiterable(w_closure)\n",
      "            n = len(closure_w)\n",
      "            if nfreevars == 0:\n",
      "                raise OperationError(space.w_ValueError, space.wrap(\"no closure needed\"))\n",
      "            elif nfreevars != n:\n",
      "                raise OperationError(space.w_ValueError, space.wrap(\"closure is wrong size\"))\n",
      "            closure = [space.interp_w(Cell, w_cell) for w_cell in closure_w]\n",
      "        func = space.allocate_instance(Function, w_subtype)\n",
      "        Function.__init__(func, space, code, w_globals, defs_w, closure, name)\n",
      "        return space.wrap(func)\n",
      "\n",
      "    def descr_function_call(self, __args__):\n",
      "        return self.call_args(__args__)\n",
      "\n",
      "    def descr_function_repr(self):\n",
      "        return self.getrepr(self.space, 'function %s' % (self.name,))\n",
      "\n",
      "\n",
      "    # delicate   \n",
      "    _all = {'': None}\n",
      "\n",
      "    def _freeze_(self):\n",
      "        from pypy.interpreter.gateway import BuiltinCode\n",
      "        if isinstance(self.code, BuiltinCode):\n",
      "            identifier = self.code.identifier\n",
      "            if Function._all.get(identifier, self) is not self:\n",
      "                print \"builtin code identifier %s used twice: %s and %s\" % (\n",
      "                    identifier, self, Function._all[identifier])\n",
      "            # we have been seen by other means so rtyping should not choke\n",
      "            # on us\n",
      "            Function._all[identifier] = self\n",
      "        return False\n",
      "\n",
      "    def find(identifier):\n",
      "        return Function._all[identifier]\n",
      "    find = staticmethod(find)\n",
      "\n",
      "    def descr_function__reduce__(self, space):\n",
      "        from pypy.interpreter.gateway import BuiltinCode\n",
      "        from pypy.interpreter.mixedmodule import MixedModule\n",
      "        w_mod    = space.getbuiltinmodule('_pickle_support')\n",
      "        mod      = space.interp_w(MixedModule, w_mod)\n",
      "        code = self.code\n",
      "        if isinstance(code, BuiltinCode):\n",
      "            new_inst = mod.get('builtin_function')\n",
      "            return space.newtuple([new_inst,\n",
      "                                   space.newtuple([space.wrap(code.identifier)])])\n",
      "            \n",
      "        new_inst = mod.get('func_new')\n",
      "        w        = space.wrap\n",
      "        if self.closure is None:\n",
      "            w_closure = space.w_None\n",
      "        else:\n",
      "            w_closure = space.newtuple([w(cell) for cell in self.closure])\n",
      "        if self.w_doc is None:\n",
      "            w_doc = space.w_None\n",
      "        else:\n",
      "            w_doc = self.w_doc\n",
      "        if self.w_func_globals is None:\n",
      "            w_func_globals = space.w_None\n",
      "        else:\n",
      "            w_func_globals = self.w_func_globals\n",
      "        if self.w_func_dict is None:\n",
      "            w_func_dict = space.w_None\n",
      "        else:\n",
      "            w_func_dict = self.w_func_dict\n",
      "\n",
      "        nt = space.newtuple\n",
      "        tup_base = []\n",
      "        tup_state = [\n",
      "            w(self.name),\n",
      "            w_doc,\n",
      "            w(self.code),\n",
      "            w_func_globals,\n",
      "            w_closure,\n",
      "            nt(self.defs_w[:]),\n",
      "            w_func_dict,\n",
      "            self.w_module,\n",
      "        ]\n",
      "        return nt([new_inst, nt(tup_base), nt(tup_state)])\n",
      "\n",
      "    def descr_function__setstate__(self, space, w_args):\n",
      "        from pypy.interpreter.pycode import PyCode\n",
      "        args_w = space.unpackiterable(w_args)\n",
      "        (w_name, w_doc, w_code, w_func_globals, w_closure, w_defs_w,\n",
      "         w_func_dict, w_module) = args_w\n",
      "\n",
      "        self.space = space\n",
      "        self.name = space.str_w(w_name)\n",
      "        self.code = space.interp_w(Code, w_code)\n",
      "        if not space.is_w(w_closure, space.w_None):\n",
      "            from pypy.interpreter.nestedscope import Cell\n",
      "            closure_w = space.unpackiterable(w_closure)\n",
      "            self.closure = [space.interp_w(Cell, w_cell) for w_cell in closure_w]\n",
      "        else:\n",
      "            self.closure = None\n",
      "        if space.is_w(w_doc, space.w_None):\n",
      "            w_doc = None\n",
      "        self.w_doc = w_doc\n",
      "        if space.is_w(w_func_globals, space.w_None):\n",
      "            w_func_globals = None\n",
      "        self.w_func_globals = w_func_globals\n",
      "        if space.is_w(w_func_dict, space.w_None):\n",
      "            w_func_dict = None\n",
      "        self.w_func_dict = w_func_dict\n",
      "        self.defs_w    = space.unpackiterable(w_defs_w)\n",
      "        self.w_module = w_module\n",
      "\n",
      "    def fget_func_defaults(space, self):\n",
      "        values_w = self.defs_w\n",
      "        if not values_w:\n",
      "            return space.w_None\n",
      "        return space.newtuple(values_w[:])\n",
      "\n",
      "    def fset_func_defaults(space, self, w_defaults):\n",
      "        if space.is_w(w_defaults, space.w_None):\n",
      "            self.defs_w = []\n",
      "            return\n",
      "        if not space.is_true( space.isinstance( w_defaults, space.w_tuple ) ):\n",
      "            raise OperationError( space.w_TypeError, space.wrap(\"func_defaults must be set to a tuple object or None\") )\n",
      "        self.defs_w = space.unpackiterable( w_defaults )\n",
      "\n",
      "    def fdel_func_defaults(space, self):\n",
      "        self.defs_w = []\n",
      "\n",
      "    def fget_func_doc(space, self):\n",
      "        if self.w_doc is None:\n",
      "            self.w_doc = self.code.getdocstring(space)\n",
      "        return self.w_doc\n",
      "\n",
      "    def fset_func_doc(space, self, w_doc):\n",
      "        self.w_doc = w_doc\n",
      "\n",
      "    def fget_func_name(space, self):\n",
      "        return space.wrap(self.name)\n",
      "\n",
      "    def fset_func_name(space, self, w_name):\n",
      "        try:\n",
      "            self.name = space.str_w(w_name)\n",
      "        except OperationError, e:\n",
      "            if e.match(space, space.w_TypeError):\n",
      "                raise OperationError(space.w_TypeError,\n",
      "                                     space.wrap(\"func_name must be set \"\n",
      "                                                \"to a string object\"))\n",
      "            raise\n",
      "\n",
      "\n",
      "    def fdel_func_doc(space, self):\n",
      "        self.w_doc = space.w_None\n",
      "\n",
      "    def fget___module__(space, self):\n",
      "        if self.w_module is None:\n",
      "            if self.w_func_globals is not None and not space.is_w(self.w_func_globals, space.w_None):\n",
      "                self.w_module = space.call_method( self.w_func_globals, \"get\", space.wrap(\"__name__\") )\n",
      "            else:\n",
      "                self.w_module = space.w_None\n",
      "        return self.w_module\n",
      "\n",
      "    def fset___module__(space, self, w_module):\n",
      "        self.w_module = w_module\n",
      "\n",
      "    def fdel___module__(space, self):\n",
      "        self.w_module = space.w_None\n",
      "\n",
      "    def fget_func_code(space, self):\n",
      "        from pypy.module.__builtin__.namespace_helpers import SLOTNAME_ALLTOKENS, SLOTNAME_NAMETOKEN, throw_access_exceptions, print_access_exceptions, _currentframe_has_access\n",
      "        from sys import stderr\n",
      "        \n",
      "        if _currentframe_has_access(space, self.creator_nametoken):\n",
      "            return space.wrap(self.code)\n",
      "        else:\n",
      "            if print_access_exceptions:\n",
      "                print >> stderr, \"\\033[1;31mAccess Error:\\033[1;m \" + self.name + \".func_code\"\n",
      "\n",
      "            if throw_access_exceptions:\n",
      "                #SRW TODO: raise\n",
      "                pass\n",
      "            else:\n",
      "                return space.w_None\n",
      "\n",
      "    def fset_func_code(space, self, w_code): #SRW !!!: This could be hazardous for untrusted code to set the code object which is executed by trusted code -- deal with in future\n",
      "        from pypy.interpreter.pycode import PyCode\n",
      "        code = space.interp_w(Code, w_code)\n",
      "        closure_len = 0\n",
      "        if self.closure:\n",
      "            closure_len = len(self.closure)\n",
      "        if isinstance(code, PyCode) and closure_len != len(code.co_freevars):\n",
      "            raise OperationError(space.w_ValueError, space.wrap(\"%s() requires a code object with %s free vars, not %s \" % (self.name, closure_len, len(code.co_freevars))))\n",
      "        self.code = code\n",
      "\n",
      "    def fget_func_closure(space, self):\n",
      "        from pypy.module.__builtin__.namespace_helpers import SLOTNAME_ALLTOKENS, SLOTNAME_NAMETOKEN, throw_access_exceptions, print_access_exceptions, _currentframe_has_access\n",
      "        from sys import stderr\n",
      "\n",
      "        if self.closure is not None:\n",
      "            if _currentframe_has_access(space, self.creator_nametoken):\n",
      "                w_res = space.newtuple( [ space.wrap(i) for i in self.closure ] )\n",
      "            else:\n",
      "                if print_access_exceptions:\n",
      "                    print >> stderr, \"\\033[1;31mAccess Error:\\033[1;m \" + self.name + \".func_closure\"\n",
      "\n",
      "                if throw_access_exceptions:\n",
      "                    #SRW TODO: raise\n",
      "                    pass\n",
      "                else:\n",
      "                    w_res = space.w_None\n",
      "        else:\n",
      "            w_res = space.w_None\n",
      "        return w_res\n",
      "\n",
      "def descr_function_get(space, w_function, w_obj, w_cls=None):\n",
      "    \"\"\"functionobject.__get__(obj[, type]) -> method\"\"\"\n",
      "    # this is not defined as a method on Function because it's generally\n",
      "    # useful logic: w_function can be any callable.  It is used by Method too.\n",
      "    asking_for_bound = (space.is_w(w_cls, space.w_None) or\n",
      "                        not space.is_w(w_obj, space.w_None) or\n",
      "                        space.is_w(w_cls, space.type(space.w_None)))\n",
      "    if asking_for_bound:\n",
      "        return space.wrap(Method(space, w_function, w_obj, w_cls))\n",
      "    else:\n",
      "        return space.wrap(Method(space, w_function, None, w_cls))\n",
      "\n",
      "\n",
      "class Method(Wrappable):\n",
      "    \"\"\"A method is a function bound to a specific instance or class.\"\"\"\n",
      "\n",
      "    def __init__(self, space, w_function, w_instance, w_class):\n",
      "        self.space = space\n",
      "        self.w_function = w_function\n",
      "        self.w_instance = w_instance   # or None\n",
      "        self.w_class = w_class         # possibly space.w_None\n",
      "\n",
      "    def descr_method__new__(space, w_subtype, w_function, w_instance, w_class=None):\n",
      "        if space.is_w( w_instance, space.w_None ):\n",
      "            w_instance = None\n",
      "        method = space.allocate_instance(Method, w_subtype)\n",
      "        Method.__init__(method, space, w_function, w_instance, w_class)\n",
      "        return space.wrap(method)\n",
      "\n",
      "    def __repr__(self):\n",
      "        if self.w_instance:\n",
      "            pre = \"bound\"\n",
      "        else:\n",
      "            pre = \"unbound\"\n",
      "        return \"%s method %s\" % (pre, self.w_function.getname(self.space, '?'))\n",
      "\n",
      "    def call_args(self, args):\n",
      "        space = self.space\n",
      "        if self.w_instance is not None:\n",
      "            # bound method\n",
      "            return space.call_obj_args(self.w_function, self.w_instance, args)\n",
      "\n",
      "        # unbound method\n",
      "        w_firstarg = args.firstarg()\n",
      "        if w_firstarg is not None and (\n",
      "                space.abstract_isinstance_w(w_firstarg, self.w_class)):\n",
      "            pass  # ok\n",
      "        else:\n",
      "            myname = self.getname(space,\"\")\n",
      "            clsdescr = self.w_class.getname(space,\"\")\n",
      "            if clsdescr:\n",
      "                clsdescr+=\" \"\n",
      "            if w_firstarg is None:\n",
      "                instdescr = \"nothing\"\n",
      "            else:\n",
      "                instname = space.abstract_getclass(w_firstarg).getname(space,\"\")\n",
      "                if instname:\n",
      "                    instname += \" \"\n",
      "                instdescr = \"%sinstance\" %instname\n",
      "            msg = (\"unbound method %s() must be called with %s\"\n",
      "                   \"instance as first argument (got %s instead)\")  % (myname, clsdescr, instdescr)\n",
      "            raise OperationError(space.w_TypeError,\n",
      "                                 space.wrap(msg))\n",
      "        return space.call_args(self.w_function, args)\n",
      "\n",
      "    def descr_method_get(self, w_obj, w_cls=None):\n",
      "        space = self.space\n",
      "        if self.w_instance is not None:\n",
      "            return space.wrap(self)    # already bound\n",
      "        else:\n",
      "            # only allow binding to a more specific class than before\n",
      "            if (w_cls is not None and\n",
      "                not space.is_w(w_cls, space.w_None) and\n",
      "                not space.abstract_issubclass_w(w_cls, self.w_class)):\n",
      "                return space.wrap(self)    # subclass test failed\n",
      "            else:\n",
      "                return descr_function_get(space, self.w_function, w_obj, w_cls)\n",
      "\n",
      "    def descr_method_call(self, __args__):\n",
      "        return self.call_args(__args__)\n",
      "\n",
      "    def descr_method_repr(self):\n",
      "        space = self.space\n",
      "        name = self.w_function.getname(self.space, '?')\n",
      "        # XXX do we handle all cases sanely here?\n",
      "        if space.is_w(self.w_class, space.w_None):\n",
      "            w_class = space.type(self.w_instance)\n",
      "        else:\n",
      "            w_class = self.w_class\n",
      "        typename = w_class.getname(self.space, '?')\n",
      "        if self.w_instance is None:\n",
      "            s = \"<unbound method %s.%s>\" % (typename, name)\n",
      "            return space.wrap(s)\n",
      "        else:\n",
      "            objrepr = space.str_w(space.repr(self.w_instance))\n",
      "            info = 'bound method %s.%s of %s' % (typename, name, objrepr)\n",
      "            # info = \"method %s of %s object\" % (name, typename)\n",
      "            return self.w_instance.getrepr(self.space, info)\n",
      "\n",
      "    def descr_method_getattribute(self, w_attr):\n",
      "        space = self.space\n",
      "        if space.str_w(w_attr) != '__doc__':\n",
      "            try:\n",
      "                return space.call_method(space.w_object, '__getattribute__',\n",
      "                                         space.wrap(self), w_attr)\n",
      "            except OperationError, e:\n",
      "                if not e.match(space, space.w_AttributeError):\n",
      "                    raise\n",
      "        # fall-back to the attribute of the underlying 'im_func'\n",
      "        return space.getattr(self.w_function, w_attr)\n",
      "\n",
      "    def descr_method_eq(self, w_other):\n",
      "        space = self.space\n",
      "        other = space.interpclass_w(w_other)\n",
      "        if not isinstance(other, Method):\n",
      "            return space.w_False\n",
      "        if self.w_instance is None:\n",
      "            if other.w_instance is not None:\n",
      "                return space.w_False\n",
      "        else:\n",
      "            if other.w_instance is None:\n",
      "                return space.w_False\n",
      "            if not space.eq_w(self.w_instance, other.w_instance):\n",
      "                return space.w_False\n",
      "        return space.eq(self.w_function, other.w_function)\n",
      "\n",
      "    def descr_method_hash(self):\n",
      "        space = self.space\n",
      "        w_result = space.hash(self.w_function)\n",
      "        if self.w_instance is not None:\n",
      "            w_result = space.xor(w_result, space.hash(self.w_instance))\n",
      "        return w_result\n",
      "\n",
      "    def descr_method__reduce__(self, space):\n",
      "        from pypy.interpreter.mixedmodule import MixedModule\n",
      "        from pypy.interpreter.gateway import BuiltinCode\n",
      "        w_mod    = space.getbuiltinmodule('_pickle_support')\n",
      "        mod      = space.interp_w(MixedModule, w_mod)\n",
      "        new_inst = mod.get('method_new')\n",
      "        w        = space.wrap\n",
      "        w_instance = self.w_instance or space.w_None\n",
      "        function = space.interpclass_w(self.w_function)\n",
      "        if isinstance(function, Function) and isinstance(function.code, BuiltinCode):\n",
      "            new_inst = mod.get('builtin_method_new')\n",
      "            if space.is_w(w_instance, space.w_None):\n",
      "                tup = [self.w_class, space.wrap(function.name)]\n",
      "            else:\n",
      "                tup = [w_instance, space.wrap(function.name)]\n",
      "        elif space.is_w( self.w_class, space.w_None ):\n",
      "            tup = [self.w_function, w_instance]\n",
      "        else:\n",
      "            tup = [self.w_function, w_instance, self.w_class]\n",
      "        return space.newtuple([new_inst, space.newtuple(tup)])\n",
      "        \n",
      "class StaticMethod(Wrappable):\n",
      "    \"\"\"The staticmethod objects.\"\"\"\n",
      "\n",
      "    def __init__(self, w_function):\n",
      "        self.w_function = w_function\n",
      "\n",
      "    def descr_staticmethod_get(self, w_obj, w_cls=None):\n",
      "        \"\"\"staticmethod(x).__get__(obj[, type]) -> x\"\"\"\n",
      "        return self.w_function\n",
      "\n",
      "    def descr_staticmethod__new__(space, w_type, w_function):\n",
      "        return space.wrap(StaticMethod(w_function))\n",
      "\n",
      "class ClassMethod(Wrappable):\n",
      "    \"\"\"The classmethod objects.\"\"\"\n",
      "\n",
      "    def __init__(self, w_function):\n",
      "        self.w_function = w_function\n",
      "\n",
      "    def descr_classmethod_get(self, space, w_obj, w_klass=None):\n",
      "        if space.is_w(w_klass, space.w_None):\n",
      "            w_klass = space.type(w_obj)\n",
      "        return space.wrap(Method(space, self.w_function, w_klass, space.w_None))\n",
      "\n",
      "    def descr_classmethod__new__(space, w_type, w_function):\n",
      "        if not space.is_true(space.callable(w_function)):\n",
      "            typename = space.type(w_function).getname(space, '?')\n",
      "            raise OperationError(space.w_TypeError, space.wrap(\n",
      "                                 \"'%s' object is not callable\" % typename))\n",
      "        return space.wrap(ClassMethod(w_function))\n",
      "\n",
      "class BuiltinFunction(Function):\n",
      "\n",
      "    def __init__(self, func):\n",
      "        assert isinstance(func, Function)\n",
      "        Function.__init__(self, func.space, func.code, func.w_func_globals,\n",
      "                          func.defs_w, func.closure, func.name)\n",
      "        self.w_doc = func.w_doc\n",
      "        self.w_func_dict = func.w_func_dict\n",
      "        self.w_module = func.w_module\n",
      "\n",
      "    def descr_builtinfunction__new__(space, w_subtype, w_func):\n",
      "        func = space.interp_w(Function, w_func)\n",
      "        bltin = space.allocate_instance(BuiltinFunction, w_subtype)\n",
      "        BuiltinFunction.__init__(bltin, func)\n",
      "        return space.wrap(bltin)\n",
      "\n",
      "    def descr_function_repr(self):\n",
      "        return self.space.wrap('<built-in function %s>' % (self.name,))\n",
      "\n",
      "def is_builtin_code(w_func):\n",
      "    from pypy.interpreter.gateway import BuiltinCode\n",
      "    if isinstance(w_func, Method):\n",
      "        w_func = w_func.w_function\n",
      "    if isinstance(w_func, Function):\n",
      "        code = w_func.getcode()\n",
      "    else:\n",
      "        code = None\n",
      "    return isinstance(code, BuiltinCode)\n",
      "\n",
      "\"\"\"Timer class based on the timeit.Timer class, but torch aware.\"\"\"\n",
      "import enum\n",
      "import timeit\n",
      "import textwrap\n",
      "from typing import Any, Callable, Dict, List, NoReturn, Optional, Type, Union\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "from torch.utils.benchmark.utils import common, cpp_jit\n",
      "from torch.utils.benchmark.utils._stubs import TimerClass, TimeitModuleType\n",
      "from torch.utils.benchmark.utils.valgrind_wrapper import timer_interface as valgrind_timer_interface\n",
      "\n",
      "\n",
      "__all__ = [\"Timer\", \"timer\", \"Language\"]\n",
      "\n",
      "\n",
      "if torch.has_cuda and torch.cuda.is_available():\n",
      "    def timer() -> float:\n",
      "        torch.cuda.synchronize()\n",
      "        return timeit.default_timer()\n",
      "else:\n",
      "    timer = timeit.default_timer\n",
      "\n",
      "\n",
      "class Language(enum.Enum):\n",
      "    PYTHON = 0\n",
      "    CPP = 1\n",
      "\n",
      "\n",
      "class CPPTimer:\n",
      "    def __init__(\n",
      "        self,\n",
      "        stmt: str,\n",
      "        setup: str,\n",
      "        timer: Callable[[], float],\n",
      "        globals: Dict[str, Any],\n",
      "    ) -> None:\n",
      "        if timer is not timeit.default_timer:\n",
      "            raise NotImplementedError(\n",
      "                \"PyTorch was built with CUDA and a GPU is present; however \"\n",
      "                \"Timer does not yet support GPU measurements. If your \"\n",
      "                \"code is CPU only, pass `timer=timeit.default_timer` to the \"\n",
      "                \"Timer's constructor to indicate this. (Note that this will \"\n",
      "                \"produce incorrect results if the GPU is in fact used, as \"\n",
      "                \"Timer will not synchronize CUDA.)\"\n",
      "            )\n",
      "\n",
      "        if globals:\n",
      "            raise ValueError(\"C++ timing does not support globals.\")\n",
      "\n",
      "        self._stmt: str = textwrap.dedent(stmt)\n",
      "        self._setup: str = textwrap.dedent(setup)\n",
      "        self._timeit_module: Optional[TimeitModuleType] = None\n",
      "\n",
      "    def timeit(self, number: int) -> float:\n",
      "        if self._timeit_module is None:\n",
      "            self._timeit_module = cpp_jit.compile_timeit_template(\n",
      "                self._stmt,\n",
      "                self._setup,\n",
      "            )\n",
      "\n",
      "        return self._timeit_module.timeit(number)\n",
      "\n",
      "\n",
      "class Timer(object):\n",
      "    \"\"\"Helper class for measuring execution time of PyTorch statements.\n",
      "\n",
      "    For a full tutorial on how to use this class, see:\n",
      "    https://pytorch.org/tutorials/recipes/recipes/benchmark.html\n",
      "\n",
      "    The PyTorch Timer is based on `timeit.Timer` (and in fact uses\n",
      "    `timeit.Timer` internally), but with several key differences:\n",
      "\n",
      "    1) Runtime aware:\n",
      "        Timer will perform warmups (important as some elements of PyTorch are\n",
      "        lazily initialized), set threadpool size so that comparisons are\n",
      "        apples-to-apples, and synchronize asynchronous CUDA functions when\n",
      "        necessary.\n",
      "\n",
      "    2) Focus on replicates:\n",
      "        When measuring code, and particularly complex kernels / models,\n",
      "        run-to-run variation is a significant confounding factor. It is\n",
      "        expected that all measurements should include replicates to quantify\n",
      "        noise and allow median computation, which is more robust than mean.\n",
      "        To that effect, this class deviates from the `timeit` API by\n",
      "        conceptually merging `timeit.Timer.repeat` and `timeit.Timer.autorange`.\n",
      "        (Exact algorithms are discussed in method docstrings.) The `timeit`\n",
      "        method is replicated for cases where an adaptive strategy is not\n",
      "        desired.\n",
      "\n",
      "    3) Optional metadata:\n",
      "        When defining a Timer, one can optionally specify `label`, `sub_label`,\n",
      "        `description`, and `env`. (Defined later) These fields are included in\n",
      "        the representation of result object and by the `Compare` class to group\n",
      "        and display results for comparison.\n",
      "\n",
      "    4) Instruction counts\n",
      "        In addition to wall times, Timer can run a statement under Callgrind\n",
      "        and report instructions executed.\n",
      "\n",
      "    Directly analogous to `timeit.Timer` constructor arguments:\n",
      "\n",
      "        `stmt`, `setup`, `timer`, `globals`\n",
      "\n",
      "    PyTorch Timer specific constructor arguments:\n",
      "\n",
      "        `label`, `sub_label`, `description`, `env`, `num_threads`\n",
      "\n",
      "    Args:\n",
      "        stmt: Code snippet to be run in a loop and timed.\n",
      "\n",
      "        setup: Optional setup code. Used to define variables used in `stmt`\n",
      "\n",
      "        timer:\n",
      "            Callable which returns the current time. If PyTorch was built\n",
      "            without CUDA or there is no GPU present, this defaults to\n",
      "            `timeit.default_timer`; otherwise it will synchronize CUDA before\n",
      "            measuring the time.\n",
      "\n",
      "        globals:\n",
      "            A dict which defines the global variables when `stmt` is being\n",
      "            executed. This is the other method for providing variables which\n",
      "            `stmt` needs.\n",
      "\n",
      "        label:\n",
      "            String which summarizes `stmt`. For instance, if `stmt` is\n",
      "            \"torch.nn.functional.relu(torch.add(x, 1, out=out))\"\n",
      "            one might set label to \"ReLU(x + 1)\" to improve readability.\n",
      "\n",
      "        sub_label:\n",
      "            Provide supplemental information to disambiguate measurements\n",
      "            with identical stmt or label. For instance, in our example\n",
      "            above sub_label might be \"float\" or \"int\", so that it is easy\n",
      "            to differentiate:\n",
      "            \"ReLU(x + 1): (float)\"\n",
      "\n",
      "            \"ReLU(x + 1): (int)\"\n",
      "            when printing Measurements or summarizing using `Compare`.\n",
      "\n",
      "        description:\n",
      "            String to distinguish measurements with identical label and\n",
      "            sub_label. The principal use of `description` is to signal to\n",
      "            `Compare` the columns of data. For instance one might set it\n",
      "            based on the input size  to create a table of the form: ::\n",
      "\n",
      "                                        | n=1 | n=4 | ...\n",
      "                                        ------------- ...\n",
      "                ReLU(x + 1): (float)    | ... | ... | ...\n",
      "                ReLU(x + 1): (int)      | ... | ... | ...\n",
      "\n",
      "\n",
      "            using `Compare`. It is also included when printing a Measurement.\n",
      "\n",
      "        env:\n",
      "            This tag indicates that otherwise identical tasks were run in\n",
      "            different environments, and are therefore not equivilent, for\n",
      "            instance when A/B testing a change to a kernel. `Compare` will\n",
      "            treat Measurements with different `env` specification as distinct\n",
      "            when merging replicate runs.\n",
      "\n",
      "        num_threads:\n",
      "            The size of the PyTorch threadpool when executing `stmt`. Single\n",
      "            threaded performace is important as both a key inference workload\n",
      "            and a good indicator of intrinsic algorithmic efficiency, so the\n",
      "            default is set to one. This is in contrast to the default PyTorch\n",
      "            threadpool size which tries to utilize all cores.\n",
      "    \"\"\"\n",
      "\n",
      "    _timer_cls: Type[TimerClass] = timeit.Timer\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        stmt: str = \"pass\",\n",
      "        setup: str = \"pass\",\n",
      "        timer: Callable[[], float] = timer,\n",
      "        globals: Optional[Dict[str, Any]] = None,\n",
      "        label: Optional[str] = None,\n",
      "        sub_label: Optional[str] = None,\n",
      "        description: Optional[str] = None,\n",
      "        env: Optional[str] = None,\n",
      "        num_threads: int = 1,\n",
      "        language: Union[Language, str] = Language.PYTHON,\n",
      "    ):\n",
      "        if not isinstance(stmt, str):\n",
      "            raise ValueError(\"Currently only a `str` stmt is supported.\")\n",
      "\n",
      "        # We copy `globals` to prevent mutations from leaking.\n",
      "        # (For instance, `eval` adds the `__builtins__` key)\n",
      "        self._globals = dict(globals or {})\n",
      "        if language in (Language.PYTHON, \"py\", \"python\"):\n",
      "            # Include `torch` if not specified as a convenience feature.\n",
      "            self._globals.setdefault(\"torch\", torch)\n",
      "            self._language: Language = Language.PYTHON\n",
      "\n",
      "        elif language in (Language.CPP, \"cpp\", \"c++\"):\n",
      "            assert self._timer_cls is timeit.Timer, \"_timer_cls has already been swapped.\"\n",
      "            self._timer_cls = CPPTimer\n",
      "            setup = (\"\" if setup == \"pass\" else setup)\n",
      "            self._language = Language.CPP\n",
      "\n",
      "        else:\n",
      "            raise ValueError(f\"Invalid language `{language}`.\")\n",
      "\n",
      "        # Convenience adjustment so that multi-line code snippets defined in\n",
      "        # functions do not IndentationError (Python) or look odd (C++). The\n",
      "        # leading newline removal is for the initial newline that appears when\n",
      "        # defining block strings. For instance:\n",
      "        #   textwrap.dedent(\"\"\"\n",
      "        #     print(\"This is a stmt\")\n",
      "        #   \"\"\")\n",
      "        # produces '\\nprint(\"This is a stmt\")\\n'.\n",
      "        #\n",
      "        # Stripping this down to 'print(\"This is a stmt\")' doesn't change\n",
      "        # what gets executed, but it makes __repr__'s nicer.\n",
      "        stmt = textwrap.dedent(stmt)\n",
      "        stmt = (stmt[1:] if stmt and stmt[0] == \"\\n\" else stmt).rstrip()\n",
      "        setup = textwrap.dedent(setup)\n",
      "        setup = (setup[1:] if setup and setup[0] == \"\\n\" else setup).rstrip()\n",
      "\n",
      "        self._timer = self._timer_cls(\n",
      "            stmt=stmt,\n",
      "            setup=setup,\n",
      "            timer=timer,\n",
      "            globals=valgrind_timer_interface.CopyIfCallgrind.unwrap_all(self._globals),\n",
      "        )\n",
      "        self._task_spec = common.TaskSpec(\n",
      "            stmt=stmt,\n",
      "            setup=setup,\n",
      "            label=label,\n",
      "            sub_label=sub_label,\n",
      "            description=description,\n",
      "            env=env,\n",
      "            num_threads=num_threads,\n",
      "        )\n",
      "\n",
      "    def timeit(self, number: int = 1000000) -> common.Measurement:\n",
      "        \"\"\"Mirrors the semantics of timeit.Timer.timeit().\n",
      "\n",
      "        Execute the main statement (`stmt`) `number` times.\n",
      "        https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\n",
      "        \"\"\"\n",
      "        with common.set_torch_threads(self._task_spec.num_threads):\n",
      "            # Warmup\n",
      "            self._timer.timeit(number=max(int(number // 100), 1))\n",
      "\n",
      "            return common.Measurement(\n",
      "                number_per_run=number,\n",
      "                raw_times=[self._timer.timeit(number=number)],\n",
      "                task_spec=self._task_spec\n",
      "            )\n",
      "\n",
      "    def repeat(self, repeat: int = -1, number: int = -1) -> None:\n",
      "        raise NotImplementedError(\"See `Timer.blocked_autorange.`\")\n",
      "\n",
      "    def autorange(self, callback: Optional[Callable[[int, float], NoReturn]] = None) -> None:\n",
      "        raise NotImplementedError(\"See `Timer.blocked_autorange.`\")\n",
      "\n",
      "    def _threaded_measurement_loop(\n",
      "        self,\n",
      "        number: int,\n",
      "        time_hook: Callable[[], float],\n",
      "        stop_hook: Callable[[List[float]], bool],\n",
      "        min_run_time: float,\n",
      "        max_run_time: Optional[float] = None,\n",
      "        callback: Optional[Callable[[int, float], NoReturn]] = None\n",
      "    ) -> List[float]:\n",
      "        total_time = 0.0\n",
      "        can_stop = False\n",
      "        times: List[float] = []\n",
      "        with common.set_torch_threads(self._task_spec.num_threads):\n",
      "            while (total_time < min_run_time) or (not can_stop):\n",
      "                time_spent = time_hook()\n",
      "                times.append(time_spent)\n",
      "                total_time += time_spent\n",
      "                if callback:\n",
      "                    callback(number, time_spent)\n",
      "                can_stop = stop_hook(times)\n",
      "                if max_run_time and total_time > max_run_time:\n",
      "                    break\n",
      "        return times\n",
      "\n",
      "    def _estimate_block_size(self, min_run_time: float) -> int:\n",
      "        with common.set_torch_threads(self._task_spec.num_threads):\n",
      "            # Estimate the block size needed for measurement to be negligible\n",
      "            # compared to the inner loop. This also serves as a warmup.\n",
      "            overhead = np.median([self._timer.timeit(0) for _ in range(5)])\n",
      "            number = 1\n",
      "            while True:\n",
      "                time_taken = self._timer.timeit(number)\n",
      "                relative_overhead = overhead / time_taken\n",
      "                if relative_overhead <= 1e-4 and time_taken >= min_run_time / 1000:\n",
      "                    break\n",
      "                if time_taken > min_run_time:\n",
      "                    break\n",
      "                number *= 10\n",
      "        return number\n",
      "\n",
      "    def adaptive_autorange(\n",
      "            self,\n",
      "            threshold: float = 0.1,\n",
      "            *,\n",
      "            min_run_time: float = 0.01,\n",
      "            max_run_time: float = 10.0,\n",
      "            callback: Optional[Callable[[int, float], NoReturn]] = None,\n",
      "    ) -> common.Measurement:\n",
      "        number = self._estimate_block_size(min_run_time=0.05)\n",
      "\n",
      "        def time_hook() -> float:\n",
      "            return self._timer.timeit(number)\n",
      "\n",
      "        def stop_hook(times: List[float]) -> bool:\n",
      "            if len(times) > 3:\n",
      "                return common.Measurement(\n",
      "                    number_per_run=number,\n",
      "                    raw_times=times,\n",
      "                    task_spec=self._task_spec\n",
      "                ).meets_confidence(threshold=threshold)\n",
      "            return False\n",
      "        times = self._threaded_measurement_loop(\n",
      "            number, time_hook, stop_hook, min_run_time, max_run_time, callback=callback)\n",
      "\n",
      "        return common.Measurement(\n",
      "            number_per_run=number,\n",
      "            raw_times=times,\n",
      "            task_spec=self._task_spec\n",
      "        )\n",
      "\n",
      "    def blocked_autorange(\n",
      "        self,\n",
      "        callback: Optional[Callable[[int, float], NoReturn]] = None,\n",
      "        min_run_time: float = 0.2,\n",
      "    ) -> common.Measurement:\n",
      "        \"\"\"Measure many replicates while keeping timer overhead to a minimum.\n",
      "\n",
      "        At a high level, blocked_autorange executes the following pseudo-code::\n",
      "\n",
      "            `setup`\n",
      "\n",
      "            total_time = 0\n",
      "            while total_time < min_run_time\n",
      "                start = timer()\n",
      "                for _ in range(block_size):\n",
      "                    `stmt`\n",
      "                total_time += (timer() - start)\n",
      "\n",
      "        Note the variable `block_size` in the inner loop. The choice of block\n",
      "        size is important to measurement quality, and must balance two\n",
      "        competing objectives:\n",
      "\n",
      "            1) A small block size results in more replicates and generally\n",
      "               better statistics.\n",
      "\n",
      "            2) A large block size better amortizes the cost of `timer`\n",
      "               invocation, and results in a less biased measurement. This is\n",
      "               important because CUDA syncronization time is non-trivial\n",
      "               (order single to low double digit microseconds) and would\n",
      "               otherwise bias the measurement.\n",
      "\n",
      "        blocked_autorange sets block_size by running a warmup period,\n",
      "        increasing block size until timer overhead is less than 0.1% of\n",
      "        the overall computation. This value is then used for the main\n",
      "        measurement loop.\n",
      "\n",
      "        Returns:\n",
      "            A `Measurement` object that contains measured runtimes and\n",
      "            repetition counts, and can be used to compute statistics.\n",
      "            (mean, median, etc.)\n",
      "        \"\"\"\n",
      "        number = self._estimate_block_size(min_run_time)\n",
      "\n",
      "        def time_hook() -> float:\n",
      "            return self._timer.timeit(number)\n",
      "\n",
      "        def stop_hook(times: List[float]) -> bool:\n",
      "            return True\n",
      "\n",
      "        times = self._threaded_measurement_loop(\n",
      "            number, time_hook, stop_hook,\n",
      "            min_run_time=min_run_time,\n",
      "            callback=callback)\n",
      "\n",
      "        return common.Measurement(\n",
      "            number_per_run=number,\n",
      "            raw_times=times,\n",
      "            task_spec=self._task_spec\n",
      "        )\n",
      "\n",
      "    def collect_callgrind(\n",
      "        self,\n",
      "        number: int = 100,\n",
      "        collect_baseline: bool = True\n",
      "    ) -> valgrind_timer_interface.CallgrindStats:\n",
      "        \"\"\"Collect instruction counts using Callgrind.\n",
      "\n",
      "        Unlike wall times, instruction counts are deterministic\n",
      "        (modulo non-determinism in the program itself and small amounts of\n",
      "        jitter from the Python interpreter.) This makes them ideal for detailed\n",
      "        performance analysis. This method runs `stmt` in a separate process\n",
      "        so that Valgrind can instrument the program. Performance is severely\n",
      "        degraded due to the instrumentation, howevever this is ameliorated by\n",
      "        the fact that a small number of iterations is generally sufficient to\n",
      "        obtain good measurements.\n",
      "\n",
      "        In order to to use this method `valgrind`, `callgrind_control`, and\n",
      "        `callgrind_annotate` must be installed.\n",
      "\n",
      "        Because there is a process boundary between the caller (this process)\n",
      "        and the `stmt` execution, `globals` cannot contain arbitrary in-memory\n",
      "        data structures. (Unlike timing methods) Instead, globals are\n",
      "        restricted to builtins, `nn.Modules`'s, and TorchScripted functions/modules\n",
      "        to reduce the surprise factor from serialization and subsequent\n",
      "        deserialization. The `GlobalsBridge` class provides more detail on this\n",
      "        subject. Take particular care with nn.Modules: they rely on pickle and\n",
      "        you may need to add an import to `setup` for them to transfer properly.\n",
      "\n",
      "        By default, a profile for an empty statement will be collected and\n",
      "        cached to indicate how many instructions are from the Python loop which\n",
      "        drives `stmt`.\n",
      "\n",
      "        Returns:\n",
      "            A `CallgrindStats` object which provides instruction counts and\n",
      "            some basic facilities for analyzing and manipulating results.\n",
      "        \"\"\"\n",
      "        if not isinstance(self._task_spec.stmt, str):\n",
      "            raise ValueError(\"`collect_callgrind` currently only supports string `stmt`\")\n",
      "\n",
      "        # Check that the statement is valid. It doesn't guarantee success, but it's much\n",
      "        # simpler and quicker to raise an exception for a faulty `stmt` or `setup` in\n",
      "        # the parent process rather than the valgrind subprocess.\n",
      "        self._timer.timeit(1)\n",
      "        is_python = (self._language == Language.PYTHON)\n",
      "        assert is_python or not self._globals\n",
      "        return valgrind_timer_interface.wrapper_singleton().collect_callgrind(\n",
      "            task_spec=self._task_spec,\n",
      "            globals=self._globals,\n",
      "            number=number,\n",
      "            collect_baseline=collect_baseline and is_python,\n",
      "            is_python=is_python)\n",
      "\n",
      "from Ranger.src.Range.Cut import Cut\n",
      "\n",
      "class Range(object):\n",
      "    \"\"\"\n",
      "    Class used to represent a range along some 1-D domain. The range\n",
      "    is represented by 2 cutpoints can can be unbounded by specifying an\n",
      "    aboveAll or belowAll Cut.\n",
      "    \"\"\"\n",
      "    def __init__(self, lowerCut, upperCut):\n",
      "        \"\"\" Instantiates a Range\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        lowerCut : Cut object\n",
      "            Specifies the lower cut for the range\n",
      "        upperCut : Cut object\n",
      "            Specifies the upper cut for the range\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If bound(s) are not Cut objects or lower > upper\n",
      "        \"\"\"\n",
      "        if not all(map(lambda x: isinstance(x, Cut), (lowerCut,upperCut))):\n",
      "            raise ValueError(\"Bounds must be Cut objects\")\n",
      "        elif lowerCut > upperCut:\n",
      "            raise ValueError(\"Lower bound cannot be greater than upper bound\")\n",
      "        self.lowerCut = lowerCut\n",
      "        self.upperCut = upperCut\n",
      "    def __repr__(self):\n",
      "        try:\n",
      "            return_str = '[' if self.isLowerBoundClosed() else '('\n",
      "        except TypeError:\n",
      "            return_str = '('\n",
      "        return_str += (str(self.lowerCut.point) if not self.lowerCut.belowAll \\\n",
      "          else '')\n",
      "        return_str += ' , '\n",
      "        return_str += (str(self.upperCut.point) if not self.upperCut.aboveAll \\\n",
      "          else '')\n",
      "        try:\n",
      "            return_str += ']' if self.isUpperBoundClosed() else ')'\n",
      "        except TypeError:\n",
      "            return_str += ')'\n",
      "        return return_str\n",
      "    def __hash__(self):\n",
      "        return (hash(self.lowerCut)*31 + hash(self.upperCut))\n",
      "    def __eq__(self, other):\n",
      "        if not isinstance(other, Range):\n",
      "            return False\n",
      "        else:\n",
      "            return ((self.lowerCut == other.lowerCut) and \\\n",
      "                    (self.upperCut == other.upperCut))\n",
      "    def __ne__(self, other):\n",
      "        return not self.__eq__(other)\n",
      "    def contains(self, val):\n",
      "        \"\"\" Returns true if the range contains the value\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        val : Comparable object of the appropriate type for the range\n",
      "            Value to query whether in the range\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If the value type not compatible with cutpoint type\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        True if the range contains the value\n",
      "        \"\"\"\n",
      "        return (self.lowerCut < val and \\\n",
      "                self.upperCut > val)\n",
      "    def containsAll(self, vals):\n",
      "        \"\"\" Returns True if the range contains all values in some\n",
      "        iterable\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        vals : Iterable of comparable object of appropriate type for range\n",
      "            Values to query against the range\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If there is a value type not compatible with the cutpoint type\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        True if the range contains all values\n",
      "        \"\"\"\n",
      "        for val in vals:\n",
      "            if not self.contains(val):\n",
      "                return False\n",
      "        return True\n",
      "    def getDistanceFromPoint(self, val, distFunc = lambda x1, x2: abs(x1-x2)):\n",
      "        \"\"\" Returns the minimum distance of a Range from a Point, returning 0\n",
      "        if there is an overlap.\n",
      "\n",
      "        Note that both upper and lower bounds must be closed for this function\n",
      "        to work\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        val : comparable, compatible with cutpoint type\n",
      "            The value of the point where the distance is desired\n",
      "        distFunc : callable\n",
      "            Function that calculates the distance between two points in the\n",
      "            domain of the Range\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the upper and/or lower bounds of this Range are not closed\n",
      "            or if the distFunc not compatible with the type\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        The minimum distance between the Range and the Point. Returns 0\n",
      "        if there is an overlap\n",
      "        \"\"\"\n",
      "        if not all((self.isLowerBoundClosed(), self.isUpperBoundClosed())):\n",
      "            raise TypeError(\"Range is not closed\")\n",
      "        if self.contains(val):\n",
      "            return 0.\n",
      "        else:\n",
      "            return min(distFunc(self.lowerCut.point, val),\n",
      "                       distFunc(self.upperCut.point, val))\n",
      "    def getDistanceFromRange(self, other, distFunc = lambda x1,x2: abs(x1-x2)):\n",
      "        \"\"\" Returns the minimum distance of a Range from another Range, returning\n",
      "        0 if there is any overlap\n",
      "\n",
      "        Note that both Ranges must be closed for this function to work\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : Range, compatible with this Range's domain\n",
      "            The Range to compare to\n",
      "        distFunc : callable\n",
      "            Function that calculates the distance between two points in the\n",
      "            domain of the Range\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the upper and/or lower bounds of this Range are not closed\n",
      "            or if the distFunc not compatible with the type\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        Minimum distance between the ranges        \n",
      "        \"\"\"\n",
      "        if not isinstance(other, Range):\n",
      "            raise TypeError(\"other is not a Range\")\n",
      "        if not all((self.isLowerBoundClosed(), self.isUpperBoundClosed(),\n",
      "                    other.isLowerBoundClosed(), other.isUpperBoundClosed())):\n",
      "            raise TypeError(\"Not all Ranges closed\")\n",
      "        if self.isConnected(other):\n",
      "            return 0.\n",
      "        else:\n",
      "            return min(distFunc(self.lowerCut.point, other.upperCut.point),\n",
      "                       distFunc(other.lowerCut.point, self.upperCut.point))\n",
      "    def hasLowerBound(self):\n",
      "        \"\"\" Returns True if the range has a lower endpoint (not unbounded\n",
      "        at the lower end)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        True if the range has a lower endpoint\n",
      "        \"\"\"\n",
      "        return (not self.lowerCut.belowAll)\n",
      "    def hasUpperBound(self):\n",
      "        \"\"\" Returns True if the range has an upper endpoint (not unbounded\n",
      "        at the upper end)\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        True if the range has an upper endpoint\n",
      "        \"\"\"\n",
      "        return (not self.upperCut.aboveAll)\n",
      "    def lowerEndpoint(self):\n",
      "        \"\"\" Returns the lower endpoint of the range if it exists. Otherwise\n",
      "        raises a TypeError\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the range is unbounded below\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The lower endpoint of the range\n",
      "        \"\"\"\n",
      "        if self.lowerCut.point is None:\n",
      "            raise TypeError(\"Range unbounded below\")\n",
      "        else:\n",
      "            return self.lowerCut.point\n",
      "    def upperEndpoint(self):\n",
      "        \"\"\" Returns the upper endpoint of the range if it exists. Otherwise\n",
      "        raises a TypeError\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the range is unbounded above\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The upper endpoint of the range\n",
      "        \"\"\"\n",
      "        if self.upperCut.point is None:\n",
      "            raise TypeError(\"Range unbounded above\")\n",
      "        else:\n",
      "            return self.upperCut.point\n",
      "    def isLowerBoundClosed(self):\n",
      "        \"\"\" Returns whether the lower bound is closed (if there is a\n",
      "        lower bound)\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the range is unbounded below\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        True if the lower bound is closed\n",
      "        \"\"\"\n",
      "        if self.lowerCut.point is None:\n",
      "            raise TypeError(\"Range unbounded below\")\n",
      "        else:\n",
      "            return self.lowerCut.below\n",
      "    def isUpperBoundClosed(self):\n",
      "        \"\"\" Returns whether the upper bound is closed (if there is an\n",
      "        upper bound)\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        TypeError\n",
      "            If the range is unbounded above\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        True if the upper bound is closed\n",
      "        \"\"\"\n",
      "        if self.upperCut.point is None:\n",
      "            raise TypeError(\"Range unbounded above\")\n",
      "        else:\n",
      "            return (not self.upperCut.below)\n",
      "    def isEmpty(self):\n",
      "        \"\"\" Returns True if the range is of form [v, v) or (v, v]\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "\n",
      "        True if the range is of the form [v,v) or (v,v]\n",
      "        \"\"\"\n",
      "        return self.lowerCut == self.upperCut\n",
      "    def encloses(self, other):\n",
      "        \"\"\" Returns True if the bounds of the other range do not extend\n",
      "        outside the bounds of this range\n",
      "\n",
      "        Examples:\n",
      "            [3,6] encloses [4,5]\n",
      "            (3,6) encloses (3,6)\n",
      "            [3,6] encloses [4,4]\n",
      "            (3,6] does not enclose [3,6]\n",
      "            [4,5] does not enclose (3,6)\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : A Range\n",
      "            The range to compare to\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If object passed in is not a Range\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        True if the bounds of the other range do not extend outside\n",
      "        the bounds of this range\n",
      "        \"\"\"\n",
      "        if not isinstance(other, Range):\n",
      "            raise ValueError(\"Range required\")\n",
      "        return ((self.lowerCut <= other.lowerCut) and \\\n",
      "            (self.upperCut >= other.upperCut))\n",
      "    def isConnected(self, other):\n",
      "        \"\"\" Returns True if there is a (possibly empty) range that is\n",
      "        enclosed by both this range and other\n",
      "\n",
      "        Examples:\n",
      "            [2,4] and [5,7] are not connected\n",
      "            [2,4] and [3,5] are connected\n",
      "            [2,4] and [4,6] are connected\n",
      "            [3,5] and (5,10) are connected\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        other : A range\n",
      "            The range to compare to\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If object passed in is not a Range\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        True if there is a (possibly empty) range that is enclosed by\n",
      "        both this range and other\n",
      "        \"\"\"\n",
      "        if not isinstance(other, Range):\n",
      "            raise ValueError(\"Range required\")\n",
      "        return ((self.lowerCut <= other.upperCut) and \\\n",
      "                (other.lowerCut <= self.upperCut))\n",
      "    def intersection(self, other):\n",
      "        \"\"\" Returns the maximal range enclosed by both this range and the\n",
      "        other range, if such a range exists\n",
      "\n",
      "        Examples:\n",
      "            Intersection of [1,5] and [3,7] is [3,5]\n",
      "            Intersection of [1,5] and [5,7] is [5,5]\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : A range\n",
      "            The range to compare to\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If object passed in is not a Range or if there is no intersection\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        The intersection range\n",
      "        \"\"\"\n",
      "        if not isinstance(other, Range):\n",
      "            raise ValueError(\"Range required\")\n",
      "        if ((self.lowerCut >= other.lowerCut) and \\\n",
      "            (self.upperCut <= other.upperCut)):\n",
      "            return Range(self.lowerCut, self.upperCut)\n",
      "        elif ((self.lowerCut <= other.lowerCut) and \\\n",
      "              (self.upperCut >= other.upperCut)):\n",
      "            return Range(other.lowerCut, other.upperCut)\n",
      "        else:\n",
      "            newLower = self.lowerCut if (self.lowerCut >= other.lowerCut) else \\\n",
      "                                         other.lowerCut\n",
      "            newUpper = self.upperCut if (self.upperCut <= other.upperCut) else \\\n",
      "                                         other.upperCut\n",
      "            return Range(newLower, newUpper)\n",
      "    def span(self, other):\n",
      "        \"\"\" Returns the minimal range that encloses both this range and\n",
      "        the other. Note that if the input ranges are not connected, the span can\n",
      "        contain values that are not contained within either input range\n",
      "\n",
      "        Examples:\n",
      "            Span of [1,3] and [5,7] is [1,7]\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        other : A range\n",
      "            A range to span with\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If object passed in is not a Range or if there is no intersection\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        The minimal range enclosing both with and the other range\n",
      "        \"\"\"\n",
      "        if ((self.lowerCut <= other.lowerCut) and \\\n",
      "            (self.upperCut >= other.upperCut)):\n",
      "            return Range(self.lowerCut, self.upperCut)\n",
      "        elif ((self.lowerCut >= other.lowerCut) and \\\n",
      "              (self.upperCut <= other.upperCut)):\n",
      "            return Range(other.lowerCut, other.upperCut)\n",
      "        else:\n",
      "            newLower = self.lowerCut if (self.lowerCut <= other.lowerCut) else \\\n",
      "                other.lowerCut\n",
      "            newUpper = self.upperCut if (self.upperCut >= other.upperCut) else \\\n",
      "                other.upperCut\n",
      "            return Range(newLower, newUpper)\n",
      "    ##################\n",
      "    # Static methods #\n",
      "    ##################\n",
      "    @staticmethod\n",
      "    def _validate_cutpoints(*pts):\n",
      "        if not all(map(lambda x: (hasattr(x, \"__lt__\") and \\\n",
      "                hasattr(x, \"__gt__\")) or hasattr(x,'__cmp__'), pts)):\n",
      "            raise ValueError(\"Cutpoint type(s) not comparable\")\n",
      "        if len(pts) == 2:\n",
      "            if not (issubclass(type(pts[0]),type(pts[1])) or \\\n",
      "              issubclass(type(pts[1]),type(pts[0]))):\n",
      "                raise ValueError(\"Cutpoints are not compatible\")\n",
      "        return True\n",
      "    @staticmethod\n",
      "    def _get_type(*pts):\n",
      "        if len(pts) == 1: return type(pts[0])\n",
      "        elif len(pts) == 2:\n",
      "            if issubclass(type(pts[0]),type(pts[1])):\n",
      "                return type(pts[1])\n",
      "            elif issubclass(type(pts[1]),type(pts[0])):\n",
      "                return type(pts[0])\n",
      "            else:\n",
      "                raise ValueError(\"Cutpoints are not compatible\")\n",
      "    @staticmethod\n",
      "    def closed(lower, upper):\n",
      "        \"\"\" Creates a range including the endpoints (i.e. [lower, upper])\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        lower : comparable, of same type as or subclass of upper type\n",
      "            The lower bound\n",
      "        upper : comparable, of same type as or subclass of lower type\n",
      "            The upper bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type(s) are not comparable or compatible\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A Range object [lower, upper]\n",
      "        \"\"\"\n",
      "        # Ensure cutpoints are of compatible, appropriate types\n",
      "        Range._validate_cutpoints(lower, upper)\n",
      "        theType = Range._get_type(lower,upper)\n",
      "        return Range(Cut.belowValue(lower, theType=theType),\n",
      "                     Cut.aboveValue(upper, theType=theType))\n",
      "    @staticmethod\n",
      "    def closedOpen(lower, upper):\n",
      "        \"\"\" Creates a range including the lower endpoint (i.e. [lower, upper))\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        lower : comparable, of same type as or subclass of upper type\n",
      "            The lower bound\n",
      "        upper : comparable, of same type as or subclass of lower type\n",
      "            The upper bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type(s) are not comparable or compatible\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A Range object [lower, upper)\n",
      "        \"\"\"\n",
      "        # Ensure cutpoints are of compatible, appropriate types\n",
      "        Range._validate_cutpoints(lower, upper)\n",
      "        theType = Range._get_type(lower,upper)\n",
      "        return Range(Cut.belowValue(lower, theType=theType),\n",
      "                     Cut.belowValue(upper, theType=theType))\n",
      "    @staticmethod\n",
      "    def openClosed(lower, upper):\n",
      "        \"\"\" Creates a range including the upper (i.e. (lower, upper])\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        lower : comparable, of same type as or subclass of upper type\n",
      "            The lower bound\n",
      "        upper : comparable, of same type as or subclass of lower type\n",
      "            The upper bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type(s) are not comparable or compatible\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A Range object (lower, upper]\n",
      "        \"\"\"\n",
      "        # Ensure cutpoints are of compatible, appropriate types\n",
      "        Range._validate_cutpoints(lower, upper)\n",
      "        theType = Range._get_type(lower,upper)\n",
      "        return Range(Cut.aboveValue(lower, theType=theType),\n",
      "                     Cut.aboveValue(upper, theType=theType))\n",
      "    @staticmethod\n",
      "    def open(lower, upper):\n",
      "        \"\"\" Creates a range excluding the endpoints (i.e. (lower, upper))\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        lower : comparable, of same type as or subclass of upper type\n",
      "            The lower bound\n",
      "        upper : comparable, of same type as or subclass of lower type\n",
      "            The upper bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type(s) are not comparable or compatible or if constructing\n",
      "            a range of type (v,v), which is invalid\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        A Range object (lower, upper)\n",
      "        \"\"\"\n",
      "        # Ensure cutpoints are of compatible, appropriate types\n",
      "        Range._validate_cutpoints(lower, upper)\n",
      "        theType = Range._get_type(lower,upper)\n",
      "        if lower == upper:\n",
      "            raise TypeError(\"Range of type (v,v) is not valid\")        \n",
      "        return Range(Cut.aboveValue(lower, theType=theType),\n",
      "                     Cut.belowValue(upper, theType=theType))\n",
      "    @staticmethod\n",
      "    def lessThan(val):\n",
      "        \"\"\" Makes range including all values less than some value\n",
      "        (i.e. (-inf, val))\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        val : comparable\n",
      "            The upper bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type not comparable\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        A Range object (-inf, val)\n",
      "        \"\"\"\n",
      "        Range._validate_cutpoints(val)\n",
      "        theType = Range._get_type(val)\n",
      "        return Range(Cut.belowAll(theType=theType),\n",
      "                     Cut.belowValue(val, theType=theType))\n",
      "    @staticmethod\n",
      "    def atMost(val):\n",
      "        \"\"\" Makes range including all values less than or equal to\n",
      "        some value (i.e. (-inf, val])\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        val : comparable\n",
      "            The upper bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type not comparable\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        A Range object (-inf, val]\n",
      "        \"\"\"\n",
      "        Range._validate_cutpoints(val)\n",
      "        theType = Range._get_type(val)\n",
      "        return Range(Cut.belowAll(theType=theType),\n",
      "                     Cut.aboveValue(val, theType=theType))\n",
      "    @staticmethod\n",
      "    def greaterThan(val):\n",
      "        \"\"\" Makes range including all values greater than\n",
      "        some value (i.e. (val, inf])\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        val : comparable\n",
      "            The lower bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type not comparable\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        A Range object (val, inf)\n",
      "        \"\"\"\n",
      "        Range._validate_cutpoints(val)\n",
      "        theType = Range._get_type(val)\n",
      "        return Range(Cut.aboveValue(val,theType=theType),\n",
      "                     Cut.aboveAll(theType=theType))\n",
      "    @staticmethod\n",
      "    def atLeast(val):\n",
      "        \"\"\" Makes range including all values greater than or equal to\n",
      "        some value (i.e. [val, inf))\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        val : comparable\n",
      "            The lower bound\n",
      "\n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            If type not comparable\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        A Range object [val, inf)\n",
      "        \"\"\"\n",
      "        Range._validate_cutpoints(val)\n",
      "        theType = Range._get_type(val)\n",
      "        return Range(Cut.belowValue(val, theType=theType),\n",
      "                     Cut.aboveAll(theType=theType))\n",
      "\n",
      "from pyrelational.data.data_manager import GenericDataManager\n",
      "\n",
      "from selfdrive.car import dbc_dict\n",
      "from cereal import car\n",
      "Ecu = car.CarParams.Ecu\n",
      "\n",
      "\n",
      "class CarControllerParams:\n",
      "  ANGLE_DELTA_BP = [0., 5., 15.]\n",
      "  ANGLE_DELTA_V = [5., .8, .15]     # windup limit\n",
      "  ANGLE_DELTA_VU = [5., 3.5, 0.4]   # unwind limit\n",
      "  LKAS_MAX_TORQUE = 1               # A value of 1 is easy to overpower\n",
      "  STEER_THRESHOLD = 1.0\n",
      "\n",
      "class CAR:\n",
      "  XTRAIL = \"NISSAN X-TRAIL 2017\"\n",
      "  LEAF = \"NISSAN LEAF 2018\"\n",
      "  # Leaf with ADAS ECU found behind instrument cluster instead of glovebox\n",
      "  # Currently the only known difference between them is the inverted seatbelt signal.\n",
      "  LEAF_IC = \"NISSAN LEAF 2018 Instrument Cluster\"\n",
      "  ROGUE = \"NISSAN ROGUE 2019\"\n",
      "  ALTIMA = \"NISSAN ALTIMA 2020\"\n",
      "\n",
      "\n",
      "FINGERPRINTS = {\n",
      "  CAR.XTRAIL: [\n",
      "    {\n",
      "      2: 5, 42: 6, 346: 6, 347: 5, 348: 8, 349: 7, 361: 8, 386: 8, 389: 8, 397: 8, 398: 8, 403: 8, 520: 2, 523: 6, 548: 8, 645: 8, 658: 8, 665: 8, 666: 8, 674: 2, 682: 8, 683: 8, 689: 8, 723: 8, 758: 3, 768: 2, 783: 3, 851: 8, 855: 8, 1041: 8, 1055: 2, 1104: 4, 1105: 6, 1107: 4, 1108: 8, 1111: 4, 1227: 8, 1228: 8, 1247: 4, 1266: 8, 1273: 7, 1342: 1, 1376: 6, 1401: 8, 1474: 2, 1497: 3, 1821: 8, 1823: 8, 1837: 8, 2015: 8, 2016: 8, 2024: 8\n",
      "    },\n",
      "    {\n",
      "      2: 5, 42: 6, 346: 6, 347: 5, 348: 8, 349: 7, 361: 8, 386: 8, 389: 8, 397: 8, 398: 8, 403: 8, 520: 2, 523: 6, 527: 1, 548: 8, 637: 4, 645: 8, 658: 8, 665: 8, 666: 8, 674: 2, 682: 8, 683: 8, 689: 8, 723: 8, 758: 3, 768: 6, 783: 3, 851: 8, 855: 8, 1041: 8, 1055: 2, 1104: 4, 1105: 6, 1107: 4, 1108: 8, 1111: 4, 1227: 8, 1228: 8, 1247: 4, 1266: 8, 1273: 7, 1342: 1, 1376: 6, 1401: 8, 1474: 8, 1497: 3, 1534: 6, 1792: 8, 1821: 8, 1823: 8, 1837: 8, 1872: 8, 1937: 8, 1953: 8, 1968: 8, 2015: 8, 2016: 8, 2024: 8\n",
      "    },\n",
      "  ],\n",
      "  CAR.LEAF: [\n",
      "    {\n",
      "      2: 5, 42: 6, 264: 3, 361: 8, 372: 8, 384: 8, 389: 8, 403: 8, 459: 7, 460: 4, 470: 8, 520: 1, 569: 8, 581: 8, 634: 7, 640: 8, 644: 8, 645: 8, 646: 5, 658: 8, 682: 8, 683: 8, 689: 8, 724: 6, 758: 3, 761: 2, 783: 3, 852: 8, 853: 8, 856: 8, 861: 8, 944: 1, 976: 6, 1008: 7, 1011: 7, 1057: 3, 1227: 8, 1228: 8, 1261: 5, 1342: 1, 1354: 8, 1361: 8, 1459: 8, 1477: 8, 1497: 3, 1549: 8, 1573: 6, 1821: 8, 1837: 8, 1856: 8, 1859: 8, 1861: 8, 1864: 8, 1874: 8, 1888: 8, 1891: 8, 1893: 8, 1906: 8, 1947: 8, 1949: 8, 1979: 8, 1981: 8, 2016: 8, 2017: 8, 2021: 8, 643: 5, 1792: 8, 1872: 8, 1937: 8, 1953: 8, 1968: 8, 1988: 8, 2000: 8, 2001: 8, 2004: 8, 2005: 8, 2015: 8\n",
      "    },\n",
      "    # 2020 Leaf SV Plus\n",
      "    {\n",
      "      2: 5, 42: 8, 264: 3, 361: 8, 372: 8, 384: 8, 389: 8, 403: 8, 459: 7, 460: 4, 470: 8, 520: 1, 569: 8, 581: 8, 634: 7, 640: 8, 643: 5, 644: 8, 645: 8, 646: 5, 658: 8, 682: 8, 683: 8, 689: 8, 724: 6, 758: 3, 761: 2, 772: 8, 773: 6, 774: 7, 775: 8, 776: 6, 777: 7, 778: 6, 783: 3, 852: 8, 853: 8, 856: 8, 861: 8, 943: 8, 944: 1, 976: 6, 1008: 7, 1009: 8, 1010: 8, 1011: 7, 1012: 8, 1013: 8, 1019: 8, 1020: 8, 1021: 8, 1022: 8, 1057: 3, 1227: 8, 1228: 8, 1261: 5, 1342: 1, 1354: 8, 1361: 8, 1402: 8, 1459: 8, 1477: 8, 1497: 3, 1549: 8, 1573: 6, 1821: 8, 1837: 8\n",
      "    },\n",
      "  ],\n",
      "  CAR.LEAF_IC: [\n",
      "    {\n",
      "      2: 5, 42: 6, 264: 3, 282: 8, 361: 8, 372: 8, 384: 8, 389: 8, 403: 8, 459: 7, 460: 4, 470: 8, 520: 1, 569: 8, 581: 8, 634: 7, 640: 8, 643: 5, 644: 8, 645: 8, 646: 5, 658: 8, 682: 8, 683: 8, 689: 8, 756: 5, 758: 3, 761: 2, 783: 3, 830: 2, 852: 8, 853: 8, 856: 8, 861: 8, 943: 8, 944: 1, 1001: 6, 1057: 3, 1227: 8, 1228: 8, 1229: 8, 1342: 1, 1354: 8, 1361: 8, 1459: 8, 1477: 8, 1497: 3, 1514: 6, 1549: 8, 1573: 6, 1792: 8, 1821: 8, 1822: 8, 1837: 8, 1838: 8, 1872: 8, 1937: 8, 1953: 8, 1968: 8, 1988: 8, 2000: 8, 2001: 8, 2004: 8, 2005: 8, 2015: 8, 2016: 8, 2017: 8\n",
      "    },\n",
      "  ],\n",
      "  CAR.ROGUE: [\n",
      "    {\n",
      "      2: 5, 42: 6, 346: 6, 347: 5, 348: 8, 349: 7, 361: 8, 386: 8, 389: 8, 397: 8, 398: 8, 403: 8, 520: 2, 523: 6, 548: 8, 634: 7, 643: 5, 645: 8, 658: 8, 665: 8, 666: 8, 674: 2, 682: 8, 683: 8, 689: 8, 723: 8, 758: 3, 772: 8, 773: 6, 774: 7, 775: 8, 776: 6, 777: 7, 778: 6, 783: 3, 851: 8, 855: 8, 1041: 8, 1042: 8, 1055: 2, 1104: 4, 1105: 6, 1107: 4, 1108: 8, 1110: 7, 1111: 7, 1227: 8, 1228: 8, 1247: 4, 1266: 8, 1273: 7, 1342: 1, 1376: 6, 1401: 8, 1474: 2, 1497: 3, 1534: 7, 1792: 8, 1821: 8, 1823: 8, 1837: 8, 1839: 8, 1872: 8, 1937: 8, 1953: 8, 1968: 8, 1988: 8, 2000: 8, 2001: 8, 2004: 8, 2005: 8, 2015: 8, 2016: 8, 2017: 8, 2024: 8, 2025: 8\n",
      "    },\n",
      "  ],\n",
      "  CAR.ALTIMA: [\n",
      "    {\n",
      "      2: 5, 42: 6, 346: 6, 347: 5, 348: 8, 349: 7, 361: 8, 386: 8, 389: 8, 397: 8, 398: 8, 403: 8, 438: 8, 451: 8, 517: 8, 520: 2, 522: 8, 523: 6, 539: 8, 541: 7, 542: 8, 543: 8, 544: 8, 545: 8, 546: 8, 547: 8, 548: 8, 570: 8, 576: 8, 577: 8, 582: 8, 583: 8, 584: 8, 586: 8, 587: 8, 588: 8, 589: 8, 590: 8, 591: 8, 592: 8, 600: 8, 601: 8, 610: 8, 611: 8, 612: 8, 614: 8, 615: 8, 616: 8, 617: 8, 622: 8, 623: 8, 634: 7, 638: 8, 645: 8, 648: 5, 654: 6, 658: 8, 659: 8, 660: 8, 661: 8, 665: 8, 666: 8, 674: 2, 675: 8, 676: 8, 682: 8, 683: 8, 684: 8, 685: 8, 686: 8, 687: 8, 689: 8, 690: 8, 703: 8, 708: 7, 709: 7, 711: 7, 712: 7, 713: 7, 714: 8, 715: 8, 716: 8, 717: 7, 718: 7, 719: 7, 720: 7, 723: 8, 726: 7, 727: 7, 728: 7, 735: 8, 746: 8, 748: 6, 749: 6, 750: 8, 758: 3, 772: 8, 773: 6, 774: 7, 775: 8, 776: 6, 777: 7, 778: 6, 779: 7, 781: 7, 782: 7, 783: 3, 851: 8, 855: 5, 1001: 6, 1041: 8, 1042: 8, 1055: 3, 1100: 7, 1104: 4, 1105: 6, 1107: 4, 1108: 8, 1110: 7, 1111: 7, 1144: 7, 1145: 7, 1227: 8, 1228: 8, 1229: 8, 1232: 8, 1247: 4, 1258: 8, 1259: 8, 1266: 8, 1273: 7, 1306: 1, 1314: 8, 1323: 8, 1324: 8, 1342: 1, 1376: 8, 1401: 8, 1454: 8, 1497: 3, 1514: 6, 1526: 8, 1527: 5, 1792: 8, 1821: 8, 1823: 8, 1837: 8, 1872: 8, 1937: 8, 1953: 8, 1968: 8, 1988: 8, 2000: 8, 2001: 8, 2004: 8, 2005: 8, 2015: 8, 2016: 8, 2017: 8, 2024: 8, 2025: 8\n",
      "    },\n",
      "  ]\n",
      "}\n",
      "\n",
      "FW_VERSIONS = {\n",
      "  CAR.ALTIMA: {\n",
      "    (Ecu.fwdCamera, 0x707, None): [\n",
      "      b'284N86CA1D',\n",
      "    ],\n",
      "    (Ecu.eps, 0x742, None): [\n",
      "      b'6CA2B\\xa9A\\x02\\x02G8A89P90D6A\\x00\\x00\\x01\\x80',\n",
      "    ],\n",
      "    (Ecu.engine, 0x7e0, None): [\n",
      "      b'237109HE2B',\n",
      "    ],\n",
      "    (Ecu.gateway, 0x18dad0f1, None): [\n",
      "      b'284U29HE0A',\n",
      "    ],\n",
      "  },\n",
      "  CAR.LEAF_IC: {\n",
      "    (Ecu.fwdCamera, 0x707, None): [\n",
      "      b'5SH1BDB\\x04\\x18\\x00\\x00\\x00\\x00\\x00_-?\\x04\\x91\\xf2\\x00\\x00\\x00\\x80',\n",
      "      b'5SK0ADB\\x04\\x18\\x00\\x00\\x00\\x00\\x00_(5\\x07\\x9aQ\\x00\\x00\\x00\\x80',\n",
      "    ],\n",
      "    (Ecu.esp, 0x740, None): [\n",
      "      b'476605SH1D',\n",
      "      b'476605SK2A',\n",
      "    ],\n",
      "    (Ecu.eps, 0x742, None): [\n",
      "      b'5SH2A\\x99A\\x05\\x02N123F\\x15\\x81\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "      b'5SK3A\\x99A\\x05\\x02N123F\\x15u\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "    ],\n",
      "    (Ecu.gateway, 0x18dad0f1, None): [\n",
      "      b'284U25SH3A',\n",
      "      b'284U25SK2D',\n",
      "    ],\n",
      "  },\n",
      "  CAR.XTRAIL: {\n",
      "    (Ecu.fwdCamera, 0x707, None): [\n",
      "      b'284N86FR2A',\n",
      "    ],\n",
      "    (Ecu.esp, 0x740, None): [\n",
      "      b'6FU1BD\\x11\\x02\\x00\\x02e\\x95e\\x80iX#\\x01\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "      b'6FU0AD\\x11\\x02\\x00\\x02e\\x95e\\x80iQ#\\x01\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "    ],\n",
      "    (Ecu.eps, 0x742, None): [\n",
      "      b'6FP2A\\x99A\\x05\\x02N123F\\x18\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "    ],\n",
      "    (Ecu.combinationMeter, 0x743, None): [\n",
      "      b'6FR2A\\x18B\\x05\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "    ],\n",
      "    (Ecu.engine, 0x7e0, None): [\n",
      "      b'6FU9B\\xa0A\\x06\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "      b'6FR9A\\xa0A\\x06\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80',\n",
      "    ],\n",
      "    (Ecu.gateway, 0x18dad0f1, None): [\n",
      "      b'284U26FR0E',\n",
      "    ],\n",
      "  },\n",
      "}\n",
      "\n",
      "DBC = {\n",
      "  CAR.XTRAIL: dbc_dict('nissan_x_trail_2017', None),\n",
      "  CAR.LEAF: dbc_dict('nissan_leaf_2018', None),\n",
      "  CAR.LEAF_IC: dbc_dict('nissan_leaf_2018', None),\n",
      "  CAR.ROGUE: dbc_dict('nissan_x_trail_2017', None),\n",
      "  CAR.ALTIMA: dbc_dict('nissan_x_trail_2017', None),\n",
      "}\n",
      "\n",
      "from .core import Core, Settings\n",
      "\n",
      "\n",
      "class Download(Core):\n",
      "\n",
      "    host = 'https://artifacts.elastic.co/downloads/beats/elastic-agent/{endpoint}'\n",
      "    endpoint = Settings.download_endpoint\n",
      "    kwargs = {\n",
      "        'stream': True\n",
      "    }\n",
      "\n",
      "    def parse_response(self, response):\n",
      "        self.__logger.debug('Saving file to download path: {}'.format(Settings.download_path))\n",
      "        with open(Settings.download_path, 'wb+') as f:\n",
      "            for chunk in response.raw.stream(1024, decode_content=False):\n",
      "                if chunk:\n",
      "                    f.write(chunk)\n",
      "        self.__logger.debug('File saved successfully')\n",
      "\n",
      "\n",
      "from .single_stage import SingleStageDetector\n",
      "from ..registry import DETECTORS\n",
      "from mmdet.core import bbox2result\n",
      "import torch.nn as nn\n",
      "import torch\n",
      "from .. import builder\n",
      "import numpy as np\n",
      "import cv2\n",
      "from mmdet.core import bbox2roi, bbox2result, build_assigner, build_sampler\n",
      "\n",
      "@DETECTORS.register_module\n",
      "class CSP(SingleStageDetector):\n",
      "\n",
      "    def __init__(self,\n",
      "                 backbone,\n",
      "                 neck,\n",
      "                 bbox_head,\n",
      "                 refine_roi_extractor=None,\n",
      "                 refine_head=None,\n",
      "                 train_cfg=None,\n",
      "                 test_cfg=None,\n",
      "                 pretrained=None,\n",
      "                 detached=True,\n",
      "                 return_feature_maps=False):\n",
      "        super(CSP, self).__init__(backbone, neck, bbox_head, train_cfg,\n",
      "                                   test_cfg, pretrained)\n",
      "        if refine_head is not None:\n",
      "            self.refine_roi_extractor = builder.build_roi_extractor(\n",
      "                refine_roi_extractor)\n",
      "            self.refine_head = builder.build_head(refine_head)\n",
      "        self.return_feature_maps = return_feature_maps\n",
      "        self.train_cfg = train_cfg\n",
      "        self.test_cfg = test_cfg\n",
      "        self.detached = detached\n",
      "\n",
      "    def show_input_debug(self, img, classification_maps, scale_maps, offset_maps):\n",
      "        img_numpy = img.cpu().numpy().copy()[0]\n",
      "        # img_numpy = np.transpose(img_numpy, [1, 2, 0]) * [58.395, 57.12, 57.375] + [123.675, 116.28, 103.53]\n",
      "        img_numpy = np.transpose(img_numpy, [1, 2, 0]) + [102.9801, 115.9465, 122.7717]\n",
      "        img_numpy = img_numpy[:, :, ::-1]\n",
      "        img_numpy = img_numpy.astype(np.uint8)\n",
      "        strides = [8, 16, 32, 64, 128]\n",
      "        img_nows = []\n",
      "        for i, stride in enumerate(strides):\n",
      "            img_now = img_numpy.copy()\n",
      "            # cls_numpy = classification_maps[0][i].cpu().numpy().copy()[0][2]\n",
      "            cls_numpy = classification_maps[0][i].cpu().numpy().copy()[0][:80]\n",
      "            scale_numpy = scale_maps[0][i].cpu().numpy().copy()[0][0] * stride\n",
      "            offset_numpy = offset_maps[0][i].cpu().numpy().copy()[0][:2]\n",
      "            cs, ys, xs = cls_numpy.nonzero()\n",
      "            print(len(ys))\n",
      "            for c, x, y in zip(cs, xs, ys):\n",
      "                cv2.imshow(str(c), classification_maps[0][i].cpu().numpy().copy()[0][80+c])\n",
      "                realx = x\n",
      "                realy = y\n",
      "                height = scale_numpy[y, x]\n",
      "                realy = realy + 0.5 + offset_numpy[0][y, x]\n",
      "                realx = realx + 0.5 + offset_numpy[1][y, x]\n",
      "                realy = realy * stride\n",
      "                realx = realx * stride\n",
      "                top_y = int(realy - height/2)\n",
      "                top_x = int(realx)\n",
      "                down_y = int(realy + height/2)\n",
      "                down_x = int(realx)\n",
      "                top_left = (int(top_x - height * 0.1), int(top_y))\n",
      "                down_right = (int(down_x + height * 0.1), down_y)\n",
      "                cv2.rectangle(img_now, top_left, down_right, (255, 255, 5*int(c)), 2)\n",
      "                img_nows.append(img_now)\n",
      "            cv2.imshow(str(i) +'img', img_now)\n",
      "        cv2.waitKey(0)\n",
      "\n",
      "    def show_input_debug_caltech(self, img, classification_maps, scale_maps, offset_maps):\n",
      "        for j in range(img.shape[0]):\n",
      "            img_numpy = img.cpu().numpy().copy()[j]\n",
      "            img_numpy = np.transpose(img_numpy, [1, 2, 0]) * [58.395, 57.12, 57.375] + [123.675, 116.28, 103.53]\n",
      "            img_numpy = img_numpy[:, :, ::-1]\n",
      "            img_numpy = img_numpy.astype(np.uint8)\n",
      "            strides = [4]\n",
      "            img_nows = []\n",
      "            for i, stride in enumerate(strides):\n",
      "                img_now = img_numpy.copy()\n",
      "                cls_numpy = classification_maps[j][i].cpu().numpy().copy()[0][2]\n",
      "                ignore_numpy = classification_maps[j][i].cpu().numpy().copy()[0][1]\n",
      "                cv2.imshow('ignore', ignore_numpy)\n",
      "                scale_numpy = scale_maps[j][i].cpu().numpy().copy()[0][0] * stride\n",
      "                offset_numpy = offset_maps[j][i].cpu().numpy().copy()[0][:2]\n",
      "                ys, xs = cls_numpy.nonzero()\n",
      "                print(len(ys))\n",
      "                for x, y in zip(xs, ys):\n",
      "                    # cv2.imshow(str(c), classification_maps[j][i].cpu().numpy().copy()[0][c])\n",
      "                    realx = x\n",
      "                    realy = y\n",
      "                    height = scale_numpy[y, x]\n",
      "                    realy = realy + 0.5 + offset_numpy[0][y, x]\n",
      "                    realx = realx + 0.5 + offset_numpy[1][y, x]\n",
      "                    realy = realy * stride\n",
      "                    realx = realx * stride\n",
      "                    top_y = int(realy - height/2)\n",
      "                    top_x = int(realx)\n",
      "                    down_y = int(realy + height/2)\n",
      "                    down_x = int(realx)\n",
      "                    top_left = (int(top_x - height * 0.1), int(top_y))\n",
      "                    down_right = (int(down_x + height * 0.1), down_y)\n",
      "                    cv2.rectangle(img_now, top_left, down_right, (255, 255, 125), 2)\n",
      "                    img_nows.append(img_now)\n",
      "                cv2.imshow(str(i) +'img', img_now)\n",
      "            cv2.waitKey(0)\n",
      "\n",
      "    def show_input_debug_head(self, img, classification_maps, scale_maps, offset_maps):\n",
      "        for j in range(img.shape[0]):\n",
      "            img_numpy = img.cpu().numpy().copy()[j]\n",
      "            img_numpy = np.transpose(img_numpy, [1, 2, 0]) * [58.395, 57.12, 57.375] + [123.675, 116.28, 103.53]\n",
      "            img_numpy = img_numpy[:, :, ::-1]\n",
      "            img_numpy = img_numpy.astype(np.uint8)\n",
      "            strides = [4]\n",
      "            img_nows = []\n",
      "            for i, stride in enumerate(strides):\n",
      "                img_now = img_numpy.copy()\n",
      "                cls_numpy = classification_maps[j][i].cpu().numpy().copy()[0][2]\n",
      "                ignore_numpy = classification_maps[j][i].cpu().numpy().copy()[0][1]\n",
      "                cv2.imshow('ignore', ignore_numpy)\n",
      "                scale_numpy = scale_maps[j][i].exp().cpu().numpy().copy()[0][0] * stride\n",
      "                offset_numpy = offset_maps[j][i].cpu().numpy().copy()[0][:2]\n",
      "                ys, xs = cls_numpy.nonzero()\n",
      "                for x, y in zip(xs, ys):\n",
      "                    # cv2.imshow(str(c), classification_maps[j][i].cpu().numpy().copy()[0][c])\n",
      "                    realx = x\n",
      "                    realy = y\n",
      "                    height = scale_numpy[y, x]\n",
      "                    realy = realy + 0.5 + offset_numpy[0][y, x]\n",
      "                    realx = realx + 0.5 + offset_numpy[1][y, x]\n",
      "                    realy = realy * stride\n",
      "                    realx = realx * stride\n",
      "                    top_y = int(realy)\n",
      "                    top_x = int(realx)\n",
      "                    down_y = int(realy + height)\n",
      "                    down_x = int(realx)\n",
      "                    top_left = (int(top_x - height * 0.41/2), int(top_y))\n",
      "                    down_right = (int(down_x + height * 0.41/2), down_y)\n",
      "                    cv2.rectangle(img_now, top_left, down_right, (255, 255, 125), 2)\n",
      "                    img_nows.append(img_now)\n",
      "                cv2.imshow(str(i) +'img', img_now)\n",
      "            cv2.waitKey(0)\n",
      "\n",
      "    def show_mot_input_debug(self, img, classification_maps, scale_maps, offset_maps):\n",
      "        for j in range(img.shape[0]):\n",
      "            img_numpy = img.cpu().numpy().copy()[j]\n",
      "            img_numpy = np.transpose(img_numpy, [1, 2, 0]) * [58.395, 57.12, 57.375] + [123.675, 116.28, 103.53]\n",
      "            # img_numpy = np.transpose(img_numpy, [1, 2, 0]) + [102.9801, 115.9465, 122.7717]\n",
      "            img_numpy = img_numpy[:, :, ::-1]\n",
      "            img_numpy = img_numpy.astype(np.uint8)\n",
      "            strides = [4]\n",
      "            img_nows = []\n",
      "            for i, stride in enumerate(strides):\n",
      "                img_now = img_numpy.copy()\n",
      "                # cls_numpy = classification_maps[0][i].cpu().numpy().copy()[0][2]\n",
      "                cls_numpy = classification_maps[j][i].cpu().numpy().copy()[0][2]\n",
      "                instance_numpy = classification_maps[j][i].cpu().numpy().copy()[0][3]\n",
      "                scale_numpy = scale_maps[j][i].cpu().numpy().copy()[0][0] * stride\n",
      "                offset_numpy = offset_maps[j][i].cpu().numpy().copy()[0][:2]\n",
      "                ys, xs = cls_numpy.nonzero()\n",
      "                for x, y in zip(xs, ys):\n",
      "                    c=0\n",
      "                    cv2.imshow(str(c), classification_maps[j][i].cpu().numpy().copy()[0][2])\n",
      "                    realx = x\n",
      "                    realy = y\n",
      "                    height = scale_numpy[y, x]\n",
      "                    realy = realy + 0.5 + offset_numpy[0][y, x]\n",
      "                    realx = realx + 0.5 + offset_numpy[1][y, x]\n",
      "                    realy = realy * stride\n",
      "                    realx = realx * stride\n",
      "                    top_y = int(realy - height/2)\n",
      "                    top_x = int(realx)\n",
      "                    down_y = int(realy + height/2)\n",
      "                    down_x = int(realx)\n",
      "                    top_left = (int(top_x - height * 0.1), int(top_y))\n",
      "                    down_right = (int(down_x + height * 0.1), down_y)\n",
      "                    cv2.rectangle(img_now, top_left, down_right, (255, 255, 5*int(c)), 2)\n",
      "                    instance = instance_numpy[y, x]\n",
      "                    cv2.putText(img_now, str(instance), top_left, cv2.FONT_HERSHEY_COMPLEX, 1, 255)\n",
      "                    img_nows.append(img_now)\n",
      "                cv2.imshow(str(i) +'img', img_now)\n",
      "            cv2.waitKey(0)\n",
      "\n",
      "    @property\n",
      "    def refine(self):\n",
      "        return hasattr(self, 'refine_head') and self.refine_head is not None\n",
      "\n",
      "    def forward_train(self,\n",
      "                      img,\n",
      "                      img_metas,\n",
      "                      gt_bboxes,\n",
      "                      gt_labels,\n",
      "                      gt_bboxes_ignore=None,\n",
      "                      classification_maps=None,\n",
      "                      scale_maps=None,\n",
      "                      offset_maps=None):\n",
      "        # for tracking data which batch is produced by dataset instead of data loader\n",
      "        if type(img) == list:\n",
      "            img=img[0]\n",
      "            img_metas=img_metas[0]\n",
      "            gt_bboxes=gt_bboxes[0]\n",
      "            gt_labels=gt_labels[0]\n",
      "            gt_bboxes_ignore = gt_bboxes_ignore[0]\n",
      "            classification_maps = classification_maps[0]\n",
      "            scale_maps = scale_maps[0]\n",
      "            offset_maps = offset_maps[0]\n",
      "\n",
      "        losses = dict()\n",
      "        x = self.extract_feat(img)\n",
      "        # self.show_input_debug(img, classification_maps, scale_maps, offset_maps)\n",
      "        # self.show_input_debug_caltech(img, classification_maps, scale_maps, offset_maps)\n",
      "        # self.show_mot_input_debug(img, classification_maps, scale_maps, offset_maps)\n",
      "        # self.show_input_debug_head(img, classification_maps, scale_maps, offset_maps)\n",
      "\n",
      "        outs = self.bbox_head(x)\n",
      "        loss_inputs = outs + (gt_bboxes, gt_labels, classification_maps, scale_maps, offset_maps, img_metas, self.train_cfg.csp_head if self.refine else self.train_cfg)\n",
      "        losses_bbox = self.bbox_head.loss(\n",
      "            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n",
      "        losses.update(losses_bbox)\n",
      "                \n",
      "        if self.refine:\n",
      "            if self.detached:\n",
      "                x = tuple([i.detach() for i in x])\n",
      "            bbox_inputs = outs + (img_metas, self.train_cfg.csp_head, False)\n",
      "            bbox_list = self.bbox_head.get_bboxes(*bbox_inputs, no_strides=False)  # no_strides to not upscale yet\n",
      "            \n",
      "            bbox_list = [\n",
      "                bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)[0]\n",
      "                for det_bboxes, det_labels in bbox_list\n",
      "            ]\n",
      "\n",
      "            bbox_assigner = build_assigner(self.train_cfg.rcnn.assigner)\n",
      "            bbox_sampler = build_sampler(\n",
      "                self.train_cfg.rcnn.sampler, context=self)\n",
      "            num_imgs = img.size(0)\n",
      "            if gt_bboxes_ignore is None:\n",
      "                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n",
      "            sampling_results = []\n",
      "            \n",
      "            for i in range(num_imgs):\n",
      "                if bbox_list[i].shape[0] == 0 or gt_bboxes[i].shape[0] == 0:\n",
      "                    continue\n",
      "                bbox = torch.tensor(bbox_list[i]).float().cuda()\n",
      "                assign_result = bbox_assigner.assign(\n",
      "                    bbox, gt_bboxes[i], gt_bboxes_ignore[i],\n",
      "                    gt_labels[i])\n",
      "                sampling_result = bbox_sampler.sample(\n",
      "                    assign_result,\n",
      "                    bbox,\n",
      "                    gt_bboxes[i],\n",
      "                    gt_labels[i])\n",
      "                sampling_results.append(sampling_result)\n",
      "\n",
      "            samp_list = [res.bboxes for res in sampling_results]\n",
      "            if len(samp_list) == 0:\n",
      "                losses.update(dict(loss_refine_cls=torch.tensor(0).float().cuda(), acc=torch.tensor(0).float().cuda()))\n",
      "                return losses\n",
      "            rois = bbox2roi(samp_list).float()\n",
      "            if self.refine_head.loss_opinion is not None:\n",
      "                pred_scores = torch.cat([torch.tensor(bbox[:, 4]).float().cuda() for bbox in bbox_list], dim=0)\n",
      "                pred_rois = bbox2roi([torch.tensor(bbox).float().cuda() for bbox in bbox_list])\n",
      "                pred_feats = self.refine_roi_extractor(\n",
      "                    x, pred_rois)\n",
      "                pred_scores_refine = self.refine_head(pred_feats)\n",
      "                loss_opinion = self.refine_head.compute_opinion_loss(pred_scores, pred_scores_refine)\n",
      "                losses.update(loss_opinion)\n",
      "            bbox_feats = self.refine_roi_extractor(\n",
      "                x, rois)\n",
      "            cls_score = self.refine_head(bbox_feats)\n",
      "            bbox_targets = self.refine_head.get_target(\n",
      "                sampling_results, gt_bboxes, gt_labels, self.train_cfg.rcnn)\n",
      "            loss_refine = self.refine_head.loss(cls_score,\n",
      "                                            *bbox_targets[:2])\n",
      "            losses.update(dict(loss_refine_cls=loss_refine[\"loss_cls\"], distL1=loss_refine[\"dist\"]))\n",
      "\n",
      "        return losses\n",
      "\n",
      "    def simple_test_accuracy(self, img, img_meta):\n",
      "        gts = img_meta[0][\"gts\"]\n",
      "        x = self.extract_feat(img)\n",
      "        if self.detached:\n",
      "            x = (x[0].detach(),)\n",
      "\n",
      "        rois = bbox2roi(gts)\n",
      "        if rois.shape[0] == 0:\n",
      "            return 0, 0\n",
      "\n",
      "        roi_feats = self.refine_roi_extractor(\n",
      "            x, rois)\n",
      "        cls_score = self.refine_head.get_scores(roi_feats)\n",
      "\n",
      "        return (cls_score > 0.5).float().sum(), rois.size(0)\n",
      "\n",
      "    def simple_test(self, img, img_meta, rescale=False, return_id=False):\n",
      "        x = self.extract_feat(img)\n",
      "        outs = self.bbox_head(x)\n",
      "        bbox_inputs = outs + (img_meta, self.test_cfg.csp_head if self.refine else self.test_cfg, False) # TODO://Handle rescalling\n",
      "        if self.return_feature_maps:\n",
      "            return self.bbox_head.get_bboxes_features(*bbox_inputs)\n",
      "        bbox_list = self.bbox_head.get_bboxes(*bbox_inputs, no_strides=False)\n",
      "        im_scale = img_meta[0][\"scale_factor\"]\n",
      "        if \"id\" in img_meta[0]:\n",
      "            img_id = img_meta[0][\"id\"]\n",
      "        else:\n",
      "            img_id = 0\n",
      "        if self.refine:\n",
      "            if self.detached:\n",
      "                x = (x[0].detach(),)\n",
      "            bbox_list = [\n",
      "                bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)[0]\n",
      "                for det_bboxes, det_labels in bbox_list\n",
      "            ]\n",
      "            refine_cfg = self.test_cfg.get('rcnn', None)\n",
      "            bbox_list = [torch.tensor(bbox).float().cuda() for bbox in bbox_list]\n",
      "            rois = bbox2roi(bbox_list)\n",
      "            bbox_list = [bbox/im_scale for bbox in bbox_list]\n",
      "            if rois.shape[0] == 0:\n",
      "                cls_score = None\n",
      "            else:\n",
      "                roi_feats = self.refine_roi_extractor(\n",
      "                    x, rois)\n",
      "                cls_score = self.refine_head.get_scores(roi_feats)\n",
      "\n",
      "            res_buffer = []\n",
      "            if cls_score is not None:\n",
      "                if refine_cfg is not None:\n",
      "                    res_buffer = self.refine_head.suppress_boxes(rois, cls_score, img_meta, cfg=refine_cfg)\n",
      "                else:\n",
      "                    res_buffer = self.refine_head.combine_scores(bbox_list, cls_score)\n",
      "            if return_id:\n",
      "                return res_buffer, img_id\n",
      "            return res_buffer\n",
      "\n",
      "        bbox_results = [\n",
      "            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)\n",
      "            for det_bboxes, det_labels in bbox_list\n",
      "        ]\n",
      "        if return_id:\n",
      "            return bbox_results[0], img_id\n",
      "        return bbox_results[0]\n",
      "\n",
      "    def foward_features(self, features):\n",
      "        bbox_list = self.bbox_head.get_bboxes(*features)\n",
      "        bbox_results = [\n",
      "            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)\n",
      "            for det_bboxes, det_labels in bbox_list\n",
      "        ]\n",
      "        return bbox_results[0]\n",
      "\n",
      "# Simple demo of sending and recieving data with the RFM95 LoRa radio.\n",
      "# Author: Tony DiCola\n",
      "import board\n",
      "import busio\n",
      "import digitalio\n",
      "\n",
      "import adafruit_rfm9x\n",
      "\n",
      "\n",
      "# Define radio parameters.\n",
      "RADIO_FREQ_MHZ = 915.0  # Frequency of the radio in Mhz. Must match your\n",
      "# module! Can be a value like 915.0, 433.0, etc.\n",
      "\n",
      "# Define pins connected to the chip, use these if wiring up the breakout according to the guide:\n",
      "CS = digitalio.DigitalInOut(board.D5)\n",
      "RESET = digitalio.DigitalInOut(board.D6)\n",
      "# Or uncomment and instead use these if using a Feather M0 RFM9x board and the appropriate\n",
      "# CircuitPython build:\n",
      "# CS = digitalio.DigitalInOut(board.RFM9X_CS)\n",
      "# RESET = digitalio.DigitalInOut(board.RFM9X_RST)\n",
      "\n",
      "# Define the onboard LED\n",
      "LED = digitalio.DigitalInOut(board.D13)\n",
      "LED.direction = digitalio.Direction.OUTPUT\n",
      "\n",
      "# Initialize SPI bus.\n",
      "spi = busio.SPI(board.SCK, MOSI=board.MOSI, MISO=board.MISO)\n",
      "\n",
      "# Initialze RFM radio\n",
      "rfm9x = adafruit_rfm9x.RFM9x(spi, CS, RESET, RADIO_FREQ_MHZ)\n",
      "\n",
      "# Note that the radio is configured in LoRa mode so you can't control sync\n",
      "# word, encryption, frequency deviation, or other settings!\n",
      "\n",
      "# You can however adjust the transmit power (in dB).  The default is 13 dB but\n",
      "# high power radios like the RFM95 can go up to 23 dB:\n",
      "rfm9x.tx_power = 23\n",
      "\n",
      "# Send a packet.  Note you can only send a packet up to 252 bytes in length.\n",
      "# This is a limitation of the radio packet size, so if you need to send larger\n",
      "# amounts of data you will need to break it into smaller send calls.  Each send\n",
      "# call will wait for the previous one to finish before continuing.\n",
      "rfm9x.send(bytes(\"Hello world!\\r\\n\", \"utf-8\"))\n",
      "print(\"Sent Hello World message!\")\n",
      "\n",
      "# Wait to receive packets.  Note that this library can't receive data at a fast\n",
      "# rate, in fact it can only receive and process one 252 byte packet at a time.\n",
      "# This means you should only use this for low bandwidth scenarios, like sending\n",
      "# and receiving a single message at a time.\n",
      "print(\"Waiting for packets...\")\n",
      "\n",
      "while True:\n",
      "    packet = rfm9x.receive()\n",
      "    # Optionally change the receive timeout from its default of 0.5 seconds:\n",
      "    # packet = rfm9x.receive(timeout=5.0)\n",
      "    # If no packet was received during the timeout then None is returned.\n",
      "    if packet is None:\n",
      "        # Packet has not been received\n",
      "        LED.value = False\n",
      "        print(\"Received nothing! Listening again...\")\n",
      "    else:\n",
      "        # Received a packet!\n",
      "        LED.value = True\n",
      "        # Print out the raw bytes of the packet:\n",
      "        print(\"Received (raw bytes): {0}\".format(packet))\n",
      "        # And decode to ASCII text and print it too.  Note that you always\n",
      "        # receive raw bytes and need to convert to a text format like ASCII\n",
      "        # if you intend to do string processing on your data.  Make sure the\n",
      "        # sending side is sending ASCII data before you try to decode!\n",
      "        packet_text = str(packet, \"ascii\")\n",
      "        print(\"Received (ASCII): {0}\".format(packet_text))\n",
      "        # Also read the RSSI (signal strength) of the last received message and\n",
      "        # print it.\n",
      "        rssi = rfm9x.last_rssi\n",
      "        print(\"Received signal strength: {0} dB\".format(rssi))\n",
      "\n",
      "\"\"\"\n",
      "Class to hold clinical outcome model.\n",
      "Predicts probability of good outcome of patient(s) or group(s) of patients.\n",
      "\n",
      "Call `calculate_outcome_for_all(args)` from outside of the object\n",
      "\n",
      "Inputs\n",
      "======\n",
      "\n",
      "All inputs take np arrays (for multiple groups of patients).\n",
      "\n",
      "mimic: proportion of patients with stroke mimic\n",
      "\n",
      "ich: proportion of patients with intracerebral haemorrhage (ICH). \n",
      "Or probability of a patient having an ICH, when using for a single patient.\n",
      "\n",
      "nlvo: proportion of patients with non-large vessel occlusions (nLVO). \n",
      "Or probability of a patient having an NLVO, when using for a single patient.\n",
      "\n",
      "lvo: proportion of patients with large vessel occlusions (LVO). \n",
      "Or probability of a patient having a LVO, when using for a single patient.\n",
      "\n",
      "onset_to_needle: minutes from onset to thrombolysis\n",
      "\n",
      "onset_to_ouncture: minutes from onset to thrombectomy\n",
      "\n",
      "nlvo_eligible_for_treatment: proportion of patients with NLVO suitable for \n",
      "treatment with thrombolysis. Or probability of a patient with NVLO being \n",
      "eligible for treatment.\n",
      "\n",
      "lvo_eligible_for_treatment: proportion of patients with LVO suitable for \n",
      "treatment with thrombolysis and/or thrombectomy. Or probability of a patient \n",
      "with LVO being eligible for treatment.\n",
      "\n",
      "Returns\n",
      "=======\n",
      "\n",
      "Probability of good outcome: The probability of having a good outcome (modified\n",
      "Rankin Scale 0-1) for the patient or group of patients (np array).\n",
      "\n",
      "\n",
      "References for decay of effect of thrombolysis and thrombectomy\n",
      "===============================================================\n",
      "\n",
      "Decay of effect of thrombolysis without image selection of patients taken from:\n",
      "Emberson, Jonathan, Kennedy R. Lees, Patrick Lyden, Lisa Blackwell, \n",
      "Gregory Albers, Erich Bluhmki, Thomas Brott, et al (2014). “Effect of Treatment \n",
      "Delay, Age, and Stroke Severity on the Effects of Intravenous Thrombolysis with\n",
      "Alteplase for Acute Ischaemic Stroke: A Meta-Analysis of Individual Patient\n",
      "Data from Randomised Trials.” The Lancet 384: 1929–1935.\n",
      "https://doi.org/10.1016/S0140-6736(14)60584-5.\n",
      "\n",
      "* Time to no effect = 6.3hrs\n",
      "\n",
      "Decay of effect of thrombectomy without image selection of patients taken from:\n",
      "Fransen, Puck S. S., Olvert A. Berkhemer, Hester F. Lingsma, Debbie Beumer, \n",
      "Lucie A. van den Berg, Albert J. Yoo, Wouter J. Schonewille, et al. (2016)\n",
      "“Time to Reperfusion and Treatment Effect for Acute Ischemic Stroke: A \n",
      "Randomized Clinical Trial.” JAMA Neurology 73: 190–96. \n",
      "https://doi.org/10.1001/jamaneurol.2015.3886.\n",
      "\n",
      "* Time to no effect = 8hrs\n",
      "\"\"\"\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "class Clinical_outcome:\n",
      "    def __init__(self):\n",
      "        \"\"\"Constructor for clinical outcome model\n",
      "        \"\"\"\n",
      "        self.name = \"Clinical outcome model\"\n",
      "        self.thrombectomy_time_no_effect = 8 * 60\n",
      "        self.thrombolysis_time_no_effect = 6.3 * 60\n",
      "        self.maximum_permitted_time_to_thrombectomy = 360\n",
      "        self.maximum_permitted_time_to_thrombolysis = 270\n",
      "\n",
      "    def calculate_outcome_for_all(self,\n",
      "                                  mimic,\n",
      "                                  ich,\n",
      "                                  nlvo,\n",
      "                                  lvo,\n",
      "                                  onset_to_needle,\n",
      "                                  onset_to_puncture,\n",
      "                                  nlvo_eligible_for_treatment,\n",
      "                                  lvo_eligible_for_treatment,\n",
      "                                  prop_thrombolysed_lvo_receiving_thrombectomy):\n",
      "        \"\"\"\n",
      "        Calculates the probability of good outcome for all patients admitted\n",
      "        with acute stroke. \n",
      "\n",
      "        Based on:\n",
      "        Holodinsky JK, Williamson TS, Demchuk AM, et al. Modeling Stroke Patient\n",
      "        Transport for All Patients With Suspected Large-Vessel Occlusion. JAMA \n",
      "        Neurol. 2018;75(12):1477-1486. doi:10.1001/jamaneurol.2018.2424\n",
      "        \n",
      "        Sums outcomes for:\n",
      "\n",
      "        1) mimics\n",
      "        2) ICH\n",
      "        3) non-LVO\n",
      "        4) LVO treated with thrombolysis\n",
      "        5) LVO treated with thrombectomy (if thrombolysis not successful in a\n",
      "            drip and ship configuration)\n",
      "\n",
      "        arguments\n",
      "        ---------\n",
      "\n",
      "        np arrays (each row is a given geographic area with different \n",
      "        characteristics)\n",
      "\n",
      "        mimic: proportion of patients with stroke mimic\n",
      "        ich: proportion of patients with ICH\n",
      "        nlvo: proportion of patients with non-lvo\n",
      "        lvo: proportion of patients with lvo\n",
      "        onset_to_needle: minutes from onset to thrombolysis\n",
      "        onset_to_ounctureL minutes from onset to thrombectomy\n",
      "        nlvo_eligible_for_treatment: proportion of nlvo suitable for treatment\n",
      "        lvo_eligible_for_treatment: proportion of lvo suitable for treatment\n",
      "\n",
      "        returns\n",
      "        -------\n",
      "\n",
      "        probability of good outcome for all (np array)\n",
      "        \"\"\"\n",
      "        \n",
      "        # Get outcomes\n",
      "        # ------------\n",
      "        \n",
      "        outcomes = pd.DataFrame()\n",
      "\n",
      "        # Calculate good outcomes for mimics\n",
      "        outcomes['mimic'] = self._calculate_outcome_for_stroke_mimics(\n",
      "            mimic.shape)\n",
      "\n",
      "        # Calculate good outcomes for ich   \n",
      "        outcomes['ich']  = self._calculate_outcome_for_ICH(mimic.shape)\n",
      "\n",
      "        # Calculate good outcomes for nlvo without treatment\n",
      "        outcomes['nlvo_base'] = \\\n",
      "            np.full(nlvo.shape, 0.4622)\n",
      "            \n",
      "        # Calculate good outcomes for nlvo with thrombolysis\n",
      "        outcomes['nlvo_add_ivt'] = \\\n",
      "            self._calculate_thrombolysis_outcome_for_nlvo(onset_to_needle)\n",
      "\n",
      "        # Calculate good outcomes for lvo without treatment\n",
      "        outcomes['lvo_base'] = \\\n",
      "            np.full(nlvo.shape, 0.1328)\n",
      "        \n",
      "        # Calculate good outcomes for lvo with thrombolysis\n",
      "        outcomes['lvo_add_ivt'] = \\\n",
      "            self._calculate_thrombolysis_outcome_for_lvo(onset_to_needle)\n",
      "\n",
      "        # Calculate good outcomes for lvo with thrombolysis\n",
      "        outcomes['lvo_add_et'] = \\\n",
      "            self._calculate_thrombectomy_outcome_for_lvo(onset_to_puncture)\n",
      "\n",
      "        \n",
      "        # Weight outcome results by proportion of patients\n",
      "        # ------------------------------------------------\n",
      "        \n",
      "        # 'Results' are good outcomes\n",
      "        results = pd.DataFrame()\n",
      "        \n",
      "        # Results for mimic\n",
      "        results['mimic'] = outcomes['mimic']  * mimic\n",
      "        \n",
      "        # Results for ich\n",
      "        results['ich'] = outcomes['ich']  * ich\n",
      "        \n",
      "        # Results for nlvo\n",
      "        results['nlvo_base'] = nlvo * outcomes['nlvo_base']\n",
      "        \n",
      "        results['nlvo_ivt'] = \\\n",
      "            nlvo * outcomes['nlvo_add_ivt'] * nlvo_eligible_for_treatment\n",
      "        \n",
      "        # Results for lvo\n",
      "        results['lvo_base'] = lvo * outcomes['lvo_base']\n",
      "        \n",
      "        results['lvo_ivt'] = \\\n",
      "            lvo * outcomes['lvo_add_ivt'] * lvo_eligible_for_treatment\n",
      "                \n",
      "        # Adjust thrombectomy/thrombolysis ratio for LVO   \n",
      "        # Reduce thrombectomy treatment by LVO responding to IVT\n",
      "        lvo_receiving_et = ((lvo * lvo_eligible_for_treatment * \n",
      "            prop_thrombolysed_lvo_receiving_thrombectomy) - \n",
      "            results['lvo_ivt'])\n",
      "\n",
      "        results['lvo_et'] = lvo_receiving_et * outcomes['lvo_add_et']\n",
      "\n",
      "        p_good = results.sum(axis=1).values\n",
      "\n",
      "        return p_good\n",
      "\n",
      "    @staticmethod\n",
      "    def _calculate_outcome_for_ICH(array_shape):\n",
      "        \"\"\"\n",
      "        Calculates the probability of good outcome for patients with intra-\n",
      "        cranial haemorrhage (ICH).\n",
      "\n",
      "        Sets all values to 0.24 \n",
      "\n",
      "        Based on Holodinsky et al. (2018) Drip-and-Ship vs. Mothership: \n",
      "        Modelling Stroke Patient Transport for All Suspected Large Vessel\n",
      "        Occlusion Patients. JAMA Neuro (in press)\n",
      "\n",
      "        arguments\n",
      "        ---------\n",
      "\n",
      "        array size\n",
      "\n",
      "        returns\n",
      "        -------\n",
      "\n",
      "        probability of good outcome for ICH (np array)\n",
      "        \"\"\"\n",
      "\n",
      "        # Create an array of required length and set all values to 0.24\n",
      "        p_good = np.zeros(array_shape)\n",
      "        p_good[:] = 0.24\n",
      "\n",
      "        return p_good        \n",
      "\n",
      "    @staticmethod\n",
      "    def _calculate_outcome_for_stroke_mimics(array_shape):\n",
      "        \"\"\"\n",
      "        Calculates the probability of good outcome for patients with stroke\n",
      "        mimic\n",
      "\n",
      "        Sets all values to 1\n",
      "\n",
      "        Based on Holodinsky et al. (2018) Drip-and-Ship vs. Mothership: \n",
      "        Modelling Stroke Patient Transport for All Suspected Large Vessel\n",
      "        Occlusion Patients. JAMA Neuro (in press)\n",
      "\n",
      "        arguments\n",
      "        ---------\n",
      "\n",
      "        array size\n",
      "\n",
      "        returns\n",
      "        -------\n",
      "\n",
      "        probability of good outcome for stroke mimiccs (np array)\n",
      "        \"\"\"\n",
      "\n",
      "        # Create an array of required length and set all values to 0.9\n",
      "        p_good = np.zeros(array_shape)\n",
      "        p_good[:] = 1\n",
      "\n",
      "        return p_good\n",
      "    \n",
      "    def _calculate_thrombectomy_outcome_for_lvo(self, onset_to_puncture):\n",
      "        \"\"\"\n",
      "        Calculates the probability of additional good outcome for LVO patients\n",
      "        receiving thrombectomy.\n",
      "\n",
      "        arguments\n",
      "        ---------\n",
      "\n",
      "        onset_to_puncture : np array in minutes\n",
      "\n",
      "        returns\n",
      "        -------\n",
      "\n",
      "        probability of additional good outcome if given thrombectomy (np array)\n",
      "        \"\"\"\n",
      "\n",
      "        p_good_max = 0.5208\n",
      "        p_good_min = 0.1328\n",
      "        \n",
      "        # Convert probability to odds\n",
      "        odds_good_max = p_good_max / (1 - p_good_max)\n",
      "        odds_good_min = p_good_min / (1 - p_good_min)\n",
      "        \n",
      "        # Calculate fraction of effective time used\n",
      "        fraction_max_effect_time_used = \\\n",
      "            onset_to_puncture / self.thrombectomy_time_no_effect\n",
      "        \n",
      "        # Calculate odds of good outcome with treatment\n",
      "        odds_good = np.exp(np.log(odds_good_max) - \n",
      "            ((np.log(odds_good_max) - np.log(odds_good_min)) \n",
      "            * fraction_max_effect_time_used))\n",
      "        \n",
      "        # Convert odds to probability\n",
      "        prob_good = odds_good / (1 + odds_good)\n",
      "        prob_good[prob_good < p_good_min] = p_good_min\n",
      "        \n",
      "        # Calculate probability of additional good outcome\n",
      "        p_good_add = prob_good - p_good_min\n",
      "        \n",
      "        # Set additional good outcomes to zero if past permitted treatment time\n",
      "        mask = onset_to_puncture > self.maximum_permitted_time_to_thrombectomy\n",
      "        p_good_add[mask] = 0   \n",
      "        \n",
      "        # Ensure no negative outcomes\n",
      "        mask = p_good_add < 0\n",
      "        p_good_add[mask] = 0  \n",
      "\n",
      "        return p_good_add        \n",
      "\n",
      "    def _calculate_thrombolysis_outcome_for_lvo(self, onset_to_needle):\n",
      "        \"\"\"\n",
      "        Calculates the probability of additional good outcome for LVO patients\n",
      "        receiving thrombolysis. Does not include baseline untreated good\n",
      "        comes.\n",
      "\n",
      "        arguments\n",
      "        ---------\n",
      "        \n",
      "        onset_to_needle : np array in minutes\n",
      "\n",
      "\n",
      "        returns\n",
      "        -------\n",
      "\n",
      "        probability of additional good outcome if given thrombolysis \n",
      "        (np array)\n",
      "        \"\"\"\n",
      "        \n",
      "        p_good_max = 0.2441\n",
      "        p_good_min = 0.1328\n",
      "        \n",
      "        # Convert probability to odds\n",
      "        odds_good_max = p_good_max / (1 - p_good_max)\n",
      "        odds_good_min = p_good_min / (1 - p_good_min)\n",
      "        \n",
      "        # Calculate fraction of effective time used        \n",
      "        fraction_max_effect_time_used = \\\n",
      "            onset_to_needle / self.thrombolysis_time_no_effect\n",
      "\n",
      "        # Calculate odds of good outcome with treatment\n",
      "        odds_good = np.exp(np.log(odds_good_max) - \n",
      "            ((np.log(odds_good_max) - np.log(odds_good_min)) \n",
      "            * fraction_max_effect_time_used))\n",
      "\n",
      "        # Convert odds to probability\n",
      "        prob_good = odds_good / (1 + odds_good)\n",
      "        prob_good[prob_good < p_good_min] = p_good_min\n",
      "        \n",
      "        # Calculate probability of additional good outcome\n",
      "        p_good_add = prob_good - p_good_min\n",
      "        \n",
      "        # Set additional good outcomes to zero if past permitted treatment time\n",
      "        mask = onset_to_needle> self.maximum_permitted_time_to_thrombolysis\n",
      "        p_good_add[mask] = 0   \n",
      "        \n",
      "        # Ensure no negative outcomes\n",
      "        mask = p_good_add < 0\n",
      "        p_good_add[mask] = 0  \n",
      "\n",
      "        # return outcome and proportion of treated who respond\n",
      "        return p_good_add\n",
      "\n",
      "    def _calculate_thrombolysis_outcome_for_nlvo(self, onset_to_needle):\n",
      "        \"\"\"\n",
      "        Calculates the probability of good outcome for non-LVO patients\n",
      "        receiving thrombolysis.\n",
      "\n",
      "        arguments\n",
      "        ---------\n",
      "\n",
      "        onset_to_needle : np array in minutes\n",
      "\n",
      "        returns\n",
      "        -------\n",
      "\n",
      "        probability of good outcome if given thrombolysis (np array)\n",
      "        \"\"\"\n",
      "\n",
      "        p_good_max = 0.6444\n",
      "        p_good_min = 0.4622\n",
      "        \n",
      "        # Convert probability to odds\n",
      "        odds_good_max = p_good_max / (1 - p_good_max)\n",
      "        odds_good_min = p_good_min / (1 - p_good_min)\n",
      "        \n",
      "        # Calculate fraction of effective time used \n",
      "        fraction_max_effect_time_used = (onset_to_needle / \n",
      "                                         self.thrombolysis_time_no_effect)\n",
      "        \n",
      "        # Calculate odds of good outcome with treatment\n",
      "        odds_good = np.exp(np.log(odds_good_max) - \n",
      "            ((np.log(odds_good_max) - np.log(odds_good_min)) \n",
      "            * fraction_max_effect_time_used))\n",
      "        \n",
      "        # Convert odds to probability\n",
      "        prob_good = odds_good / (1 + odds_good)\n",
      "        prob_good[prob_good < p_good_min] = p_good_min\n",
      "        \n",
      "        # Calculate probability of additional good outcome\n",
      "        p_good_add = prob_good - p_good_min\n",
      "        \n",
      "        mask = onset_to_needle> self.maximum_permitted_time_to_thrombolysis\n",
      "        p_good_add[mask] = 0   \n",
      "        \n",
      "        # Ensure no negative outcomes\n",
      "        mask = p_good_add < 0\n",
      "        p_good_add[mask] = 0  \n",
      "\n",
      "        # return outcome and proportion of treated who respond\n",
      "        return p_good_add\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "from django.conf.urls.defaults import patterns, url\n",
      "from djangopypi.feeds import ReleaseFeed\n",
      "\n",
      "urlpatterns = patterns(\"djangopypi.views\",\n",
      "    url(r'^$', \"root\", name=\"djangopypi-root\"),\n",
      "    url(r'^packages/$','packages.index', name='djangopypi-package-index'),\n",
      "    url(r'^simple/$','packages.simple_index', name='djangopypi-package-index-simple'),\n",
      "    url(r'^search/$','packages.search',name='djangopypi-search'),\n",
      "    url(r'^pypi/$', 'root', name='djangopypi-release-index'),\n",
      "    url(r'^rss/$', ReleaseFeed(), name='djangopypi-rss'),\n",
      "    \n",
      "    url(r'^simple/(?P<package>[\\w\\d_\\.\\-]+)/$','packages.simple_details',\n",
      "        name='djangopypi-package-simple'),\n",
      "    \n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/$','packages.details',\n",
      "        name='djangopypi-package'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/rss/$', ReleaseFeed(),\n",
      "        name='djangopypi-package-rss'),    \n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/doap.rdf$','packages.doap',\n",
      "        name='djangopypi-package-doap'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/manage/$','packages.manage',\n",
      "        name='djangopypi-package-manage'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/manage/versions/$','packages.manage_versions',\n",
      "        name='djangopypi-package-manage-versions'),\n",
      "    \n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/$',\n",
      "        'releases.details',name='djangopypi-release'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/doap.rdf$',\n",
      "        'releases.doap',name='djangopypi-release-doap'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/manage/$',\n",
      "        'releases.manage',name='djangopypi-release-manage'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/metadata/$',\n",
      "        'releases.manage_metadata',name='djangopypi-release-manage-metadata'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/files/$',\n",
      "        'releases.manage_files',name='djangopypi-release-manage-files'),\n",
      "    url(r'^pypi/(?P<package>[\\w\\d_\\.\\-]+)/(?P<version>[\\w\\d_\\.\\-]+)/files/upload/$',\n",
      "        'releases.upload_file',name='djangopypi-release-upload-file'),\n",
      ")\n",
      "import pandas as pd\n",
      "from pandas.compat import StringIO\n",
      "import numpy\n",
      "numpy.set_printoptions(threshold=numpy.nan)\n",
      "\n",
      "\n",
      "def main():\n",
      "    df = pd.read_csv(StringIO(earnings), sep=\",\", header=None,\n",
      "                     names=['symbol', 'exchange', 'eps_pct_diff_surp', 'asof_date'])\n",
      "    df = df.sort_values(by=['asof_date'])\n",
      "    print(df.head())\n",
      "    print(len(df))\n",
      "    df.to_csv('../../data/events/nyse_earnings_surprises_2013.csv', index=False)\n",
      "\n",
      "    myString = ', '.join('\"{0}\"'.format(s) for s in df.symbol.unique())\n",
      "    myString = myString.replace(\" \", \"\")\n",
      "    print(myString)\n",
      "\n",
      "#earnings = 'CFN,  NYSE, -21.82, 2013-02-09\\nNDZ,  NYSE,  30.77, 2013-01-29\\nAZZ,  NYSE,  -1.64, 2013-01-10'\n",
      "earnings = 'CFN,  NYSE,   -21.82, 2013-02-09\\n  NDZ,  NYSE,    30.77, 2013-01-29\\n  AZZ,  NYSE,    -1.64, 2013-01-10\\n  CLC,  NYSE,     2.86, 2013-01-17\\n  CMC,  NYSE,    64.71, 2013-01-08\\n   FC,  NYSE,    15.38, 2013-01-04\\n  FDO,  NYSE,    -6.76, 2013-01-04\\n  FUL,  NYSE,    14.29, 2013-01-17\\n  LEN,  NYSE,    30.23, 2013-01-16\\n  LNN,  NYSE,    53.33, 2013-01-09\\n  MKC,  NYSE,    -3.48, 2013-01-25\\n   RT,  NYSE,     0.00, 2013-01-10\\n  MSM,  NYSE,     1.00, 2013-01-11\\n  RPM,  NYSE,    -4.76, 2013-01-09\\n  SVU,  NYSE,   -50.00, 2013-01-11\\n TISI,  NYSE,    10.00, 2013-01-08\\n  TXI,  NYSE,    -5.88, 2013-01-10\\n  UNF,  NYSE,    15.79, 2013-01-04\\n  WOR,  NYSE,    12.20, 2013-01-04\\n  GBX,  NYSE,    12.90, 2013-01-10\\n  SJR,  NYSE,    11.11, 2013-01-10\\n  OMN,  NYSE,   -50.00, 2013-01-23\\n  MON,  NYSE,    67.57, 2013-01-09\\n  GPN,  NYSE,     6.90, 2013-01-09\\n  AYI,  NYSE,   -13.75, 2013-01-09\\n  STZ,  NYSE,    14.55, 2013-01-10\\n  SNX,  NYSE,    11.54, 2013-01-11\\n  TAL,  NYSE,   600.00, 2013-01-23\\n  IHS,  NYSE,    12.35, 2013-01-09\\n  EDU,  NYSE,  -150.00, 2013-01-30\\n  SAR,  NYSE,    28.57, 2013-01-15\\n  ZEP,  NYSE,    11.11, 2013-01-08\\n   MG,  NYSE,     0.00, 2013-01-09\\n  MOS,  NYSE,     7.14, 2013-01-04\\n  ABT,  NYSE,     1.33, 2013-01-24\\n  ABX,  NYSE,     1.83, 2013-02-15\\n   AB,  NYSE,    21.21, 2013-02-13\\n  TAP,  NYSE,     7.81, 2013-02-15\\n  ACO,  NYSE,   -15.91, 2013-01-26\\n  ADM,  NYSE,   -26.83, 2013-02-05\\n  AEM,  NYSE,   -13.33, 2013-02-14\\n  AEP,  NYSE,    11.11, 2013-02-16\\n  AES,  NYSE,     6.67, 2013-02-28\\n  AET,  NYSE,    -2.08, 2013-02-01\\n  AFL,  NYSE,     0.00, 2013-02-06\\n AGCO,  NYSE,     1.02, 2013-02-06\\n  HES,  NYSE,    -2.44, 2013-01-31\\n  AIG,  NYSE,   322.22, 2013-02-22\\n  AIN,  NYSE,    -9.68, 2013-02-07\\n  AJG,  NYSE,     2.63, 2013-01-30\\n  ALU,  NYSE,     0.00, 2013-02-08\\n MATX,  NYSE,    24.14, 2013-02-08\\n  ALK,  NYSE,    -4.11, 2013-01-25\\n  ALX,  NYSE,   -11.52, 2013-02-27\\n BEAM,  NYSE,     0.00, 2013-02-02\\n  AME,  NYSE,     2.08, 2013-01-25\\n  TWX,  NYSE,     6.36, 2013-02-07\\n  AVD,  NYSE,    11.43, 2013-03-01\\n  AMN,  NYSE,    36.36, 2013-02-22\\n   AN,  NYSE,     3.08, 2013-02-01\\n  AON,  NYSE,     1.60, 2013-02-02\\n   AP,  NYSE,    77.78, 2013-02-05\\n  APA,  NYSE,    -1.30, 2013-02-15\\n  APC,  NYSE,    30.00, 2013-02-05\\n  APD,  NYSE,     0.78, 2013-01-24\\n  APH,  NYSE,     4.44, 2013-01-18\\n  ARG,  NYSE,    -3.70, 2013-01-25\\n  AAN,  NYSE,    -4.00, 2013-02-08\\n  ARW,  NYSE,    13.89, 2013-02-08\\n ASGN,  NYSE,   -25.00, 2013-02-15\\n  ASH,  NYSE,   -17.65, 2013-01-30\\n  ASR,  NYSE,    56.88, 2013-02-26\\n  GAS,  NYSE,    -9.90, 2013-02-07\\n  ATO,  NYSE,    -5.13, 2013-02-07\\n  ATW,  NYSE,    17.02, 2013-01-31\\n   AU,  NYSE,   -67.44, 2013-02-21\\n  AVP,  NYSE,    37.04, 2013-02-13\\n  AVT,  NYSE,    21.69, 2013-01-25\\n  AVY,  NYSE,    10.20, 2013-01-31\\n  AXP,  NYSE,     0.00, 2013-01-18\\n    B,  NYSE,     7.84, 2013-02-23\\n   BA,  NYSE,     7.56, 2013-01-31\\n  BAC,  NYSE,    50.00, 2013-01-18\\n  BAX,  NYSE,     0.00, 2013-01-25\\n   BC,  NYSE,   122.22, 2013-01-25\\n  OMX,  NYSE,     6.67, 2013-02-21\\n  BCE,  NYSE,    -2.99, 2013-02-08\\n  BCR,  NYSE,     1.80, 2013-02-01\\n  BCS,  NYSE,    40.74, 2013-02-13\\n  BDX,  NYSE,     9.76, 2013-02-06\\n  BEN,  NYSE,     1.68, 2013-02-02\\n  BGG,  NYSE,   250.00, 2013-01-25\\n  BHE,  NYSE,    10.00, 2013-02-05\\n  BHI,  NYSE,     1.64, 2013-01-24\\n  BID,  NYSE,     0.92, 2013-03-01\\n  BIO,  NYSE,    15.67, 2013-02-27\\n   BK,  NYSE,     0.00, 2013-01-16\\n  BKH,  NYSE,     9.68, 2013-02-01\\n  WRB,  NYSE,    28.00, 2013-01-29\\n  BLC,  NYSE,     5.71, 2013-02-09\\n  BLL,  NYSE,    -3.03, 2013-02-01\\n  BLX,  NYSE,    20.75, 2013-02-08\\n  BMI,  NYSE,   -11.36, 2013-02-07\\n  BMS,  NYSE,     4.00, 2013-02-01\\n  BMY,  NYSE,     9.30, 2013-01-25\\n  BOH,  NYSE,     1.12, 2013-01-31\\n  BXS,  NYSE,   -25.00, 2013-01-24\\n  BPL,  NYSE,    18.52, 2013-02-09\\nBRK.A,  NYSE,   175.73, 2013-03-02\\n  BRO,  NYSE,     7.41, 2013-02-02\\n  BSX,  NYSE,    63.64, 2013-01-30\\n   BT,  NYSE,   -89.22, 2013-02-02\\n MTRN,  NYSE,    17.14, 2013-03-01\\n CACI,  NYSE,     3.66, 2013-01-31\\n  CAT,  NYSE,   -13.10, 2013-01-29\\n   CB,  NYSE,    10.00, 2013-01-30\\n  CBI,  NYSE,     9.64, 2013-02-28\\n  CBM,  NYSE,   100.00, 2013-02-07\\n  CBU,  NYSE,    -3.70, 2013-01-23\\n  CBT,  NYSE,   -28.57, 2013-01-31\\n  CCC,  NYSE,    35.71, 2013-02-22\\n  CCE,  NYSE,     4.65, 2013-02-08\\n    C,  NYSE,   -20.69, 2013-01-18\\n  CCK,  NYSE,    -7.27, 2013-01-31\\n  CCU,  NYSE,   -12.21, 2013-02-01\\n  CDE,  NYSE,   -15.15, 2013-02-22\\n  CDI,  NYSE,     8.70, 2013-02-27\\n  CAH,  NYSE,     9.41, 2013-02-06\\n  CFR,  NYSE,     5.38, 2013-01-31\\n  CHD,  NYSE,     0.00, 2013-02-06\\n  CKP,  NYSE,   -50.00, 2013-03-06\\n  CPK,  NYSE,    18.60, 2013-03-08\\n   CI,  NYSE,     6.08, 2013-02-08\\n  CIA,  NYSE,  -100.00, 2013-03-12\\n  CKH,  NYSE,   -93.55, 2013-02-28\\n   CL,  NYSE,     0.71, 2013-02-01\\n  CLF,  NYSE,   -25.45, 2013-02-13\\n  CLH,  NYSE,   -25.00, 2013-02-21\\n  CLX,  NYSE,    11.11, 2013-02-05\\n  CMA,  NYSE,     7.81, 2013-01-17\\n  CMO,  NYSE,    -6.06, 2013-01-31\\n  CRK,  NYSE,   -77.42, 2013-02-12\\n  CMS,  NYSE,     4.17, 2013-02-22\\n  CNA,  NYSE,  -150.00, 2013-02-12\\n  CNW,  NYSE,   -10.34, 2013-02-07\\n  CHG,  NYSE,    -4.12, 2013-02-27\\n  CNL,  NYSE,    12.50, 2013-02-20\\n  COG,  NYSE,    14.29, 2013-02-22\\n  COT,  NYSE,   -66.67, 2013-02-16\\n   CP,  NYSE,    -0.78, 2013-01-30\\n  CPF,  NYSE,    11.54, 2013-02-01\\n  CQB,  NYSE,   -17.65, 2013-03-12\\n   CR,  NYSE,    -5.15, 2013-01-29\\nCRD.B,  NYSE,    52.38, 2013-02-14\\n  CRS,  NYSE,     1.64, 2013-02-01\\n  CSC,  NYSE,    22.22, 2013-02-06\\n  CSL,  NYSE,     6.49, 2013-02-09\\n  CTB,  NYSE,    35.29, 2013-02-26\\n  CTL,  NYSE,    -1.47, 2013-02-14\\n  CTS,  NYSE,   -21.74, 2013-01-29\\n  CUB,  NYSE,   -32.86, 2013-02-12\\n  CMI,  NYSE,    14.94, 2013-02-07\\n  CUZ,  NYSE,    40.00, 2013-02-14\\n  CVC,  NYSE,  -400.00, 2013-03-01\\n  CVH,  NYSE,    35.82, 2013-02-07\\n   CW,  NYSE,     4.40, 2013-02-21\\n  CWT,  NYSE,    33.33, 2013-02-28\\n   CX,  NYSE,  -258.33, 2013-02-08\\n  CYN,  NYSE,   -13.00, 2013-01-25\\n    D,  NYSE,     1.47, 2013-02-01\\n  DBD,  NYSE,    -8.16, 2013-02-13\\n  DCO,  NYSE,   -23.81, 2013-03-05\\n   DD,  NYSE,    22.22, 2013-01-23\\n  CVA,  NYSE,   -13.04, 2013-02-07\\n  DHR,  NYSE,     0.00, 2013-01-30\\n  DIS,  NYSE,     2.60, 2013-02-06\\n  DLX,  NYSE,    11.76, 2013-01-25\\n  DNB,  NYSE,    -1.24, 2013-02-12\\n  RRD,  NYSE,    16.22, 2013-02-27\\n  DOV,  NYSE,     1.87, 2013-01-25\\n  DOW,  NYSE,    -2.94, 2013-02-01\\n  DRE,  NYSE,     0.00, 2013-01-31\\n  DHI,  NYSE,    42.86, 2013-01-30\\n  UFS,  NYSE,    -7.09, 2013-02-02\\n  DTE,  NYSE,     0.00, 2013-02-21\\n  DUK,  NYSE,     7.69, 2013-02-14\\n  DVN,  NYSE,     2.63, 2013-02-21\\n   DV,  NYSE,    55.36, 2013-02-07\\n  EAT,  NYSE,     0.00, 2013-01-23\\n  ECL,  NYSE,     0.00, 2013-02-27\\n   ED,  NYSE,    -6.85, 2013-02-01\\n  EDE,  NYSE,    27.78, 2013-02-15\\n  EFX,  NYSE,     4.00, 2013-02-07\\n  EGN,  NYSE,   -15.58, 2013-01-24\\n  EGP,  NYSE,     0.00, 2013-02-13\\n  ELY,  NYSE,     2.00, 2013-01-31\\n  EMC,  NYSE,     6.98, 2013-01-30\\n  EMR,  NYSE,     0.00, 2013-02-06\\n  EOG,  NYSE,    19.26, 2013-02-14\\n  EQT,  NYSE,    14.29, 2013-01-25\\n  ESE,  NYSE,   -44.44, 2013-02-08\\n  ESV,  NYSE,     7.87, 2013-02-21\\n  ETN,  NYSE,   -10.87, 2013-02-06\\n  ETR,  NYSE,    21.99, 2013-02-09\\n EXAR,  NYSE,   -14.29, 2013-01-24\\n    F,  NYSE,    19.23, 2013-01-30\\n  OPY,  NYSE,   115.79, 2013-02-02\\n CLGX,  NYSE,    -3.12, 2013-02-22\\n  FNB,  NYSE,     4.55, 2013-01-24\\n  FCF,  NYSE,   -18.18, 2013-01-31\\n  FBP,  NYSE,   -30.00, 2013-02-06\\n FICO,  NYSE,     6.94, 2013-01-31\\n  FLO,  NYSE,    12.00, 2013-02-08\\n  FMC,  NYSE,     0.00, 2013-02-07\\n  FOE,  NYSE,  -250.00, 2013-03-06\\n    S,  NYSE,     4.35, 2013-02-08\\n  NEE,  NYSE,     9.57, 2013-01-30\\n  FRT,  NYSE,     0.91, 2013-02-13\\n  FRX,  NYSE,   -61.54, 2013-01-16\\n  FUN,  NYSE,  -433.33, 2013-02-20\\n  FUR,  NYSE,   -48.15, 2013-03-08\\n  GBL,  NYSE,   -28.72, 2013-02-06\\n  GVA,  NYSE,   -29.03, 2013-03-01\\n  BGC,  NYSE,    -3.45, 2013-02-26\\n   GD,  NYSE,   -26.84, 2013-01-24\\n   GE,  NYSE,     2.33, 2013-01-19\\n  RHP,  NYSE,   -50.00, 2013-02-13\\n AXLL,  NYSE,    95.08, 2013-02-13\\n  GGG,  NYSE,    13.33, 2013-01-29\\n  GHM,  NYSE,   -22.22, 2013-02-02\\n  GIB,  NYSE,    -4.35, 2013-01-31\\n  GLT,  NYSE,   -25.71, 2013-02-08\\n  GLW,  NYSE,     3.03, 2013-01-30\\n  GSK,  NYSE,     8.33, 2013-02-07\\n  GLF,  NYSE,  -160.71, 2013-02-26\\n  GNI,  NYSE,   -14.44, 2013-01-30\\n  GPC,  NYSE,     0.00, 2013-02-20\\n  GRA,  NYSE,     4.72, 2013-02-07\\n  GTY,  NYSE,   -10.34, 2013-03-01\\n  GWW,  NYSE,    -7.28, 2013-01-25\\n  HAE,  NYSE,     4.17, 2013-01-31\\n  HAL,  NYSE,     3.28, 2013-01-26\\n  HAR,  NYSE,   -32.95, 2013-02-01\\n  HVT,  NYSE,    30.43, 2013-02-26\\n  HRC,  NYSE,     6.82, 2013-01-24\\n  HCC,  NYSE,    43.75, 2013-02-13\\n  HCN,  NYSE,     1.19, 2013-02-26\\n  HCP,  NYSE,     1.41, 2013-02-13\\n  HOG,  NYSE,     0.00, 2013-01-30\\n   HE,  NYSE,    21.88, 2013-02-16\\n   HL,  NYSE,   -25.00, 2013-02-26\\n  HMA,  NYSE,    -5.00, 2013-02-15\\n  HMC,  NYSE,   -29.58, 2013-02-01\\n  HMN,  NYSE,    91.43, 2013-02-06\\n  HFC,  NYSE,    -8.97, 2013-02-27\\n  HOT,  NYSE,     7.69, 2013-02-08\\n   HP,  NYSE,     8.53, 2013-02-01\\n  HLS,  NYSE,    40.63, 2013-02-19\\n  HRS,  NYSE,     4.17, 2013-01-30\\n  HSC,  NYSE,    -3.23, 2013-02-15\\n  HSY,  NYSE,    -1.33, 2013-02-01\\n HUBB,  NYSE,     0.00, 2013-01-25\\n  HUM,  NYSE,    11.21, 2013-02-05\\n  HXL,  NYSE,    -5.26, 2013-01-24\\n  IBM,  NYSE,     2.67, 2013-01-23\\n  IDA,  NYSE,    10.00, 2013-02-22\\n  IEX,  NYSE,     2.99, 2013-02-05\\n  IFF,  NYSE,    -1.19, 2013-02-08\\n  DIN,  NYSE,     1.22, 2013-02-28\\n  INT,  NYSE,     0.00, 2013-02-22\\n   IP,  NYSE,     6.15, 2013-01-30\\n  IPG,  NYSE,     3.70, 2013-02-23\\n   IO,  NYSE,    30.77, 2013-02-14\\n   IR,  NYSE,     8.57, 2013-02-02\\n  IRF,  NYSE,     6.38, 2013-01-29\\n  ITW,  NYSE,    -1.11, 2013-01-30\\n  IVC,  NYSE,   -56.00, 2013-02-09\\n  JEC,  NYSE,     0.00, 2013-01-24\\n  JNJ,  NYSE,     1.71, 2013-01-23\\n  JNY,  NYSE,    75.00, 2013-02-14\\n    K,  NYSE,     3.08, 2013-02-06\\n KAMN,  NYSE,     0.00, 2013-02-26\\n  KDN,  NYSE,     0.00, 2013-02-22\\n  KEX,  NYSE,     9.30, 2013-01-31\\n  KEY,  NYSE,    -4.55, 2013-01-25\\n  KIM,  NYSE,     6.45, 2013-02-06\\n  KMB,  NYSE,     0.74, 2013-01-26\\n  KEM,  NYSE,    53.33, 2013-02-01\\n  KMT,  NYSE,   -21.88, 2013-01-25\\n   KO,  NYSE,     2.27, 2013-02-13\\n  KSU,  NYSE,    10.98, 2013-01-23\\n  LDL,  NYSE,   -10.53, 2013-02-27\\n  LDR,  NYSE,    10.42, 2013-02-12\\n  LEE,  NYSE,    25.00, 2013-01-23\\n  LEG,  NYSE,    10.34, 2013-02-05\\n  LLY,  NYSE,     8.97, 2013-01-30\\n   LM,  NYSE,    29.63, 2013-02-02\\n  LNC,  NYSE,     3.77, 2013-02-07\\n  LPX,  NYSE,   -10.00, 2013-02-09\\n  LXU,  NYSE,   145.00, 2013-03-01\\n  LTC,  NYSE,    -1.72, 2013-02-22\\n    L,  NYSE,   -37.93, 2013-02-12\\n  LUK,  NYSE,   210.17, 2013-02-26\\n  LUV,  NYSE,    28.57, 2013-01-25\\n  LUX,  NYSE,     4.35, 2013-03-01\\n  MKL,  NYSE,   314.07, 2013-02-05\\n  MAN,  NYSE,    18.18, 2013-01-31\\n  MTW,  NYSE,    12.50, 2013-02-01\\n   SM,  NYSE,    95.65, 2013-02-21\\n  MAS,  NYSE,   500.00, 2013-02-12\\n  MTZ,  NYSE,     2.22, 2013-03-01\\n  MCD,  NYSE,     3.76, 2013-01-24\\n  MDC,  NYSE,    40.48, 2013-02-01\\n  MDP,  NYSE,     1.14, 2013-01-25\\n  MDR,  NYSE,    13.04, 2013-03-01\\n  MDU,  NYSE,     2.56, 2013-02-05\\n  MED,  NYSE,    12.00, 2013-03-08\\n  CVS,  NYSE,     2.73, 2013-02-07\\n  MFC,  NYSE,   -12.50, 2013-02-08\\n  MGA,  NYSE,    36.84, 2013-03-02\\n  MGM,  NYSE,     0.00, 2013-02-21\\n  MLR,  NYSE,   -11.76, 2013-03-07\\n  MLI,  NYSE,    14.29, 2013-02-06\\n  MMC,  NYSE,     0.00, 2013-02-13\\n  MMM,  NYSE,     0.00, 2013-01-25\\n  MSA,  NYSE,     3.64, 2013-02-14\\n  MNR,  NYSE,    38.46, 2013-02-08\\n   MO,  NYSE,     1.85, 2013-02-01\\n  MOD,  NYSE,   -75.00, 2013-02-02\\nMOG.A,  NYSE,    -8.54, 2013-01-26\\n  MHK,  NYSE,     7.45, 2013-02-22\\n  MSI,  NYSE,     7.61, 2013-01-24\\n  MCY,  NYSE,  -168.00, 2013-02-05\\n  MRK,  NYSE,     2.47, 2013-02-02\\n  MRO,  NYSE,   -19.12, 2013-02-07\\n POWR,  NYSE,    18.18, 2013-03-08\\n  MTG,  NYSE,   -37.87, 2013-03-01\\n  MTB,  NYSE,     2.76, 2013-01-17\\n  MTX,  NYSE,     6.38, 2013-02-01\\n  MUR,  NYSE,    59.23, 2013-01-31\\n  MYE,  NYSE,    -7.14, 2013-02-14\\n  NBL,  NYSE,    54.21, 2013-02-08\\n  NBR,  NYSE,     3.45, 2013-02-20\\n   NE,  NYSE,   -19.35, 2013-01-24\\n  NEM,  NYSE,    13.27, 2013-02-22\\n  NFG,  NYSE,     6.58, 2013-02-08\\n  NHI,  NYSE,     1.20, 2013-02-15\\n   NI,  NYSE,     0.00, 2013-02-20\\n  NJR,  NYSE,   -17.48, 2013-02-08\\n  THC,  NYSE,   -24.64, 2013-02-27\\n  NNN,  NYSE,     4.55, 2013-02-08\\n  NOC,  NYSE,    18.39, 2013-01-31\\n  NPK,  NYSE,   -11.23, 2013-02-16\\n   NR,  NYSE,     0.00, 2013-02-15\\n  NSC,  NYSE,     9.24, 2013-01-23\\n  NUE,  NYSE,    55.17, 2013-01-30\\n  NVR,  NYSE,     8.22, 2013-01-25\\n  NWL,  NYSE,     2.38, 2013-02-02\\n  NWN,  NYSE,    -4.55, 2013-03-02\\n  NYT,  NYSE,     3.23, 2013-02-08\\n  OCR,  NYSE,     1.18, 2013-02-20\\n  OGE,  NYSE,    14.71, 2013-02-28\\n  OHI,  NYSE,     3.57, 2013-02-12\\n   OI,  NYSE,     8.11, 2013-01-31\\n  OII,  NYSE,     2.78, 2013-02-14\\n  OKE,  NYSE,    17.78, 2013-02-26\\n  OLN,  NYSE,     2.94, 2013-01-29\\n  BRS,  NYSE,    32.95, 2013-02-05\\n  OLP,  NYSE,     0.00, 2013-03-15\\n  OMC,  NYSE,     3.67, 2013-02-13\\n  OMI,  NYSE,   -12.77, 2013-02-12\\n  ORB,  NYSE,    31.82, 2013-02-15\\n  ORI,  NYSE,   -28.57, 2013-01-25\\n  OSK,  NYSE,    93.55, 2013-01-26\\n  OXY,  NYSE,    10.24, 2013-02-01\\n  PHX,  NYSE,   -18.75, 2013-02-08\\n FCFS,  NYSE,     2.20, 2013-01-24\\n  PBI,  NYSE,     7.69, 2013-02-01\\n  PCG,  NYSE,     3.51, 2013-02-22\\n  PCL,  NYSE,    68.97, 2013-01-29\\n  PCP,  NYSE,    -3.23, 2013-01-25\\n  TPC,  NYSE,     0.00, 2013-02-22\\n  PDS,  NYSE,   250.00, 2013-02-15\\n  PEG,  NYSE,     5.13, 2013-02-22\\n  PEI,  NYSE,     0.00, 2013-02-26\\n  PEP,  NYSE,     3.81, 2013-02-15\\n  PFE,  NYSE,     6.82, 2013-01-30\\n   PG,  NYSE,     9.91, 2013-01-26\\n  PGR,  NYSE,     0.00, 2013-01-19\\n   PH,  NYSE,     6.25, 2013-01-19\\n  PHG,  NYSE,    -4.17, 2013-01-30\\n  PHM,  NYSE,     9.68, 2013-02-01\\n  PKD,  NYSE,  -150.00, 2013-02-22\\n  PKY,  NYSE,    17.39, 2013-02-12\\n  PNC,  NYSE,    24.82, 2013-01-18\\n  PNM,  NYSE,    18.18, 2013-03-02\\n  PNR,  NYSE,     6.82, 2013-01-30\\n  PNW,  NYSE,    41.18, 2013-02-23\\n  POM,  NYSE,    -5.00, 2013-03-02\\n  POT,  NYSE,   -11.86, 2013-02-01\\n  PPG,  NYSE,    -0.65, 2013-01-15\\n  PPL,  NYSE,     6.52, 2013-02-15\\n PRGO,  NYSE,     3.82, 2013-02-02\\n   PL,  NYSE,    11.36, 2013-02-07\\n  PSB,  NYSE,     5.04, 2013-02-20\\n  CSH,  NYSE,    12.61, 2013-01-25\\n  PWR,  NYSE,    36.11, 2013-02-22\\n   PX,  NYSE,     0.00, 2013-01-24\\n  KWR,  NYSE,    26.32, 2013-03-07\\n    R,  NYSE,     6.36, 2013-02-01\\n  RBC,  NYSE,     2.70, 2013-02-05\\n  RDC,  NYSE,    28.57, 2013-03-01\\n HTSI,  NYSE,   -20.69, 2013-02-01\\n  RES,  NYSE,     8.33, 2013-01-24\\n  RGS,  NYSE,   -76.92, 2013-02-01\\n  RGR,  NYSE,    36.99, 2013-02-28\\n  RHI,  NYSE,     2.44, 2013-01-30\\n  RJF,  NYSE,     0.00, 2013-01-24\\n  RLI,  NYSE,   102.27, 2013-01-24\\n  ROG,  NYSE,    -8.62, 2013-02-20\\n  ROK,  NYSE,    -2.38, 2013-01-31\\n  ROL,  NYSE,    -5.88, 2013-01-24\\n  ROP,  NYSE,     1.37, 2013-01-29\\n  RTI,  NYSE,    25.00, 2013-02-07\\n  RTN,  NYSE,    23.08, 2013-01-25\\n  RYL,  NYSE,    12.00, 2013-01-30\\n BSAC,  NYSE,    -1.96, 2013-02-05\\n    T,  NYSE,    -6.38, 2013-01-25\\n  SCG,  NYSE,     0.00, 2013-02-22\\n SCHW,  NYSE,     0.00, 2013-01-17\\n  SCL,  NYSE,    -5.56, 2013-02-20\\n  SMG,  NYSE,     0.88, 2013-02-07\\n  SEE,  NYSE,    17.24, 2013-02-20\\n   SF,  NYSE,     5.17, 2013-02-26\\n  SFE,  NYSE,  -121.74, 2013-03-08\\n  SHW,  NYSE,    -0.87, 2013-02-01\\n  STC,  NYSE,    29.27, 2013-02-15\\n  SJI,  NYSE,    -6.67, 2013-03-01\\n  JOE,  NYSE, -1000.00, 2013-03-01\\n  SJW,  NYSE,    72.22, 2013-02-20\\n  SLB,  NYSE,     0.00, 2013-01-19\\n  HSH,  NYSE,    29.17, 2013-02-01\\n  AOS,  NYSE,    12.35, 2013-01-25\\n  SNA,  NYSE,     4.38, 2013-02-08\\n  PII,  NYSE,     0.81, 2013-01-30\\n  SNV,  NYSE,     0.00, 2013-01-23\\n   SO,  NYSE,    12.82, 2013-01-31\\n  SON,  NYSE,     3.70, 2013-02-14\\n  SPA,  NYSE,    30.00, 2013-02-06\\n  TRV,  NYSE,   500.00, 2013-01-23\\n   SR,  NYSE,    14.68, 2013-02-06\\n  NVE,  NYSE,     0.00, 2013-02-23\\n  SCI,  NYSE,    10.00, 2013-02-13\\n  SSP,  NYSE,    -3.85, 2013-02-27\\n  STT,  NYSE,    11.00, 2013-01-19\\n  STI,  NYSE,     6.56, 2013-01-19\\n  STJ,  NYSE,     2.22, 2013-01-24\\n  STL,  NYSE,    14.29, 2013-01-24\\n  STR,  NYSE,     8.57, 2013-02-21\\n  STE,  NYSE,     3.57, 2013-02-07\\n  SYK,  NYSE,     0.88, 2013-01-24\\n  SUN,  NYSE,    -4.88, 2013-03-30\\n  SUP,  NYSE,   -61.54, 2013-03-02\\n  SWK,  NYSE,     3.01, 2013-01-25\\n  SWN,  NYSE,     2.33, 2013-02-21\\n  SWS,  NYSE,     0.00, 2013-02-07\\n  SWX,  NYSE,    -2.44, 2013-02-27\\n  SWY,  NYSE,    23.68, 2013-02-22\\n  SXI,  NYSE,     1.10, 2013-02-02\\n  SYY,  NYSE,    19.51, 2013-02-05\\n  TNC,  NYSE,     6.90, 2013-02-20\\n  TCB,  NYSE,   -16.67, 2013-01-31\\n  TCO,  NYSE,     5.15, 2013-02-14\\n  TDS,  NYSE,  -725.00, 2013-02-27\\n  TDW,  NYSE,    38.64, 2013-02-02\\n  TDY,  NYSE,     8.33, 2013-01-25\\n   TE,  NYSE,     0.00, 2013-02-06\\n  TER,  NYSE,   600.00, 2013-01-24\\n TEVA,  NYSE,    -0.75, 2013-02-08\\n  TEX,  NYSE,   -51.28, 2013-02-20\\n  TFX,  NYSE,     1.79, 2013-02-22\\n  TEN,  NYSE,    -2.94, 2013-02-01\\n  TKR,  NYSE,    25.00, 2013-01-25\\n  TMK,  NYSE,     1.53, 2013-02-05\\n  TMO,  NYSE,     6.25, 2013-02-01\\n  TOT,  NYSE,    -1.12, 2013-02-14\\n   TM,  NYSE,   -44.72, 2013-02-06\\n   TR,  NYSE,    37.50, 2013-02-14\\n  TRN,  NYSE,     7.14, 2013-02-21\\n  TRP,  NYSE,   -15.09, 2013-02-13\\n  TRR,  NYSE,   566.67, 2013-02-07\\n  TSO,  NYSE,    -2.90, 2013-02-07\\n  TSS,  NYSE,    -3.03, 2013-01-23\\n  TTI,  NYSE,   -21.05, 2013-03-01\\n  TXT,  NYSE,    -1.75, 2013-01-24\\n  TYL,  NYSE,    10.71, 2013-02-07\\n  TSN,  NYSE,    23.08, 2013-02-02\\n  UDR,  NYSE,     2.94, 2013-02-06\\n  UFI,  NYSE,   -42.86, 2013-01-23\\n  UGI,  NYSE,   -15.89, 2013-02-01\\n  UAM,  NYSE,    45.45, 2013-02-20\\n  UHS,  NYSE,     9.89, 2013-03-01\\n  UHT,  NYSE,   268.42, 2013-02-28\\n  UIL,  NYSE,    -9.68, 2013-02-22\\n  UNH,  NYSE,     0.00, 2013-01-18\\n KMPR,  NYSE,  -250.00, 2013-02-08\\n  UNM,  NYSE,     5.13, 2013-02-06\\n  UNP,  NYSE,     1.39, 2013-01-25\\n  UNT,  NYSE,     2.06, 2013-02-20\\n  URS,  NYSE,    -1.04, 2013-02-26\\n  USG,  NYSE,   -67.86, 2013-02-07\\n  MUX,  NYSE,  -600.00, 2013-03-09\\n  USM,  NYSE, -1100.00, 2013-02-27\\n USPH,  NYSE,     3.03, 2013-03-08\\n  UTL,  NYSE,     3.13, 2013-01-31\\n  UTX,  NYSE,    26.47, 2013-01-24\\n  VMI,  NYSE,     8.48, 2013-02-13\\n  VAR,  NYSE,     3.49, 2013-01-24\\n  VFC,  NYSE,     1.32, 2013-02-16\\n  CBS,  NYSE,    -8.57, 2013-02-15\\n  VLO,  NYSE,    57.98, 2013-01-30\\n  VMC,  NYSE,   -81.82, 2013-02-15\\n  VLY,  NYSE,     0.00, 2013-01-31\\n  VNO,  NYSE,     6.09, 2013-02-27\\n  VSH,  NYSE,    37.50, 2013-02-06\\n  WTS,  NYSE,     5.17, 2013-02-20\\n  WBS,  NYSE,     6.12, 2013-01-19\\n  WEC,  NYSE,     4.88, 2013-01-31\\n  WFC,  NYSE,     3.41, 2013-01-14\\n   WG,  NYSE,    57.14, 2013-03-07\\n  WGL,  NYSE,     9.62, 2013-02-07\\n  WHR,  NYSE,     3.15, 2013-02-01\\n  WMB,  NYSE,    -3.85, 2013-02-21\\n  WMK,  NYSE,    20.29, 2013-03-06\\n  WNC,  NYSE,     3.23, 2013-02-06\\n  TEG,  NYSE,    -5.32, 2013-03-01\\n   WR,  NYSE,    80.00, 2013-03-01\\n  WRE,  NYSE,     2.17, 2013-02-14\\n  WRI,  NYSE,     4.44, 2013-02-15\\n  WPP,  NYSE,  -175.00, 2013-02-12\\n  WSO,  NYSE,   -12.77, 2013-02-15\\n  WST,  NYSE,     8.93, 2013-02-22\\n  WWW,  NYSE,   200.00, 2013-02-20\\n   WY,  NYSE,    36.84, 2013-01-26\\n    X,  NYSE,    45.33, 2013-01-30\\n   XL,  NYSE,   138.24, 2013-02-08\\n  XOM,  NYSE,    10.00, 2013-02-02\\n  XRX,  NYSE,     7.14, 2013-01-25\\n    Y,  NYSE,    54.64, 2013-02-22\\n  HRG,  NYSE,   -50.00, 2013-02-09\\n  CRY,  NYSE,    33.33, 2013-02-15\\n  CHK,  NYSE,    85.71, 2013-02-22\\n  DDR,  NYSE,     0.00, 2013-02-13\\n  ELS,  NYSE,     0.00, 2013-01-29\\n  ALG,  NYSE,    37.93, 2013-03-07\\n  ETH,  NYSE,     5.41, 2013-01-23\\n  ATR,  NYSE,     0.00, 2013-02-08\\n  GGP,  NYSE,     6.90, 2013-02-05\\n  MSL,  NYSE,   -10.00, 2013-01-30\\n  RCL,  NYSE,    66.67, 2013-02-05\\n CWEI,  NYSE,   -34.04, 2013-02-22\\n   HR,  NYSE,     0.00, 2013-02-21\\n  RGA,  NYSE,    35.56, 2013-02-01\\n  RIG,  NYSE,    12.35, 2013-03-02\\n  SKT,  NYSE,     2.22, 2013-02-13\\n  TWI,  NYSE,   -80.85, 2013-02-26\\n  BDN,  NYSE,    17.86, 2013-02-07\\n  KGC,  NYSE,    -4.55, 2013-02-14\\n  YPF,  NYSE,    26.67, 2013-03-13\\n  CPT,  NYSE,     1.04, 2013-02-01\\n  SGY,  NYSE,    67.27, 2013-02-26\\n  BFS,  NYSE,   -11.48, 2013-03-08\\n  BWA,  NYSE,     3.57, 2013-02-15\\n  EQR,  NYSE,     0.00, 2013-02-06\\n  CLP,  NYSE,   -81.25, 2013-02-08\\n  KOF,  NYSE,    -7.78, 2013-02-28\\n  OKS,  NYSE,     3.13, 2013-02-26\\n  SQM,  NYSE,   -15.63, 2013-03-06\\n  BYD,  NYSE,  -138.46, 2013-03-05\\n  CBL,  NYSE,     8.77, 2013-02-06\\n DECK,  NYSE,     7.36, 2013-03-01\\n   IT,  NYSE,     6.78, 2013-02-08\\n  GFI,  NYSE,   -36.36, 2013-02-15\\n  HST,  NYSE,     8.11, 2013-02-22\\n  LXP,  NYSE,     0.00, 2013-02-22\\n  OMG,  NYSE,  -533.33, 2013-02-20\\n  REG,  NYSE,     8.62, 2013-01-31\\n  TUC,  NYSE,    -5.56, 2013-03-08\\n   AF,  NYSE,     7.14, 2013-01-24\\n  BFR,  NYSE,    13.33, 2013-02-09\\n  HHS,  NYSE,    26.32, 2013-02-01\\n  MHO,  NYSE,    -3.45, 2013-02-01\\n  NFX,  NYSE,   -36.36, 2013-02-20\\n  SPG,  NYSE,    13.93, 2013-02-05\\n   SU,  NYSE,   -14.20, 2013-02-06\\n  SUI,  NYSE,    -2.44, 2013-02-22\\n   TV,  NYSE,     5.13, 2013-02-26\\n  CGI,  NYSE,     0.00, 2013-01-24\\n  CYT,  NYSE,    77.42, 2013-02-01\\n  EMN,  NYSE,     0.00, 2013-02-01\\n  GRT,  NYSE,     0.00, 2013-02-15\\n  MAA,  NYSE,    -1.74, 2013-02-07\\n  PLT,  NYSE,     0.00, 2013-01-30\\n  BZH,  NYSE,    24.27, 2013-01-29\\n  ELX,  NYSE,     0.00, 2013-02-01\\n  AGM,  NYSE,    -5.41, 2013-03-19\\n  MLM,  NYSE,   -13.21, 2013-02-13\\n  AKS,  NYSE,    14.29, 2013-01-30\\n  ALB,  NYSE,    18.18, 2013-01-23\\n  VRX,  NYSE,    -4.00, 2013-03-01\\n  CBR,  NYSE,   140.00, 2013-02-22\\n  MAC,  NYSE,     3.45, 2013-02-07\\n  RKT,  NYSE,     5.47, 2013-01-23\\n  RYN,  NYSE,     3.51, 2013-01-25\\n  ADC,  NYSE,     1.96, 2013-02-28\\nBRK.B,  NYSE,     0.88, 2013-03-02\\n  EXP,  NYSE,     0.00, 2013-02-07\\n  GGB,  NYSE,   -66.67, 2013-02-22\\n  SSD,  NYSE,  -100.00, 2013-02-08\\n  ESS,  NYSE,     4.02, 2013-02-01\\n   FR,  NYSE,     0.00, 2013-02-21\\n  HIW,  NYSE,     0.00, 2013-02-13\\n IMAX,  NYSE,    58.33, 2013-02-22\\n  AIV,  NYSE,     4.00, 2013-02-08\\n  FCH,  NYSE,    50.00, 2013-02-20\\n ITGR,  NYSE,     6.00, 2013-02-26\\n  GEO,  NYSE,     7.32, 2013-02-22\\n  CLI,  NYSE,     4.76, 2013-02-08\\n  DAR,  NYSE,   -20.00, 2013-02-28\\n   RS,  NYSE,     9.28, 2013-02-22\\n  CPE,  NYSE,   -66.67, 2013-03-15\\n  KNX,  NYSE,     4.76, 2013-01-31\\n    O,  NYSE,     3.70, 2013-02-15\\n  PKX,  NYSE,   -15.35, 2013-03-02\\n  COF,  NYSE,   -12.35, 2013-01-18\\n  CYD,  NYSE,   -23.14, 2013-02-28\\n  IRS,  NYSE,    57.50, 2013-02-20\\n  MCK,  NYSE,   -13.50, 2013-02-01\\n  SWC,  NYSE,   116.67, 2013-02-28\\n  STM,  NYSE,   -22.22, 2013-01-31\\n  TEO,  NYSE,    28.36, 2013-03-01\\n  TRK,  NYSE,   400.00, 2013-03-07\\n  GFF,  NYSE,   300.00, 2013-01-31\\n  LMT,  NYSE,    -0.56, 2013-01-25\\n  APU,  NYSE,   -13.89, 2013-02-01\\n  AGU,  NYSE,     6.93, 2013-02-22\\n   LH,  NYSE,    -4.35, 2013-02-09\\n  DDD,  NYSE,     0.00, 2013-02-26\\n  WEX,  NYSE,     0.94, 2013-02-07\\n  AFG,  NYSE,     3.08, 2013-02-12\\n  RMD,  NYSE,     3.92, 2013-01-25\\n  WAB,  NYSE,     2.29, 2013-02-20\\n  CIB,  NYSE,    20.39, 2013-03-05\\n  CAM,  NYSE,    -1.04, 2013-02-01\\n  FCX,  NYSE,     5.41, 2013-01-23\\n  RNR,  NYSE,    70.27, 2013-02-06\\n  AVX,  NYSE,   -20.00, 2013-01-25\\n  RWT,  NYSE,    85.19, 2013-02-22\\n  AXE,  NYSE,     0.76, 2013-01-30\\n  CLB,  NYSE,     3.54, 2013-01-31\\n   MD,  NYSE,     1.54, 2013-02-01\\n  THG,  NYSE,     6.25, 2013-02-07\\n  BAP,  NYSE,     3.72, 2013-02-06\\n   DO,  NYSE,    28.18, 2013-02-06\\n   RE,  NYSE,   175.86, 2013-02-07\\n  DST,  NYSE,    17.82, 2013-02-01\\n   EL,  NYSE,    11.54, 2013-02-06\\n  ESC,  NYSE,   -34.88, 2013-03-01\\n  MIG,  NYSE,  -100.00, 2013-02-13\\n  WAT,  NYSE,     0.63, 2013-01-23\\n  EME,  NYSE,    11.48, 2013-02-27\\n  HIG,  NYSE,    80.00, 2013-02-05\\n  ITT,  NYSE,     2.63, 2013-02-28\\n  SPN,  NYSE,     4.26, 2013-02-27\\n  SWM,  NYSE,    -9.18, 2013-02-07\\n SCCO,  NYSE,     0.00, 2013-02-02\\n  RCI,  NYSE,    20.55, 2013-02-15\\n  EIX,  NYSE,    66.04, 2013-02-27\\n  IRM,  NYSE,   -20.00, 2013-03-01\\n  REV,  NYSE,   -19.18, 2013-02-06\\n  SPH,  NYSE,   -17.46, 2013-02-08\\n  CCJ,  NYSE,    46.34, 2013-02-09\\n  PGI,  NYSE,    -6.67, 2013-02-14\\n  CRR,  NYSE,     2.30, 2013-02-01\\n  BVN,  NYSE,   -26.67, 2013-03-01\\n  FCN,  NYSE,    11.67, 2013-03-01\\n  RPT,  NYSE,     8.00, 2013-02-13\\n  TUP,  NYSE,     1.79, 2013-01-30\\n  ASB,  NYSE,     0.00, 2013-01-18\\n  GWR,  NYSE,    -2.47, 2013-02-13\\n  TBI,  NYSE,    35.71, 2013-02-07\\n  FFG,  NYSE,    24.00, 2013-02-08\\n USNA,  NYSE,     4.96, 2013-02-06\\n  CSV,  NYSE,     4.35, 2013-02-26\\n  LVB,  NYSE,    12.77, 2013-03-07\\n  ALR,  NYSE,     6.25, 2013-02-16\\n  OCN,  NYSE,    -7.84, 2013-03-01\\n  PAA,  NYSE,    42.03, 2013-02-07\\n  DNR,  NYSE,    24.14, 2013-02-22\\n  HMY,  NYSE,    50.00, 2013-02-05\\n  TGI,  NYSE,     5.80, 2013-01-31\\n  PAG,  NYSE,     7.55, 2013-02-07\\n  GEL,  NYSE,    -2.86, 2013-02-15\\n   IM,  NYSE,    23.73, 2013-02-14\\n  LIN,  NYSE,   -21.92, 2013-03-01\\n  NUS,  NYSE,     2.11, 2013-02-07\\n  CNI,  NYSE,    -0.70, 2013-01-23\\n  LAD,  NYSE,    10.45, 2013-02-21\\n  NSP,  NYSE,     4.44, 2013-02-09\\n  DEL,  NYSE,   -29.63, 2013-02-28\\n  DGX,  NYSE,    -3.81, 2013-01-24\\n  KRC,  NYSE,     3.23, 2013-01-31\\n  MTH,  NYSE,    50.00, 2013-02-01\\n  NCR,  NYSE,     4.35, 2013-02-08\\n  OFG,  NYSE,   -50.00, 2013-02-08\\n  IVZ,  NYSE,    -4.26, 2013-02-01\\n   DX,  NYSE,     9.68, 2013-02-21\\n  FBC,  NYSE,    38.27, 2013-02-09\\n  ALV,  NYSE,     9.85, 2013-02-01\\n  ARE,  NYSE,     0.87, 2013-02-08\\n  BBT,  NYSE,     2.86, 2013-01-18\\n  CGG,  NYSE,   -59.32, 2013-03-02\\n  BXP,  NYSE,     2.42, 2013-01-30\\n   MS,  NYSE,    73.08, 2013-01-19\\n  SRT,  NYSE,   200.00, 2013-02-28\\n  HLX,  NYSE,   162.86, 2013-02-21\\n  FLS,  NYSE,     0.35, 2013-02-22\\n   MT,  NYSE,  -880.00, 2013-02-07\\n  PXD,  NYSE,    -2.35, 2013-02-14\\n  SLG,  NYSE,     0.87, 2013-01-31\\n  NAT,  NYSE,     0.00, 2013-02-12\\n  CSU,  NYSE,   -22.22, 2013-03-07\\n  DRQ,  NYSE,     2.70, 2013-03-01\\n  FDP,  NYSE,  -100.00, 2013-02-20\\n  NLY,  NYSE,    35.29, 2013-02-07\\n  TLM,  NYSE,  -300.00, 2013-02-18\\n  TSM,  NYSE,     0.00, 2013-01-18\\n  YUM,  NYSE,     2.47, 2013-02-05\\n  AMG,  NYSE,     4.94, 2013-01-30\\n  EPR,  NYSE,    -4.40, 2013-02-27\\n   FE,  NYSE,     1.27, 2013-02-26\\n  LFL,  NYSE,   -80.00, 2013-05-01\\n  MTD,  NYSE,     8.44, 2013-02-07\\n  SID,  NYSE,    57.14, 2013-03-29\\n   IN,  NYSE,   -18.18, 2013-03-12\\n   AI,  NYSE,     9.91, 2013-02-07\\n  URI,  NYSE,    23.30, 2013-01-24\\n INGR,  NYSE,     4.26, 2013-02-08\\n  RAS,  NYSE,   153.85, 2013-02-14\\n  UNS,  NYSE,    12.50, 2013-02-27\\n  ASI,  NYSE,   -17.95, 2013-03-07\\n  ANH,  NYSE,     7.14, 2013-02-08\\n  OFC,  NYSE,     4.08, 2013-02-09\\n  GPX,  NYSE,     6.67, 2013-02-27\\n  WAC,  NYSE,    11.32, 2013-03-19\\n  RBA,  NYSE,   -12.50, 2013-02-27\\n  WDR,  NYSE,     5.17, 2013-01-30\\n  LHO,  NYSE,     4.44, 2013-02-21\\n  LNT,  NYSE,    -1.72, 2013-02-15\\n LVLT,  NYSE,    11.11, 2013-02-13\\n  MFA,  NYSE,     0.00, 2013-03-07\\n  OME,  NYSE,    33.33, 2013-03-06\\n  EQY,  NYSE,     7.14, 2013-02-21\\n  FII,  NYSE,    10.00, 2013-01-25\\n  FMX,  NYSE,    39.60, 2013-02-28\\n  LLL,  NYSE,     6.13, 2013-01-31\\n  VTR,  NYSE,     2.06, 2013-02-16\\n  WCN,  NYSE,    -7.69, 2013-02-15\\n  AVB,  NYSE,    -0.71, 2013-01-31\\n  GIL,  NYSE,     6.67, 2013-02-07\\n  HZO,  NYSE,    10.00, 2013-01-30\\n  AWR,  NYSE,    43.24, 2013-03-01\\n  CLS,  NYSE,    46.67, 2013-01-23\\n  EPD,  NYSE,     7.58, 2013-02-01\\n  RSG,  NYSE,   -13.95, 2013-02-08\\n   WM,  NYSE,    -5.00, 2013-02-15\\n  AKR,  NYSE,     3.57, 2013-02-06\\n  CVG,  NYSE,     4.17, 2013-02-08\\n  RRC,  NYSE,   228.57, 2013-02-27\\n  SAP,  NYSE,    -2.38, 2013-01-24\\n  CCI,  NYSE,    57.14, 2013-01-24\\n   PQ,  NYSE,   -20.00, 2013-03-01\\n  WFT,  NYSE,   -94.44, 2013-02-27\\n  CAA,  NYSE,    14.29, 2013-02-01\\n  ENB,  NYSE,    -6.67, 2013-02-16\\n  GMK,  NYSE,    -8.33, 2013-02-28\\n  MMR,  NYSE,    75.00, 2013-01-19\\n   PB,  NYSE,     1.19, 2013-01-26\\n  VIV,  NYSE,    -7.25, 2013-02-26\\n  AXL,  NYSE,  -111.76, 2013-02-09\\n   BP,  NYSE,    19.05, 2013-02-06\\n  ETM,  NYSE,    13.04, 2013-02-09\\n   HT,  NYSE,    10.00, 2013-02-21\\n  BYI,  NYSE,     5.26, 2013-02-01\\n  CEB,  NYSE,     4.84, 2013-02-07\\n INFY,  NYSE,     5.56, 2013-01-12\\n  JLL,  NYSE,    -0.38, 2013-01-30\\n  AZN,  NYSE,    24.64, 2013-02-01\\n  SFG,  NYSE,     7.23, 2013-01-30\\n TREX,  NYSE,    27.78, 2013-02-20\\n   GS,  NYSE,    61.38, 2013-01-17\\n  SYX,  NYSE,  -144.44, 2013-03-06\\n  WCC,  NYSE,    -2.75, 2013-02-01\\n JNPR,  NYSE,    26.67, 2013-01-25\\n  RDN,  NYSE,  -146.43, 2013-02-12\\n  RAI,  NYSE,     4.11, 2013-02-13\\n  SKX,  NYSE,   172.73, 2013-02-14\\n  WTM,  NYSE,   724.10, 2013-02-06\\n  NCI,  NYSE,    29.17, 2013-02-15\\n  BLT,  NYSE,   -21.74, 2013-03-08\\n  BLK,  NYSE,     5.88, 2013-01-18\\n  CIR,  NYSE,    25.45, 2013-03-01\\n  PKG,  NYSE,    -1.61, 2013-01-23\\n  PKI,  NYSE,     0.00, 2013-02-01\\n  UGP,  NYSE,    38.10, 2013-02-21\\n  WWE,  NYSE,     0.00, 2013-03-01\\n  SNN,  NYSE,     2.86, 2013-02-08\\n  UPS,  NYSE,    -4.35, 2013-02-01\\n XOXO,  NYSE,    62.50, 2013-03-07\\n  SLF,  NYSE,    36.36, 2013-02-14\\n  CDR,  NYSE,    33.33, 2013-03-08\\n  RLH,  NYSE,   -21.43, 2013-03-01\\n   EW,  NYSE,    16.88, 2013-02-05\\n  MET,  NYSE,     5.93, 2013-02-13\\n  FBR,  NYSE,   -28.57, 2013-01-31\\n  VVC,  NYSE,    23.81, 2013-02-15\\n  BAM,  NYSE,   148.28, 2013-02-16\\n  NVS,  NYSE,     0.00, 2013-01-24\\n  VGR,  NYSE,   -43.75, 2013-02-27\\n BHLB,  NYSE,     0.00, 2013-01-29\\n  CRL,  NYSE,     6.67, 2013-02-14\\n  CYH,  NYSE,     0.00, 2013-02-22\\n  MBT,  NYSE,    65.71, 2013-03-20\\n MTOR,  NYSE,  -375.00, 2013-01-31\\n  CNQ,  NYSE,   -29.55, 2013-03-08\\n  ERJ,  NYSE,   -25.27, 2013-03-13\\n   VZ,  NYSE,   -28.30, 2013-01-23\\n  EVC,  NYSE,    12.50, 2013-02-28\\n  PBR,  NYSE,     0.00, 2013-02-05\\n  XEL,  NYSE,     3.57, 2013-02-01\\n  ALE,  NYSE,     0.00, 2013-02-16\\n   HW,  NYSE,   -20.00, 2013-01-30\\n  POL,  NYSE,     0.00, 2013-01-30\\n  UMC,  NYSE,     0.00, 2013-02-07\\n  ASX,  NYSE,    41.43, 2013-01-31\\n  COH,  NYSE,    -4.65, 2013-01-23\\n  CXW,  NYSE,     7.32, 2013-02-14\\n  DVA,  NYSE,     6.33, 2013-02-15\\n  EXC,  NYSE,    -1.54, 2013-02-08\\n  MCO,  NYSE,     7.14, 2013-02-09\\n BRFS,  NYSE,    43.48, 2013-03-06\\n   TU,  NYSE,    -1.15, 2013-02-16\\n  WIT,  NYSE,     0.00, 2013-01-18\\n  ERF,  NYSE,   462.50, 2013-02-22\\n   GG,  NYSE,   -22.22, 2013-02-15\\n  HNT,  NYSE,    -2.70, 2013-01-31\\n  NXY,  NYSE,   -23.44, 2013-02-26\\n NYCB,  NYSE,    -3.45, 2013-01-31\\n  SXT,  NYSE,    -8.33, 2013-02-08\\n  CPG,  NYSE,  -191.67, 2013-03-15\\n  AMX,  NYSE,   -40.00, 2013-02-13\\n  MPX,  NYSE,   -50.00, 2013-01-24\\n  OIS,  NYSE,    -5.82, 2013-02-20\\n   BH,  NYSE,   -35.35, 2013-01-26\\n  MMP,  NYSE,     6.15, 2013-02-06\\n  PES,  NYSE,   250.00, 2013-02-14\\n  ABB,  NYSE,   -18.75, 2013-02-15\\n  RDY,  NYSE,   -27.27, 2013-02-15\\n  KMR,  NYSE,   -19.23, 2013-02-22\\n  GEN,  NYSE,   -20.00, 2013-02-12\\n  ADS,  NYSE,     2.38, 2013-02-01\\n  CVI,  NYSE,     5.15, 2013-03-13\\n  FTI,  NYSE,     0.00, 2013-02-13\\n  PRA,  NYSE,    10.64, 2013-02-20\\n  STO,  NYSE,    26.47, 2013-02-08\\n  BEL,  NYSE,  -266.67, 2013-02-21\\n  FIS,  NYSE,    -8.82, 2013-02-13\\n  COL,  NYSE,     4.44, 2013-01-19\\n  KAI,  NYSE,     7.32, 2013-02-27\\n  FRM,  NYSE,   233.33, 2013-03-09\\n  ABC,  NYSE,     0.00, 2013-01-25\\n   BG,  NYSE,   -76.15, 2013-02-08\\n  FRO,  NYSE,   106.52, 2013-02-22\\n  ECA,  NYSE,    -3.12, 2013-02-15\\n   CS,  NYSE,   -54.76, 2013-02-08\\n  EEP,  NYSE,   -30.77, 2013-02-14\\n  CVX,  NYSE,    -1.65, 2013-02-02\\n   DB,  NYSE,   280.49, 2013-02-01\\n  GXP,  NYSE,   200.00, 2013-03-01\\n  JHX,  NYSE,   371.43, 2013-02-28\\n  PFG,  NYSE,    10.81, 2013-02-01\\n  PVR,  NYSE,  -227.78, 2013-02-21\\n  AAP,  NYSE,    17.33, 2013-02-08\\n  KND,  NYSE,     4.55, 2013-02-26\\n  WTW,  NYSE,     9.09, 2013-02-14\\n  CNC,  NYSE,    42.42, 2013-02-06\\n  PRU,  NYSE,    -2.87, 2013-02-07\\n  BCH,  NYSE,    12.94, 2013-02-06\\n   NS,  NYSE,   -19.35, 2013-02-02\\n ITUB,  NYSE,    -5.00, 2013-02-05\\n  SXL,  NYSE,    20.88, 2013-02-21\\n VALE,  NYSE,   -26.00, 2013-02-28\\n  TNP,  NYSE,  -128.57, 2013-04-20\\n  LCI,  NYSE,   233.33, 2013-02-08\\n  AUO,  NYSE,  -122.73, 2013-02-07\\n  GTI,  NYSE,    19.05, 2013-02-27\\n  HNR,  NYSE,  -127.27, 2013-05-04\\n  MWE,  NYSE,   -38.89, 2013-02-28\\n  NLS,  NYSE,     4.55, 2013-03-05\\n  RGC,  NYSE,    40.00, 2013-02-08\\n  SBS,  NYSE,    48.25, 2013-03-22\\n  JAH,  NYSE,     2.40, 2013-02-15\\n  NPO,  NYSE,   110.71, 2013-02-08\\n  TRI,  NYSE,     9.09, 2013-02-14\\n  CAE,  NYSE,    12.50, 2013-02-14\\n   LF,  NYSE,   971.43, 2013-02-07\\n  SNY,  NYSE,     1.30, 2013-02-08\\n  WHG,  NYSE,    15.91, 2013-02-08\\n BANC,  NYSE,  -300.00, 2013-03-02\\n  GTN,  NYSE,     4.35, 2013-02-21\\n  BAK,  NYSE,  -150.00, 2013-02-08\\n  COP,  NYSE,     1.42, 2013-01-31\\n  CNP,  NYSE,    40.00, 2013-02-28\\n  EEQ,  NYSE,   -18.18, 2013-02-15\\n  MRH,  NYSE,    60.26, 2013-02-08\\n  NGS,  NYSE,    26.09, 2013-03-15\\n  NRP,  NYSE,    34.88, 2013-02-14\\n  PXP,  NYSE,   -22.64, 2013-02-22\\n  XEC,  NYSE,     9.26, 2013-02-20\\n  IAG,  NYSE,   -11.11, 2013-02-21\\n   TS,  NYSE,   -16.44, 2013-02-22\\n  EGO,  NYSE,     6.67, 2013-02-23\\n  JNS,  NYSE,    35.71, 2013-01-25\\n  PFS,  NYSE,     7.41, 2013-02-02\\n  ENH,  NYSE,    21.68, 2013-02-08\\n  IHG,  NYSE,     5.56, 2013-02-20\\n  CNX,  NYSE,    95.45, 2013-02-01\\n  AMT,  NYSE,   -17.07, 2013-02-27\\n  ABG,  NYSE,    10.77, 2013-02-20\\n  LII,  NYSE,     0.00, 2013-02-06\\n  SRE,  NYSE,    11.34, 2013-02-27\\n  AEE,  NYSE,   -36.36, 2013-02-21\\n  PLD,  NYSE,     0.00, 2013-02-07\\n  SAH,  NYSE,     4.00, 2013-02-21\\n  GPI,  NYSE,   -17.50, 2013-02-20\\n  FIX,  NYSE,   -11.11, 2013-03-01\\n  MMS,  NYSE,    12.50, 2013-02-08\\n  SRI,  NYSE,   -28.57, 2013-03-02\\n RTEC,  NYSE,     6.25, 2013-02-05\\n  NOV,  NYSE,     3.47, 2013-02-02\\n   DF,  NYSE,    33.33, 2013-02-14\\n  SAM,  NYSE,     1.63, 2013-02-21\\n   RL,  NYSE,     8.60, 2013-02-07\\n  FLR,  NYSE,   132.35, 2013-02-21\\n  ALL,  NYSE,   942.86, 2013-02-07\\n  ATI,  NYSE,     5.88, 2013-01-24\\n   EE,  NYSE,   -14.29, 2013-02-20\\n  AIT,  NYSE,     0.00, 2013-02-01\\n  CHH,  NYSE,     9.76, 2013-02-12\\n  FMS,  NYSE,   105.77, 2013-02-27\\n  BCO,  NYSE,    -7.69, 2013-02-02\\n  CBB,  NYSE,  -125.00, 2013-02-28\\n  MWW,  NYSE,     0.00, 2013-02-08\\n  PSA,  NYSE,     5.68, 2013-02-22\\n    E,  NYSE,     2.83, 2013-02-16\\n  JPM,  NYSE,    15.83, 2013-01-17\\n  USB,  NYSE,     1.35, 2013-01-17\\n  HON,  NYSE,     0.92, 2013-01-26\\n  ITG,  NYSE,   100.00, 2013-02-01\\n  ARB,  NYSE,     6.25, 2013-02-26\\n  APL,  NYSE,     0.00, 2013-02-19\\n  AVA,  NYSE,   -42.22, 2013-02-21\\n  AXS,  NYSE,    64.96, 2013-02-05\\n  CHT,  NYSE,     5.26, 2013-01-31\\n  MOH,  NYSE,   145.45, 2013-02-08\\n  CVD,  NYSE,     2.82, 2013-01-25\\n  AHT,  NYSE,     2.63, 2013-02-28\\n  GPK,  NYSE,    12.50, 2013-02-08\\n  CNO,  NYSE,     8.70, 2013-02-12\\n  AUQ,  NYSE,   -28.57, 2013-03-26\\n  JRN,  NYSE,    34.62, 2013-03-08\\nGRP.U,  NYSE,   -14.92, 2013-03-06\\n  NFP,  NYSE,    11.43, 2013-02-15\\n  CRI,  NYSE,     2.30, 2013-02-28\\n  FMD,  NYSE,   -20.00, 2013-02-08\\n  FPO,  NYSE,    10.34, 2013-02-22\\n  TRQ,  NYSE,  -350.00, 2013-03-26\\n  WLL,  NYSE,     9.21, 2013-02-28\\n  AEL,  NYSE,    14.63, 2013-02-21\\n  AHL,  NYSE,    87.60, 2013-02-08\\n  AUY,  NYSE,    -3.70, 2013-02-21\\n  CMP,  NYSE,     0.00, 2013-02-07\\n  KRO,  NYSE,  -400.00, 2013-03-13\\n  TPX,  NYSE,     9.09, 2013-01-25\\n  UTI,  NYSE,    75.00, 2013-02-01\\n  PJC,  NYSE,    31.34, 2013-01-31\\n  TRW,  NYSE,    14.81, 2013-02-16\\n  AIZ,  NYSE,   122.58, 2013-02-07\\n  HTH,  NYSE,    62.50, 2013-03-16\\n  ETP,  NYSE,     0.00, 2013-02-21\\n  SMI,  NYSE,   500.00, 2013-02-07\\n  LSE,  NYSE,    -6.25, 2013-02-16\\n  BBD,  NYSE,    -2.63, 2013-01-29\\n  NRG,  NYSE,   124.14, 2013-02-28\\n  HOS,  NYSE,    29.17, 2013-02-07\\n  ABR,  NYSE,   160.00, 2013-02-16\\n  FHN,  NYSE,     0.00, 2013-01-19\\n  AGO,  NYSE,    32.39, 2013-02-28\\n  HSP,  NYSE,     1.85, 2013-02-14\\n  HNI,  NYSE,    -6.98, 2013-02-06\\n  GHL,  NYSE,   -32.43, 2013-01-24\\n  XPO,  NYSE,   -14.00, 2013-02-28\\n  CVO,  NYSE,    23.08, 2013-02-28\\n  CHE,  NYSE,    16.92, 2013-02-19\\n  GNW,  NYSE,    30.77, 2013-02-06\\n  CBG,  NYSE,    12.24, 2013-02-07\\n  SFL,  NYSE,   -26.67, 2013-02-26\\n  NEU,  NYSE,   -15.57, 2013-01-29\\n  GOL,  NYSE,  -109.09, 2013-03-26\\n  CAB,  NYSE,     4.17, 2013-02-15\\n  LTM,  NYSE,     1.82, 2013-02-22\\n  VVI,  NYSE,    10.53, 2013-02-02\\n  WCG,  NYSE,     0.00, 2013-02-14\\n  HEP,  NYSE,    -2.63, 2013-02-22\\n  DPZ,  NYSE,     8.47, 2013-03-01\\n  BDC,  NYSE,     9.86, 2013-02-08\\n  EGY,  NYSE,  -171.43, 2013-03-15\\n  LPL,  NYSE,     2.63, 2013-02-22\\n  ENS,  NYSE,    12.82, 2013-02-07\\n  BMR,  NYSE,     5.88, 2013-02-06\\n  ACC,  NYSE,     9.26, 2013-02-13\\n  KRG,  NYSE,    -9.09, 2013-02-08\\n  WLK,  NYSE,    13.60, 2013-02-20\\n  EXR,  NYSE,     4.65, 2013-02-22\\n  CNS,  NYSE,    16.67, 2013-01-24\\n  IOC,  NYSE,   264.29, 2013-02-28\\n STON,  NYSE,  -233.33, 2013-03-16\\n  CPL,  NYSE,    38.10, 2013-03-13\\n TPGI,  NYSE,  -114.29, 2013-02-14\\n  SHO,  NYSE,    -3.33, 2013-02-20\\n CUBE,  NYSE,     5.00, 2013-02-22\\n  NRF,  NYSE,   170.37, 2013-02-15\\n  BBW,  NYSE,   -68.29, 2013-02-15\\n  DLR,  NYSE,     4.31, 2013-02-16\\n  NWE,  NYSE,     2.63, 2013-02-15\\n  ORA,  NYSE,   200.00, 2013-02-28\\n   NP,  NYSE,     5.26, 2013-02-21\\n  SMA,  NYSE,   -21.05, 2013-02-22\\n  BBG,  NYSE,    25.00, 2013-02-22\\n  BXC,  NYSE,  -163.16, 2013-02-14\\n  KNL,  NYSE,    32.14, 2013-02-06\\n  LVS,  NYSE,    -8.47, 2013-01-31\\n  HLF,  NYSE,     0.96, 2013-02-20\\n  MIC,  NYSE,   -20.41, 2013-02-21\\n  PHH,  NYSE,   -11.54, 2013-02-07\\n   CE,  NYSE,     6.35, 2013-01-29\\n  EDR,  NYSE,     0.00, 2013-02-20\\n  WTI,  NYSE,     8.33, 2013-02-27\\n  ARC,  NYSE,  -100.00, 2013-03-01\\n  PBH,  NYSE,     8.82, 2013-02-08\\n  HUN,  NYSE,     0.00, 2013-02-13\\n  DLB,  NYSE,     4.44, 2013-01-30\\n  DSX,  NYSE,   -33.33, 2013-03-15\\n  LAZ,  NYSE,    84.85, 2013-02-08\\n  TGP,  NYSE,     1.82, 2013-02-22\\n  TLP,  NYSE,   -43.48, 2013-03-13\\n  DRH,  NYSE,    16.00, 2013-03-01\\n HTGC,  NYSE,     8.70, 2013-03-01\\n  KFN,  NYSE,     5.26, 2013-02-06\\n  THS,  NYSE,     0.00, 2013-02-22\\n  NSR,  NYSE,   -12.50, 2013-02-06\\n  WAL,  NYSE,     0.00, 2013-01-25\\n  SLW,  NYSE,     2.04, 2013-03-22\\n  MPW,  NYSE,     0.00, 2013-02-08\\nRDS.B,  NYSE,    16.00, 2013-02-01\\n  GNK,  NYSE,   -24.71, 2013-02-21\\n  MFB,  NYSE,     4.76, 2013-03-07\\nRDS.A,  NYSE,     9.95, 2013-02-01\\n  ITC,  NYSE,     0.93, 2013-02-28\\n  FTK,  NYSE,  -158.82, 2013-03-14\\n PIKE,  NYSE,   168.00, 2013-02-06\\n  ALJ,  NYSE,     0.00, 2013-03-07\\n  DRC,  NYSE,    -4.55, 2013-03-01\\n  STN,  NYSE,     8.06, 2013-02-22\\n  SSW,  NYSE,    -6.90, 2013-03-06\\n   CF,  NYSE,     3.41, 2013-02-20\\n  HPY,  NYSE,     0.00, 2013-02-08\\n ACCO,  NYSE,     0.00, 2013-02-14\\n  ROC,  NYSE,    -6.25, 2013-02-20\\n  WPZ,  NYSE,   -28.57, 2013-02-20\\n  LCC,  NYSE,    44.44, 2013-01-24\\n  GLP,  NYSE,    58.82, 2013-03-15\\n  AMP,  NYSE,    15.54, 2013-01-31\\n  DHT,  NYSE,   108.33, 2013-01-30\\n  FNF,  NYSE,    17.86, 2013-02-20\\n   NM,  NYSE,    20.00, 2013-02-20\\n  CCO,  NYSE,    25.00, 2013-02-20\\n  BWP,  NYSE,     0.00, 2013-02-12\\n  ICE,  NYSE,     5.14, 2013-02-07\\n  BKD,  NYSE,   -57.14, 2013-02-12\\n  AAV,  NYSE,   350.00, 2013-03-28\\n  BAS,  NYSE,   -42.11, 2013-02-20\\n  CPA,  NYSE,    -9.87, 2013-02-07\\n  LYV,  NYSE,  -147.06, 2013-02-27\\n  WNR,  NYSE,     5.84, 2013-03-01\\n  CMG,  NYSE,     0.00, 2013-02-06\\n  RGP,  NYSE,  -180.00, 2013-02-21\\n  KOP,  NYSE,    11.86, 2013-02-15\\n  UAL,  NYSE,    -7.41, 2013-01-25\\n  ETE,  NYSE,   -90.91, 2013-02-21\\n  RSO,  NYSE,   -17.65, 2013-03-05\\n  XCO,  NYSE,     6.25, 2013-02-21\\n  PAC,  NYSE,    41.18, 2013-02-28\\n  NYX,  NYSE,    10.26, 2013-02-06\\n  TDG,  NYSE,    51.65, 2013-02-05\\n  BMA,  NYSE,    18.40, 2013-02-15\\n  THI,  NYSE,    -2.82, 2013-02-22\\n  BTE,  NYSE,   -40.48, 2013-03-08\\n  CNH,  NYSE,    29.58, 2013-02-01\\n  GLA,  NYSE,    67.44, 2013-02-14\\n  POR,  NYSE,    -9.52, 2013-02-23\\n  HIL,  NYSE,  -100.00, 2013-03-12\\n  HVB,  NYSE,   -20.00, 2013-02-01\\n   KS,  NYSE,     0.00, 2013-02-14\\n   HK,  NYSE,     0.00, 2013-03-01\\n  DCP,  NYSE,    59.62, 2013-02-28\\n   DK,  NYSE,    10.10, 2013-03-08\\n CODI,  NYSE,    14.81, 2013-03-07\\n   VG,  NYSE,    25.00, 2013-02-14\\n   MA,  NYSE,     1.46, 2013-02-01\\n  MWA,  NYSE,  -200.00, 2013-02-06\\n  KOG,  NYSE,    14.29, 2013-03-01\\n  PWE,  NYSE,  -500.00, 2013-02-15\\n PGTI,  NYSE,   100.00, 2013-02-21\\n  AWH,  NYSE,    16.23, 2013-02-14\\n  NSH,  NYSE,   -65.71, 2013-02-02\\n  WYN,  NYSE,     5.00, 2013-02-07\\n  WNS,  NYSE,     0.00, 2013-01-17\\n  AYR,  NYSE,    36.84, 2013-02-22\\n  EVR,  NYSE,    55.77, 2013-01-31\\n  HBI,  NYSE,     7.00, 2013-02-06\\n   WU,  NYSE,    20.00, 2013-02-13\\n   OC,  NYSE,   -31.25, 2013-02-21\\n   MR,  NYSE,     2.08, 2013-02-26\\n  DAC,  NYSE,   -21.43, 2013-02-12\\n  AWI,  NYSE,     3.03, 2013-02-20\\n SUSS,  NYSE,   444.44, 2013-02-28\\n  DEI,  NYSE,     0.00, 2013-02-13\\n   OB,  NYSE,  -200.00, 2013-02-06\\n  SBH,  NYSE,    -5.88, 2013-02-08\\n  EBS,  NYSE,    -4.35, 2013-03-08\\n  KBR,  NYSE,   122.22, 2013-02-21\\n  AER,  NYSE,    30.95, 2013-02-21\\n  NOA,  NYSE,   -11.11, 2013-02-06\\n  SPR,  NYSE,    -2.27, 2013-02-13\\n  ANW,  NYSE,     0.00, 2013-02-28\\n  DCT,  NYSE,    10.00, 2013-02-08\\n   SE,  NYSE,    -3.03, 2013-02-06\\n  TOO,  NYSE,    16.67, 2013-02-22\\n  TSL,  NYSE,   -39.77, 2013-02-27\\n  TWC,  NYSE,     1.95, 2013-02-01\\n  MVO,  NYSE,    -5.06, 2013-03-15\\n   CO,  NYSE,    40.00, 2013-02-27\\n  EXK,  NYSE,   -45.83, 2013-03-13\\n  EIG,  NYSE,   -25.00, 2013-02-28\\n   HF,  NYSE,    21.62, 2013-03-07\\n  CEL,  NYSE,    34.78, 2013-03-05\\n  FIG,  NYSE,    53.85, 2013-02-28\\n NGLS,  NYSE,     0.00, 2013-02-15\\n TCAP,  NYSE,     3.64, 2013-03-07\\n  GFA,  NYSE,  -483.33, 2013-03-12\\n   BR,  NYSE,    -5.56, 2013-02-08\\n  SCR,  NYSE,    85.71, 2013-03-08\\n  CNK,  NYSE,   -12.82, 2013-02-21\\n  DAL,  NYSE,     0.00, 2013-01-23\\n  ORN,  NYSE,   250.00, 2013-03-01\\n  ACM,  NYSE,     9.09, 2013-02-06\\n  JMP,  NYSE,    62.50, 2013-02-14\\n  SLH,  NYSE,     1.69, 2013-02-08\\n  CLR,  NYSE,    16.85, 2013-02-28\\n  BGS,  NYSE,   -17.95, 2013-02-15\\n STAR,  NYSE,    12.50, 2013-02-27\\n  YGE,  NYSE,   -74.07, 2013-03-05\\n  DFS,  NYSE,    -9.40, 2013-03-06\\n  TEL,  NYSE,     1.56, 2013-01-24\\n   BX,  NYSE,    25.53, 2013-02-01\\n  SEP,  NYSE,     8.11, 2013-02-06\\n   BZ,  NYSE,   -30.00, 2013-02-27\\n  PPO,  NYSE,   -28.26, 2013-02-21\\n  PRO,  NYSE,    25.00, 2013-02-13\\n  WBC,  NYSE,    13.68, 2013-02-16\\n  DHX,  NYSE,     7.14, 2013-01-31\\n  PMC,  NYSE,    13.79, 2013-02-08\\n  HGG,  NYSE,     0.00, 2013-02-01\\n  OWW,  NYSE,   -14.29, 2013-02-15\\n   VR,  NYSE,    35.58, 2013-02-01\\n  CXO,  NYSE,    -5.88, 2013-02-21\\n    G,  NYSE,     4.76, 2013-02-08\\n   EJ,  NYSE,   160.00, 2013-03-13\\n   WX,  NYSE,    32.00, 2013-03-08\\n CMLP,  NYSE,   -50.00, 2013-02-06\\n  VMW,  NYSE,    -5.56, 2013-01-29\\n  CZZ,  NYSE,    63.64, 2013-02-08\\n  CGA,  NYSE,    -3.23, 2013-02-09\\n  TDC,  NYSE,     5.71, 2013-02-08\\n  FLY,  NYSE,   137.65, 2013-03-08\\n  DUF,  NYSE,     6.25, 2013-02-26\\n MAIN,  NYSE,    12.00, 2013-03-08\\n  REN,  NYSE,   -50.00, 2013-03-08\\n  TGH,  NYSE,     9.57, 2013-02-13\\n  DFT,  NYSE,    -5.00, 2013-02-07\\n   RF,  NYSE,    10.00, 2013-01-23\\n  PZN,  NYSE,   -22.22, 2013-02-13\\n   LL,  NYSE,    19.05, 2013-02-21\\n  NMM,  NYSE,     0.00, 2013-01-25\\n  OZM,  NYSE,     5.48, 2013-02-08\\n   ES,  NYSE,    -5.08, 2013-02-20\\n MSCI,  NYSE,    -1.89, 2013-02-08\\n  ARR,  NYSE,   -18.52, 2013-02-23\\n   KW,  NYSE,   275.00, 2013-03-13\\n  GTS,  NYSE,   -10.17, 2013-02-07\\n  FOR,  NYSE,   222.22, 2013-02-14\\n  LRN,  NYSE,     4.35, 2013-02-06\\n  TNK,  NYSE,  -125.00, 2013-02-22\\n    N,  NYSE,    21.43, 2013-02-01\\n  DAN,  NYSE,     5.56, 2013-02-22\\n  BIP,  NYSE,    12.07, 2013-02-09\\n  CPN,  NYSE,  -500.00, 2013-02-14\\n  SOL,  NYSE,     2.70, 2013-03-15\\n   PM,  NYSE,     1.64, 2013-02-08\\n   HI,  NYSE,     7.89, 2013-02-05\\n    V,  NYSE,     2.25, 2013-02-07\\n  IPI,  NYSE,     0.00, 2013-02-14\\n  AWK,  NYSE,   -14.29, 2013-02-27\\n  HTS,  NYSE,    37.84, 2013-02-13\\n  DPS,  NYSE,    -4.71, 2013-02-14\\n  CFX,  NYSE,     7.69, 2013-02-07\\n  WES,  NYSE,   -27.91, 2013-02-28\\n   SB,  NYSE,   -10.00, 2013-02-21\\n   LO,  NYSE,     3.95, 2013-02-14\\n  LPS,  NYSE,    10.45, 2013-02-08\\n   FF,  NYSE,   -31.82, 2013-03-19\\n  NNA,  NYSE,   150.00, 2013-02-13\\n  EPB,  NYSE,    14.55, 2013-01-17\\n  JBT,  NYSE,     3.23, 2013-03-07\\n   DL,  NYSE,    33.33, 2013-02-27\\n  RAX,  NYSE,    -4.55, 2013-02-13\\n  HCI,  NYSE,    67.61, 2013-03-06\\n   EC,  NYSE,   -20.47, 2013-02-16\\n  CLW,  NYSE,    10.53, 2013-02-21\\n  MJN,  NYSE,     5.88, 2013-02-01\\n  EPC,  NYSE,     1.85, 2013-02-01\\n  BPI,  NYSE,    -3.33, 2013-03-13\\n  RST,  NYSE,    55.56, 2013-03-01\\n  DGI,  NYSE,    92.31, 2013-02-27\\n  SWI,  NYSE,    10.34, 2013-02-05\\n  CYS,  NYSE,   -46.15, 2013-02-07\\n  IVR,  NYSE,    20.31, 2013-02-06\\n  BUD,  NYSE,    -5.08, 2013-02-28\\n  PMT,  NYSE,    -2.35, 2013-02-08\\n STWD,  NYSE,    15.38, 2013-02-28\\n  CFN,  NYSE,   -16.98, 2013-02-09\\n  SPB,  NYSE,    71.43, 2013-02-07\\n  ARI,  NYSE,   -10.34, 2013-02-28\\n CLNY,  NYSE,   -13.89, 2013-03-07\\n  ART,  NYSE,   300.00, 2013-02-15\\n  SEM,  NYSE,    12.00, 2013-02-22\\n BSBR,  NYSE,   578.57, 2013-03-28\\n DOLE,  NYSE, -6100.00, 2013-03-13\\n  VSI,  NYSE,     0.00, 2013-02-27\\n  TWO,  NYSE,   -15.15, 2013-02-07\\n  CVE,  NYSE,   -14.29, 2013-02-15\\n    H,  NYSE,    81.82, 2013-02-14\\n  LEA,  NYSE,     7.25, 2013-02-02\\n  CLD,  NYSE,     8.00, 2013-02-14\\n  AOL,  NYSE,     7.50, 2013-02-09\\n CHSP,  NYSE,     5.13, 2013-02-22\\n  PEB,  NYSE,     0.00, 2013-02-22\\n  CIT,  NYSE,    60.94, 2013-01-30\\n  KAR,  NYSE,    -4.55, 2013-02-21\\n  CIE,  NYSE,   -66.67, 2013-02-27\\n  TMH,  NYSE,     8.33, 2013-02-06\\n  KRA,  NYSE,  -300.00, 2013-02-28\\n  SYA,  NYSE,   -29.41, 2013-02-05\\n TRNO,  NYSE,  -162.50, 2013-02-16\\n  PDM,  NYSE,    -2.70, 2013-02-08\\n GNRC,  NYSE,    26.09, 2013-02-15\\n  ACW,  NYSE,    -2.17, 2013-03-07\\n BALT,  NYSE,   -11.76, 2013-02-21\\n   ST,  NYSE,     2.17, 2013-01-31\\n SEMG,  NYSE,    55.56, 2013-03-01\\n CALX,  NYSE,    20.00, 2013-02-06\\n  MXL,  NYSE,   -57.14, 2013-02-06\\n STNG,  NYSE,   -60.00, 2013-02-26\\n  PRI,  NYSE,    -1.43, 2013-02-08\\n SDRL,  NYSE,   -93.65, 2013-03-01\\n CLDT,  NYSE,     0.00, 2013-02-20\\n  EXL,  NYSE,     0.00, 2013-02-28\\n  LYB,  NYSE,    -0.88, 2013-02-02\\n  PNG,  NYSE,     7.14, 2013-02-07\\n PLOW,  NYSE,   -25.00, 2013-03-12\\n  SIX,  NYSE,   198.00, 2013-02-21\\n  NKA,  NYSE,  1066.67, 2013-02-01\\n RRTS,  NYSE,     0.00, 2013-02-07\\n  JKS,  NYSE,  -332.48, 2013-04-11\\n CODE,  NYSE,   -13.64, 2013-01-30\\n  FAF,  NYSE,    44.64, 2013-02-22\\n  QEP,  NYSE,     3.13, 2013-02-20\\n  OAS,  NYSE,     6.52, 2013-02-26\\n  VPG,  NYSE,    15.38, 2013-02-13\\n  HPP,  NYSE,     9.52, 2013-03-07\\n   FN,  NYSE,     9.09, 2013-02-05\\n  ECT,  NYSE,    65.85, 2013-03-16\\n QUAD,  NYSE,    -6.67, 2013-03-05\\n  KKR,  NYSE,    54.84, 2013-02-08\\n  RLD,  NYSE,    20.00, 2013-02-07\\n AMRC,  NYSE,    44.44, 2013-03-19\\n GDOT,  NYSE,    50.00, 2013-02-01\\n   AT,  NYSE,  -160.00, 2013-03-01\\n  ENV,  NYSE,     0.00, 2013-02-15\\n   IL,  NYSE,   200.00, 2013-02-22\\n  WSR,  NYSE,   -12.00, 2013-03-13\\n SFUN,  NYSE,    35.71, 2013-02-09\\n  COR,  NYSE,     5.00, 2013-02-23\\n   VC,  NYSE,    20.62, 2013-03-01\\n CCSC,  NYSE,   -20.00, 2013-03-07\\n  CCG,  NYSE,     0.00, 2013-02-27\\n  EFC,  NYSE,   -72.73, 2013-02-14\\n TOWR,  NYSE,   183.33, 2013-02-16\\n CHMT,  NYSE,   -53.13, 2013-02-26\\n  HBM,  NYSE,   200.00, 2013-02-21\\n EXAM,  NYSE,    55.56, 2013-02-28\\n  XUE,  NYSE,     7.69, 2013-02-28\\n CMRE,  NYSE,     6.67, 2013-01-24\\n NOAH,  NYSE,    20.00, 2013-02-26\\n IPHI,  NYSE,   -40.00, 2013-02-05\\n BITA,  NYSE,    33.33, 2013-03-08\\n  BAH,  NYSE,    11.11, 2013-01-31\\n   GM,  NYSE,    -2.04, 2013-02-15\\n TROX,  NYSE,   -60.00, 2013-02-21\\n DANG,  NYSE,    20.00, 2013-03-08\\n YOKU,  NYSE,     9.09, 2013-03-01\\n  FRC,  NYSE,   -16.44, 2013-01-17\\n  RFP,  NYSE,    52.38, 2013-02-13\\n  ISS,  NYSE,    15.38, 2013-03-09\\n   WD,  NYSE,   -14.29, 2013-03-07\\n  FLT,  NYSE,    10.00, 2013-02-08\\n GCAP,  NYSE,  -325.00, 2013-03-13\\n  FRF,  NYSE,   -25.93, 2013-03-29\\n SWFT,  NYSE,    46.15, 2013-01-24\\n   AG,  NYSE,   -10.34, 2013-02-27\\n  QRE,  NYSE,  -174.07, 2013-03-07\\n  AAT,  NYSE,    11.76, 2013-02-20\\n  MCC,  NYSE,     5.41, 2013-02-07\\n NLSN,  NYSE,     3.51, 2013-02-12\\n AGRO,  NYSE,   -71.43, 2013-03-22\\n  BKU,  NYSE,    27.08, 2013-01-30\\n INXN,  NYSE,   -38.89, 2013-02-28\\n NPTN,  NYSE,    16.67, 2013-02-22\\n  INN,  NYSE,    25.00, 2013-02-27\\n  KMI,  NYSE,    -5.88, 2013-01-17\\n  HCA,  NYSE,     9.64, 2013-02-05\\n   MX,  NYSE,   135.21, 2013-01-31\\n  HII,  NYSE,     8.89, 2013-02-28\\n QIHU,  NYSE,   175.00, 2013-03-06\\n  APO,  NYSE,   119.48, 2013-02-09\\n  GNC,  NYSE,     8.70, 2013-02-15\\n  SDT,  NYSE,    11.48, 2013-03-16\\n  UAN,  NYSE,    16.67, 2013-02-28\\n ARCO,  NYSE,     5.00, 2013-03-09\\n ELLI,  NYSE,    36.36, 2013-02-15\\n  TMS,  NYSE,   -23.81, 2013-02-15\\n SQNS,  NYSE,   -16.00, 2013-02-08\\n STAG,  NYSE,    17.24, 2013-02-21\\n   AL,  NYSE,     8.33, 2013-03-01\\n TLLP,  NYSE,    10.42, 2013-02-12\\n RENN,  NYSE,    14.29, 2013-03-12\\n   NQ,  NYSE,   800.00, 2013-03-07\\n  THR,  NYSE,   -14.29, 2013-02-08\\n  KOS,  NYSE,   125.00, 2013-02-26\\n  RLJ,  NYSE,     4.35, 2013-02-28\\n  NGL,  NYSE,    -7.41, 2013-02-16\\n FENG,  NYSE,   100.00, 2013-03-07\\n LNKD,  NYSE,   900.00, 2013-02-08\\n NMFC,  NYSE,     5.88, 2013-03-07\\n ACTV,  NYSE,     5.26, 2013-02-15\\n TAOM,  NYSE,   700.00, 2013-03-15\\n RATE,  NYSE,   -60.00, 2013-02-13\\n  VHS,  NYSE,   -22.22, 2013-01-31\\n  MPC,  NYSE,     8.13, 2013-01-31\\n MITT,  NYSE,    -1.16, 2013-03-06\\n OILT,  NYSE,     0.00, 2013-03-07\\n  SXC,  NYSE,    14.71, 2013-02-06\\n AMTG,  NYSE,    -8.57, 2013-03-07\\n AMID,  NYSE, -2500.00, 2013-04-17\\n WAIR,  NYSE,    -7.41, 2013-01-30\\n  PER,  NYSE,    -7.58, 2013-03-02\\n  PPP,  NYSE,   -44.44, 2013-02-22\\n  FNV,  NYSE,    -8.33, 2013-03-20\\n  FSM,  NYSE,    16.67, 2013-03-21\\n FBHS,  NYSE,     4.55, 2013-02-01\\n  XLS,  NYSE,     4.44, 2013-03-02\\n  XYL,  NYSE,     2.17, 2013-02-08\\n NDRO,  NYSE,     4.76, 2013-03-19\\n  RNF,  NYSE,   -33.33, 2013-03-20\\n  VAC,  NYSE,    25.53, 2013-02-22\\n CHKR,  NYSE,    -7.25, 2013-03-16\\n PACD,  NYSE,    14.29, 2013-02-28\\n INVN,  NYSE,     0.00, 2013-01-24\\n DLPH,  NYSE,     3.45, 2013-02-06\\n   MN,  NYSE,     0.00, 2013-02-14\\n RRMS,  NYSE,   -25.00, 2013-03-01\\n  WPX,  NYSE,  -400.00, 2013-03-01\\n  LPI,  NYSE,     0.00, 2013-03-13\\n   SN,  NYSE,   -80.00, 2013-03-07\\n KORS,  NYSE,    60.00, 2013-02-13\\n BCEI,  NYSE,    -7.89, 2013-03-15\\n BOXC,  NYSE,     4.78, 2013-01-29\\n  PVG,  NYSE,   -25.00, 2013-03-06\\n POST,  NYSE,    30.43, 2013-02-08\\n SLCA,  NYSE,    32.26, 2013-02-27\\n MTDR,  NYSE,  -116.67, 2013-03-14\\n GWAY,  NYSE,  -200.00, 2013-02-13\\n EPAM,  NYSE,   -10.81, 2013-02-28\\n RNDY,  NYSE,     5.56, 2013-03-01\\n CPAC,  NYSE,   -13.33, 2013-02-21\\n PRLB,  NYSE,     7.69, 2013-02-14\\n YELP,  NYSE,   -50.00, 2013-02-07\\n  NSM,  NYSE,     7.58, 2013-03-08\\n ALSN,  NYSE,   257.14, 2013-02-20\\n DWRE,  NYSE,   350.00, 2013-02-15\\n VNTV,  NYSE,    16.13, 2013-02-21\\n   ET,  NYSE,    34.78, 2013-02-22\\n VIPS,  NYSE,  1100.00, 2013-02-22\\n VCRA,  NYSE,   -33.33, 2013-02-28\\n   RM,  NYSE,    -1.89, 2013-02-28\\n BNNY,  NYSE,     0.00, 2013-02-12\\n   MM,  NYSE,   200.00, 2013-02-20\\n  RXN,  NYSE,   -15.00, 2013-02-12\\n GLOG,  NYSE,   -20.00, 2013-02-28\\n  PBA,  NYSE,    44.44, 2013-03-02\\n RPAI,  NYSE,    15.79, 2013-02-20\\n  OAK,  NYSE,    63.33, 2013-02-15\\n  FET,  NYSE,    -3.45, 2013-02-15\\n  MRC,  NYSE,    17.02, 2013-02-22\\n  PSX,  NYSE,    21.18, 2013-01-31\\n TUMI,  NYSE,     0.00, 2013-03-21\\n ACRE,  NYSE,   -38.10, 2013-04-02\\n EVER,  NYSE,    17.24, 2013-01-31\\n  PDH,  NYSE,   -13.79, 2013-02-07\\n  WMC,  NYSE,     3.23, 2013-04-03\\n WAGE,  NYSE,     0.00, 2013-02-21\\n  HTA,  NYSE,     0.00, 2013-02-21\\n ALEX,  NYSE,    42.86, 2013-02-20\\n  BKW,  NYSE,    53.33, 2013-02-16\\n  EQM,  NYSE,    51.22, 2013-01-25\\n  NOW,  NYSE,    38.46, 2013-01-31\\n  EGL,  NYSE,    18.46, 2013-03-13\\n NGVC,  NYSE,    25.00, 2013-02-01\\n  NTI,  NYSE,   -25.00, 2013-03-14\\n AMRE,  NYSE,     4.35, 2013-02-20\\n GMED,  NYSE,    15.79, 2013-02-28\\n MANU,  NYSE,   -46.43, 2013-02-15\\n HCLP,  NYSE,   -28.57, 2013-02-01\\n  ADT,  NYSE,     4.76, 2013-01-31\\n TRLA,  NYSE,   -20.00, 2013-02-13\\n  SRC,  NYSE,     8.82, 2013-02-28\\n NBHC,  NYSE,   -14.29, 2013-01-29\\n BSMX,  NYSE,    -4.17, 2013-02-19\\n   HY,  NYSE,    14.53, 2013-02-20\\n SMLP,  NYSE,    40.00, 2013-03-14\\n  DYN,  NYSE, -1714.29, 2013-03-15\\n LXFR,  NYSE,    43.75, 2013-03-12\\n LOCK,  NYSE,    16.67, 2013-02-21\\n  JMI,  NYSE,    97.78, 2013-03-22\\n BERY,  NYSE,   -40.00, 2013-02-01\\n FLTX,  NYSE,     0.00, 2013-02-21\\n ANFI,  NYSE,    30.77, 2013-02-26\\n SSTK,  NYSE,  -100.00, 2013-02-22\\n SDLP,  NYSE,    90.91, 2013-03-01\\n MPLX,  NYSE,   -25.00, 2013-01-31\\n WWAV,  NYSE,     5.88, 2013-02-14\\n  SXE,  NYSE, -4121.43, 2013-03-29\\n  DKL,  NYSE,    -5.56, 2013-03-06\\n RKUS,  NYSE,   -20.00, 2013-02-13\\n  WGP,  NYSE,    57.14, 2013-02-28\\n  PBF,  NYSE,   -92.31, 2013-03-01\\n  SBY,  NYSE,     0.00, 2013-03-01\\n RIOM,  NYSE,    77.78, 2013-03-29\\n BFAM,  NYSE, -1186.36, 2013-03-27\\n  ZTS,  NYSE,   -79.41, 2013-03-29\\n  DDC,  NYSE,   -39.13, 2013-04-04\\n  ABM,  NYSE,    18.18, 2013-03-05\\n  ANN,  NYSE,     0.00, 2013-03-09\\n  BBY,  NYSE,     5.81, 2013-03-02\\n BF.B,  NYSE,     4.29, 2013-03-07\\n  BKE,  NYSE,     2.40, 2013-03-15\\n  BNS,  NYSE,    -3.17, 2013-03-06\\n  BRC,  NYSE,   -22.45, 2013-02-22\\n CATO,  NYSE,    -3.57, 2013-03-22\\n  COO,  NYSE,     2.50, 2013-03-08\\n  CPB,  NYSE,     6.06, 2013-02-16\\n  CFI,  NYSE,    10.34, 2013-02-28\\n  DCI,  NYSE,   -10.53, 2013-02-26\\n  DDS,  NYSE,    -1.03, 2013-02-26\\n   DE,  NYSE,    17.02, 2013-02-14\\n   DY,  NYSE,    50.00, 2013-02-27\\n   EV,  NYSE,    -3.85, 2013-02-21\\n  ENZ,  NYSE,  -133.33, 2013-03-13\\n  ESL,  NYSE,    13.11, 2013-03-01\\nFCE.A,  NYSE,     9.09, 2013-03-28\\n    M,  NYSE,     3.54, 2013-02-27\\n  GCO,  NYSE,     1.41, 2013-03-09\\n  GPS,  NYSE,     2.82, 2013-03-01\\n   HD,  NYSE,     4.69, 2013-02-27\\n  HEI,  NYSE,   -12.50, 2013-02-21\\n  HNZ,  NYSE,    10.00, 2013-02-28\\n  HOV,  NYSE,   -66.67, 2013-03-07\\n  HRB,  NYSE,  -633.33, 2013-03-08\\n  HRL,  NYSE,    -2.04, 2013-02-22\\n  HPQ,  NYSE,    15.49, 2013-02-22\\n  JCP,  NYSE,  -926.32, 2013-02-28\\n   KR,  NYSE,    25.71, 2013-03-08\\n  KSS,  NYSE,     1.84, 2013-03-01\\n   LB,  NYSE,     1.15, 2013-02-28\\n  LOW,  NYSE,    13.04, 2013-02-26\\n  LZB,  NYSE,    16.67, 2013-02-20\\n  MDT,  NYSE,     2.20, 2013-02-20\\n  MEI,  NYSE,   350.00, 2013-03-01\\n  MPR,  NYSE,     0.00, 2013-03-22\\n  NAV,  NYSE,    14.11, 2013-03-08\\n  JWN,  NYSE,     4.48, 2013-02-22\\n  ODC,  NYSE,   -35.42, 2013-03-12\\n  OXM,  NYSE,    -5.80, 2013-04-03\\n  PBY,  NYSE,  -225.00, 2013-04-16\\n  PLL,  NYSE,     8.96, 2013-02-28\\n  PNY,  NYSE,     1.72, 2013-03-07\\n  PVH,  NYSE,     6.67, 2013-03-28\\n  THO,  NYSE,     0.00, 2013-03-08\\n  TIF,  NYSE,     2.19, 2013-03-23\\n  TJX,  NYSE,     1.23, 2013-02-28\\n  TOL,  NYSE,   -81.82, 2013-02-21\\n  TTC,  NYSE,    23.26, 2013-02-22\\n  VAL,  NYSE,    -9.09, 2013-02-13\\n JW.A,  NYSE,    13.41, 2013-03-08\\n  WMT,  NYSE,     6.37, 2013-02-22\\n  WSM,  NYSE,     4.69, 2013-03-20\\n   FL,  NYSE,   -11.11, 2013-03-09\\n  CHS,  NYSE,     0.00, 2013-03-01\\n  REX,  NYSE,  -800.00, 2013-03-29\\n  BKS,  NYSE,  -136.00, 2013-03-01\\n  CAL,  NYSE,    75.00, 2013-03-16\\n  SIG,  NYSE,     1.44, 2013-03-29\\n  ZLC,  NYSE,    -1.92, 2013-02-22\\n  AEO,  NYSE,     0.00, 2013-03-07\\n  FGP,  NYSE,   -10.00, 2013-03-08\\n  BMO,  NYSE,     1.37, 2013-02-27\\n   RY,  NYSE,     0.75, 2013-03-01\\n  GEF,  NYSE,   -13.21, 2013-02-28\\n  MOV,  NYSE,    70.83, 2013-03-22\\n  SKS,  NYSE,    13.33, 2013-02-27\\n   TD,  NYSE,     1.55, 2013-03-01\\n  ANF,  NYSE,    14.51, 2013-02-23\\n CIEN,  NYSE,   116.00, 2013-03-08\\n  KMG,  NYSE,   -17.65, 2013-03-09\\n IRET,  NYSE,    -5.88, 2013-03-13\\n   CM,  NYSE,     0.00, 2013-03-01\\nHEI.A,  NYSE,   -18.60, 2013-02-21\\n  UBA,  NYSE,    13.04, 2013-03-07\\n  KFY,  NYSE,     6.90, 2013-03-07\\n  TGT,  NYSE,    12.24, 2013-02-28\\n  KKD,  NYSE,     0.00, 2013-03-15\\n  NDZ,  NYSE,     0.00, 2013-03-06\\n  MVC,  NYSE,   -20.00, 2013-03-08\\n  CBK,  NYSE,    52.17, 2013-03-14\\n  SJM,  NYSE,     7.30, 2013-02-16\\n  BIG,  NYSE,     5.03, 2013-03-07\\n  IDT,  NYSE,    -7.14, 2013-03-08\\n  JOY,  NYSE,    14.91, 2013-02-28\\n  SSI,  NYSE,    -5.93, 2013-03-13\\n  GME,  NYSE,     3.35, 2013-03-29\\n  DKS,  NYSE,    -3.74, 2013-03-12\\n    A,  NYSE,    -5.97, 2013-02-15\\n  MTN,  NYSE,    -3.51, 2013-03-07\\n  GES,  NYSE,    10.47, 2013-03-21\\n  CRM,  NYSE,    66.67, 2013-03-01\\n  NWY,  NYSE,    25.00, 2013-03-22\\n  PAY,  NYSE,     8.11, 2013-03-06\\n  DSW,  NYSE,    -4.17, 2013-03-20\\n   NX,  NYSE,  -183.33, 2013-03-08\\n  AGX,  NYSE,    15.00, 2013-04-11\\n  CMD,  NYSE,    -5.26, 2013-03-08\\n   DG,  NYSE,     7.78, 2013-03-26\\n EXPR,  NYSE,     1.35, 2013-03-14\\n    P,  NYSE,     0.00, 2013-03-07\\n GWRE,  NYSE,   181.82, 2013-02-27\\n BLOX,  NYSE,   -20.00, 2013-02-22\\n TLYS,  NYSE,     6.67, 2013-03-21\\n PANW,  NYSE,  -250.00, 2013-03-01\\n WDAY,  NYSE,    24.00, 2013-03-08\\n   RH,  NYSE,     4.92, 2013-04-19\\n  AIR,  NYSE,     4.55, 2013-03-20\\n  ATU,  NYSE,    -5.41, 2013-03-21\\n  AZO,  NYSE,     0.84, 2013-02-27\\n  AZZ,  NYSE,     2.04, 2013-04-09\\n  CAG,  NYSE,    -3.51, 2013-04-04\\n  CLC,  NYSE,     2.17, 2013-03-21\\n  CMC,  NYSE,   -80.00, 2013-03-29\\n  KMX,  NYSE,     0.00, 2013-04-11\\n   FC,  NYSE,   -27.27, 2013-04-05\\n  FDO,  NYSE,    -0.82, 2013-04-11\\n  FDX,  NYSE,   -10.87, 2013-03-21\\n  FUL,  NYSE,    -3.92, 2013-03-28\\n  GIS,  NYSE,    12.28, 2013-03-21\\n  KBH,  NYSE,    30.43, 2013-03-22\\n  LEN,  NYSE,   100.00, 2013-03-21\\n  LNN,  NYSE,    16.28, 2013-03-28\\n  LUB,  NYSE,  -100.00, 2013-03-21\\n  MKC,  NYSE,     1.79, 2013-04-03\\n   RT,  NYSE,     0.00, 2013-04-11\\n  MSM,  NYSE,     0.00, 2013-04-11\\n  NKE,  NYSE,     8.96, 2013-03-22\\n ORCL,  NYSE,    -1.56, 2013-03-21\\n  PIR,  NYSE,     0.00, 2013-04-12\\n  PKE,  NYSE,   -21.43, 2013-05-10\\n  RPM,  NYSE,    16.67, 2013-04-05\\n  SVU,  NYSE,  -200.00, 2013-04-25\\n  TXI,  NYSE,    25.00, 2013-03-28\\n  UNF,  NYSE,    18.75, 2013-03-28\\n  WGO,  NYSE,    37.50, 2013-03-29\\n  WOR,  NYSE,     6.12, 2013-03-22\\n  JBL,  NYSE,    -2.17, 2013-03-21\\n  GBX,  NYSE,    21.62, 2013-04-05\\n  DRI,  NYSE,     0.99, 2013-03-23\\n  FDS,  NYSE,   -21.24, 2013-03-20\\n  SCS,  NYSE,     0.00, 2013-03-28\\n  SJR,  NYSE,     5.56, 2013-04-13\\n  RHT,  NYSE,    19.05, 2013-03-28\\n  OMN,  NYSE,   -75.00, 2013-04-04\\n  MON,  NYSE,     7.06, 2013-04-04\\n  GPN,  NYSE,    -1.14, 2013-04-03\\n  AYI,  NYSE,     0.00, 2013-04-04\\n  CCL,  NYSE,   100.00, 2013-03-16\\n  CUK,  NYSE,    33.33, 2013-03-16\\n  STZ,  NYSE,     4.44, 2013-04-11\\n  ACN,  NYSE,     3.09, 2013-03-29\\n  SNX,  NYSE,     1.15, 2013-03-28\\n  TAL,  NYSE,    50.00, 2013-04-24\\n  IHS,  NYSE,    11.90, 2013-03-22\\n  EDU,  NYSE,    63.64, 2013-04-25\\n  KED,  NYSE,   -99.22, 2013-05-02\\n CORR,  NYSE,    -9.09, 2013-05-11\\n  DFS,  NYSE,    18.75, 2013-04-24\\n  ZEP,  NYSE,    54.55, 2013-04-10\\n   MG,  NYSE,   -58.82, 2013-04-09\\n  MOS,  NYSE,     5.62, 2013-03-28\\n  ABT,  NYSE,     0.00, 2013-04-18\\n  ABX,  NYSE,     6.98, 2013-04-25\\n   AB,  NYSE,     8.57, 2013-05-02\\n  ACO,  NYSE,   -10.64, 2013-04-27\\n  ADM,  NYSE,    -5.88, 2013-05-01\\n  AEM,  NYSE,   -35.29, 2013-04-26\\n  AEP,  NYSE,     0.00, 2013-04-27\\n  AES,  NYSE,   -14.29, 2013-05-10\\n  AET,  NYSE,     8.70, 2013-05-01\\n  AFL,  NYSE,     4.32, 2013-04-25\\n AGCO,  NYSE,    35.23, 2013-05-01\\n  HES,  NYSE,    24.20, 2013-04-25\\n  AIG,  NYSE,    52.27, 2013-05-03\\n  AIN,  NYSE,     0.00, 2013-05-02\\n  AJG,  NYSE,    33.33, 2013-05-01\\n  ALU,  NYSE,   -81.82, 2013-04-27\\n MATX,  NYSE,    31.25, 2013-05-07\\n  ALK,  NYSE,    15.09, 2013-04-26\\n  ALX,  NYSE,    -2.56, 2013-05-07\\n BEAM,  NYSE,    18.52, 2013-05-03\\n  AME,  NYSE,     3.92, 2013-04-26\\n  TWX,  NYSE,     9.33, 2013-05-02\\n  AVD,  NYSE,    47.50, 2013-05-03\\n  AMN,  NYSE,    33.33, 2013-05-03\\n   AN,  NYSE,     7.94, 2013-04-19\\n  AON,  NYSE,     0.00, 2013-04-27\\n  APA,  NYSE,    -9.01, 2013-05-10\\n  APC,  NYSE,    17.39, 2013-05-07\\n  APD,  NYSE,     0.00, 2013-04-24\\n  APH,  NYSE,     1.16, 2013-04-19\\n  ARG,  NYSE,     0.88, 2013-05-03\\n  AAN,  NYSE,    -5.63, 2013-04-26\\n  ARW,  NYSE,     3.49, 2013-05-02\\n ASGN,  NYSE,    94.44, 2013-04-25\\n  ASH,  NYSE,    14.10, 2013-04-25\\n  ASR,  NYSE,   -13.25, 2013-04-23\\n  GAS,  NYSE,    -2.96, 2013-05-01\\n  ATO,  NYSE,     1.63, 2013-05-02\\n  ATW,  NYSE,     2.40, 2013-05-02\\n   AU,  NYSE,   -26.67, 2013-05-14\\n  AVP,  NYSE,    85.71, 2013-05-01\\n  AVT,  NYSE,     3.45, 2013-04-26\\n  AVY,  NYSE,     3.51, 2013-04-25\\n  AXP,  NYSE,     3.60, 2013-04-18\\n    B,  NYSE,   -11.11, 2013-04-27\\n   BA,  NYSE,    17.69, 2013-04-25\\n  BAC,  NYSE,   -13.04, 2013-04-17\\n  BAX,  NYSE,     0.96, 2013-04-19\\n   BC,  NYSE,    22.58, 2013-04-26\\n  OMX,  NYSE,   -52.17, 2013-05-08\\n  BCE,  NYSE,    10.00, 2013-05-10\\n  BCR,  NYSE,     0.00, 2013-04-24\\n  BDX,  NYSE,     6.67, 2013-05-03\\n  BEN,  NYSE,     8.47, 2013-05-01\\n  BGG,  NYSE,   -17.59, 2013-04-20\\n  BHE,  NYSE,    10.00, 2013-04-26\\n  BHI,  NYSE,     4.84, 2013-04-20\\n  BID,  NYSE,  -175.00, 2013-05-10\\n  BIO,  NYSE,   -38.18, 2013-05-08\\n   BK,  NYSE,     9.62, 2013-04-18\\n  BKH,  NYSE,    19.18, 2013-05-03\\n  WRB,  NYSE,     0.00, 2013-04-24\\n  BLC,  NYSE,     6.67, 2013-04-26\\n  BLL,  NYSE,    -9.38, 2013-04-26\\n  BLX,  NYSE,   -21.82, 2013-04-18\\n  BMI,  NYSE,   -58.33, 2013-04-17\\n  BMS,  NYSE,    -1.85, 2013-04-26\\n  BMY,  NYSE,     0.00, 2013-04-26\\n  BOH,  NYSE,    -6.90, 2013-04-23\\n  BXS,  NYSE,     4.76, 2013-04-23\\n  BPL,  NYSE,    19.44, 2013-05-04\\nBRK.A,  NYSE,   197.70, 2013-05-04\\n  BRO,  NYSE,     5.13, 2013-04-16\\n  BSX,  NYSE,     0.00, 2013-04-26\\n MTRN,  NYSE,    -2.94, 2013-04-26\\n  CAI,  NYSE,    -1.32, 2013-04-25\\n  CAT,  NYSE,    -2.24, 2013-04-23\\n   CB,  NYSE,    12.44, 2013-04-23\\n  CBI,  NYSE,    15.49, 2013-05-03\\n  CBM,  NYSE,    85.00, 2013-05-04\\n  CBU,  NYSE,    -1.96, 2013-04-24\\n  CBT,  NYSE,    -7.25, 2013-05-01\\n  CCC,  NYSE,    20.00, 2013-05-07\\n  CCE,  NYSE,     2.63, 2013-04-26\\n    C,  NYSE,     9.32, 2013-04-16\\n  CCK,  NYSE,     4.17, 2013-04-18\\n  CDE,  NYSE,   -74.07, 2013-05-10\\n  CDI,  NYSE,   -40.91, 2013-05-03\\n  CAH,  NYSE,    26.32, 2013-05-03\\n  CFR,  NYSE,    -4.21, 2013-04-25\\n  CHD,  NYSE,     5.56, 2013-05-03\\n  CPK,  NYSE,    14.93, 2013-05-03\\n   CI,  NYSE,    20.28, 2013-05-03\\n  CIA,  NYSE,     0.00, 2013-05-03\\n  CKH,  NYSE,  -156.12, 2013-04-30\\n   CL,  NYSE,     0.00, 2013-04-26\\n  CLF,  NYSE,    87.50, 2013-04-25\\n  CLH,  NYSE,    25.81, 2013-05-02\\n  CLX,  NYSE,    -5.66, 2013-05-02\\n  CMA,  NYSE,     4.48, 2013-04-17\\n  CMO,  NYSE,     3.33, 2013-04-25\\n  CRK,  NYSE,   -11.36, 2013-04-30\\n  CMS,  NYSE,    15.22, 2013-04-26\\n  CNA,  NYSE,    21.13, 2013-05-01\\n  CNW,  NYSE,   -29.63, 2013-05-02\\n  CHG,  NYSE,    19.00, 2013-05-10\\n  CNL,  NYSE,    -8.33, 2013-04-30\\n  COG,  NYSE,   -20.00, 2013-04-25\\n  COT,  NYSE,  -100.00, 2013-05-02\\n   CP,  NYSE,     2.54, 2013-04-25\\n  CPF,  NYSE,   105.00, 2013-04-27\\n  CQB,  NYSE,    28.57, 2013-05-08\\n   CR,  NYSE,    -0.95, 2013-04-23\\nCRD.B,  NYSE,   -29.17, 2013-05-09\\n  CRS,  NYSE,    -9.21, 2013-04-26\\n  CSC,  NYSE,    32.29, 2013-05-16\\n  CSL,  NYSE,     0.00, 2013-04-25\\n  CTB,  NYSE,    31.82, 2013-05-10\\n  CTL,  NYSE,    10.14, 2013-05-09\\n  CTS,  NYSE,    16.67, 2013-04-24\\n  CUB,  NYSE,    52.24, 2013-05-03\\n  CMI,  NYSE,   -22.58, 2013-05-01\\n  CUZ,  NYSE,    -8.33, 2013-05-09\\n  CVC,  NYSE,  -185.71, 2013-05-10\\n  CVH,  NYSE,    26.58, 2013-05-02\\n   CW,  NYSE,    28.21, 2013-05-02\\n  CWT,  NYSE,  -200.00, 2013-05-02\\n   CX,  NYSE,  -140.00, 2013-04-27\\n  CYN,  NYSE,    -2.17, 2013-04-19\\n    D,  NYSE,    -7.78, 2013-04-26\\n  DBD,  NYSE,  -125.00, 2013-05-01\\n  DCO,  NYSE,   -18.60, 2013-05-07\\n   DD,  NYSE,     1.30, 2013-04-24\\n  CVA,  NYSE,   -61.54, 2013-04-18\\n  DHR,  NYSE,    -1.32, 2013-04-19\\n  DIS,  NYSE,     2.60, 2013-05-08\\n  DLX,  NYSE,     3.41, 2013-04-26\\n  DNB,  NYSE,     2.26, 2013-05-03\\n  RRD,  NYSE,    12.12, 2013-04-26\\n  DOV,  NYSE,     1.85, 2013-04-18\\n  DOW,  NYSE,    15.00, 2013-04-26\\n  DRE,  NYSE,     0.00, 2013-04-25\\n  DHI,  NYSE,    60.00, 2013-04-27\\n  UFS,  NYSE,   -35.37, 2013-04-26\\n  DTE,  NYSE,    30.10, 2013-04-27\\n  DUK,  NYSE,    -1.92, 2013-05-04\\n  DVN,  NYSE,    17.86, 2013-05-02\\n   DV,  NYSE,     8.43, 2013-04-24\\n  EAT,  NYSE,     4.35, 2013-04-24\\n  ECL,  NYSE,     3.45, 2013-05-01\\n   ED,  NYSE,     4.85, 2013-05-03\\n  EDE,  NYSE,    11.11, 2013-04-26\\n  EFX,  NYSE,     0.00, 2013-04-25\\n  EGN,  NYSE,    -7.32, 2013-04-30\\n  EGP,  NYSE,    -1.30, 2013-04-19\\n  ELP,  NYSE,     0.00, 2013-05-17\\n  ELY,  NYSE,    65.00, 2013-04-26\\n  EMC,  NYSE,     3.23, 2013-04-25\\n  EMR,  NYSE,    -1.28, 2013-05-08\\n  EOG,  NYSE,    59.29, 2013-05-07\\n  EQT,  NYSE,    26.92, 2013-04-26\\n  ESE,  NYSE,   -17.65, 2013-05-08\\n  ESV,  NYSE,     5.43, 2013-04-30\\n  ETN,  NYSE,     6.33, 2013-04-30\\n  ETR,  NYSE,     0.00, 2013-04-26\\n EXAR,  NYSE,    16.67, 2013-05-01\\n    F,  NYSE,     7.89, 2013-04-25\\n CLGX,  NYSE,     8.11, 2013-04-25\\n  FNB,  NYSE,    -4.76, 2013-04-24\\n  FCF,  NYSE,     0.00, 2013-04-24\\n  FBP,  NYSE,  -122.22, 2013-05-04\\n FICO,  NYSE,    -9.38, 2013-04-25\\n  FLO,  NYSE,     6.98, 2013-05-17\\n  FMC,  NYSE,     1.85, 2013-05-01\\n  FOE,  NYSE,    66.67, 2013-04-25\\n    S,  NYSE,    38.24, 2013-04-25\\n  NEE,  NYSE,    10.89, 2013-05-01\\n  FRT,  NYSE,     0.88, 2013-05-02\\n  FRX,  NYSE,    47.06, 2013-04-24\\n  FSS,  NYSE,    20.00, 2013-05-07\\n  FUN,  NYSE,    24.32, 2013-05-09\\n  FUR,  NYSE,    77.78, 2013-05-03\\n  GBL,  NYSE,    17.86, 2013-05-08\\n  GVA,  NYSE,  -103.85, 2013-05-10\\n  BGC,  NYSE,  -319.23, 2013-05-01\\n   GD,  NYSE,     8.00, 2013-04-25\\n   GE,  NYSE,    11.43, 2013-04-20\\n  RHP,  NYSE,    26.47, 2013-05-08\\n AXLL,  NYSE,   -38.02, 2013-05-08\\n  GGG,  NYSE,    15.07, 2013-04-25\\n  GHM,  NYSE,    28.13, 2013-06-01\\n  GIB,  NYSE,    14.58, 2013-05-01\\n  GLT,  NYSE,    17.65, 2013-05-01\\n  GLW,  NYSE,    15.38, 2013-04-25\\n  GSK,  NYSE,     6.49, 2013-04-26\\n  GLF,  NYSE,   175.00, 2013-04-30\\n  GNI,  NYSE,   -14.58, 2013-04-26\\n  GPC,  NYSE,    -6.06, 2013-04-20\\n  GRA,  NYSE,     0.00, 2013-04-25\\n  GTY,  NYSE,     0.00, 2013-05-03\\n  GWW,  NYSE,     7.69, 2013-04-17\\n  HAE,  NYSE,     4.35, 2013-05-02\\n  HAL,  NYSE,    17.54, 2013-04-23\\n  HAR,  NYSE,    25.40, 2013-05-03\\n  HVT,  NYSE,    33.33, 2013-05-02\\n  HRC,  NYSE,    -2.00, 2013-04-25\\n  HCC,  NYSE,    31.71, 2013-05-01\\n  HCN,  NYSE,     1.11, 2013-05-08\\n  HCP,  NYSE,     2.78, 2013-05-01\\n  HOG,  NYSE,     2.06, 2013-04-26\\n   HE,  NYSE,   -12.82, 2013-05-09\\n   HL,  NYSE,   -66.67, 2013-05-11\\n  HMA,  NYSE,     0.00, 2013-05-03\\n  HMC,  NYSE,   -28.57, 2013-04-27\\n  HMN,  NYSE,     7.84, 2013-04-25\\n  HFC,  NYSE,    -7.91, 2013-05-08\\n  HOT,  NYSE,    43.40, 2013-05-01\\n   HP,  NYSE,     5.43, 2013-04-26\\n  HLS,  NYSE,    14.29, 2013-04-26\\n  HRS,  NYSE,     0.00, 2013-05-01\\n  HSC,  NYSE,    50.00, 2013-05-10\\n  HSY,  NYSE,     4.81, 2013-04-26\\n HUBB,  NYSE,    -0.90, 2013-04-19\\n  HUM,  NYSE,    51.12, 2013-05-02\\n  HXL,  NYSE,     4.88, 2013-04-23\\n  IBM,  NYSE,    -1.96, 2013-04-19\\n  IDA,  NYSE,    17.54, 2013-05-03\\n  IEX,  NYSE,     4.23, 2013-04-23\\n  IFF,  NYSE,     5.31, 2013-05-08\\n  DIN,  NYSE,    12.87, 2013-05-03\\n  INT,  NYSE,    14.06, 2013-05-01\\n   IP,  NYSE,   -12.16, 2013-05-03\\n  IPG,  NYSE,    -7.69, 2013-04-20\\n   IO,  NYSE,   -85.71, 2013-05-01\\n   IR,  NYSE,     2.44, 2013-04-24\\n  IRF,  NYSE,    27.50, 2013-04-30\\n  ITW,  NYSE,     0.00, 2013-04-24\\n  JEC,  NYSE,    -2.44, 2013-04-30\\n  JNJ,  NYSE,     2.13, 2013-04-17\\n  JNY,  NYSE,     0.00, 2013-05-02\\n    K,  NYSE,     0.00, 2013-05-03\\n KAMN,  NYSE,    -2.94, 2013-04-30\\n  KDN,  NYSE,     5.71, 2013-05-10\\n  KEX,  NYSE,     2.15, 2013-04-25\\n  KEY,  NYSE,     5.00, 2013-04-19\\n  KIM,  NYSE,     3.13, 2013-05-01\\n  KMB,  NYSE,    10.45, 2013-04-20\\n  KEM,  NYSE,  -133.33, 2013-05-10\\n  KMT,  NYSE,    -8.45, 2013-04-26\\n   KO,  NYSE,     2.22, 2013-04-17\\n  KSU,  NYSE,     2.30, 2013-04-20\\n  LDR,  NYSE,    -9.52, 2013-05-07\\n  LEG,  NYSE,   -13.16, 2013-04-26\\n  LLY,  NYSE,     8.57, 2013-04-25\\n   LM,  NYSE,   -13.33, 2013-05-01\\n  LNC,  NYSE,    -7.27, 2013-05-02\\n  LPX,  NYSE,     0.00, 2013-05-08\\n  LXU,  NYSE,  -110.53, 2013-05-07\\n  LTC,  NYSE,    -1.67, 2013-05-01\\n    L,  NYSE,     1.19, 2013-04-30\\n  LUV,  NYSE,   133.33, 2013-04-26\\n  LUX,  NYSE,     7.14, 2013-05-02\\n  MKL,  NYSE,    40.11, 2013-05-01\\n  MAN,  NYSE,    40.00, 2013-04-20\\n  MTW,  NYSE,   -35.71, 2013-05-01\\n   SM,  NYSE,    46.43, 2013-05-01\\n  MAS,  NYSE,    -7.14, 2013-04-30\\n  MTZ,  NYSE,    12.50, 2013-05-03\\n  MCD,  NYSE,    -0.79, 2013-04-20\\n  MDC,  NYSE,    73.08, 2013-05-03\\n  MDP,  NYSE,     4.35, 2013-04-26\\n  MDR,  NYSE,   -40.00, 2013-05-09\\n  MDU,  NYSE,    36.36, 2013-05-01\\n  MED,  NYSE,    26.47, 2013-05-09\\n  CVS,  NYSE,     5.06, 2013-05-02\\n  MFC,  NYSE,    18.52, 2013-05-03\\n  MGA,  NYSE,    13.57, 2013-05-11\\n  MGM,  NYSE,   130.00, 2013-05-03\\n  MMC,  NYSE,     4.29, 2013-05-03\\n  MMM,  NYSE,    -2.42, 2013-04-26\\n  MSA,  NYSE,   -20.31, 2013-04-25\\n  MNR,  NYSE,    -7.69, 2013-05-09\\n   MO,  NYSE,     1.89, 2013-04-26\\n  MOD,  NYSE,     5.88, 2013-05-31\\nMOG.A,  NYSE,    -1.23, 2013-04-27\\n  MHK,  NYSE,     3.57, 2013-05-03\\n  MSI,  NYSE,    -1.79, 2013-04-25\\n  MCY,  NYSE,    46.81, 2013-04-30\\n  MRK,  NYSE,     8.97, 2013-05-02\\n  MRO,  NYSE,   -28.17, 2013-05-08\\n POWR,  NYSE,     0.00, 2013-05-09\\n  MTG,  NYSE,   -60.00, 2013-05-01\\n  MTB,  NYSE,     6.19, 2013-04-16\\n  MTX,  NYSE,     0.00, 2013-04-26\\n  MUR,  NYSE,    11.34, 2013-05-02\\n  MYE,  NYSE,   -11.11, 2013-04-25\\n  NBL,  NYSE,    21.31, 2013-04-26\\n  NBR,  NYSE,    13.79, 2013-04-24\\n   NE,  NYSE,     3.51, 2013-04-18\\n  NEM,  NYSE,    -8.97, 2013-04-30\\n  NFG,  NYSE,     7.37, 2013-05-03\\n  NHI,  NYSE,     4.94, 2013-05-07\\n   NI,  NYSE,    -1.43, 2013-05-01\\n  NJR,  NYSE,     3.16, 2013-05-03\\n  THC,  NYSE,    17.86, 2013-05-01\\n  NNN,  NYSE,     4.35, 2013-05-03\\n  NOC,  NYSE,    12.14, 2013-04-25\\n   NR,  NYSE,     5.88, 2013-04-26\\n  NSC,  NYSE,     3.39, 2013-04-24\\n  NUE,  NYSE,     4.00, 2013-04-19\\n  NVR,  NYSE,    -9.64, 2013-04-23\\n  NWL,  NYSE,     9.38, 2013-05-04\\n  NWN,  NYSE,    -5.41, 2013-05-03\\n  NYT,  NYSE,   -20.00, 2013-04-26\\n  OCR,  NYSE,     4.65, 2013-04-25\\n  OGE,  NYSE,   -32.35, 2013-05-03\\n  OHI,  NYSE,     5.08, 2013-05-08\\n   OI,  NYSE,     7.14, 2013-04-24\\n  OII,  NYSE,    16.95, 2013-04-24\\n  OKE,  NYSE,    -6.90, 2013-05-01\\n  OLN,  NYSE,    10.64, 2013-04-26\\n  BRS,  NYSE,    -1.94, 2013-05-23\\n  OMC,  NYSE,     1.33, 2013-04-19\\n  OMI,  NYSE,     4.76, 2013-04-24\\n  ORB,  NYSE,    43.48, 2013-04-24\\n  ORI,  NYSE,   600.00, 2013-04-26\\n  OSK,  NYSE,    12.94, 2013-05-01\\n  OXY,  NYSE,     7.64, 2013-04-26\\n FCFS,  NYSE,     0.00, 2013-04-18\\n  PBI,  NYSE,     0.00, 2013-05-01\\n  PCG,  NYSE,   -10.00, 2013-05-03\\n  PCL,  NYSE,     9.38, 2013-04-30\\n  PCP,  NYSE,     1.81, 2013-05-10\\n  TPC,  NYSE,    34.78, 2013-05-02\\n  PDS,  NYSE,    14.29, 2013-04-26\\n  PEG,  NYSE,    14.86, 2013-05-01\\n  PEI,  NYSE,     4.76, 2013-04-23\\n  PEP,  NYSE,     8.45, 2013-04-19\\n  PFE,  NYSE,    -1.82, 2013-05-01\\n   PG,  NYSE,     3.13, 2013-04-25\\n  PGR,  NYSE,    -4.55, 2013-04-11\\n   PH,  NYSE,     0.60, 2013-04-26\\n  PHM,  NYSE,    31.25, 2013-04-26\\n  PKD,  NYSE,   200.00, 2013-05-02\\n  PKY,  NYSE,    15.38, 2013-05-07\\n  PNC,  NYSE,    12.10, 2013-04-18\\n  PNM,  NYSE,   -10.00, 2013-05-07\\n  PNR,  NYSE,     3.57, 2013-04-24\\n  PNW,  NYSE,   175.00, 2013-05-04\\n  POM,  NYSE,    -4.00, 2013-05-04\\n  POT,  NYSE,     3.28, 2013-04-26\\n  PPG,  NYSE,     1.28, 2013-04-19\\n  PPL,  NYSE,     0.00, 2013-05-03\\n PRGO,  NYSE,    -1.39, 2013-05-08\\n   PL,  NYSE,    -4.30, 2013-05-07\\n  PSB,  NYSE,     0.00, 2013-05-07\\n  WTR,  NYSE,     7.41, 2013-05-02\\n  CSH,  NYSE,     8.21, 2013-04-26\\n  PWR,  NYSE,    24.14, 2013-05-03\\n   PX,  NYSE,     0.00, 2013-04-25\\n  KWR,  NYSE,    14.29, 2013-04-30\\n    R,  NYSE,     1.28, 2013-04-24\\n  RBC,  NYSE,    -6.09, 2013-05-01\\n  RDC,  NYSE,     5.77, 2013-05-02\\n HTSI,  NYSE,    11.67, 2013-05-03\\n  RES,  NYSE,   -33.33, 2013-04-25\\n  RGS,  NYSE,   -90.77, 2013-05-08\\n  RGR,  NYSE,    15.38, 2013-04-30\\n  RHI,  NYSE,    -2.44, 2013-04-24\\n  RJF,  NYSE,    -9.33, 2013-04-25\\n  RLI,  NYSE,    -1.89, 2013-04-18\\n  ROG,  NYSE,     0.00, 2013-05-01\\n  ROK,  NYSE,     2.31, 2013-04-25\\n  ROL,  NYSE,    -5.88, 2013-04-25\\n  ROP,  NYSE,     4.10, 2013-04-30\\n  RTI,  NYSE,    20.00, 2013-05-01\\n  RTN,  NYSE,    21.88, 2013-04-26\\n  RYL,  NYSE,    43.33, 2013-04-25\\n BSAC,  NYSE,   -21.74, 2013-04-26\\n    T,  NYSE,     0.00, 2013-04-24\\n  SCG,  NYSE,     7.77, 2013-04-26\\n SCHW,  NYSE,    -6.25, 2013-04-16\\n  SCL,  NYSE,    -4.08, 2013-05-01\\n  SMG,  NYSE,   -19.60, 2013-05-07\\n  SEE,  NYSE,    -5.56, 2013-05-02\\n   SF,  NYSE,     1.75, 2013-05-10\\n  SFE,  NYSE,   -46.15, 2013-04-26\\n  SHW,  NYSE,     2.78, 2013-04-19\\n  SJI,  NYSE,    -8.43, 2013-05-04\\n  JOE,  NYSE,  -200.00, 2013-05-09\\n  SJW,  NYSE,   -12.50, 2013-04-25\\n  SLB,  NYSE,     2.02, 2013-04-20\\n  HSH,  NYSE,     9.38, 2013-05-03\\n  AOS,  NYSE,    24.68, 2013-04-24\\n  SMP,  NYSE,    31.25, 2013-05-04\\n  SNA,  NYSE,     4.48, 2013-04-19\\n  PII,  NYSE,     5.94, 2013-04-24\\n  SNV,  NYSE,     0.00, 2013-04-24\\n   SO,  NYSE,    -3.92, 2013-04-25\\n  SON,  NYSE,    -5.66, 2013-04-19\\n  SPA,  NYSE,   -46.15, 2013-05-08\\n  TRV,  NYSE,    14.93, 2013-04-24\\n   SR,  NYSE,    -3.36, 2013-05-01\\n  NVE,  NYSE,    12.50, 2013-05-04\\n  SCI,  NYSE,    21.74, 2013-04-25\\n  SSP,  NYSE,    58.33, 2013-05-07\\n  STT,  NYSE,     3.23, 2013-04-20\\n  STI,  NYSE,     3.28, 2013-04-20\\n  STJ,  NYSE,     0.00, 2013-04-18\\n  STL,  NYSE,     7.14, 2013-04-23\\n  STR,  NYSE,    -2.38, 2013-05-01\\n  STE,  NYSE,     6.06, 2013-05-08\\n  SYK,  NYSE,     1.98, 2013-04-25\\n  SUN,  NYSE,    -7.32, 2013-05-09\\n  SUP,  NYSE,     5.88, 2013-05-04\\n  SWK,  NYSE,     7.29, 2013-04-26\\n  SWN,  NYSE,     7.69, 2013-05-03\\n  SWX,  NYSE,     0.61, 2013-05-04\\n  SWY,  NYSE,    -2.78, 2013-04-26\\n  SYY,  NYSE,    16.67, 2013-05-07\\n  TAC,  NYSE,   -33.33, 2013-04-24\\n  TNC,  NYSE,   -17.14, 2013-04-23\\n  TCB,  NYSE,   -15.79, 2013-04-20\\n  TCO,  NYSE,     7.14, 2013-04-26\\n  TDS,  NYSE,   350.00, 2013-05-04\\n  TDW,  NYSE,    55.74, 2013-05-22\\n  TDY,  NYSE,    10.31, 2013-04-25\\n   TE,  NYSE,    11.76, 2013-05-01\\n  TER,  NYSE,   200.00, 2013-04-25\\n TEVA,  NYSE,     1.82, 2013-05-03\\n  TEX,  NYSE,   -17.86, 2013-04-25\\n  TFX,  NYSE,     1.98, 2013-05-01\\n  TEN,  NYSE,    10.77, 2013-04-30\\n  TKR,  NYSE,     0.00, 2013-04-25\\n  TMK,  NYSE,     1.46, 2013-04-24\\n  TMO,  NYSE,     6.20, 2013-04-25\\n  TOT,  NYSE,    -2.38, 2013-04-27\\n   TM,  NYSE,    80.67, 2013-05-09\\n   TR,  NYSE,   -11.76, 2013-04-25\\n  TRN,  NYSE,    13.75, 2013-05-01\\n  TRP,  NYSE,    -8.93, 2013-04-27\\n  TSO,  NYSE,     2.82, 2013-05-02\\n  TSS,  NYSE,    -2.94, 2013-04-24\\n  TTI,  NYSE,   -40.00, 2013-05-09\\n  TXT,  NYSE,   -14.89, 2013-04-18\\n  TYL,  NYSE,    26.09, 2013-04-25\\n  TSN,  NYSE,   -21.74, 2013-05-07\\n  UDR,  NYSE,     3.03, 2013-05-01\\n  UFI,  NYSE,   -43.75, 2013-04-25\\n  UAM,  NYSE,    17.65, 2013-04-30\\n  UHS,  NYSE,     5.17, 2013-04-25\\n  UIL,  NYSE,     3.06, 2013-05-03\\n  UIS,  NYSE,  -145.61, 2013-04-24\\n  UNH,  NYSE,     0.00, 2013-04-19\\n KMPR,  NYSE,    35.85, 2013-05-03\\n  UNM,  NYSE,     2.56, 2013-05-02\\n  UNP,  NYSE,     3.57, 2013-04-19\\n  UNT,  NYSE,     6.98, 2013-05-08\\n  URS,  NYSE,   -14.29, 2013-05-08\\n  USG,  NYSE,   -88.89, 2013-04-25\\n  MUX,  NYSE,  -300.00, 2013-05-10\\n  USM,  NYSE,   214.29, 2013-05-04\\n USPH,  NYSE,    -3.12, 2013-05-10\\n  UTL,  NYSE,    -9.20, 2013-04-24\\n  UTX,  NYSE,    -1.54, 2013-04-24\\n  VMI,  NYSE,    15.60, 2013-04-19\\n  VAR,  NYSE,     2.97, 2013-04-25\\n  CBS,  NYSE,     7.35, 2013-05-02\\n  VLO,  NYSE,    16.83, 2013-05-01\\n  VMC,  NYSE,   -24.32, 2013-05-03\\n  VLY,  NYSE,   -11.11, 2013-04-25\\n  VNO,  NYSE,   -38.38, 2013-05-07\\n  VSH,  NYSE,    63.64, 2013-05-01\\n  WTS,  NYSE,   -14.04, 2013-05-01\\n  WBS,  NYSE,    -2.22, 2013-04-16\\n  WEC,  NYSE,     7.04, 2013-05-01\\n  WFC,  NYSE,     5.75, 2013-04-13\\n   WG,  NYSE, -2400.00, 2013-05-09\\n  WGL,  NYSE,    19.05, 2013-05-02\\n  WHR,  NYSE,     1.03, 2013-04-25\\n  WMB,  NYSE,    -8.33, 2013-05-08\\n  WNC,  NYSE,     0.00, 2013-05-01\\n  TEG,  NYSE,    10.69, 2013-05-02\\n   WR,  NYSE,    33.33, 2013-05-09\\n  WRE,  NYSE,    -4.35, 2013-04-26\\n  WRI,  NYSE,     4.35, 2013-05-01\\n  WPP,  NYSE,    33.33, 2013-04-30\\n  WSO,  NYSE,    18.18, 2013-04-19\\n  WST,  NYSE,     1.16, 2013-05-03\\n  WWW,  NYSE,    50.00, 2013-04-17\\n   WY,  NYSE,    18.18, 2013-04-27\\n    X,  NYSE,   -84.21, 2013-05-01\\n   XL,  NYSE,    38.81, 2013-05-03\\n  XOM,  NYSE,     4.43, 2013-04-26\\n  XRX,  NYSE,    12.50, 2013-04-24\\n    Y,  NYSE,    53.96, 2013-05-07\\n  HRG,  NYSE,    60.00, 2013-05-10\\n  CRY,  NYSE,    28.57, 2013-05-01\\n  CHK,  NYSE,    30.43, 2013-05-02\\n  DDR,  NYSE,     0.00, 2013-05-01\\n  ELS,  NYSE,     0.71, 2013-04-23\\n  ALG,  NYSE,     5.56, 2013-05-02\\n  ETH,  NYSE,   -22.22, 2013-04-24\\n  ATR,  NYSE,    -3.03, 2013-04-26\\n  GGP,  NYSE,     4.17, 2013-04-30\\n  MSL,  NYSE,     3.70, 2013-05-01\\n  RCL,  NYSE,    84.21, 2013-04-26\\n CWEI,  NYSE,   -61.22, 2013-04-25\\n   HR,  NYSE,     0.00, 2013-05-02\\n  RGA,  NYSE,     2.48, 2013-04-26\\n  RIG,  NYSE,    -7.92, 2013-05-09\\n  SKT,  NYSE,     2.44, 2013-05-01\\n  TWI,  NYSE,   -16.28, 2013-04-25\\n  BDN,  NYSE,     2.94, 2013-04-25\\n  KGC,  NYSE,    25.00, 2013-05-08\\n  CPT,  NYSE,     2.11, 2013-05-03\\n  SGY,  NYSE,    18.84, 2013-05-07\\n  BFS,  NYSE,   -24.49, 2013-05-01\\n  BWA,  NYSE,     6.56, 2013-04-26\\n  EQR,  NYSE,    -1.54, 2013-05-01\\n  CLP,  NYSE,     3.03, 2013-04-26\\n  KOF,  NYSE,   -16.24, 2013-04-25\\n  OKS,  NYSE,   -27.59, 2013-05-01\\n  SQM,  NYSE,    -6.45, 2013-05-29\\n  BYD,  NYSE,   114.29, 2013-04-25\\n  CBL,  NYSE,     3.92, 2013-04-30\\n DECK,  NYSE,   133.33, 2013-04-26\\n   IT,  NYSE,    -2.50, 2013-05-03\\n  HST,  NYSE,    21.74, 2013-05-04\\n  LXP,  NYSE,     0.00, 2013-05-03\\n  REG,  NYSE,     3.23, 2013-05-08\\n  TUC,  NYSE,   -24.00, 2013-05-03\\n   AF,  NYSE,     7.69, 2013-04-18\\n  BFR,  NYSE,    -2.56, 2013-05-11\\n  HHS,  NYSE,    10.00, 2013-04-26\\n  MHO,  NYSE,    28.57, 2013-04-26\\n  NFX,  NYSE,    -2.17, 2013-04-24\\n  SPG,  NYSE,     1.99, 2013-04-27\\n   SU,  NYSE,    -1.41, 2013-04-30\\n  SUI,  NYSE,     2.20, 2013-04-26\\n   TV,  NYSE,   -22.50, 2013-04-26\\n  CGI,  NYSE,   -26.92, 2013-04-26\\n  CYT,  NYSE,   -12.79, 2013-04-19\\n  EMN,  NYSE,     3.18, 2013-04-26\\n  GRT,  NYSE,    14.29, 2013-04-25\\n  MAA,  NYSE,     5.04, 2013-05-02\\n  PLT,  NYSE,     4.62, 2013-05-08\\n  BZH,  NYSE,    15.38, 2013-05-03\\n  ELX,  NYSE,   114.29, 2013-05-03\\n  MLM,  NYSE,   -69.44, 2013-05-01\\n  AKS,  NYSE,    41.67, 2013-04-24\\n  ALB,  NYSE,    -7.00, 2013-04-18\\n  VRX,  NYSE,     1.56, 2013-05-03\\n  CBR,  NYSE,     0.00, 2013-05-01\\n  MAC,  NYSE,     8.86, 2013-05-02\\n  RKT,  NYSE,     9.80, 2013-04-24\\n  RYN,  NYSE,    27.42, 2013-04-26\\n  ADC,  NYSE,    -2.00, 2013-04-30\\nBRK.B,  NYSE,    52.31, 2013-05-04\\n  EXP,  NYSE,     5.00, 2013-05-15\\n  GGB,  NYSE,   -66.67, 2013-05-08\\n  SSD,  NYSE,   -52.38, 2013-04-26\\n  ESS,  NYSE,    -0.53, 2013-05-02\\n   FR,  NYSE,    -7.69, 2013-04-26\\n  HIW,  NYSE,    -2.90, 2013-05-01\\n IMAX,  NYSE,     0.00, 2013-04-26\\n  AIV,  NYSE,     2.13, 2013-05-03\\n  FCH,  NYSE,     0.00, 2013-05-01\\n ITGR,  NYSE,     2.33, 2013-04-26\\n  NOK,  NYSE,    33.33, 2013-04-19\\n  GEO,  NYSE,    -3.51, 2013-05-09\\n  CLI,  NYSE,     0.00, 2013-04-26\\n   RS,  NYSE,    -5.22, 2013-04-26\\n  CPE,  NYSE,   100.00, 2013-05-10\\n  KNX,  NYSE,     0.00, 2013-04-25\\n    O,  NYSE,     1.69, 2013-04-26\\n  COF,  NYSE,    17.79, 2013-04-19\\n  IRS,  NYSE,    10.34, 2013-05-18\\n  MCK,  NYSE,    -0.43, 2013-05-08\\n  SWC,  NYSE,   200.00, 2013-04-30\\n  STM,  NYSE,    23.53, 2013-04-23\\n  TEO,  NYSE,     1.30, 2013-04-30\\n  TRK,  NYSE,  -400.00, 2013-05-02\\n  LMT,  NYSE,    23.38, 2013-04-24\\n  APU,  NYSE,   -35.48, 2013-05-16\\n  AGU,  NYSE,   -12.15, 2013-05-10\\n   LH,  NYSE,    -1.69, 2013-04-20\\n  DDD,  NYSE,   -10.00, 2013-05-01\\n  AFG,  NYSE,    10.84, 2013-05-09\\n  RMD,  NYSE,     3.51, 2013-04-26\\n  WAB,  NYSE,     3.60, 2013-04-25\\n  CIB,  NYSE,     6.78, 2013-05-08\\n  CAM,  NYSE,    -5.41, 2013-04-26\\n  FCX,  NYSE,     1.39, 2013-04-19\\n  RNR,  NYSE,    34.25, 2013-05-02\\n  AVX,  NYSE,     7.14, 2013-04-25\\n  RWT,  NYSE,    46.81, 2013-05-03\\n  AXE,  NYSE,    -6.62, 2013-04-24\\n  CLB,  NYSE,     6.09, 2013-04-18\\n   MD,  NYSE,     0.92, 2013-05-03\\n  THG,  NYSE,    30.69, 2013-04-30\\n  BAP,  NYSE,   -10.94, 2013-05-07\\n   DO,  NYSE,    10.43, 2013-04-26\\n   RE,  NYSE,    36.11, 2013-04-23\\n  DST,  NYSE,    -6.60, 2013-04-26\\n   EL,  NYSE,    36.36, 2013-05-03\\n  ESC,  NYSE,   -57.14, 2013-05-03\\n  LXK,  NYSE,    -7.55, 2013-04-24\\n  MIG,  NYSE,     7.69, 2013-05-01\\n  WAT,  NYSE,    -1.83, 2013-04-24\\n  EME,  NYSE,     2.27, 2013-04-26\\n  HIG,  NYSE,    10.84, 2013-04-30\\n  ITT,  NYSE,     9.30, 2013-05-03\\n  SPN,  NYSE,     0.00, 2013-04-26\\n  SWM,  NYSE,     8.60, 2013-05-09\\n SCCO,  NYSE,    -4.84, 2013-04-27\\n  RCI,  NYSE,    -1.27, 2013-04-23\\n  EIX,  NYSE,    20.31, 2013-05-01\\n  IRM,  NYSE,     0.00, 2013-05-02\\n  SPH,  NYSE,    -4.82, 2013-05-10\\n  CCJ,  NYSE,     0.00, 2013-05-02\\n  PGI,  NYSE,     0.00, 2013-04-19\\n  CRR,  NYSE,   -14.61, 2013-04-26\\n  BVN,  NYSE,   -40.30, 2013-04-30\\n  FCN,  NYSE,    13.46, 2013-05-10\\n  RPT,  NYSE,     6.90, 2013-04-24\\n  TUP,  NYSE,     4.42, 2013-04-25\\n  ASB,  NYSE,     8.00, 2013-04-19\\n  GWR,  NYSE,   -10.11, 2013-05-02\\n  TBI,  NYSE,   -50.00, 2013-04-25\\n  FFG,  NYSE,    12.66, 2013-05-03\\n USNA,  NYSE,    14.29, 2013-04-24\\n  CSV,  NYSE,    -3.03, 2013-05-08\\n  LVB,  NYSE,    10.53, 2013-05-09\\n  ALR,  NYSE,     6.25, 2013-05-10\\n  OCN,  NYSE,     0.00, 2013-05-03\\n  PAA,  NYSE,    37.50, 2013-05-07\\n  DNR,  NYSE,    13.79, 2013-05-03\\n  HMY,  NYSE,  -119.23, 2013-05-04\\n  TGI,  NYSE,     5.66, 2013-05-02\\n  PAG,  NYSE,     1.61, 2013-04-30\\n  GEL,  NYSE,   -17.65, 2013-05-03\\n   IM,  NYSE,     0.00, 2013-04-26\\n  NUS,  NYSE,    13.92, 2013-05-03\\n  CNI,  NYSE,    -1.67, 2013-04-23\\n  LAD,  NYSE,    16.67, 2013-04-25\\n  NSP,  NYSE,     0.00, 2013-04-30\\n  DGX,  NYSE,   -14.42, 2013-04-18\\n  KRC,  NYSE,     0.00, 2013-05-01\\n  MTH,  NYSE,    32.00, 2013-04-25\\n  NCR,  NYSE,    35.00, 2013-05-01\\n  OFG,  NYSE,     2.78, 2013-04-26\\n  IVZ,  NYSE,    10.64, 2013-05-01\\n   DX,  NYSE,     9.68, 2013-05-02\\n  FBC,  NYSE,   -65.98, 2013-04-24\\n  ALV,  NYSE,     1.57, 2013-04-27\\n  ARE,  NYSE,     0.00, 2013-04-30\\n  BBT,  NYSE,     2.99, 2013-04-19\\n  CGG,  NYSE,     6.25, 2013-05-04\\n  BXP,  NYSE,    -0.83, 2013-05-01\\n  CBD,  NYSE,   -23.73, 2013-05-01\\n   MS,  NYSE,     7.02, 2013-04-19\\n  SRT,  NYSE,  -314.29, 2013-05-10\\n  HLX,  NYSE,    38.89, 2013-04-22\\n  FLS,  NYSE,     3.61, 2013-04-25\\n   MT,  NYSE,  -400.00, 2013-05-11\\n  PXD,  NYSE,     5.15, 2013-05-02\\n  SLG,  NYSE,     0.83, 2013-04-24\\n  NAT,  NYSE,   -16.22, 2013-05-14\\n  CSU,  NYSE,   -36.36, 2013-05-07\\n  DRQ,  NYSE,    22.50, 2013-05-04\\n  FDP,  NYSE,   -24.47, 2013-05-01\\n  NLY,  NYSE,    30.56, 2013-05-02\\n  TLM,  NYSE,  -250.00, 2013-05-02\\n  TSM,  NYSE,    13.04, 2013-04-19\\n  YUM,  NYSE,    12.90, 2013-04-24\\n  AMG,  NYSE,    12.38, 2013-05-01\\n  EPR,  NYSE,    -1.05, 2013-05-01\\n   FE,  NYSE,    10.14, 2013-05-08\\n  LFL,  NYSE,    80.00, 2013-05-15\\n  MTD,  NYSE,     2.79, 2013-05-03\\n  SID,  NYSE,   -66.67, 2013-05-16\\n   IN,  NYSE,  -271.43, 2013-05-04\\n  CBZ,  NYSE,    25.64, 2013-05-03\\n  URI,  NYSE,    11.54, 2013-04-17\\n INGR,  NYSE,     6.82, 2013-05-03\\n  RAS,  NYSE,   181.82, 2013-05-03\\n  UNS,  NYSE,    35.00, 2013-04-30\\n  ASI,  NYSE,    18.92, 2013-05-09\\n  ANH,  NYSE,    15.38, 2013-04-30\\n  OFC,  NYSE,    17.07, 2013-04-27\\n  GPX,  NYSE,     0.00, 2013-05-03\\n  WAC,  NYSE,  1427.27, 2013-05-10\\n  RBA,  NYSE,   -13.33, 2013-05-01\\n  WDR,  NYSE,     1.61, 2013-04-24\\n  LHO,  NYSE,     8.00, 2013-04-18\\n  LNT,  NYSE,    18.03, 2013-05-04\\n LVLT,  NYSE,     7.14, 2013-04-26\\n  MFA,  NYSE,    -4.76, 2013-05-02\\n  OME,  NYSE,    50.00, 2013-05-08\\n  EQY,  NYSE,     6.90, 2013-05-02\\n  FII,  NYSE,    -2.38, 2013-04-26\\n  FMX,  NYSE,   -37.89, 2013-04-25\\n  LLL,  NYSE,     3.63, 2013-04-26\\n  VTR,  NYSE,     4.04, 2013-04-27\\n  WCN,  NYSE,    20.00, 2013-05-02\\n  AVB,  NYSE,     0.74, 2013-05-01\\n  GIL,  NYSE,     5.36, 2013-05-03\\n  HZO,  NYSE,   -92.86, 2013-04-26\\n  AWR,  NYSE,    38.00, 2013-05-11\\n  CLS,  NYSE,    10.00, 2013-04-24\\n  EPD,  NYSE,    16.67, 2013-05-01\\n  RSG,  NYSE,    15.00, 2013-04-26\\n   WM,  NYSE,    -2.44, 2013-04-25\\n  AKR,  NYSE,     3.33, 2013-04-24\\n  CVG,  NYSE,    17.39, 2013-05-01\\n  RRC,  NYSE,   -38.89, 2013-04-26\\n  SAP,  NYSE,    41.51, 2013-04-20\\n  CCI,  NYSE,     0.00, 2013-04-25\\n   PQ,  NYSE,   100.00, 2013-05-08\\n  WFT,  NYSE,     0.00, 2013-05-03\\n  CAA,  NYSE,     0.00, 2013-05-03\\n  ENB,  NYSE,    13.21, 2013-05-09\\n  GMK,  NYSE,    60.00, 2013-04-25\\n  MMR,  NYSE,     0.00, 2013-05-07\\n   PB,  NYSE,     2.38, 2013-04-25\\n  VIV,  NYSE,   -20.00, 2013-05-08\\n  AXL,  NYSE,    53.33, 2013-05-04\\n   BP,  NYSE,    33.33, 2013-05-01\\n  ETM,  NYSE,     0.00, 2013-05-09\\n   HT,  NYSE,     0.00, 2013-05-01\\n  BYI,  NYSE,    10.71, 2013-04-25\\n  CEB,  NYSE,     1.64, 2013-05-02\\n INFY,  NYSE,     5.41, 2013-04-13\\n  JLL,  NYSE,    56.52, 2013-05-01\\n  AZN,  NYSE,     5.22, 2013-04-26\\n  SFG,  NYSE,    33.75, 2013-04-24\\n TREX,  NYSE,    14.68, 2013-05-04\\n   GS,  NYSE,    11.43, 2013-04-17\\n  SYX,  NYSE,  -157.14, 2013-05-01\\n  WCC,  NYSE,    -4.27, 2013-04-19\\n JNPR,  NYSE,    33.33, 2013-04-24\\n  RDN,  NYSE,    28.57, 2013-05-02\\n  RAI,  NYSE,     4.35, 2013-04-24\\n  SKX,  NYSE,   -27.78, 2013-05-16\\n  WTM,  NYSE,   178.02, 2013-04-30\\n  NCI,  NYSE,    12.50, 2013-04-26\\n  BLT,  NYSE,   -17.39, 2013-05-08\\n  QTM,  NYSE,   -33.33, 2013-05-09\\n  BLK,  NYSE,     1.67, 2013-04-17\\n  CIR,  NYSE,     4.00, 2013-05-03\\n  MSO,  NYSE,    12.50, 2013-05-01\\n  PKG,  NYSE,    10.71, 2013-04-23\\n  PKI,  NYSE,   -25.00, 2013-04-26\\n  WWE,  NYSE,   -37.50, 2013-05-03\\n  SNN,  NYSE,    -2.11, 2013-05-03\\n  UPS,  NYSE,     2.97, 2013-04-26\\n XOXO,  NYSE,    16.67, 2013-05-10\\n  SLF,  NYSE,     7.25, 2013-05-09\\n  CDR,  NYSE,     9.09, 2013-05-10\\n   EW,  NYSE,    -5.26, 2013-04-24\\n  MET,  NYSE,    13.85, 2013-05-01\\n  FBR,  NYSE,   -89.47, 2013-04-24\\n  VVC,  NYSE,    -7.58, 2013-05-02\\n  BAM,  NYSE,    70.00, 2013-05-10\\n  NVS,  NYSE,     4.00, 2013-04-25\\n BHLB,  NYSE,    -1.82, 2013-04-30\\n  CRL,  NYSE,    -2.82, 2013-05-02\\n  CYH,  NYSE,     3.57, 2013-04-30\\n  MBT,  NYSE,   -13.04, 2013-06-08\\n MTOR,  NYSE,   500.00, 2013-05-01\\n  CNQ,  NYSE,   -44.19, 2013-05-03\\n  ERJ,  NYSE,   -62.79, 2013-04-30\\n   VZ,  NYSE,     3.03, 2013-04-19\\n  EVC,  NYSE,     0.00, 2013-05-03\\n  PBR,  NYSE,     0.00, 2013-04-27\\n  XEL,  NYSE,    11.63, 2013-05-03\\n  ALE,  NYSE,    10.67, 2013-05-09\\n   HW,  NYSE,   -30.00, 2013-05-01\\n  POL,  NYSE,    14.81, 2013-05-02\\n  COH,  NYSE,     3.70, 2013-04-24\\n  CXW,  NYSE,     6.38, 2013-05-09\\n  DVA,  NYSE,     3.37, 2013-05-08\\n  EXC,  NYSE,     4.41, 2013-05-02\\n  MCO,  NYSE,    11.49, 2013-05-04\\n BRFS,  NYSE,    23.53, 2013-04-30\\n   TU,  NYSE,     3.77, 2013-05-10\\n  WIT,  NYSE,     0.00, 2013-04-20\\n  ERF,  NYSE,   100.00, 2013-05-11\\n   GG,  NYSE,   -35.00, 2013-05-03\\n  HNT,  NYSE,    34.15, 2013-04-30\\n NYCB,  NYSE,     3.85, 2013-04-25\\n  SXT,  NYSE,     3.33, 2013-04-19\\n  CPG,  NYSE,   -20.00, 2013-05-10\\n  AMX,  NYSE,    16.67, 2013-04-20\\n  MPX,  NYSE,     0.00, 2013-04-25\\n  OIS,  NYSE,    -2.70, 2013-04-25\\n  MMP,  NYSE,     4.08, 2013-05-03\\n  PES,  NYSE,    33.33, 2013-05-01\\n  ABB,  NYSE,   -12.12, 2013-04-25\\n  KMR,  NYSE,    -3.28, 2013-05-02\\n  GEN,  NYSE,   -41.18, 2013-05-07\\n  ADS,  NYSE,    -2.88, 2013-04-19\\n  CVI,  NYSE,    25.00, 2013-05-03\\n  FTI,  NYSE,    -6.52, 2013-04-24\\n  PRA,  NYSE,    27.63, 2013-05-07\\n  STO,  NYSE,   -16.46, 2013-05-03\\n  BEL,  NYSE,    41.67, 2013-05-02\\n  FIS,  NYSE,     1.64, 2013-05-01\\n  COL,  NYSE,     0.86, 2013-04-20\\n  KAI,  NYSE,    20.51, 2013-04-30\\n  ABC,  NYSE,    -2.25, 2013-04-26\\n   BG,  NYSE,    18.56, 2013-04-26\\n  FRO,  NYSE,    27.08, 2013-05-31\\n  ECA,  NYSE,   150.00, 2013-04-24\\n  CIG,  NYSE,   108.33, 2013-05-17\\n  EEP,  NYSE,    16.67, 2013-05-01\\n  CVX,  NYSE,     3.25, 2013-04-27\\n  GXP,  NYSE,    41.67, 2013-05-10\\n  JHX,  NYSE,    -2.78, 2013-05-24\\n  PFG,  NYSE,     5.33, 2013-04-26\\n  PVR,  NYSE,    14.29, 2013-04-26\\n  AAP,  NYSE,     2.48, 2013-05-24\\n  KND,  NYSE,    36.11, 2013-05-02\\n  WTW,  NYSE,    38.10, 2013-05-03\\n  CNC,  NYSE,     5.00, 2013-04-24\\n  BCH,  NYSE,     3.70, 2013-05-09\\n   NS,  NYSE,   -86.67, 2013-04-25\\n ITUB,  NYSE,    -4.88, 2013-04-26\\n  SXL,  NYSE,    26.74, 2013-05-09\\n VALE,  NYSE,    50.00, 2013-04-25\\n  TNP,  NYSE,   150.00, 2013-05-25\\n  LCI,  NYSE,    40.00, 2013-05-09\\n  GTI,  NYSE,    50.00, 2013-04-26\\n  HNR,  NYSE,   -26.67, 2013-06-06\\n  MWE,  NYSE,   -90.00, 2013-05-09\\n  NLS,  NYSE,    50.00, 2013-05-07\\n  RGC,  NYSE,    -7.14, 2013-05-01\\n  JAH,  NYSE,    30.43, 2013-04-25\\n  NPO,  NYSE,   -23.29, 2013-05-03\\n  TRI,  NYSE,    22.58, 2013-05-01\\n  CAE,  NYSE,    10.53, 2013-05-17\\n   LF,  NYSE,    28.57, 2013-05-02\\n  SNY,  NYSE,   -10.11, 2013-05-03\\n BANC,  NYSE,   400.00, 2013-05-09\\n  COP,  NYSE,     0.00, 2013-04-26\\n  CNP,  NYSE,    -8.11, 2013-05-03\\n  EEQ,  NYSE,  -321.43, 2013-05-02\\n  MRH,  NYSE,    32.58, 2013-04-25\\n  NGS,  NYSE,    23.08, 2013-05-10\\n  NRP,  NYSE,     4.88, 2013-05-07\\n  PXP,  NYSE,    17.98, 2013-05-03\\n  XEC,  NYSE,    -0.93, 2013-05-08\\n  IAG,  NYSE,     7.14, 2013-05-08\\n  EGO,  NYSE,     0.00, 2013-05-03\\n  JNS,  NYSE,    -6.25, 2013-04-24\\n  PFS,  NYSE,    14.81, 2013-04-27\\n  ENH,  NYSE,    74.79, 2013-05-02\\n  CNX,  NYSE,    -5.00, 2013-04-26\\n  AMT,  NYSE,   -10.42, 2013-05-02\\n  ABG,  NYSE,    13.43, 2013-04-25\\n  LII,  NYSE,    22.22, 2013-04-23\\n  SRE,  NYSE,    -4.90, 2013-05-03\\n  AEE,  NYSE,   -21.43, 2013-05-03\\n  PLD,  NYSE,     0.00, 2013-04-25\\n  SAH,  NYSE,    -2.38, 2013-04-24\\n  GPI,  NYSE,    11.54, 2013-05-03\\n  FIX,  NYSE,   800.00, 2013-05-02\\n  MMS,  NYSE,     1.41, 2013-05-10\\n  SRI,  NYSE,    50.00, 2013-05-10\\n RTEC,  NYSE,    50.00, 2013-05-03\\n  NOV,  NYSE,    -5.84, 2013-04-27\\n   DF,  NYSE,    11.54, 2013-05-10\\n  SAM,  NYSE,   -17.74, 2013-05-02\\n   RL,  NYSE,     8.46, 2013-05-24\\n  FLR,  NYSE,     6.25, 2013-05-03\\n  ALL,  NYSE,     2.27, 2013-05-02\\n  ATI,  NYSE,     0.00, 2013-04-25\\n   EE,  NYSE,    72.73, 2013-05-02\\n  AIT,  NYSE,     0.00, 2013-05-03\\n  CHH,  NYSE,    -3.70, 2013-04-30\\n  FMS,  NYSE,   -17.78, 2013-05-01\\n  BCO,  NYSE,    16.67, 2013-04-26\\n  CBB,  NYSE,   133.33, 2013-05-10\\n  MWW,  NYSE,    14.29, 2013-05-03\\n  PSA,  NYSE,    -3.09, 2013-05-10\\n    E,  NYSE,     0.00, 2013-04-25\\n  JPM,  NYSE,    15.22, 2013-04-13\\n  USB,  NYSE,     0.00, 2013-04-17\\n  HON,  NYSE,     6.14, 2013-04-20\\n  ITG,  NYSE,    50.00, 2013-05-03\\n  ARB,  NYSE,   -15.49, 2013-05-08\\n  APL,  NYSE,   -28.95, 2013-04-30\\n  AVA,  NYSE,     0.00, 2013-05-02\\n  AXS,  NYSE,    85.71, 2013-04-26\\n  MOH,  NYSE,   146.15, 2013-04-26\\n  CVD,  NYSE,     4.17, 2013-05-02\\n  AHT,  NYSE,     2.94, 2013-05-09\\n  GPK,  NYSE,    25.00, 2013-04-26\\n  CNO,  NYSE,     0.00, 2013-04-25\\n  AUQ,  NYSE,   -60.00, 2013-05-10\\n  NFP,  NYSE,    -5.45, 2013-05-04\\n  CRI,  NYSE,    12.86, 2013-05-10\\n  FMD,  NYSE,    27.27, 2013-04-30\\n  FPO,  NYSE,     3.45, 2013-04-26\\n  TRQ,  NYSE,   -25.00, 2013-05-14\\n  WLL,  NYSE,     2.17, 2013-04-25\\n  AEL,  NYSE,    11.36, 2013-05-02\\n  AHL,  NYSE,     0.95, 2013-04-25\\n  AUY,  NYSE,   -23.81, 2013-05-01\\n  CMP,  NYSE,    24.32, 2013-04-30\\n  KRO,  NYSE,  -800.00, 2013-05-09\\n  TPX,  NYSE,     3.33, 2013-05-03\\n  UTI,  NYSE,  -300.00, 2013-05-01\\n  PJC,  NYSE,     9.09, 2013-04-18\\n  TRW,  NYSE,     3.42, 2013-05-01\\n  AIZ,  NYSE,   -14.56, 2013-04-25\\n  HTH,  NYSE,    11.43, 2013-05-07\\n  ETP,  NYSE,    33.33, 2013-05-09\\n  LSE,  NYSE,     0.00, 2013-05-09\\n  BBD,  NYSE,     0.00, 2013-04-23\\n  NRG,  NYSE,   -37.04, 2013-05-08\\n  HOS,  NYSE,    96.67, 2013-05-02\\n  ABR,  NYSE,    84.62, 2013-05-04\\n  FHN,  NYSE,     0.00, 2013-04-20\\n  AGO,  NYSE,    86.11, 2013-05-10\\n  HSP,  NYSE,    18.18, 2013-05-02\\n  HNI,  NYSE,   250.00, 2013-04-18\\n  GHL,  NYSE,   -34.78, 2013-04-18\\n  XPO,  NYSE,   -16.44, 2013-05-08\\n  CVO,  NYSE,  -200.00, 2013-05-09\\n  CHE,  NYSE,     9.92, 2013-04-19\\n  GNW,  NYSE,    11.11, 2013-05-01\\n  CBG,  NYSE,    -5.88, 2013-04-26\\n  SFL,  NYSE,   -43.33, 2013-05-31\\n  NEU,  NYSE,     3.28, 2013-04-25\\n  GOL,  NYSE, -1200.00, 2013-05-14\\n  CAB,  NYSE,    18.64, 2013-04-26\\n  LTM,  NYSE,     3.08, 2013-04-26\\n  VVI,  NYSE,    68.00, 2013-04-27\\n  WCG,  NYSE,    -8.70, 2013-05-04\\n  HEP,  NYSE,   -36.36, 2013-05-01\\n  DPZ,  NYSE,     5.36, 2013-05-01\\n  BDC,  NYSE,     6.33, 2013-05-03\\n  ENS,  NYSE,     2.56, 2013-05-29\\n  BMR,  NYSE,     7.89, 2013-05-02\\n  ACC,  NYSE,    -1.54, 2013-04-24\\n  KRG,  NYSE,    27.27, 2013-05-03\\n  WLK,  NYSE,    42.64, 2013-05-07\\n  EXR,  NYSE,     4.55, 2013-04-30\\n  CNS,  NYSE,     7.32, 2013-04-18\\n  IOC,  NYSE,   161.54, 2013-05-14\\n STON,  NYSE,  -150.00, 2013-05-08\\n  TTM,  NYSE,    60.56, 2013-05-30\\n  CPL,  NYSE,     7.69, 2013-05-11\\n TPGI,  NYSE,  -460.00, 2013-05-07\\n  SHO,  NYSE,     0.00, 2013-05-07\\n CUBE,  NYSE,     0.00, 2013-05-03\\n  NRF,  NYSE,   -51.35, 2013-05-04\\n  DLR,  NYSE,    -1.69, 2013-04-27\\n  MTL,  NYSE,   100.00, 2013-06-19\\n  NWE,  NYSE,     8.60, 2013-04-26\\n  ORA,  NYSE,   550.00, 2013-05-08\\n   NP,  NYSE,     7.25, 2013-05-09\\n  SMA,  NYSE,   -73.33, 2013-05-03\\n  BBG,  NYSE, -2600.00, 2013-05-03\\n  BXC,  NYSE,    35.29, 2013-05-02\\n  KNL,  NYSE,     8.33, 2013-04-19\\n  LVS,  NYSE,     7.58, 2013-05-02\\n  HLF,  NYSE,    18.69, 2013-04-30\\n  MIC,  NYSE,   -89.09, 2013-04-30\\n  PHH,  NYSE,   -81.13, 2013-05-02\\n   CE,  NYSE,    44.30, 2013-04-19\\n  EDR,  NYSE,     0.00, 2013-04-30\\n  WTI,  NYSE,    34.62, 2013-05-08\\n  ARC,  NYSE,     0.00, 2013-05-08\\n  PBH,  NYSE,     5.88, 2013-05-17\\n  HUN,  NYSE,    18.75, 2013-05-01\\n  WEX,  NYSE,     3.16, 2013-05-02\\n  DLB,  NYSE,    14.29, 2013-04-26\\n  DSX,  NYSE,    66.67, 2013-05-23\\n  LAZ,  NYSE,   -17.65, 2013-04-27\\n  TGP,  NYSE,    14.29, 2013-05-10\\n  TLP,  NYSE,     7.69, 2013-05-08\\n  DRH,  NYSE,    55.56, 2013-05-11\\n HTGC,  NYSE,     8.00, 2013-05-03\\n  KFN,  NYSE,    27.78, 2013-05-02\\n  THS,  NYSE,     5.71, 2013-05-10\\n  NSR,  NYSE,    -8.86, 2013-05-03\\n  WAL,  NYSE,    14.29, 2013-04-19\\n  SLW,  NYSE,    -9.76, 2013-05-11\\n  MPW,  NYSE,    -3.85, 2013-04-27\\n  GNK,  NYSE,    -2.75, 2013-05-02\\n  MFB,  NYSE,    28.57, 2013-05-09\\nRDS.A,  NYSE,    21.74, 2013-05-03\\n  ITC,  NYSE,    -3.45, 2013-04-24\\n  FTK,  NYSE,   -11.76, 2013-05-10\\n PIKE,  NYSE,   -20.00, 2013-05-07\\n  ALJ,  NYSE,    63.27, 2013-05-09\\n  DRC,  NYSE,     2.38, 2013-04-26\\n  STN,  NYSE,     0.00, 2013-05-10\\n  SSW,  NYSE,    -8.70, 2013-04-30\\n   CF,  NYSE,     0.50, 2013-05-09\\n  HPY,  NYSE,    12.50, 2013-05-01\\n  ROC,  NYSE,     1.49, 2013-05-01\\n  WPZ,  NYSE,   -57.58, 2013-05-01\\n  LCC,  NYSE,    29.17, 2013-04-24\\n  GLP,  NYSE,    -7.27, 2013-05-10\\n  AMP,  NYSE,     1.27, 2013-04-23\\n  DHT,  NYSE,    58.33, 2013-04-30\\n  FNF,  NYSE,     5.00, 2013-05-02\\n   NM,  NYSE,    52.38, 2013-05-22\\n  CCO,  NYSE,   -57.14, 2013-05-03\\n  BWP,  NYSE,     5.00, 2013-04-30\\n  ICE,  NYSE,     2.53, 2013-05-02\\n  BKD,  NYSE,    50.00, 2013-05-02\\n  BAS,  NYSE,    12.00, 2013-04-25\\n  CPA,  NYSE,    21.21, 2013-05-14\\n  LYV,  NYSE,     8.33, 2013-05-08\\n  WNR,  NYSE,    -6.93, 2013-05-03\\n  CMG,  NYSE,     9.81, 2013-04-19\\n  RGP,  NYSE,   -50.00, 2013-05-09\\n  KOP,  NYSE,   -16.92, 2013-05-04\\n   TX,  NYSE,    40.43, 2013-05-01\\n  UAL,  NYSE,    10.09, 2013-04-26\\n  ETE,  NYSE,   -27.03, 2013-05-09\\n  RSO,  NYSE,   -45.00, 2013-05-08\\n  XCO,  NYSE,    62.50, 2013-05-01\\n  PAC,  NYSE,    30.00, 2013-04-26\\n  NYX,  NYSE,     1.79, 2013-05-01\\n  TDG,  NYSE,     0.61, 2013-05-08\\n  BMA,  NYSE,    11.68, 2013-05-09\\n  THI,  NYSE,     1.67, 2013-05-09\\n  BTE,  NYSE,  -112.00, 2013-05-10\\n  CNH,  NYSE,    41.49, 2013-05-01\\n  GLA,  NYSE,   -82.35, 2013-05-02\\n  POR,  NYSE,     0.00, 2013-05-02\\n  HIL,  NYSE,    50.00, 2013-05-03\\n  HVB,  NYSE,    12.50, 2013-04-24\\n   KS,  NYSE,    -9.30, 2013-05-08\\n   HK,  NYSE,   -28.57, 2013-05-03\\n  DCP,  NYSE,     3.28, 2013-05-07\\n   DK,  NYSE,     7.56, 2013-05-09\\n CODI,  NYSE,     0.00, 2013-05-08\\n   MA,  NYSE,     0.65, 2013-05-02\\n  MWA,  NYSE,   150.00, 2013-05-01\\n  KOG,  NYSE,   -21.43, 2013-05-03\\n  PWE,  NYSE,  -150.00, 2013-05-03\\n PGTI,  NYSE,   100.00, 2013-05-02\\n  AWH,  NYSE,     8.45, 2013-04-25\\n  NSH,  NYSE,   -29.73, 2013-04-25\\n  WYN,  NYSE,     7.58, 2013-04-25\\n  WNS,  NYSE,    15.38, 2013-04-18\\n  PGH,  NYSE,     0.00, 2013-05-02\\n  AYR,  NYSE,    34.48, 2013-05-03\\n  EVR,  NYSE,   -24.49, 2013-04-25\\n  HBI,  NYSE,     2.00, 2013-04-24\\n   WU,  NYSE,    12.12, 2013-05-01\\n   OC,  NYSE,    45.00, 2013-04-25\\n  DAC,  NYSE,    44.44, 2013-04-30\\n  AWI,  NYSE,   -43.59, 2013-04-30\\n SUSS,  NYSE,     0.00, 2013-05-09\\n  DEI,  NYSE,     5.71, 2013-05-08\\n   OB,  NYSE,    79.31, 2013-04-30\\n  SBH,  NYSE,    -7.69, 2013-05-03\\n  EBS,  NYSE,  -144.44, 2013-05-03\\n  KBR,  NYSE,    25.53, 2013-04-26\\n  AER,  NYSE,    23.40, 2013-05-08\\n  NOA,  NYSE,  -442.86, 2013-06-11\\n  SPR,  NYSE,    29.79, 2013-05-03\\n  ANW,  NYSE,    -7.14, 2013-05-16\\n  DCT,  NYSE,    10.00, 2013-05-03\\n   SE,  NYSE,     6.25, 2013-05-04\\n  TOO,  NYSE,   -17.86, 2013-05-10\\n  TSL,  NYSE,   -27.78, 2013-05-30\\n  TWC,  NYSE,     2.92, 2013-04-26\\n  MVO,  NYSE,   -13.92, 2013-05-09\\n   CO,  NYSE,   150.00, 2013-06-19\\n  EXK,  NYSE,   -18.75, 2013-05-07\\n  EIG,  NYSE,    22.22, 2013-05-09\\n   HF,  NYSE,   -50.00, 2013-05-02\\n  FIG,  NYSE,    33.33, 2013-05-03\\n NGLS,  NYSE,   -20.00, 2013-05-04\\n TCAP,  NYSE,    -1.75, 2013-05-09\\n  GFA,  NYSE,  -211.11, 2013-05-14\\n   BR,  NYSE,    18.18, 2013-05-08\\n  SCR,  NYSE,    12.50, 2013-05-10\\n  CNK,  NYSE,    12.00, 2013-05-08\\n  DAL,  NYSE,    42.86, 2013-04-24\\n  ORN,  NYSE,    42.86, 2013-05-03\\n  ACM,  NYSE,     3.92, 2013-05-08\\n  SLH,  NYSE,     5.00, 2013-05-08\\n  CLR,  NYSE,     2.63, 2013-05-09\\n  BGS,  NYSE,    -5.13, 2013-04-19\\n STAR,  NYSE,    26.42, 2013-05-01\\n  YGE,  NYSE,   -40.00, 2013-05-31\\n  DFS,  NYSE,    18.75, 2013-04-24\\n  TEL,  NYSE,     7.04, 2013-04-25\\n   BX,  NYSE,     1.85, 2013-04-19\\n  SEP,  NYSE,     4.65, 2013-05-04\\n   BZ,  NYSE,   -77.78, 2013-05-03\\n  PPO,  NYSE,   -41.18, 2013-05-09\\n  PRO,  NYSE,   100.00, 2013-05-03\\n  WBC,  NYSE,     7.34, 2013-04-26\\n  DHX,  NYSE,     0.00, 2013-04-24\\n  PMC,  NYSE,    23.53, 2013-05-02\\n  HGG,  NYSE,     3.33, 2013-05-21\\n  OWW,  NYSE,   -33.33, 2013-05-10\\n   VR,  NYSE,    35.97, 2013-04-26\\n  CXO,  NYSE,   -27.50, 2013-05-02\\n    G,  NYSE,     5.00, 2013-05-02\\n   EJ,  NYSE,    89.47, 2013-05-16\\n   WX,  NYSE,    11.11, 2013-05-14\\n CMLP,  NYSE,   -92.86, 2013-05-08\\n  VMW,  NYSE,    10.87, 2013-04-24\\n  CZZ,  NYSE,   -40.00, 2013-06-06\\n  CGA,  NYSE,     6.67, 2013-05-14\\n  TDC,  NYSE,   -26.92, 2013-05-03\\n  FLY,  NYSE,    61.73, 2013-05-03\\n MAIN,  NYSE,     2.04, 2013-05-10\\n  REN,  NYSE,   100.00, 2013-05-07\\n  TGH,  NYSE,   -12.90, 2013-05-08\\n  DFT,  NYSE,    -5.00, 2013-05-08\\n   RF,  NYSE,    15.00, 2013-04-24\\n  PZN,  NYSE,     0.00, 2013-04-25\\n   LL,  NYSE,    29.55, 2013-04-25\\n  NMM,  NYSE,     0.00, 2013-04-26\\n  OZM,  NYSE,    81.25, 2013-05-03\\n   ES,  NYSE,    12.31, 2013-05-02\\n MSCI,  NYSE,     5.56, 2013-05-02\\n  ARR,  NYSE,   -21.74, 2013-05-03\\n   KW,  NYSE,    62.50, 2013-05-08\\n  GTS,  NYSE,    52.78, 2013-05-02\\n  FOR,  NYSE,   450.00, 2013-05-09\\n  LRN,  NYSE,    34.78, 2013-05-04\\n  TNK,  NYSE,  -100.00, 2013-05-10\\n    N,  NYSE,   -21.43, 2013-04-26\\n  DAN,  NYSE,   -33.33, 2013-04-26\\n  BIP,  NYSE,     0.00, 2013-05-03\\n  CPN,  NYSE,    -6.67, 2013-05-03\\n  SOL,  NYSE,   -15.38, 2013-05-17\\n   PM,  NYSE,    -4.44, 2013-04-19\\n    V,  NYSE,     6.08, 2013-05-02\\n  IPI,  NYSE,     5.26, 2013-05-02\\n  AWK,  NYSE,    -5.88, 2013-05-08\\n  HTS,  NYSE,    -7.46, 2013-04-23\\n  DPS,  NYSE,    12.77, 2013-04-25\\n  CFX,  NYSE,     8.33, 2013-04-26\\n  WES,  NYSE,   -22.50, 2013-05-02\\n   SB,  NYSE,     0.00, 2013-05-16\\n   LO,  NYSE,     4.76, 2013-04-25\\n  LPS,  NYSE,     0.00, 2013-04-25\\n   FF,  NYSE,    -6.90, 2013-05-08\\n  NNA,  NYSE,   200.00, 2013-05-03\\n  EPB,  NYSE,     7.41, 2013-04-18\\n  JBT,  NYSE,   -17.65, 2013-05-08\\n   DL,  NYSE,   -33.33, 2013-05-22\\n  RAX,  NYSE,    -5.00, 2013-05-09\\n  GSL,  NYSE,   -50.00, 2013-05-10\\n  HCI,  NYSE,    66.06, 2013-05-03\\n   EC,  NYSE,   -18.58, 2013-05-04\\n  CLW,  NYSE,   -98.08, 2013-04-25\\n  MJN,  NYSE,    -1.16, 2013-04-26\\n  EPC,  NYSE,    39.53, 2013-05-02\\n  BPI,  NYSE,     0.00, 2013-05-07\\n  RST,  NYSE,    25.00, 2013-05-09\\n  DGI,  NYSE,    22.22, 2013-05-08\\n  SWI,  NYSE,     6.25, 2013-05-01\\n  CYS,  NYSE,   -45.16, 2013-04-18\\n  IVR,  NYSE,     1.59, 2013-05-02\\n  BUD,  NYSE,    50.65, 2013-05-01\\n  SLD,  NYSE,   -66.67, 2013-05-15\\n  PMT,  NYSE,    11.11, 2013-04-24\\n STWD,  NYSE,   -20.93, 2013-05-09\\n  CFN,  NYSE,    11.32, 2013-05-10\\n  SPB,  NYSE,     7.32, 2013-05-01\\n  ARI,  NYSE,    33.33, 2013-05-02\\n CLNY,  NYSE,   -26.47, 2013-05-07\\n  ART,  NYSE,  -800.00, 2013-05-07\\n  SEM,  NYSE,   -11.11, 2013-05-03\\n BSBR,  NYSE,   -71.43, 2013-04-26\\n DOLE,  NYSE,   -50.00, 2013-05-03\\n  VSI,  NYSE,     2.86, 2013-05-08\\n  TWO,  NYSE,    -9.38, 2013-05-08\\n  CVE,  NYSE,    -6.38, 2013-04-25\\n    H,  NYSE,    12.50, 2013-05-02\\n  LEA,  NYSE,    19.27, 2013-04-26\\n  SVN,  NYSE,   -81.82, 2013-05-14\\n  CLD,  NYSE,   -59.26, 2013-05-01\\n  AOL,  NYSE,     6.25, 2013-05-09\\n CHSP,  NYSE,    25.00, 2013-05-08\\n  PEB,  NYSE,     5.88, 2013-04-26\\n  CIT,  NYSE,    -8.99, 2013-04-24\\n  KAR,  NYSE,    -3.03, 2013-05-02\\n  CIE,  NYSE,   -15.38, 2013-05-01\\n  TMH,  NYSE,     0.00, 2013-05-01\\n  KRA,  NYSE,   -75.00, 2013-05-02\\n  SYA,  NYSE,     8.82, 2013-04-25\\n TRNO,  NYSE,   -11.11, 2013-05-09\\n  PDM,  NYSE,     0.00, 2013-05-03\\n GNRC,  NYSE,    23.47, 2013-05-03\\n  ACW,  NYSE,    -9.68, 2013-04-24\\n BALT,  NYSE,    -9.52, 2013-05-02\\n   ST,  NYSE,     4.35, 2013-04-24\\n SEMG,  NYSE,   -15.00, 2013-05-09\\n CALX,  NYSE,    50.00, 2013-04-26\\n  MXL,  NYSE,    33.33, 2013-05-01\\n STNG,  NYSE,    60.00, 2013-04-30\\n  PRI,  NYSE,    -4.35, 2013-05-08\\n SDRL,  NYSE,    16.95, 2013-05-29\\n CLDT,  NYSE,     7.50, 2013-05-08\\n  EXL,  NYSE,     5.00, 2013-05-02\\n  LYB,  NYSE,     9.09, 2013-04-27\\n  PNG,  NYSE,     4.35, 2013-05-07\\n PLOW,  NYSE,    13.33, 2013-05-07\\n  SIX,  NYSE,    19.61, 2013-04-23\\n  NKA,  NYSE,  -140.00, 2013-05-10\\n RRTS,  NYSE,     3.57, 2013-05-02\\n  JKS,  NYSE,    66.27, 2013-06-08\\n CODE,  NYSE,     7.69, 2013-05-01\\n  FAF,  NYSE,   -31.71, 2013-04-26\\n  QEP,  NYSE,    -6.67, 2013-05-01\\n  OAS,  NYSE,    31.37, 2013-05-08\\n  HPP,  NYSE,    18.18, 2013-05-07\\n   FN,  NYSE,     3.70, 2013-04-30\\n  ECT,  NYSE,     7.32, 2013-05-11\\n QUAD,  NYSE,   -88.10, 2013-05-08\\n  KKR,  NYSE,     4.76, 2013-04-26\\n  RLD,  NYSE,    70.00, 2013-06-07\\n AMRC,  NYSE,  -200.00, 2013-05-10\\n GDOT,  NYSE,     9.37, 2013-05-01\\n   AT,  NYSE,    40.00, 2013-05-09\\n  ENV,  NYSE,     0.00, 2013-05-17\\n  COR,  NYSE,     0.00, 2013-04-25\\n   VC,  NYSE,    75.65, 2013-05-10\\n  CCG,  NYSE,     5.88, 2013-05-01\\n  EFC,  NYSE,   -32.00, 2013-05-07\\n TOWR,  NYSE,   255.56, 2013-05-03\\n CHMT,  NYSE,   -21.05, 2013-05-03\\n  HBM,  NYSE,   200.00, 2013-05-02\\n EXAM,  NYSE,     0.00, 2013-05-09\\n  XUE,  NYSE,   -25.00, 2013-05-17\\n CMRE,  NYSE,    26.09, 2013-04-25\\n NOAH,  NYSE,   112.50, 2013-05-07\\n IPHI,  NYSE,    18.18, 2013-05-02\\n BITA,  NYSE,     0.00, 2013-05-10\\n  BAH,  NYSE,    11.43, 2013-05-23\\n   GM,  NYSE,    19.64, 2013-05-03\\n  XNY,  NYSE,    28.57, 2013-05-20\\n TROX,  NYSE,  -181.25, 2013-05-09\\n TRGP,  NYSE,    52.38, 2013-05-04\\n DANG,  NYSE,    21.05, 2013-05-17\\n YOKU,  NYSE,     0.00, 2013-05-16\\n  FRC,  NYSE,     0.00, 2013-04-16\\n  RFP,  NYSE,    64.29, 2013-05-01\\n  ISS,  NYSE,    50.00, 2013-05-18\\n   WD,  NYSE,   -45.65, 2013-05-09\\n  FLT,  NYSE,    10.39, 2013-05-03\\n GCAP,  NYSE,   -15.38, 2013-05-08\\n  FRF,  NYSE,   -27.27, 2013-05-14\\n SWFT,  NYSE,    23.53, 2013-04-23\\n   AG,  NYSE,    -8.00, 2013-05-16\\n  QRE,  NYSE,     0.00, 2013-05-09\\n  AAT,  NYSE,     8.57, 2013-05-01\\n  MCC,  NYSE,    -2.70, 2013-05-03\\n NLSN,  NYSE,     9.09, 2013-04-26\\n AGRO,  NYSE,  -100.00, 2013-05-17\\n  BKU,  NYSE,     4.44, 2013-04-25\\n INXN,  NYSE,    -7.14, 2013-05-09\\n NPTN,  NYSE,    10.00, 2013-05-10\\n  INN,  NYSE,     5.88, 2013-05-07\\n  KMI,  NYSE,   -12.50, 2013-04-18\\n  HCA,  NYSE,    -4.82, 2013-05-03\\n   MX,  NYSE,    13.04, 2013-05-01\\n  HII,  NYSE,     0.00, 2013-05-09\\n QIHU,  NYSE,   100.00, 2013-05-20\\n  APO,  NYSE,    56.20, 2013-05-07\\n  GNC,  NYSE,     1.39, 2013-04-27\\n  SDT,  NYSE,    16.07, 2013-05-11\\n  UAN,  NYSE,     4.26, 2013-05-02\\n ARCO,  NYSE,  -142.86, 2013-05-01\\n ELLI,  NYSE,   -16.67, 2013-05-01\\n  TMS,  NYSE,   -12.00, 2013-04-26\\n SQNS,  NYSE,     0.00, 2013-04-26\\n STAG,  NYSE,     3.13, 2013-05-07\\n   AL,  NYSE,     5.13, 2013-05-10\\n TLLP,  NYSE,   -14.89, 2013-05-07\\n RENN,  NYSE,    85.71, 2013-05-14\\n   NQ,  NYSE,   -16.67, 2013-05-16\\n  KOS,  NYSE,   -37.50, 2013-05-10\\n  RLJ,  NYSE,    10.81, 2013-05-09\\n  NGL,  NYSE,   -62.86, 2013-06-15\\n FENG,  NYSE,    60.00, 2013-05-15\\n LNKD,  NYSE,   340.00, 2013-05-03\\n NMFC,  NYSE,    -2.86, 2013-05-07\\n ACTV,  NYSE,    32.14, 2013-05-03\\n  FIO,  NYSE,    20.00, 2013-04-25\\n TAOM,  NYSE,   -25.00, 2013-05-24\\n RATE,  NYSE,    10.00, 2013-05-01\\n  VHS,  NYSE,     8.33, 2013-05-01\\n  MPC,  NYSE,     0.00, 2013-05-01\\n MITT,  NYSE,    -9.64, 2013-05-07\\n OILT,  NYSE,    17.07, 2013-05-09\\n  SXC,  NYSE,   -40.00, 2013-04-26\\n AMTG,  NYSE,    14.06, 2013-05-07\\n AMID,  NYSE,  -200.00, 2013-05-14\\n WAIR,  NYSE,    22.22, 2013-04-30\\n  PER,  NYSE,    -7.58, 2013-05-11\\n  PPP,  NYSE,   260.00, 2013-05-09\\n  FSM,  NYSE,   -28.57, 2013-05-08\\n FBHS,  NYSE,    41.18, 2013-05-03\\n  XLS,  NYSE,    73.91, 2013-05-04\\n  XYL,  NYSE,    -3.57, 2013-05-01\\n  GNE,  NYSE,  -550.00, 2013-05-08\\n NDRO,  NYSE,    -8.11, 2013-05-04\\n  RNF,  NYSE,   -29.63, 2013-05-10\\n  VAC,  NYSE,    10.20, 2013-04-26\\n CHKR,  NYSE,    -2.90, 2013-05-10\\n PACD,  NYSE,   250.00, 2013-05-07\\n INVN,  NYSE,   -13.33, 2013-05-03\\n DLPH,  NYSE,    11.46, 2013-05-02\\n   MN,  NYSE,     0.00, 2013-05-02\\n RRMS,  NYSE,    51.28, 2013-05-10\\n  WPX,  NYSE,    -4.17, 2013-05-03\\n  LPI,  NYSE,   -15.38, 2013-05-10\\n   SN,  NYSE,   -82.61, 2013-05-08\\n KORS,  NYSE,    35.14, 2013-05-30\\n BCEI,  NYSE,   -20.93, 2013-05-10\\n BOXC,  NYSE,     2.56, 2013-04-23\\n  PVG,  NYSE,   -25.00, 2013-05-11\\n POST,  NYSE,   -29.63, 2013-05-14\\n SLCA,  NYSE,    -2.78, 2013-05-01\\n MTDR,  NYSE,     0.00, 2013-05-09\\n GWAY,  NYSE,  -120.00, 2013-05-07\\n EPAM,  NYSE,   -14.71, 2013-05-09\\n RNDY,  NYSE,    -9.52, 2013-05-10\\n PRLB,  NYSE,     0.00, 2013-04-26\\n YELP,  NYSE,   -40.00, 2013-05-02\\n  NSM,  NYSE,    23.19, 2013-05-08\\n ALSN,  NYSE,    95.24, 2013-04-30\\n DWRE,  NYSE,   -22.73, 2013-05-08\\n VNTV,  NYSE,     3.70, 2013-05-07\\n   ET,  NYSE,     0.00, 2013-05-10\\n VCRA,  NYSE,  -160.00, 2013-05-03\\n   RM,  NYSE,    -1.82, 2013-05-03\\n BNNY,  NYSE,     3.57, 2013-06-11\\n   MM,  NYSE,    25.00, 2013-05-09\\n  RXN,  NYSE,     0.00, 2013-05-22\\n GLOG,  NYSE,   -16.67, 2013-05-16\\n RPAI,  NYSE,     9.52, 2013-05-07\\n  OAK,  NYSE,    39.86, 2013-05-08\\n  FET,  NYSE,     3.03, 2013-04-26\\n  MRC,  NYSE,     4.65, 2013-05-03\\n  PSX,  NYSE,    17.74, 2013-05-02\\n TUMI,  NYSE,     6.67, 2013-05-09\\n ACRE,  NYSE,    -5.88, 2013-05-16\\n EVER,  NYSE,    13.79, 2013-04-25\\n  PDH,  NYSE,   -13.24, 2013-04-25\\n ROYT,  NYSE,    10.00, 2013-05-11\\n  WMC,  NYSE,    -2.15, 2013-05-16\\n WAGE,  NYSE,    35.71, 2013-05-10\\n  HTA,  NYSE,     6.67, 2013-05-08\\n ALEX,  NYSE,   -28.57, 2013-05-10\\n  BKW,  NYSE,     0.00, 2013-04-27\\n CNCO,  NYSE,   -88.24, 2013-05-31\\n  EQM,  NYSE,    41.30, 2013-04-26\\n  NOW,  NYSE,     0.00, 2013-04-25\\n  EGL,  NYSE,   -11.24, 2013-05-14\\n NGVC,  NYSE,     7.69, 2013-05-10\\n  NTI,  NYSE,     3.51, 2013-05-14\\n AMRE,  NYSE,     4.00, 2013-05-08\\n GMED,  NYSE,     5.00, 2013-05-03\\n MANU,  NYSE,   -25.00, 2013-05-03\\n HCLP,  NYSE,   -23.08, 2013-05-15\\n  ADT,  NYSE,    -4.65, 2013-05-02\\n TRLA,  NYSE,   -75.00, 2013-05-01\\n  SRC,  NYSE,    19.44, 2013-05-09\\n NBHC,  NYSE,   -50.00, 2013-04-30\\n BSMX,  NYSE,    30.43, 2013-04-27\\n   HY,  NYSE,    67.05, 2013-05-02\\n SMLP,  NYSE,   -10.71, 2013-05-14\\n  DYN,  NYSE,  -254.55, 2013-05-03\\n LXFR,  NYSE,     0.00, 2013-05-08\\n LOCK,  NYSE,    25.00, 2013-05-02\\n  JMI,  NYSE,   224.44, 2013-05-08\\n BERY,  NYSE,    16.67, 2013-05-03\\n FLTX,  NYSE,     8.33, 2013-05-09\\n ANFI,  NYSE,     0.00, 2013-06-11\\n SSTK,  NYSE,    23.08, 2013-05-09\\n RLGY,  NYSE,   -13.33, 2013-05-02\\n SDLP,  NYSE,    88.64, 2013-05-29\\n MPLX,  NYSE,    -7.14, 2013-05-01\\n WWAV,  NYSE,     6.67, 2013-05-10\\n  SXE,  NYSE,   -44.44, 2013-05-09\\n  DKL,  NYSE,    31.58, 2013-05-08\\n  SCM,  NYSE,    -8.82, 2013-05-10\\n RKUS,  NYSE,  -100.00, 2013-05-07\\n ALDW,  NYSE,    -1.32, 2013-05-08\\n  WGP,  NYSE,     0.00, 2013-05-02\\n ABBV,  NYSE,     3.03, 2013-04-27\\n  PBF,  NYSE,   -54.72, 2013-05-03\\n  SBY,  NYSE,  -433.33, 2013-05-14\\n RIOM,  NYSE,     0.00, 2013-05-15\\n USAC,  NYSE,   -30.00, 2013-05-10\\n CVRR,  NYSE,    -2.56, 2013-05-03\\n SXCP,  NYSE,    -9.76, 2013-04-26\\n BFAM,  NYSE,    81.82, 2013-05-10\\n  TPH,  NYSE,   200.00, 2013-05-15\\n  ZTS,  NYSE,     5.88, 2013-05-01\\n  BCC,  NYSE,   146.15, 2013-04-23\\n  AGI,  NYSE,     0.00, 2013-04-26\\n APAM,  NYSE,   -11.32, 2013-05-02\\n SSNI,  NYSE, -1211.77, 2013-05-02\\n MODN,  NYSE,     0.00, 2013-05-08\\n AVIV,  NYSE,   150.00, 2013-05-08\\n OAKS,  NYSE,   509.09, 2013-05-04\\n MRIN,  NYSE,    -7.50, 2013-05-09\\n   PF,  NYSE,    17.24, 2013-05-16\\n TMHC,  NYSE,   -66.67, 2013-05-16\\n ARPI,  NYSE,  -600.00, 2013-06-25\\n CSTM,  NYSE,  -105.08, 2013-06-18\\n  DDC,  NYSE,   -80.00, 2013-06-06\\n  ABM,  NYSE,     9.09, 2013-06-04\\n  ANN,  NYSE,     4.76, 2013-06-07\\n  BBY,  NYSE,    28.00, 2013-05-22\\n BF.B,  NYSE,    -2.17, 2013-06-06\\n  BKE,  NYSE,    -4.88, 2013-05-24\\n  NCS,  NYSE,   -21.74, 2013-06-05\\n  BNS,  NYSE,    -0.83, 2013-05-29\\n  BRC,  NYSE,    -6.78, 2013-05-17\\n CATO,  NYSE,     1.94, 2013-05-24\\n  COO,  NYSE,     9.49, 2013-06-07\\n  CPB,  NYSE,    10.71, 2013-05-21\\n  CFI,  NYSE,    10.81, 2013-06-13\\n  DCI,  NYSE,    -4.17, 2013-05-18\\n  DDS,  NYSE,    15.38, 2013-05-15\\n   DE,  NYSE,     0.73, 2013-05-16\\n   DY,  NYSE,     0.00, 2013-05-22\\n   EV,  NYSE,     0.00, 2013-05-23\\n  ESL,  NYSE,   -11.81, 2013-05-31\\n    M,  NYSE,     3.77, 2013-05-16\\n  GCO,  NYSE,    11.90, 2013-06-01\\n  GPS,  NYSE,     2.90, 2013-05-24\\n   HD,  NYSE,     7.79, 2013-05-22\\n  HEI,  NYSE,    10.00, 2013-05-23\\n  HOV,  NYSE,   120.00, 2013-06-06\\n  HRB,  NYSE,    -1.93, 2013-06-13\\n  HRL,  NYSE,     0.00, 2013-05-24\\n  HPQ,  NYSE,     7.41, 2013-05-23\\n  JCP,  NYSE,   -12.93, 2013-05-17\\n   KR,  NYSE,     4.55, 2013-06-21\\n  KSS,  NYSE,    15.79, 2013-05-17\\n   LB,  NYSE,     4.35, 2013-05-23\\n  LOW,  NYSE,    -3.92, 2013-05-23\\n  LZB,  NYSE,     7.14, 2013-06-19\\n  MDT,  NYSE,     6.80, 2013-05-22\\n  MEI,  NYSE,    60.00, 2013-06-21\\n  MPR,  NYSE,   -33.33, 2013-06-07\\n  NAV,  NYSE,  -302.75, 2013-06-11\\n  JWN,  NYSE,    -3.95, 2013-05-17\\n  OXM,  NYSE,     5.13, 2013-06-12\\n  PBY,  NYSE,   -85.71, 2013-06-11\\n  PLL,  NYSE,     1.37, 2013-05-31\\n  PNY,  NYSE,     0.00, 2013-06-08\\n  PVH,  NYSE,    39.42, 2013-06-13\\n  THO,  NYSE,    -7.87, 2013-06-07\\n  TIF,  NYSE,    32.08, 2013-05-29\\n  TJX,  NYSE,     0.00, 2013-05-22\\n  TOL,  NYSE,     0.00, 2013-05-23\\n  TTC,  NYSE,    10.92, 2013-05-24\\n  VAL,  NYSE,     2.25, 2013-05-15\\n JW.A,  NYSE,   -16.47, 2013-06-19\\n  TGT,  NYSE,    23.53, 2013-05-23\\n  WMT,  NYSE,    -0.87, 2013-05-17\\n  WSM,  NYSE,    11.11, 2013-05-24\\n   FL,  NYSE,     3.41, 2013-05-25\\n  CHS,  NYSE,   -11.11, 2013-05-30\\n  BKS,  NYSE,    52.22, 2013-06-26\\n  CAL,  NYSE,    45.45, 2013-05-30\\n  SIG,  NYSE,     0.89, 2013-05-24\\n  ZLC,  NYSE,  1200.00, 2013-05-23\\n  AEO,  NYSE,     5.88, 2013-05-23\\n  FGP,  NYSE,    15.69, 2013-06-07\\n  BMO,  NYSE,    -4.73, 2013-05-30\\n   RY,  NYSE,    -2.34, 2013-05-31\\n  GEF,  NYSE,     1.45, 2013-06-06\\n  SKS,  NYSE,     0.00, 2013-05-22\\n   TD,  NYSE,     1.09, 2013-05-24\\n  ANF,  NYSE,   -80.00, 2013-05-25\\n CIEN,  NYSE,    20.00, 2013-06-07\\n  KMG,  NYSE,     8.70, 2013-06-11\\n IRET,  NYSE,    11.76, 2013-07-02\\n   CM,  NYSE,     0.00, 2013-05-31\\n  UBA,  NYSE,    12.00, 2013-06-08\\n  KFY,  NYSE,     3.23, 2013-06-18\\n  KKD,  NYSE,    25.00, 2013-05-31\\n  MVC,  NYSE,   -37.50, 2013-06-11\\n  CBK,  NYSE,   150.00, 2013-06-08\\n  SJM,  NYSE,    12.17, 2013-06-07\\n  BIG,  NYSE,     0.00, 2013-05-31\\n  JOY,  NYSE,    11.61, 2013-05-31\\n  SSI,  NYSE,  -122.22, 2013-05-18\\n  GME,  NYSE,    15.00, 2013-05-24\\n  DKS,  NYSE,     0.00, 2013-05-22\\n    A,  NYSE,    14.93, 2013-05-15\\n  MTN,  NYSE,    -3.62, 2013-06-07\\n  GES,  NYSE,    75.00, 2013-05-31\\n  CRM,  NYSE,  -600.00, 2013-05-24\\n  NWY,  NYSE,   128.57, 2013-05-24\\n  PAY,  NYSE,    -7.69, 2013-06-06\\n  DSW,  NYSE,    11.11, 2013-05-30\\n   NX,  NYSE,  -300.00, 2013-06-08\\n   DG,  NYSE,    -1.39, 2013-06-05\\n EXPR,  NYSE,     5.56, 2013-05-31\\n    P,  NYSE,     0.00, 2013-05-23\\n GWRE,  NYSE,    44.44, 2013-05-29\\n BLOX,  NYSE,   100.00, 2013-05-24\\n TLYS,  NYSE,    14.29, 2013-05-30\\n PANW,  NYSE,  -900.00, 2013-05-31\\n WDAY,  NYSE,    13.04, 2013-05-23\\n   RH,  NYSE,    50.00, 2013-06-14\\n RALY,  NYSE,    14.78, 2013-06-07\\n  AIR,  NYSE,    13.64, 2013-07-26\\n  ATU,  NYSE,    -1.59, 2013-06-20\\n  AZO,  NYSE,     0.69, 2013-05-22\\n  AZZ,  NYSE,    -8.20, 2013-06-29\\n  CAG,  NYSE,     1.69, 2013-06-28\\n  CLC,  NYSE,    -1.49, 2013-06-20\\n  CMC,  NYSE,   -15.79, 2013-06-28\\n   FC,  NYSE,    18.18, 2013-07-10\\n  FDO,  NYSE,     1.94, 2013-07-11\\n  FDX,  NYSE,     8.67, 2013-06-20\\n  FUL,  NYSE,    -5.63, 2013-06-27\\n  GIS,  NYSE,    -1.85, 2013-06-27\\n  KBH,  NYSE,    20.00, 2013-06-28\\n  LEN,  NYSE,    30.30, 2013-06-26\\n  LNN,  NYSE,    12.92, 2013-06-27\\n  MKC,  NYSE,     0.00, 2013-06-28\\n   RT,  NYSE,   -36.84, 2013-07-25\\n  MCS,  NYSE,    -6.25, 2013-07-26\\n  MSM,  NYSE,     9.37, 2013-07-11\\n  NKE,  NYSE,     2.70, 2013-06-28\\n ORCL,  NYSE,     0.00, 2013-06-21\\n  PIR,  NYSE,     0.00, 2013-06-21\\n  PKE,  NYSE,   -13.79, 2013-06-27\\n  RAD,  NYSE,     0.00, 2013-06-21\\n  RPM,  NYSE,     7.46, 2013-07-23\\n  SVU,  NYSE,   250.00, 2013-07-19\\n TISI,  NYSE,     0.00, 2013-08-07\\n  TXI,  NYSE,   116.00, 2013-07-11\\n  UNF,  NYSE,     2.88, 2013-06-27\\n  WGO,  NYSE,     0.00, 2013-06-28\\n  WOR,  NYSE,    -7.46, 2013-06-28\\n  JBL,  NYSE,     4.35, 2013-06-20\\n  GBX,  NYSE,    -5.66, 2013-07-03\\n  DRI,  NYSE,    -1.94, 2013-06-22\\n  FDS,  NYSE,    -1.71, 2013-06-19\\n  KMX,  NYSE,    12.28, 2013-06-22\\n  SCS,  NYSE,     0.00, 2013-06-20\\n  SJR,  NYSE,    16.28, 2013-06-29\\n  RHT,  NYSE,     9.09, 2013-06-20\\n  OMN,  NYSE,    14.29, 2013-06-28\\n  MON,  NYSE,     3.75, 2013-06-27\\n  GPN,  NYSE,    -3.92, 2013-07-26\\n  AYI,  NYSE,     7.78, 2013-07-03\\n  CCL,  NYSE,    50.00, 2013-06-26\\n  CUK,  NYSE,    50.00, 2013-06-26\\n  STZ,  NYSE,    -7.32, 2013-07-03\\n  ACN,  NYSE,     0.00, 2013-06-28\\n  SNX,  NYSE,     0.00, 2013-06-26\\n  TAL,  NYSE,    66.67, 2013-07-23\\n  IHS,  NYSE,     1.45, 2013-06-21\\n  EDU,  NYSE,    20.00, 2013-07-24\\n  ZEP,  NYSE,   -31.71, 2013-07-03\\n   MG,  NYSE,    -5.88, 2013-08-08\\n  MOS,  NYSE,    -0.88, 2013-07-16\\n  ABT,  NYSE,     4.55, 2013-07-18\\n  ABX,  NYSE,    17.86, 2013-08-02\\n   AB,  NYSE,     7.89, 2013-08-01\\n  TAP,  NYSE,     8.63, 2013-08-07\\n  ACO,  NYSE,     1.79, 2013-07-27\\n  ADM,  NYSE,     9.52, 2013-08-07\\n  AEM,  NYSE,   -85.71, 2013-07-25\\n  AEP,  NYSE,    -5.19, 2013-07-26\\n  AES,  NYSE,    23.08, 2013-08-09\\n  AET,  NYSE,     9.35, 2013-07-31\\n  AFL,  NYSE,     6.58, 2013-07-31\\n AGCO,  NYSE,    18.78, 2013-08-01\\n  AGN,  NYSE,     1.01, 2013-07-26\\n  HES,  NYSE,     7.09, 2013-08-01\\n  AIG,  NYSE,    31.76, 2013-08-02\\n  AIN,  NYSE,   -23.08, 2013-08-01\\n  AJG,  NYSE,     5.80, 2013-07-31\\n  ALU,  NYSE,    33.33, 2013-07-31\\n MATX,  NYSE,     6.82, 2013-08-08\\n  ALK,  NYSE,    -0.68, 2013-07-26\\n BEAM,  NYSE,     6.67, 2013-08-09\\n  AME,  NYSE,     0.00, 2013-08-08\\n  TWX,  NYSE,    10.67, 2013-08-08\\n  AVD,  NYSE,   -17.14, 2013-08-06\\n  AMN,  NYSE,    20.00, 2013-08-02\\n   AN,  NYSE,    -1.35, 2013-07-19\\n  AON,  NYSE,     0.91, 2013-07-27\\n  APA,  NYSE,    -0.50, 2013-08-02\\n  APC,  NYSE,    16.67, 2013-07-30\\n  APD,  NYSE,     0.00, 2013-07-24\\n  APH,  NYSE,     1.06, 2013-07-19\\n  ARG,  NYSE,    -0.87, 2013-07-26\\n  AAN,  NYSE,     0.00, 2013-07-25\\n  ARW,  NYSE,     8.74, 2013-07-25\\n ASGN,  NYSE,    14.29, 2013-07-25\\n  ASH,  NYSE,    -8.29, 2013-07-26\\n  ASR,  NYSE,    21.90, 2013-07-23\\n  GAS,  NYSE,    51.85, 2013-08-01\\n  ATO,  NYSE,    13.51, 2013-08-07\\n  ATW,  NYSE,     0.74, 2013-08-01\\n  AVP,  NYSE,    11.54, 2013-08-02\\n  AVT,  NYSE,     3.16, 2013-08-08\\n  AVY,  NYSE,     2.90, 2013-07-24\\n  AXP,  NYSE,     4.96, 2013-07-18\\n    B,  NYSE,     0.00, 2013-07-27\\n   BA,  NYSE,     5.70, 2013-07-25\\n  BAC,  NYSE,    28.00, 2013-07-18\\n  BAX,  NYSE,     2.65, 2013-07-19\\n   BC,  NYSE,    13.89, 2013-07-26\\n  OMX,  NYSE,   -33.33, 2013-08-07\\n  BCE,  NYSE,    -2.67, 2013-08-09\\n  BCR,  NYSE,     2.90, 2013-07-24\\n  BDX,  NYSE,     7.48, 2013-08-02\\n  BEN,  NYSE,     1.18, 2013-07-30\\n  BGG,  NYSE,    15.79, 2013-08-16\\n  BHE,  NYSE,    10.71, 2013-07-26\\n  BHI,  NYSE,    -6.15, 2013-07-20\\n  BID,  NYSE,    -9.56, 2013-08-07\\n  BIO,  NYSE,     7.14, 2013-08-07\\n   BK,  NYSE,     6.90, 2013-07-18\\n  BKH,  NYSE,    -2.38, 2013-08-06\\n  WRB,  NYSE,    -2.99, 2013-07-23\\n  BLC,  NYSE,     9.09, 2013-07-31\\n  BLL,  NYSE,     1.19, 2013-07-26\\n  BLX,  NYSE,     5.56, 2013-07-19\\n  BMI,  NYSE,   -20.00, 2013-07-19\\n  BMS,  NYSE,     1.67, 2013-07-26\\n  BMY,  NYSE,     0.00, 2013-07-26\\n  BOH,  NYSE,     2.41, 2013-07-23\\n  BXS,  NYSE,    10.00, 2013-07-23\\n  BPL,  NYSE,    -8.86, 2013-08-03\\nBRK.A,  NYSE,   176.30, 2013-08-03\\n  BRO,  NYSE,     2.86, 2013-07-16\\n  BSX,  NYSE,    12.50, 2013-07-26\\n   BT,  NYSE,     6.17, 2013-07-26\\n MTRN,  NYSE,     7.50, 2013-07-27\\n  CAI,  NYSE,    -8.54, 2013-07-31\\n  CAT,  NYSE,   -15.20, 2013-07-25\\n   CB,  NYSE,    19.27, 2013-07-24\\n  CBI,  NYSE,     0.00, 2013-07-31\\n  CBM,  NYSE,   -64.29, 2013-08-02\\n  CBU,  NYSE,     4.00, 2013-07-24\\n  CBT,  NYSE,    -4.35, 2013-08-01\\n  CCC,  NYSE,    14.29, 2013-08-07\\n  CCE,  NYSE,     2.67, 2013-07-26\\n    C,  NYSE,     5.93, 2013-07-16\\n  CCK,  NYSE,     3.23, 2013-07-18\\n  CCU,  NYSE,    25.00, 2013-08-08\\n  CDE,  NYSE, -1100.00, 2013-08-09\\n  CDI,  NYSE,     6.25, 2013-08-02\\n  CAH,  NYSE,     2.60, 2013-08-02\\n  CFR,  NYSE,     0.00, 2013-07-25\\n  CHD,  NYSE,     1.67, 2013-08-03\\n  CKP,  NYSE,   -15.38, 2013-08-07\\n  CPK,  NYSE,    -7.02, 2013-08-10\\n   CI,  NYSE,    11.95, 2013-08-02\\n  CKH,  NYSE,    51.67, 2013-07-31\\n   CL,  NYSE,     0.00, 2013-07-26\\n  CLF,  NYSE,    85.25, 2013-07-26\\n  CLH,  NYSE,   -25.00, 2013-08-08\\n  CLX,  NYSE,     2.99, 2013-08-02\\n  CMA,  NYSE,     8.57, 2013-07-17\\n  CMO,  NYSE,   -15.63, 2013-07-25\\n  CRK,  NYSE,    -6.67, 2013-07-30\\n  CMS,  NYSE,   -14.71, 2013-07-26\\n  CNA,  NYSE,    17.19, 2013-07-31\\n  CNW,  NYSE,    13.56, 2013-08-01\\n  CNL,  NYSE,    -6.06, 2013-08-01\\n  COG,  NYSE,    35.48, 2013-07-25\\n  COT,  NYSE,    -4.76, 2013-08-02\\n   CP,  NYSE,    -4.14, 2013-07-25\\n  CPF,  NYSE,    25.93, 2013-07-26\\n  CQB,  NYSE,    43.48, 2013-08-09\\n   CR,  NYSE,     0.00, 2013-07-23\\nCRD.B,  NYSE,    42.86, 2013-08-06\\n  CRS,  NYSE,    11.59, 2013-07-31\\n  CSC,  NYSE,    42.19, 2013-08-07\\n  CSL,  NYSE,   -14.93, 2013-07-24\\n  CTB,  NYSE,   -38.20, 2013-08-09\\n  CTL,  NYSE,     2.99, 2013-08-08\\n  CTS,  NYSE,    33.33, 2013-07-23\\n  CUB,  NYSE,     9.52, 2013-08-02\\n  CMI,  NYSE,    11.11, 2013-07-31\\n  CUZ,  NYSE,     9.09, 2013-07-30\\n  CVC,  NYSE,    80.00, 2013-08-03\\n   CW,  NYSE,     6.06, 2013-08-01\\n  CWT,  NYSE,     0.00, 2013-08-01\\n   CX,  NYSE,     0.00, 2013-07-26\\n  CYN,  NYSE,     8.33, 2013-07-19\\n    D,  NYSE,    -4.62, 2013-08-07\\n  DBD,  NYSE,     0.00, 2013-08-15\\n  DCO,  NYSE,    30.77, 2013-08-06\\n   DD,  NYSE,     0.79, 2013-07-24\\n  CVA,  NYSE,   150.00, 2013-07-18\\n  DHR,  NYSE,     2.35, 2013-07-19\\n  DIS,  NYSE,     0.00, 2013-08-07\\n  DLX,  NYSE,    10.34, 2013-07-26\\n  DNB,  NYSE,     2.00, 2013-08-08\\n  RRD,  NYSE,     4.65, 2013-07-30\\n  DOV,  NYSE,     5.43, 2013-07-19\\n  DOW,  NYSE,     1.59, 2013-07-26\\n  DRE,  NYSE,     0.00, 2013-08-01\\n  DHI,  NYSE,    23.53, 2013-07-26\\n  UFS,  NYSE,   -25.00, 2013-07-26\\n  DTE,  NYSE,   -21.52, 2013-07-27\\n  DUK,  NYSE,    -6.45, 2013-08-08\\n  DVN,  NYSE,    28.72, 2013-08-08\\n   DV,  NYSE,    31.71, 2013-08-09\\n  EAT,  NYSE,     4.05, 2013-08-03\\n  ECL,  NYSE,     2.38, 2013-07-31\\n   ED,  NYSE,    -5.26, 2013-08-02\\n  EDE,  NYSE,     8.00, 2013-07-26\\n  EFX,  NYSE,     2.22, 2013-07-25\\n  EGN,  NYSE,     8.20, 2013-08-01\\n  EGP,  NYSE,     2.56, 2013-07-19\\n  ELP,  NYSE,    17.65, 2013-08-16\\n  ELY,  NYSE,    20.00, 2013-07-26\\n  EMC,  NYSE,     2.94, 2013-07-25\\n  EMR,  NYSE,    -2.02, 2013-08-07\\n  EOG,  NYSE,    19.32, 2013-08-07\\n  EQT,  NYSE,     3.64, 2013-07-26\\n  ESE,  NYSE,   -41.07, 2013-08-09\\n  ESV,  NYSE,     3.33, 2013-07-30\\n  ETN,  NYSE,    -1.80, 2013-08-03\\n  ETR,  NYSE,     3.06, 2013-07-31\\n EXAR,  NYSE,    14.29, 2013-07-25\\n    F,  NYSE,    21.62, 2013-07-25\\n CLGX,  NYSE,    13.64, 2013-07-25\\n  FNB,  NYSE,     0.00, 2013-07-24\\n  FCF,  NYSE,   -50.00, 2013-07-25\\n  FBP,  NYSE,   -11.11, 2013-07-25\\n FICO,  NYSE,     6.35, 2013-07-31\\n  FLO,  NYSE,     4.35, 2013-08-14\\n  FMC,  NYSE,     0.00, 2013-07-30\\n  FOE,  NYSE,    27.27, 2013-08-01\\n    S,  NYSE,     6.06, 2013-07-31\\n  NEE,  NYSE,    13.18, 2013-07-31\\n  FRT,  NYSE,     0.88, 2013-08-01\\n  FRX,  NYSE,   300.00, 2013-07-24\\n  FSS,  NYSE,    64.29, 2013-08-10\\n  FUN,  NYSE,     2.41, 2013-08-09\\n  FUR,  NYSE,   -48.15, 2013-08-02\\n  GBL,  NYSE,    17.20, 2013-08-07\\n  GVA,  NYSE,   -78.13, 2013-08-02\\n  BGC,  NYSE,    23.21, 2013-08-01\\n   GD,  NYSE,    11.73, 2013-07-25\\n   GE,  NYSE,     0.00, 2013-07-20\\n  RHP,  NYSE,   -26.85, 2013-08-07\\n AXLL,  NYSE,     2.59, 2013-08-01\\n  GGG,  NYSE,     9.52, 2013-07-25\\n  GHM,  NYSE,    52.00, 2013-07-26\\n  GIB,  NYSE,    10.71, 2013-08-01\\n  GLT,  NYSE,    20.00, 2013-07-31\\n  GLW,  NYSE,     3.23, 2013-07-31\\n  GSK,  NYSE,    -5.88, 2013-07-25\\n  GLF,  NYSE,    25.71, 2013-07-23\\n  GPC,  NYSE,    14.88, 2013-07-19\\n  GRA,  NYSE,     2.75, 2013-07-26\\n  GTY,  NYSE,    36.00, 2013-08-08\\n  GWW,  NYSE,     2.71, 2013-07-18\\n  HAE,  NYSE,     0.00, 2013-07-30\\n  HAL,  NYSE,     1.39, 2013-07-23\\n  HAR,  NYSE,     4.60, 2013-08-07\\n  HVT,  NYSE,    31.25, 2013-08-01\\n  HRC,  NYSE,     0.00, 2013-07-25\\n  HCC,  NYSE,    21.69, 2013-07-31\\n  HCN,  NYSE,     1.09, 2013-08-07\\n  HCP,  NYSE,    -2.70, 2013-07-31\\n  HOG,  NYSE,     3.42, 2013-07-26\\n   HE,  NYSE,     7.89, 2013-08-09\\n  HMA,  NYSE,   -46.15, 2013-08-10\\n  HMN,  NYSE,    30.00, 2013-07-25\\n  HFC,  NYSE,     0.00, 2013-08-08\\n  HOT,  NYSE,     8.22, 2013-07-26\\n   HP,  NYSE,     6.67, 2013-07-27\\n  HLS,  NYSE,    18.60, 2013-07-26\\n  HRS,  NYSE,    23.68, 2013-07-31\\n  HSC,  NYSE,   -11.76, 2013-08-09\\n  HSY,  NYSE,     1.41, 2013-07-26\\n HUBB,  NYSE,     5.38, 2013-07-19\\n  HUM,  NYSE,     6.91, 2013-08-01\\n  HXL,  NYSE,     2.13, 2013-07-23\\n  IBM,  NYSE,     3.44, 2013-07-18\\n  IDA,  NYSE,    33.82, 2013-08-02\\n  IEX,  NYSE,     2.70, 2013-07-23\\n  IFF,  NYSE,    -3.39, 2013-08-07\\n  DIN,  NYSE,    12.09, 2013-07-31\\n  INT,  NYSE,    11.76, 2013-08-01\\n   IP,  NYSE,    -5.45, 2013-07-26\\n  IPG,  NYSE,   -14.29, 2013-07-20\\n   IO,  NYSE,  -100.00, 2013-08-08\\n   IR,  NYSE,     5.56, 2013-07-20\\n  IRF,  NYSE,    81.82, 2013-08-20\\n  ITW,  NYSE,    -0.92, 2013-07-24\\n  JEC,  NYSE,    -1.19, 2013-07-30\\n  JNJ,  NYSE,     5.71, 2013-07-17\\n  JNY,  NYSE,   116.67, 2013-08-01\\n    K,  NYSE,     3.09, 2013-08-02\\n KAMN,  NYSE,    13.56, 2013-07-30\\n  KDN,  NYSE,    10.53, 2013-07-26\\n  KEX,  NYSE,     0.94, 2013-07-25\\n  KEY,  NYSE,     5.00, 2013-07-19\\n  KIM,  NYSE,     6.06, 2013-07-30\\n  KMB,  NYSE,     1.44, 2013-07-23\\n  KEM,  NYSE,   -95.00, 2013-07-26\\n  KMT,  NYSE,     4.11, 2013-07-26\\n   KO,  NYSE,     0.00, 2013-07-17\\n  KSU,  NYSE,     1.05, 2013-07-20\\n  LDR,  NYSE,   -19.64, 2013-08-06\\n  LEG,  NYSE,     0.00, 2013-07-26\\n  LLY,  NYSE,    13.73, 2013-07-25\\n   LM,  NYSE,    -1.45, 2013-07-26\\n  LNC,  NYSE,    10.43, 2013-08-01\\n  LPX,  NYSE,    32.26, 2013-08-07\\n  LXU,  NYSE,    29.17, 2013-08-09\\n  LTC,  NYSE,    -3.39, 2013-08-09\\n    L,  NYSE,    -5.48, 2013-07-30\\n  LUV,  NYSE,    -2.56, 2013-07-26\\n  LUX,  NYSE,    -1.67, 2013-07-26\\n  MKL,  NYSE,     7.46, 2013-08-08\\n  MAN,  NYSE,    17.98, 2013-07-20\\n  MTW,  NYSE,    25.00, 2013-07-30\\n   SM,  NYSE,     0.00, 2013-07-31\\n  MAS,  NYSE,    21.05, 2013-07-30\\n  MTZ,  NYSE,     2.33, 2013-08-02\\n  MCD,  NYSE,    -1.43, 2013-07-23\\n  MDC,  NYSE,    38.18, 2013-07-31\\n  MDP,  NYSE,     5.63, 2013-07-26\\n  MDR,  NYSE, -1966.67, 2013-08-06\\n  MDU,  NYSE,    -3.85, 2013-08-01\\n  MED,  NYSE,     2.00, 2013-08-07\\n  CVS,  NYSE,     1.04, 2013-08-07\\n  MFC,  NYSE,    -3.12, 2013-08-09\\n  MGA,  NYSE,    11.25, 2013-08-10\\n  MGM,  NYSE,   300.00, 2013-08-07\\n  MMC,  NYSE,     2.94, 2013-08-08\\n  MMM,  NYSE,     0.59, 2013-07-26\\n  MSA,  NYSE,     0.00, 2013-07-25\\n  MNR,  NYSE,   -27.78, 2013-08-07\\n   MO,  NYSE,    -1.59, 2013-07-24\\n  MOD,  NYSE,   145.45, 2013-08-02\\nMOG.A,  NYSE,     8.43, 2013-07-27\\n  MHK,  NYSE,    10.84, 2013-08-02\\n  MSI,  NYSE,    11.96, 2013-07-25\\n  MCY,  NYSE,     3.28, 2013-07-30\\n  MRK,  NYSE,     2.44, 2013-07-31\\n  MRO,  NYSE,    -5.63, 2013-08-07\\n POWR,  NYSE,    20.00, 2013-08-08\\n  MTG,  NYSE,   118.75, 2013-07-24\\n  MTB,  NYSE,    26.19, 2013-07-18\\n  MTX,  NYSE,     8.62, 2013-07-26\\n  MUR,  NYSE,    12.90, 2013-08-01\\n  MYE,  NYSE,    19.05, 2013-07-19\\n  NBL,  NYSE,    -5.48, 2013-07-26\\n  NBR,  NYSE,   -11.11, 2013-07-24\\n   NE,  NYSE,    12.50, 2013-07-18\\n  NEM,  NYSE,  -124.39, 2013-07-27\\n  NFG,  NYSE,     6.15, 2013-08-09\\n  NHI,  NYSE,    -1.14, 2013-08-07\\n   NI,  NYSE,    -4.17, 2013-08-01\\n  NJR,  NYSE,    15.00, 2013-08-08\\n  THC,  NYSE,    -4.35, 2013-08-07\\n  NNN,  NYSE,     0.00, 2013-08-02\\n  NOC,  NYSE,    20.59, 2013-07-25\\n   NR,  NYSE,    -5.26, 2013-07-26\\n  NSC,  NYSE,    -2.67, 2013-07-24\\n  NUE,  NYSE,   -10.00, 2013-07-19\\n  NVR,  NYSE,   -18.34, 2013-07-23\\n  NWL,  NYSE,     2.04, 2013-07-27\\n  NWN,  NYSE,   -11.11, 2013-08-08\\n  NYT,  NYSE,    16.67, 2013-08-02\\n  OCR,  NYSE,     4.65, 2013-07-25\\n  OGE,  NYSE,    -2.13, 2013-08-09\\n  OHI,  NYSE,     1.64, 2013-08-01\\n   OI,  NYSE,     2.53, 2013-07-25\\n  OII,  NYSE,     8.33, 2013-07-25\\n  OKE,  NYSE,  -225.93, 2013-07-31\\n  OLN,  NYSE,     3.85, 2013-07-26\\n  BRS,  NYSE,     1.01, 2013-08-06\\n  OMC,  NYSE,     0.00, 2013-07-19\\n  OMI,  NYSE,     0.00, 2013-07-30\\n  ORB,  NYSE,    17.39, 2013-07-19\\n  ORI,  NYSE,  1750.00, 2013-07-26\\n  OSK,  NYSE,    53.21, 2013-07-31\\n  OXY,  NYSE,    -1.86, 2013-07-31\\n FCFS,  NYSE,     1.79, 2013-07-18\\n  PBI,  NYSE,    15.56, 2013-07-31\\n  PCG,  NYSE,     9.72, 2013-08-01\\n  PCL,  NYSE,    21.74, 2013-07-30\\n  PCP,  NYSE,    -0.69, 2013-07-26\\n  TPC,  NYSE,   -11.11, 2013-08-10\\n  PEG,  NYSE,     4.35, 2013-07-31\\n  PEI,  NYSE,     7.69, 2013-07-24\\n  PEP,  NYSE,    10.08, 2013-07-25\\n  PFE,  NYSE,     3.70, 2013-07-31\\n   PG,  NYSE,     2.60, 2013-08-02\\n  PGR,  NYSE,    -2.44, 2013-07-12\\n   PH,  NYSE,    -8.72, 2013-08-07\\n  PHM,  NYSE,   -10.34, 2013-07-26\\n  PKD,  NYSE,     0.00, 2013-08-07\\n  PKY,  NYSE,     0.00, 2013-08-06\\n  PNC,  NYSE,    21.34, 2013-07-18\\n  PNM,  NYSE,    15.15, 2013-08-03\\n  PNR,  NYSE,     2.22, 2013-07-24\\n  PNW,  NYSE,     3.51, 2013-08-03\\n  POM,  NYSE,    -8.33, 2013-08-08\\n  POT,  NYSE,   -10.98, 2013-07-26\\n  PPG,  NYSE,     4.70, 2013-07-19\\n  PPL,  NYSE,     0.00, 2013-08-02'\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "# Copyright (C) 2013 eNovance SAS <licensing@enovance.com>\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
      "# not use this file except in compliance with the License. You may obtain\n",
      "# a copy of the License at\n",
      "#\n",
      "#      http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
      "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
      "# License for the specific language governing permissions and limitations\n",
      "# under the License.\n",
      "\n",
      "import mock\n",
      "from oslo_config import cfg\n",
      "from oslo_utils import fixture as utils_fixture\n",
      "from oslo_utils import timeutils\n",
      "from oslo_utils import uuidutils\n",
      "\n",
      "from neutron.conf.services import metering_agent as metering_agent_config\n",
      "from neutron.services.metering.agents import metering_agent\n",
      "from neutron.tests import base\n",
      "from neutron.tests import fake_notifier\n",
      "\n",
      "_uuid = uuidutils.generate_uuid\n",
      "\n",
      "TENANT_ID = _uuid()\n",
      "LABEL_ID = _uuid()\n",
      "ROUTERS = [{'status': 'ACTIVE',\n",
      "            'name': 'router1',\n",
      "            'gw_port_id': None,\n",
      "            'admin_state_up': True,\n",
      "            'tenant_id': TENANT_ID,\n",
      "            '_metering_labels': [{'rules': [],\n",
      "                                  'id': LABEL_ID}],\n",
      "            'id': _uuid()}]\n",
      "\n",
      "ROUTERS_WITH_RULE = [{'status': 'ACTIVE',\n",
      "                      'name': 'router1',\n",
      "                      'gw_port_id': None,\n",
      "                      'admin_state_up': True,\n",
      "                      'tenant_id': TENANT_ID,\n",
      "                      '_metering_labels': [{'rule': {},\n",
      "                                            'id': LABEL_ID}],\n",
      "                      'id': _uuid()}]\n",
      "\n",
      "\n",
      "class TestMeteringOperations(base.BaseTestCase):\n",
      "\n",
      "    def setUp(self):\n",
      "        super(TestMeteringOperations, self).setUp()\n",
      "        metering_agent_config.register_metering_agent_opts()\n",
      "\n",
      "        self.noop_driver = ('neutron.services.metering.drivers.noop.'\n",
      "                            'noop_driver.NoopMeteringDriver')\n",
      "        cfg.CONF.set_override('driver', 'noop')\n",
      "        cfg.CONF.set_override('measure_interval', 0)\n",
      "        cfg.CONF.set_override('report_interval', 0)\n",
      "\n",
      "        self.setup_notification_driver()\n",
      "\n",
      "        metering_rpc = ('neutron.services.metering.agents.metering_agent.'\n",
      "                        'MeteringPluginRpc._get_sync_data_metering')\n",
      "        self.metering_rpc_patch = mock.patch(metering_rpc, return_value=[])\n",
      "        self.metering_rpc_patch.start()\n",
      "\n",
      "        self.driver_patch = mock.patch(self.noop_driver, spec=True)\n",
      "        self.driver_patch.start()\n",
      "\n",
      "        loopingcall_patch = mock.patch(\n",
      "            'oslo_service.loopingcall.FixedIntervalLoopingCall')\n",
      "        loopingcall_patch.start()\n",
      "\n",
      "        self.agent = metering_agent.MeteringAgent('my agent', cfg.CONF)\n",
      "        self.driver = self.agent.metering_driver\n",
      "\n",
      "    def test_add_metering_label(self):\n",
      "        self.agent.add_metering_label(None, ROUTERS)\n",
      "        self.assertEqual(1, self.driver.add_metering_label.call_count)\n",
      "\n",
      "    def test_remove_metering_label(self):\n",
      "        self.agent.remove_metering_label(None, ROUTERS)\n",
      "        self.assertEqual(1, self.driver.remove_metering_label.call_count)\n",
      "\n",
      "    def test_update_metering_label_rule(self):\n",
      "        self.agent.update_metering_label_rules(None, ROUTERS)\n",
      "        self.assertEqual(1, self.driver.update_metering_label_rules.call_count)\n",
      "\n",
      "    def test_add_metering_label_rule(self):\n",
      "        self.agent.add_metering_label_rule(None, ROUTERS_WITH_RULE)\n",
      "        self.assertEqual(1, self.driver.add_metering_label_rule.call_count)\n",
      "\n",
      "    def test_remove_metering_label_rule(self):\n",
      "        self.agent.remove_metering_label_rule(None, ROUTERS_WITH_RULE)\n",
      "        self.assertEqual(1, self.driver.remove_metering_label_rule.call_count)\n",
      "\n",
      "    def test_routers_updated(self):\n",
      "        self.agent.routers_updated(None, ROUTERS)\n",
      "        self.assertEqual(1, self.driver.update_routers.call_count)\n",
      "\n",
      "    def test_get_traffic_counters(self):\n",
      "        self.agent._get_traffic_counters(None, ROUTERS)\n",
      "        self.assertEqual(1, self.driver.get_traffic_counters.call_count)\n",
      "\n",
      "    def test_sync_router_namespaces(self):\n",
      "        self.agent._sync_router_namespaces(None, ROUTERS)\n",
      "        self.assertEqual(1, self.driver.sync_router_namespaces.call_count)\n",
      "\n",
      "    def test_notification_report(self):\n",
      "        self.agent.routers_updated(None, ROUTERS)\n",
      "\n",
      "        self.driver.get_traffic_counters.return_value = {LABEL_ID:\n",
      "                                                         {'pkts': 88,\n",
      "                                                          'bytes': 444}}\n",
      "        self.agent._metering_loop()\n",
      "\n",
      "        self.assertNotEqual(len(fake_notifier.NOTIFICATIONS), 0)\n",
      "        for n in fake_notifier.NOTIFICATIONS:\n",
      "            if n['event_type'] == 'l3.meter':\n",
      "                break\n",
      "\n",
      "        self.assertEqual('l3.meter', n['event_type'])\n",
      "\n",
      "        payload = n['payload']\n",
      "        self.assertEqual(TENANT_ID, payload['tenant_id'])\n",
      "        self.assertEqual(LABEL_ID, payload['label_id'])\n",
      "        self.assertEqual(88, payload['pkts'])\n",
      "        self.assertEqual(444, payload['bytes'])\n",
      "\n",
      "    def test_notification_report_interval(self):\n",
      "        measure_interval = 30\n",
      "        report_interval = 600\n",
      "\n",
      "        now = timeutils.utcnow()\n",
      "        time_fixture = self.useFixture(utils_fixture.TimeFixture(now))\n",
      "\n",
      "        self.agent.routers_updated(None, ROUTERS)\n",
      "\n",
      "        self.driver.get_traffic_counters.return_value = {LABEL_ID:\n",
      "                                                         {'pkts': 889,\n",
      "                                                          'bytes': 4440}}\n",
      "\n",
      "        cfg.CONF.set_override('measure_interval', measure_interval)\n",
      "        cfg.CONF.set_override('report_interval', report_interval)\n",
      "\n",
      "        for i in range(report_interval):\n",
      "            self.agent._metering_loop()\n",
      "            count = 0\n",
      "\n",
      "            if len(fake_notifier.NOTIFICATIONS) > 1:\n",
      "                for n in fake_notifier.NOTIFICATIONS:\n",
      "                    if n['event_type'] == 'l3.meter':\n",
      "                        # skip the first notification because the time is 0\n",
      "                        count += 1\n",
      "                        if count > 1:\n",
      "                            break\n",
      "\n",
      "            time_fixture.advance_time_seconds(measure_interval)\n",
      "\n",
      "        self.assertEqual('l3.meter', n['event_type'])\n",
      "\n",
      "        payload = n['payload']\n",
      "        self.assertEqual(TENANT_ID, payload['tenant_id'])\n",
      "        self.assertEqual(LABEL_ID, payload['label_id'])\n",
      "        self.assertLess((payload['time'] - report_interval),\n",
      "                        measure_interval, payload)\n",
      "        interval = (payload['last_update'] - payload['first_update']) \\\n",
      "            - report_interval\n",
      "        self.assertLess(interval, measure_interval, payload)\n",
      "\n",
      "    def test_router_deleted(self):\n",
      "        label_id = _uuid()\n",
      "        self.driver.get_traffic_counters = mock.MagicMock()\n",
      "        self.driver.get_traffic_counters.return_value = {label_id:\n",
      "                                                         {'pkts': 44,\n",
      "                                                          'bytes': 222}}\n",
      "        self.agent._add_metering_info = mock.MagicMock()\n",
      "\n",
      "        self.agent.routers_updated(None, ROUTERS)\n",
      "        self.agent.router_deleted(None, ROUTERS[0]['id'])\n",
      "\n",
      "        self.assertEqual(1, self.agent._add_metering_info.call_count)\n",
      "        self.assertEqual(1, self.driver.remove_router.call_count)\n",
      "\n",
      "        self.agent._add_metering_info.assert_called_with(label_id, 44, 222)\n",
      "\n",
      "    @mock.patch('time.time')\n",
      "    def _test_purge_metering_info(self, current_timestamp, is_empty,\n",
      "                                  mock_time):\n",
      "        mock_time.return_value = current_timestamp\n",
      "        self.agent.metering_infos = {'fake': {'last_update': 1}}\n",
      "        self.config(report_interval=1)\n",
      "\n",
      "        self.agent._purge_metering_info()\n",
      "        self.assertEqual(0 if is_empty else 1, len(self.agent.metering_infos))\n",
      "        self.assertEqual(1, mock_time.call_count)\n",
      "\n",
      "    def test_purge_metering_info(self):\n",
      "        # 1 < 2 - 1 -> False\n",
      "        self._test_purge_metering_info(2, False)\n",
      "\n",
      "    def test_purge_metering_info_delete(self):\n",
      "        # 1 < 3 - 1 -> False\n",
      "        self._test_purge_metering_info(3, True)\n",
      "\n",
      "    @mock.patch('time.time')\n",
      "    def _test_add_metering_info(self, expected_info, current_timestamp,\n",
      "                                mock_time):\n",
      "        mock_time.return_value = current_timestamp\n",
      "        actual_info = self.agent._add_metering_info('fake_label_id', 1, 1)\n",
      "        self.assertEqual(1, len(self.agent.metering_infos))\n",
      "        self.assertEqual(expected_info, actual_info)\n",
      "        self.assertEqual(expected_info,\n",
      "                         self.agent.metering_infos['fake_label_id'])\n",
      "        self.assertEqual(1, mock_time.call_count)\n",
      "\n",
      "    def test_add_metering_info_create(self):\n",
      "        expected_info = {'bytes': 1, 'pkts': 1, 'time': 0, 'first_update': 1,\n",
      "                         'last_update': 1}\n",
      "        self._test_add_metering_info(expected_info, 1)\n",
      "\n",
      "    def test_add_metering_info_update(self):\n",
      "        expected_info = {'bytes': 1, 'pkts': 1, 'time': 0, 'first_update': 1,\n",
      "                         'last_update': 1}\n",
      "        self.agent.metering_infos = {'fake_label_id': expected_info}\n",
      "        expected_info.update({'bytes': 2, 'pkts': 2, 'time': 1,\n",
      "                              'last_update': 2})\n",
      "        self._test_add_metering_info(expected_info, 2)\n",
      "\n",
      "    def test_metering_agent_host_value(self):\n",
      "        expected_host = 'my agent'\n",
      "        self.assertEqual(expected_host, self.agent.host)\n",
      "\n",
      "\n",
      "class TestMeteringDriver(base.BaseTestCase):\n",
      "    def setUp(self):\n",
      "        super(TestMeteringDriver, self).setUp()\n",
      "        metering_agent_config.register_metering_agent_opts()\n",
      "\n",
      "        cfg.CONF.set_override('driver', 'noop')\n",
      "\n",
      "        self.agent = metering_agent.MeteringAgent('my agent', cfg.CONF)\n",
      "        self.driver = mock.Mock()\n",
      "        self.agent.metering_driver = self.driver\n",
      "\n",
      "    def test_add_metering_label_with_bad_driver_impl(self):\n",
      "        del self.driver.add_metering_label\n",
      "\n",
      "        with mock.patch.object(metering_agent, 'LOG') as log:\n",
      "            self.agent.add_metering_label(None, ROUTERS)\n",
      "            log.exception.assert_called_with(mock.ANY,\n",
      "                                             {'driver': 'noop',\n",
      "                                              'func': 'add_metering_label'})\n",
      "\n",
      "    def test_add_metering_label_runtime_error(self):\n",
      "        self.driver.add_metering_label.side_effect = RuntimeError\n",
      "\n",
      "        with mock.patch.object(metering_agent, 'LOG') as log:\n",
      "            self.agent.add_metering_label(None, ROUTERS)\n",
      "            log.exception.assert_called_with(mock.ANY,\n",
      "                                             {'driver': 'noop',\n",
      "                                              'func':\n",
      "                                              'add_metering_label'})\n",
      "\n",
      "    def test_init_chain(self):\n",
      "        with mock.patch('oslo_service.'\n",
      "                        'periodic_task.PeriodicTasks.__init__') as init:\n",
      "            metering_agent.MeteringAgent('my agent', cfg.CONF)\n",
      "        init.assert_called_once_with(cfg.CONF)\n",
      "\n",
      "from PIL import ImageChops, Image as PILImage\n",
      "from http.client import HTTPConnection\n",
      "from time import sleep\n",
      "from traceback import format_stack, print_exc\n",
      "\n",
      "\n",
      "def Tint(image, color):\n",
      "    return ImageChops.blend(image, PILImage.new('RGB', image.size, color), 0.36)\n",
      "\n",
      "def GetStatusCode(host, path=\"/\"):\n",
      "    \"\"\" This function retreives the status code of a website by requesting\n",
      "        HEAD data from the host. This means that it only requests the headers.\n",
      "        If the host cannot be reached or something else goes wrong, it returns\n",
      "        None instead.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        conn = HTTPConnection(host)\n",
      "        conn.request(\"HEAD\", path)\n",
      "        return conn.getresponse().status\n",
      "    except Exception:\n",
      "        return None\n",
      "    \n",
      "def WaitOK(host, path=\"/\"):\n",
      "    while GetStatusCode(host, path) != 200:\n",
      "        sleep(5)\n",
      "        \n",
      "# Copyright 2016 Quantopian, Inc.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "from abc import (\n",
      "    ABCMeta,\n",
      "    abstractmethod,\n",
      "    abstractproperty,\n",
      ")\n",
      "\n",
      "from numpy import concatenate\n",
      "from lru import LRU\n",
      "from pandas import isnull\n",
      "from pandas.tslib import normalize_date\n",
      "from toolz import sliding_window\n",
      "\n",
      "from six import with_metaclass\n",
      "\n",
      "from zipline.assets import Equity, Future\n",
      "from zipline.assets.continuous_futures import ContinuousFuture\n",
      "from zipline.lib._int64window import AdjustedArrayWindow as Int64Window\n",
      "from zipline.lib._float64window import AdjustedArrayWindow as Float64Window\n",
      "from zipline.lib.adjustment import Float64Multiply, Float64Add\n",
      "from zipline.utils.cache import ExpiringCache\n",
      "from zipline.utils.math_utils import number_of_decimal_places\n",
      "from zipline.utils.memoize import lazyval\n",
      "from zipline.utils.numpy_utils import float64_dtype\n",
      "from zipline.utils.pandas_utils import find_in_sorted_index\n",
      "\n",
      "# Default number of decimal places used for rounding asset prices.\n",
      "DEFAULT_ASSET_PRICE_DECIMALS = 3\n",
      "\n",
      "\n",
      "class HistoryCompatibleUSEquityAdjustmentReader(object):\n",
      "\n",
      "    def __init__(self, adjustment_reader):\n",
      "        self._adjustments_reader = adjustment_reader\n",
      "\n",
      "    def load_adjustments(self, columns, dts, assets):\n",
      "        \"\"\"\n",
      "        Returns\n",
      "        -------\n",
      "        adjustments : list[dict[int -> Adjustment]]\n",
      "            A list, where each element corresponds to the `columns`, of\n",
      "            mappings from index to adjustment objects to apply at that index.\n",
      "        \"\"\"\n",
      "        out = [None] * len(columns)\n",
      "        for i, column in enumerate(columns):\n",
      "            adjs = {}\n",
      "            for asset in assets:\n",
      "                adjs.update(self._get_adjustments_in_range(\n",
      "                    asset, dts, column))\n",
      "            out[i] = adjs\n",
      "        return out\n",
      "\n",
      "    def _get_adjustments_in_range(self, asset, dts, field):\n",
      "        \"\"\"\n",
      "        Get the Float64Multiply objects to pass to an AdjustedArrayWindow.\n",
      "\n",
      "        For the use of AdjustedArrayWindow in the loader, which looks back\n",
      "        from current simulation time back to a window of data the dictionary is\n",
      "        structured with:\n",
      "        - the key into the dictionary for adjustments is the location of the\n",
      "        day from which the window is being viewed.\n",
      "        - the start of all multiply objects is always 0 (in each window all\n",
      "          adjustments are overlapping)\n",
      "        - the end of the multiply object is the location before the calendar\n",
      "          location of the adjustment action, making all days before the event\n",
      "          adjusted.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        asset : Asset\n",
      "            The assets for which to get adjustments.\n",
      "        dts : iterable of datetime64-like\n",
      "            The dts for which adjustment data is needed.\n",
      "        field : str\n",
      "            OHLCV field for which to get the adjustments.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        out : dict[loc -> Float64Multiply]\n",
      "            The adjustments as a dict of loc -> Float64Multiply\n",
      "        \"\"\"\n",
      "        sid = int(asset)\n",
      "        start = normalize_date(dts[0])\n",
      "        end = normalize_date(dts[-1])\n",
      "        adjs = {}\n",
      "        if field != 'volume':\n",
      "            mergers = self._adjustments_reader.get_adjustments_for_sid(\n",
      "                'mergers', sid)\n",
      "            for m in mergers:\n",
      "                dt = m[0]\n",
      "                if start < dt <= end:\n",
      "                    end_loc = dts.searchsorted(dt)\n",
      "                    adj_loc = end_loc\n",
      "                    mult = Float64Multiply(0,\n",
      "                                           end_loc - 1,\n",
      "                                           0,\n",
      "                                           0,\n",
      "                                           m[1])\n",
      "                    try:\n",
      "                        adjs[adj_loc].append(mult)\n",
      "                    except KeyError:\n",
      "                        adjs[adj_loc] = [mult]\n",
      "            divs = self._adjustments_reader.get_adjustments_for_sid(\n",
      "                'dividends', sid)\n",
      "            for d in divs:\n",
      "                dt = d[0]\n",
      "                if start < dt <= end:\n",
      "                    end_loc = dts.searchsorted(dt)\n",
      "                    adj_loc = end_loc\n",
      "                    mult = Float64Multiply(0,\n",
      "                                           end_loc - 1,\n",
      "                                           0,\n",
      "                                           0,\n",
      "                                           d[1])\n",
      "                    try:\n",
      "                        adjs[adj_loc].append(mult)\n",
      "                    except KeyError:\n",
      "                        adjs[adj_loc] = [mult]\n",
      "        splits = self._adjustments_reader.get_adjustments_for_sid(\n",
      "            'splits', sid)\n",
      "        for s in splits:\n",
      "            dt = s[0]\n",
      "            if start < dt <= end:\n",
      "                if field == 'volume':\n",
      "                    ratio = 1.0 / s[1]\n",
      "                else:\n",
      "                    ratio = s[1]\n",
      "                end_loc = dts.searchsorted(dt)\n",
      "                adj_loc = end_loc\n",
      "                mult = Float64Multiply(0,\n",
      "                                       end_loc - 1,\n",
      "                                       0,\n",
      "                                       0,\n",
      "                                       ratio)\n",
      "                try:\n",
      "                    adjs[adj_loc].append(mult)\n",
      "                except KeyError:\n",
      "                    adjs[adj_loc] = [mult]\n",
      "        return adjs\n",
      "\n",
      "\n",
      "class ContinuousFutureAdjustmentReader(object):\n",
      "    \"\"\"\n",
      "    Calculates adjustments for continuous futures, based on the\n",
      "    close and open of the contracts on the either side of each roll.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self,\n",
      "                 trading_calendar,\n",
      "                 asset_finder,\n",
      "                 bar_reader,\n",
      "                 roll_finders,\n",
      "                 frequency):\n",
      "        self._trading_calendar = trading_calendar\n",
      "        self._asset_finder = asset_finder\n",
      "        self._bar_reader = bar_reader\n",
      "        self._roll_finders = roll_finders\n",
      "        self._frequency = frequency\n",
      "\n",
      "    def load_adjustments(self, columns, dts, assets):\n",
      "        \"\"\"\n",
      "        Returns\n",
      "        -------\n",
      "        adjustments : list[dict[int -> Adjustment]]\n",
      "            A list, where each element corresponds to the `columns`, of\n",
      "            mappings from index to adjustment objects to apply at that index.\n",
      "        \"\"\"\n",
      "        out = [None] * len(columns)\n",
      "        for i, column in enumerate(columns):\n",
      "            adjs = {}\n",
      "            for asset in assets:\n",
      "                adjs.update(self._get_adjustments_in_range(\n",
      "                    asset, dts, column))\n",
      "            out[i] = adjs\n",
      "        return out\n",
      "\n",
      "    def _make_adjustment(self,\n",
      "                         adjustment_type,\n",
      "                         front_close,\n",
      "                         back_close,\n",
      "                         end_loc):\n",
      "        adj_base = back_close - front_close\n",
      "        if adjustment_type == 'mul':\n",
      "            adj_value = 1.0 + adj_base / front_close\n",
      "            adj_class = Float64Multiply\n",
      "        elif adjustment_type == 'add':\n",
      "            adj_value = adj_base\n",
      "            adj_class = Float64Add\n",
      "        return adj_class(0,\n",
      "                         end_loc,\n",
      "                         0,\n",
      "                         0,\n",
      "                         adj_value)\n",
      "\n",
      "    def _get_adjustments_in_range(self, cf, dts, field):\n",
      "        if field == 'volume' or field == 'sid':\n",
      "            return {}\n",
      "        if cf.adjustment is None:\n",
      "            return {}\n",
      "        rf = self._roll_finders[cf.roll_style]\n",
      "        partitions = []\n",
      "\n",
      "        rolls = rf.get_rolls(cf.root_symbol, dts[0], dts[-1],\n",
      "                             cf.offset)\n",
      "\n",
      "        tc = self._trading_calendar\n",
      "\n",
      "        adjs = {}\n",
      "\n",
      "        for front, back in sliding_window(2, rolls):\n",
      "            front_sid, roll_dt = front\n",
      "            back_sid = back[0]\n",
      "            dt = tc.previous_session_label(roll_dt)\n",
      "            if self._frequency == 'minute':\n",
      "                dt = tc.open_and_close_for_session(dt)[1]\n",
      "                roll_dt = tc.open_and_close_for_session(roll_dt)[0]\n",
      "            partitions.append((front_sid,\n",
      "                               back_sid,\n",
      "                               dt,\n",
      "                               roll_dt))\n",
      "        for partition in partitions:\n",
      "            front_sid, back_sid, dt, roll_dt = partition\n",
      "            last_front_dt = self._bar_reader.get_last_traded_dt(\n",
      "                self._asset_finder.retrieve_asset(front_sid), dt)\n",
      "            last_back_dt = self._bar_reader.get_last_traded_dt(\n",
      "                self._asset_finder.retrieve_asset(back_sid), dt)\n",
      "            if isnull(last_front_dt) or isnull(last_back_dt):\n",
      "                continue\n",
      "            front_close = self._bar_reader.get_value(\n",
      "                front_sid, last_front_dt, 'close')\n",
      "            back_close = self._bar_reader.get_value(\n",
      "                back_sid, last_back_dt, 'close')\n",
      "            adj_loc = dts.searchsorted(roll_dt)\n",
      "            end_loc = adj_loc - 1\n",
      "            adj = self._make_adjustment(cf.adjustment,\n",
      "                                        front_close,\n",
      "                                        back_close,\n",
      "                                        end_loc)\n",
      "            try:\n",
      "                adjs[adj_loc].append(adj)\n",
      "            except KeyError:\n",
      "                adjs[adj_loc] = [adj]\n",
      "        return adjs\n",
      "\n",
      "\n",
      "class SlidingWindow(object):\n",
      "    \"\"\"\n",
      "    Wrapper around an AdjustedArrayWindow which supports monotonically\n",
      "    increasing (by datetime) requests for a sized window of data.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    window : AdjustedArrayWindow\n",
      "       Window of pricing data with prefetched values beyond the current\n",
      "       simulation dt.\n",
      "    cal_start : int\n",
      "       Index in the overall calendar at which the window starts.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, window, size, cal_start, offset):\n",
      "        self.window = window\n",
      "        self.cal_start = cal_start\n",
      "        self.current = next(window)\n",
      "        self.offset = offset\n",
      "        self.most_recent_ix = self.cal_start + size\n",
      "\n",
      "    def get(self, end_ix):\n",
      "        \"\"\"\n",
      "        Returns\n",
      "        -------\n",
      "        out : A np.ndarray of the equity pricing up to end_ix after adjustments\n",
      "              and rounding have been applied.\n",
      "        \"\"\"\n",
      "        if self.most_recent_ix == end_ix:\n",
      "            return self.current\n",
      "\n",
      "        target = end_ix - self.cal_start - self.offset + 1\n",
      "        self.current = self.window.seek(target)\n",
      "\n",
      "        self.most_recent_ix = end_ix\n",
      "        return self.current\n",
      "\n",
      "\n",
      "class HistoryLoader(with_metaclass(ABCMeta)):\n",
      "    \"\"\"\n",
      "    Loader for sliding history windows, with support for adjustments.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    trading_calendar: TradingCalendar\n",
      "        Contains the grouping logic needed to assign minutes to periods.\n",
      "    reader : DailyBarReader, MinuteBarReader\n",
      "        Reader for pricing bars.\n",
      "    adjustment_reader : SQLiteAdjustmentReader\n",
      "        Reader for adjustment data.\n",
      "    \"\"\"\n",
      "    FIELDS = ('open', 'high', 'low', 'close', 'volume', 'sid')\n",
      "\n",
      "    def __init__(self, trading_calendar, reader, equity_adjustment_reader,\n",
      "                 asset_finder,\n",
      "                 roll_finders=None,\n",
      "                 sid_cache_size=1000,\n",
      "                 prefetch_length=0):\n",
      "        self.trading_calendar = trading_calendar\n",
      "        self._asset_finder = asset_finder\n",
      "        self._reader = reader\n",
      "        self._adjustment_readers = {}\n",
      "        if equity_adjustment_reader is not None:\n",
      "            self._adjustment_readers[Equity] = \\\n",
      "                HistoryCompatibleUSEquityAdjustmentReader(\n",
      "                    equity_adjustment_reader)\n",
      "        if roll_finders:\n",
      "            self._adjustment_readers[ContinuousFuture] =\\\n",
      "                ContinuousFutureAdjustmentReader(trading_calendar,\n",
      "                                                 asset_finder,\n",
      "                                                 reader,\n",
      "                                                 roll_finders,\n",
      "                                                 self._frequency)\n",
      "        self._window_blocks = {\n",
      "            field: ExpiringCache(LRU(sid_cache_size))\n",
      "            for field in self.FIELDS\n",
      "        }\n",
      "        self._prefetch_length = prefetch_length\n",
      "\n",
      "    @abstractproperty\n",
      "    def _frequency(self):\n",
      "        pass\n",
      "\n",
      "    @abstractproperty\n",
      "    def _calendar(self):\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def _array(self, start, end, assets, field):\n",
      "        pass\n",
      "\n",
      "    def _decimal_places_for_asset(self, asset, reference_date):\n",
      "        if isinstance(asset, Future) and asset.tick_size:\n",
      "            return number_of_decimal_places(asset.tick_size)\n",
      "        elif isinstance(asset, ContinuousFuture):\n",
      "            # Tick size should be the same for all contracts of a continuous\n",
      "            # future, so arbitrarily get the contract with next upcoming auto\n",
      "            # close date.\n",
      "            oc = self._asset_finder.get_ordered_contracts(asset.root_symbol)\n",
      "            contract_sid = oc.contract_before_auto_close(reference_date.value)\n",
      "            if contract_sid is not None:\n",
      "                contract = self._asset_finder.retrieve_asset(contract_sid)\n",
      "                if contract.tick_size:\n",
      "                    return number_of_decimal_places(contract.tick_size)\n",
      "        return DEFAULT_ASSET_PRICE_DECIMALS\n",
      "\n",
      "    def _ensure_sliding_windows(self, assets, dts, field,\n",
      "                                is_perspective_after):\n",
      "        \"\"\"\n",
      "        Ensure that there is a Float64Multiply window for each asset that can\n",
      "        provide data for the given parameters.\n",
      "        If the corresponding window for the (assets, len(dts), field) does not\n",
      "        exist, then create a new one.\n",
      "        If a corresponding window does exist for (assets, len(dts), field), but\n",
      "        can not provide data for the current dts range, then create a new\n",
      "        one and replace the expired window.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        assets : iterable of Assets\n",
      "            The assets in the window\n",
      "        dts : iterable of datetime64-like\n",
      "            The datetimes for which to fetch data.\n",
      "            Makes an assumption that all dts are present and contiguous,\n",
      "            in the calendar.\n",
      "        field : str\n",
      "            The OHLCV field for which to retrieve data.\n",
      "        is_perspective_after : bool\n",
      "            see: `PricingHistoryLoader.history`\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        out : list of Float64Window with sufficient data so that each asset's\n",
      "        window can provide `get` for the index corresponding with the last\n",
      "        value in `dts`\n",
      "        \"\"\"\n",
      "        end = dts[-1]\n",
      "        size = len(dts)\n",
      "        asset_windows = {}\n",
      "        needed_assets = []\n",
      "        cal = self._calendar\n",
      "\n",
      "        assets = self._asset_finder.retrieve_all(assets)\n",
      "        end_ix = find_in_sorted_index(cal, end)\n",
      "\n",
      "        for asset in assets:\n",
      "            try:\n",
      "                window = self._window_blocks[field].get(\n",
      "                    (asset, size, is_perspective_after), end)\n",
      "            except KeyError:\n",
      "                needed_assets.append(asset)\n",
      "            else:\n",
      "                if end_ix < window.most_recent_ix:\n",
      "                    # Window needs reset. Requested end index occurs before the\n",
      "                    # end index from the previous history call for this window.\n",
      "                    # Grab new window instead of rewinding adjustments.\n",
      "                    needed_assets.append(asset)\n",
      "                else:\n",
      "                    asset_windows[asset] = window\n",
      "\n",
      "        if needed_assets:\n",
      "            offset = 0\n",
      "            start_ix = find_in_sorted_index(cal, dts[0])\n",
      "\n",
      "            prefetch_end_ix = min(end_ix + self._prefetch_length, len(cal) - 1)\n",
      "            prefetch_end = cal[prefetch_end_ix]\n",
      "            prefetch_dts = cal[start_ix:prefetch_end_ix + 1]\n",
      "            if is_perspective_after:\n",
      "                adj_end_ix = min(prefetch_end_ix + 1, len(cal) - 1)\n",
      "                adj_dts = cal[start_ix:adj_end_ix + 1]\n",
      "            else:\n",
      "                adj_dts = prefetch_dts\n",
      "            prefetch_len = len(prefetch_dts)\n",
      "            array = self._array(prefetch_dts, needed_assets, field)\n",
      "\n",
      "            if field == 'sid':\n",
      "                window_type = Int64Window\n",
      "            else:\n",
      "                window_type = Float64Window\n",
      "\n",
      "            view_kwargs = {}\n",
      "            if field == 'volume':\n",
      "                array = array.astype(float64_dtype)\n",
      "\n",
      "            for i, asset in enumerate(needed_assets):\n",
      "                adj_reader = None\n",
      "                try:\n",
      "                    adj_reader = self._adjustment_readers[type(asset)]\n",
      "                except KeyError:\n",
      "                    adj_reader = None\n",
      "                if adj_reader is not None:\n",
      "                    adjs = adj_reader.load_adjustments(\n",
      "                        [field], adj_dts, [asset])[0]\n",
      "                else:\n",
      "                    adjs = {}\n",
      "                window = window_type(\n",
      "                    array[:, i].reshape(prefetch_len, 1),\n",
      "                    view_kwargs,\n",
      "                    adjs,\n",
      "                    offset,\n",
      "                    size,\n",
      "                    int(is_perspective_after),\n",
      "                    self._decimal_places_for_asset(asset, dts[-1]),\n",
      "                )\n",
      "                sliding_window = SlidingWindow(window, size, start_ix, offset)\n",
      "                asset_windows[asset] = sliding_window\n",
      "                self._window_blocks[field].set(\n",
      "                    (asset, size, is_perspective_after),\n",
      "                    sliding_window,\n",
      "                    prefetch_end)\n",
      "\n",
      "        return [asset_windows[asset] for asset in assets]\n",
      "\n",
      "    def history(self, assets, dts, field, is_perspective_after):\n",
      "        \"\"\"\n",
      "        A window of pricing data with adjustments applied assuming that the\n",
      "        end of the window is the day before the current simulation time.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        assets : iterable of Assets\n",
      "            The assets in the window.\n",
      "        dts : iterable of datetime64-like\n",
      "            The datetimes for which to fetch data.\n",
      "            Makes an assumption that all dts are present and contiguous,\n",
      "            in the calendar.\n",
      "        field : str\n",
      "            The OHLCV field for which to retrieve data.\n",
      "        is_perspective_after : bool\n",
      "            True, if the window is being viewed immediately after the last dt\n",
      "            in the sliding window.\n",
      "            False, if the window is viewed on the last dt.\n",
      "\n",
      "            This flag is used for handling the case where the last dt in the\n",
      "            requested window immediately precedes a corporate action, e.g.:\n",
      "\n",
      "            - is_perspective_after is True\n",
      "\n",
      "            When the viewpoint is after the last dt in the window, as when a\n",
      "            daily history window is accessed from a simulation that uses a\n",
      "            minute data frequency, the history call to this loader will not\n",
      "            include the current simulation dt. At that point in time, the raw\n",
      "            data for the last day in the window will require adjustment, so the\n",
      "            most recent adjustment with respect to the simulation time is\n",
      "            applied to the last dt in the requested window.\n",
      "\n",
      "            An example equity which has a 0.5 split ratio dated for 05-27,\n",
      "            with the dts for a history call of 5 bars with a '1d' frequency at\n",
      "            05-27 9:31. Simulation frequency is 'minute'.\n",
      "\n",
      "            (In this case this function is called with 4 daily dts, and the\n",
      "             calling function is responsible for stitching back on the\n",
      "             'current' dt)\n",
      "\n",
      "            |       |       |       |       | last dt | <-- viewer is here |\n",
      "            |       | 05-23 | 05-24 | 05-25 | 05-26   | 05-27 9:31         |\n",
      "            | raw   | 10.10 | 10.20 | 10.30 | 10.40   |                    |\n",
      "            | adj   |  5.05 |  5.10 |  5.15 |  5.25   |                    |\n",
      "\n",
      "            The adjustment is applied to the last dt, 05-26, and all previous\n",
      "            dts.\n",
      "\n",
      "            - is_perspective_after is False, daily\n",
      "\n",
      "            When the viewpoint is the same point in time as the last dt in the\n",
      "            window, as when a daily history window is accessed from a\n",
      "            simulation that uses a daily data frequency, the history call will\n",
      "            include the current dt. At that point in time, the raw data for the\n",
      "            last day in the window will be post-adjustment, so no adjustment\n",
      "            is applied to the last dt.\n",
      "\n",
      "            An example equity which has a 0.5 split ratio dated for 05-27,\n",
      "            with the dts for a history call of 5 bars with a '1d' frequency at\n",
      "            05-27 0:00. Simulation frequency is 'daily'.\n",
      "\n",
      "            |       |       |       |       |       | <-- viewer is here |\n",
      "            |       |       |       |       |       | last dt            |\n",
      "            |       | 05-23 | 05-24 | 05-25 | 05-26 | 05-27              |\n",
      "            | raw   | 10.10 | 10.20 | 10.30 | 10.40 | 5.25               |\n",
      "            | adj   |  5.05 |  5.10 |  5.15 |  5.20 | 5.25               |\n",
      "\n",
      "            Adjustments are applied 05-23 through 05-26 but not to the last dt,\n",
      "            05-27\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        out : np.ndarray with shape(len(days between start, end), len(assets))\n",
      "        \"\"\"\n",
      "        block = self._ensure_sliding_windows(assets,\n",
      "                                             dts,\n",
      "                                             field,\n",
      "                                             is_perspective_after)\n",
      "        end_ix = self._calendar.searchsorted(dts[-1])\n",
      "\n",
      "        return concatenate(\n",
      "            [window.get(end_ix) for window in block],\n",
      "            axis=1,\n",
      "        )\n",
      "\n",
      "\n",
      "class DailyHistoryLoader(HistoryLoader):\n",
      "\n",
      "    @property\n",
      "    def _frequency(self):\n",
      "        return 'daily'\n",
      "\n",
      "    @property\n",
      "    def _calendar(self):\n",
      "        return self._reader.sessions\n",
      "\n",
      "    def _array(self, dts, assets, field):\n",
      "        return self._reader.load_raw_arrays(\n",
      "            [field],\n",
      "            dts[0],\n",
      "            dts[-1],\n",
      "            assets,\n",
      "        )[0]\n",
      "\n",
      "\n",
      "class MinuteHistoryLoader(HistoryLoader):\n",
      "\n",
      "    @property\n",
      "    def _frequency(self):\n",
      "        return 'minute'\n",
      "\n",
      "    @lazyval\n",
      "    def _calendar(self):\n",
      "        mm = self.trading_calendar.all_minutes\n",
      "        start = mm.searchsorted(self._reader.first_trading_day)\n",
      "        end = mm.searchsorted(self._reader.last_available_dt, side='right')\n",
      "        return mm[start:end]\n",
      "\n",
      "    def _array(self, dts, assets, field):\n",
      "        return self._reader.load_raw_arrays(\n",
      "            [field],\n",
      "            dts[0],\n",
      "            dts[-1],\n",
      "            assets,\n",
      "        )[0]\n",
      "\n",
      "import requests\n",
      "\n",
      "API_URL = 'https://secure.techfortesco.com/tescolabsapi/restservice.aspx'\n",
      "\n",
      "\n",
      "class TescoLabsApi(object):\n",
      "    def __init__(self, url, developerkey, applicationkey):\n",
      "        self.url = url\n",
      "        self.developerkey = developerkey\n",
      "        self.applicationkey = applicationkey\n",
      "        res = requests.get(self.url,\n",
      "                           params={'command': 'login',\n",
      "                                   'email': '', 'password': '',\n",
      "                                   'developerkey': self.developerkey,\n",
      "                                   'applicationkey': self.applicationkey,\n",
      "                                   })\n",
      "        self.sessionkey = res.json()['SessionKey']\n",
      "\n",
      "    def _command(self, command, **kwargs):\n",
      "        params = kwargs\n",
      "        params.update({'command': command, 'sessionkey': self.sessionkey})\n",
      "        res = requests.get(self.url, params=params)\n",
      "        return res\n",
      "\n",
      "    def listproductcategories(self):\n",
      "        return self._command('listproductcategories')\n",
      "\n",
      "    def listproductsincategory(self, category):\n",
      "        return self._command('listproductsincategory', category=category)\n",
      "\n",
      "    def listproductoffers(self):\n",
      "        return self._command('listproductoffers')\n",
      "\n",
      "    def productsearch(self, searchtext, page=1, extendedinfo=False):\n",
      "        return self._command('productsearch', searchtext=searchtext,\n",
      "                             page=page, extendedinfo=extendedinfo)\n",
      "\n",
      "\n",
      "# Generated by Django 3.2.12 on 2022-03-21 09:04\n",
      "\n",
      "from django.db import migrations, models\n",
      "\n",
      "\n",
      "class Migration(migrations.Migration):\n",
      "\n",
      "    dependencies = [\n",
      "        ('catalog', '0002_tag'),\n",
      "    ]\n",
      "\n",
      "    operations = [\n",
      "        migrations.AddField(\n",
      "            model_name='item',\n",
      "            name='tags',\n",
      "            field=models.ManyToManyField(related_name='items', to='catalog.Tag', verbose_name='Теги'),\n",
      "        ),\n",
      "    ]\n",
      "\n",
      "#\n",
      "# Licensed to the Apache Software Foundation (ASF) under one\n",
      "# or more contributor license agreements.  See the NOTICE file\n",
      "# distributed with this work for additional information\n",
      "# regarding copyright ownership.  The ASF licenses this file\n",
      "# to you under the Apache License, Version 2.0 (the\n",
      "# \"License\"); you may not use this file except in compliance\n",
      "# with the License.  You may obtain a copy of the License at\n",
      "#\n",
      "#   http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing,\n",
      "# software distributed under the License is distributed on an\n",
      "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
      "# KIND, either express or implied.  See the License for the\n",
      "# specific language governing permissions and limitations\n",
      "# under the License.\n",
      "\"\"\"Base task runner\"\"\"\n",
      "import os\n",
      "import subprocess\n",
      "import threading\n",
      "from pwd import getpwnam\n",
      "from tempfile import NamedTemporaryFile\n",
      "from typing import Optional, Union\n",
      "\n",
      "from airflow.configuration import conf\n",
      "from airflow.exceptions import AirflowConfigException\n",
      "from airflow.models.taskinstance import load_error_file\n",
      "from airflow.utils.configuration import tmp_configuration_copy\n",
      "from airflow.utils.log.logging_mixin import LoggingMixin\n",
      "from airflow.utils.net import get_hostname\n",
      "from airflow.utils.platform import getuser\n",
      "\n",
      "PYTHONPATH_VAR = 'PYTHONPATH'\n",
      "\n",
      "\n",
      "class BaseTaskRunner(LoggingMixin):\n",
      "    \"\"\"\n",
      "    Runs Airflow task instances by invoking the `airflow tasks run` command with raw\n",
      "    mode enabled in a subprocess.\n",
      "\n",
      "    :param local_task_job: The local task job associated with running the\n",
      "        associated task instance.\n",
      "    :type local_task_job: airflow.jobs.local_task_job.LocalTaskJob\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, local_task_job):\n",
      "        # Pass task instance context into log handlers to setup the logger.\n",
      "        super().__init__(local_task_job.task_instance)\n",
      "        self._task_instance = local_task_job.task_instance\n",
      "\n",
      "        popen_prepend = []\n",
      "        if self._task_instance.run_as_user:\n",
      "            self.run_as_user = self._task_instance.run_as_user\n",
      "        else:\n",
      "            try:\n",
      "                self.run_as_user = conf.get('core', 'default_impersonation')\n",
      "            except AirflowConfigException:\n",
      "                self.run_as_user = None\n",
      "\n",
      "        # Add sudo commands to change user if we need to. Needed to handle SubDagOperator\n",
      "        # case using a SequentialExecutor.\n",
      "        self.log.debug(\"Planning to run as the %s user\", self.run_as_user)\n",
      "        if self.run_as_user and (self.run_as_user != getuser()):\n",
      "            # We want to include any environment variables now, as we won't\n",
      "            # want to have to specify them in the sudo call - they would show\n",
      "            # up in `ps` that way! And run commands now, as the other user\n",
      "            # might not be able to run the cmds to get credentials\n",
      "            cfg_path = tmp_configuration_copy(chmod=0o600)\n",
      "\n",
      "            # Give ownership of file to user; only they can read and write\n",
      "            subprocess.call(['sudo', 'chown', self.run_as_user, cfg_path], close_fds=True)\n",
      "\n",
      "            # propagate PYTHONPATH environment variable\n",
      "            pythonpath_value = os.environ.get(PYTHONPATH_VAR, '')\n",
      "            popen_prepend = ['sudo', '-E', '-H', '-u', self.run_as_user]\n",
      "\n",
      "            if pythonpath_value:\n",
      "                popen_prepend.append(f'{PYTHONPATH_VAR}={pythonpath_value}')\n",
      "\n",
      "        else:\n",
      "            # Always provide a copy of the configuration file settings. Since\n",
      "            # we are running as the same user, and can pass through environment\n",
      "            # variables then we don't need to include those in the config copy\n",
      "            # - the runner can read/execute those values as it needs\n",
      "            cfg_path = tmp_configuration_copy(chmod=0o600)\n",
      "\n",
      "        self._error_file = NamedTemporaryFile(delete=True)\n",
      "        if self.run_as_user:\n",
      "            try:\n",
      "                os.chown(self._error_file.name, getpwnam(self.run_as_user).pw_uid, -1)\n",
      "            except KeyError:\n",
      "                # No user `run_as_user` found\n",
      "                pass\n",
      "\n",
      "        self._cfg_path = cfg_path\n",
      "        self._command = (\n",
      "            popen_prepend\n",
      "            + self._task_instance.command_as_list(\n",
      "                raw=True,\n",
      "                pickle_id=local_task_job.pickle_id,\n",
      "                mark_success=local_task_job.mark_success,\n",
      "                job_id=local_task_job.id,\n",
      "                pool=local_task_job.pool,\n",
      "                cfg_path=cfg_path,\n",
      "            )\n",
      "            + [\"--error-file\", self._error_file.name]\n",
      "        )\n",
      "        self.process = None\n",
      "\n",
      "    def deserialize_run_error(self) -> Optional[Union[str, Exception]]:\n",
      "        \"\"\"Return task runtime error if its written to provided error file.\"\"\"\n",
      "        return load_error_file(self._error_file)\n",
      "\n",
      "    def _read_task_logs(self, stream):\n",
      "        while True:\n",
      "            line = stream.readline()\n",
      "            if isinstance(line, bytes):\n",
      "                line = line.decode('utf-8')\n",
      "            if not line:\n",
      "                break\n",
      "            self.log.info(\n",
      "                'Job %s: Subtask %s %s',\n",
      "                self._task_instance.job_id,\n",
      "                self._task_instance.task_id,\n",
      "                line.rstrip('\\n'),\n",
      "            )\n",
      "\n",
      "    def run_command(self, run_with=None):\n",
      "        \"\"\"\n",
      "        Run the task command.\n",
      "\n",
      "        :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``\n",
      "        :type run_with: list\n",
      "        :return: the process that was run\n",
      "        :rtype: subprocess.Popen\n",
      "        \"\"\"\n",
      "        run_with = run_with or []\n",
      "        full_cmd = run_with + self._command\n",
      "\n",
      "        self.log.info(\"Running on host: %s\", get_hostname())\n",
      "        self.log.info('Running: %s', full_cmd)\n",
      "\n",
      "        proc = subprocess.Popen(\n",
      "            full_cmd,\n",
      "            stdout=subprocess.PIPE,\n",
      "            stderr=subprocess.STDOUT,\n",
      "            universal_newlines=True,\n",
      "            close_fds=True,\n",
      "            env=os.environ.copy(),\n",
      "            preexec_fn=os.setsid,\n",
      "        )\n",
      "\n",
      "        # Start daemon thread to read subprocess logging output\n",
      "        log_reader = threading.Thread(\n",
      "            target=self._read_task_logs,\n",
      "            args=(proc.stdout,),\n",
      "        )\n",
      "        log_reader.daemon = True\n",
      "        log_reader.start()\n",
      "        return proc\n",
      "\n",
      "    def start(self):\n",
      "        \"\"\"Start running the task instance in a subprocess.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def return_code(self) -> Optional[int]:\n",
      "        \"\"\"\n",
      "        :return: The return code associated with running the task instance or\n",
      "            None if the task is not yet done.\n",
      "        :rtype: int\n",
      "        \"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def terminate(self) -> None:\n",
      "        \"\"\"Force kill the running task instance.\"\"\"\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def on_finish(self) -> None:\n",
      "        \"\"\"A callback that should be called when this is done running.\"\"\"\n",
      "        if self._cfg_path and os.path.isfile(self._cfg_path):\n",
      "            if self.run_as_user:\n",
      "                subprocess.call(['sudo', 'rm', self._cfg_path], close_fds=True)\n",
      "            else:\n",
      "                os.remove(self._cfg_path)\n",
      "        try:\n",
      "            self._error_file.close()\n",
      "        except FileNotFoundError:\n",
      "            # The subprocess has deleted this file before we do\n",
      "            # so we ignore\n",
      "            pass\n",
      "\n",
      "# =======================================================================================================================================\n",
      "# VNU-HCM, University of Science\n",
      "# Department Computer Science, Faculty of Information Technology\n",
      "# Authors: Nhut-Nam Le (Tich Phan Suy Rong)\n",
      "# © 2020\n",
      "\n",
      "\n",
      "import unittest\n",
      "\n",
      "\"\"\"\n",
      "Given two strings, return True if either of the strings appears at the very end of the other string, ignoring upper/lower case differences (in other words, the computation should not be \"case sensitive\"). Note: s.lower() returns the lowercase version of a string.\n",
      "\n",
      "\n",
      "end_other('Hiabc', 'abc') → True\n",
      "end_other('AbC', 'HiaBc') → True\n",
      "end_other('abc', 'abXabc') → True\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "def end_other(a, b):\n",
      "    a = a.lower()\n",
      "    b = b.lower()\n",
      "    return (b[(len(b) - len(a)):] == a, a[(len(a) - len(b)):] == b)[len(a) >= len(b)]\n",
      "\n",
      "\n",
      "class TestEndOther(unittest.TestCase):\n",
      "    def test_case_00(self):\n",
      "        self.assertEqual(end_other('Hiabc', 'abc'), True)\n",
      "\n",
      "    def test_case_01(self):\n",
      "        self.assertEqual(end_other('AbC', 'HiaBc'), True)\n",
      "\n",
      "    def test_case_02(self):\n",
      "        self.assertEqual(end_other('abc', 'abXabc'), True)\n",
      "\n",
      "    def test_case_03(self):\n",
      "        self.assertEqual(end_other('Hiabc', 'abcd'), False)\n",
      "\n",
      "    def test_case_04(self):\n",
      "        self.assertEqual(end_other('Hiabc', 'bc'), True)\n",
      "\n",
      "    def test_case_05(self):\n",
      "        self.assertEqual(end_other('Hiabcx', 'bc'), False)\n",
      "\n",
      "    def test_case_06(self):\n",
      "        self.assertEqual(end_other('abc', 'abc'), True)\n",
      "\n",
      "    def test_case_07(self):\n",
      "        self.assertEqual(end_other('xyz', '12xyz'), True)\n",
      "\n",
      "    def test_case_08(self):\n",
      "        self.assertEqual(end_other('yz', '12xz'), False)\n",
      "\n",
      "    def test_case_09(self):\n",
      "        self.assertEqual(end_other('Z', '12xz'), True)\n",
      "\n",
      "    def test_case_10(self):\n",
      "        self.assertEqual(end_other('12', '12'), True)\n",
      "\n",
      "    def test_case_11(self):\n",
      "        self.assertEqual(end_other('abcXYZ', 'abcDEF'), False)\n",
      "\n",
      "    def test_case_12(self):\n",
      "        self.assertEqual(end_other('ab', 'ab12'), False)\n",
      "\n",
      "    def test_case_13(self):\n",
      "        self.assertEqual(end_other('ab', '12ab'), True)\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main()\n",
      "\n",
      "# ----------------------------------------------------------------------------\n",
      "# pyglet\n",
      "# Copyright (c) 2006-2008 Alex Holkner\n",
      "# All rights reserved.\n",
      "#\n",
      "# Redistribution and use in source and binary forms, with or without\n",
      "# modification, are permitted provided that the following conditions\n",
      "# are met:\n",
      "#\n",
      "#  * Redistributions of source code must retain the above copyright\n",
      "#    notice, this list of conditions and the following disclaimer.\n",
      "#  * Redistributions in binary form must reproduce the above copyright\n",
      "#    notice, this list of conditions and the following disclaimer in\n",
      "#    the documentation and/or other materials provided with the\n",
      "#    distribution.\n",
      "#  * Neither the name of pyglet nor the names of its\n",
      "#    contributors may be used to endorse or promote products\n",
      "#    derived from this software without specific prior written\n",
      "#    permission.\n",
      "#\n",
      "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n",
      "# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n",
      "# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n",
      "# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n",
      "# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n",
      "# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n",
      "# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n",
      "# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n",
      "# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n",
      "# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
      "# POSSIBILITY OF SUCH DAMAGE.\n",
      "# ----------------------------------------------------------------------------\n",
      "\"\"\"Document formats.\n",
      "\n",
      ":since: pyglet 1.1\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "from dotenv import load_dotenv\n",
      "load_dotenv()\n",
      "\n",
      "class Config:\n",
      "    SECRET_KEY = os.environ.get('SECRET_KEY')\n",
      "    SQLALCHEMY_DATABASE_URI = 'postgresql+psycopg2://fidel:fidel@localhost/blog'\n",
      "    UPLOADED_PHOTOS_DEST = 'app/static/photos'\n",
      "    QUOTES_URL = 'http://quotes.stormconsultancy.co.uk/random.json'\n",
      "    MAIL_SERVER = 'smtp.googlemail.com'\n",
      "    MAIL_PORT = 587 \n",
      "    MAIL_USE_TLS = True\n",
      "    MAIL_USERNAME = os.environ.get(\"MAIL_USERNAME\")\n",
      "    MAIL_PASSWORD = os.environ.get(\"MAIL_PASSWORD\")\n",
      "    \n",
      "class ProdConfig(Config):\n",
      "    SQLALCHEMY_DATABASE_URI =os.environ.get('DATABASE_URL')\n",
      "    \n",
      "\n",
      "class DevConfig(Config):\n",
      "    #SQLALCHEMY_DATABASE_URI = 'postgresql+psycopg2://fidel:fidel@localhost/blog'\n",
      "    DEBUG = True\n",
      "\n",
      "\n",
      "config_options = {\n",
      "'development':DevConfig,\n",
      "'production':ProdConfig\n",
      "}\n",
      "_TF_INCLUDE_PATH = \"TF_INCLUDE_PATH\"\n",
      "_TF_LIB_PATH = \"TF_LIB_PATH\"\n",
      "\n",
      "def _get_env_var_with_default(repository_ctx, env_var):\n",
      "  \"\"\"Returns evironment variable value.\"\"\"\n",
      "  if env_var in repository_ctx.os.environ:\n",
      "    value = repository_ctx.os.environ[env_var]\n",
      "    return value\n",
      "  else:\n",
      "    fail(\"Environment variable '%s' was not set.\" % env_var)\n",
      "\n",
      "def _get_tf_conf(repository_ctx):\n",
      "  \"\"\"Returns structure containing all required information about tensorflow\n",
      "     configuration on host platform.\n",
      "  \"\"\"\n",
      "  include_path = _get_env_var_with_default(repository_ctx, _TF_INCLUDE_PATH)\n",
      "  lib_path = _get_env_var_with_default(repository_ctx, _TF_LIB_PATH)\n",
      "  return struct(\n",
      "    include_path = include_path,\n",
      "    lib_path = lib_path\n",
      "  )\n",
      "\n",
      "def _tensorflow_autoconf_impl(repository_ctx):\n",
      "  \"\"\"Implementation of tensorflow autoconf. rule.\"\"\"\n",
      "  tf_conf = _get_tf_conf(repository_ctx)\n",
      "  print(\"Using %s=%s\" % (_TF_INCLUDE_PATH, tf_conf.include_path))\n",
      "  print(\"Using %s=%s\" % (_TF_LIB_PATH, tf_conf.lib_path))\n",
      "  repository_ctx.symlink(tf_conf.include_path, 'include')\n",
      "  repository_ctx.symlink(tf_conf.lib_path, 'lib')\n",
      "  repository_ctx.template('BUILD', Label(\"//third_party/tensorflow:tensorflow.BUILD\"))\n",
      "\n",
      "\n",
      "\n",
      "tensorflow_configure = repository_rule(\n",
      "  implementation = _tensorflow_autoconf_impl,\n",
      "  environ = [\n",
      "    _TF_INCLUDE_PATH,\n",
      "    _TF_LIB_PATH\n",
      "  ]\n",
      ")\n",
      "import yaml\n",
      "from os import path\n",
      "from netmiko import ConnectHandler\n",
      "\n",
      "\n",
      "home_dir = path.expanduser(\"~\")\n",
      "filename = path.join(home_dir, \".netmiko.yml\")\n",
      "\n",
      "with open(filename) as f:\n",
      "    yaml_out = yaml.safe_load(f)\n",
      "\n",
      "cisco3 = yaml_out[\"cisco3\"]\n",
      "net_connect = ConnectHandler(**cisco3)\n",
      "\n",
      "print()\n",
      "print(net_connect.find_prompt())\n",
      "print()\n",
      "\n",
      "# -*- coding: iso-8859-1 -*-\n",
      "\n",
      "# Copyright 1999-2017 Tencent Ltd.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#    http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "from QcloudApi.modules import base\n",
      "\n",
      "\n",
      "class Trade(base.Base):\n",
      "    requestHost = 'trade.api.qcloud.com'\n",
      "\n",
      "\"\"\"Support for ICS Calendar.\"\"\"\n",
      "import copy\n",
      "import logging\n",
      "from datetime import datetime, timedelta\n",
      "from urllib.error import ContentTooShortError, HTTPError, URLError\n",
      "from urllib.request import (\n",
      "    HTTPPasswordMgrWithDefaultRealm,\n",
      "    HTTPBasicAuthHandler,\n",
      "    HTTPDigestAuthHandler,\n",
      "    build_opener,\n",
      "    install_opener,\n",
      "    urlopen,\n",
      ")\n",
      "\n",
      "import voluptuous as vol\n",
      "from homeassistant.components.calendar import (\n",
      "    ENTITY_ID_FORMAT,\n",
      "    PLATFORM_SCHEMA,\n",
      "    CalendarEventDevice,\n",
      "    calculate_offset,\n",
      "    is_offset_reached,\n",
      ")\n",
      "from homeassistant.const import CONF_NAME, CONF_PASSWORD, CONF_URL, CONF_USERNAME\n",
      "import homeassistant.helpers.config_validation as cv\n",
      "from homeassistant.helpers.entity import generate_entity_id\n",
      "from homeassistant.util import Throttle\n",
      "from .icalendarparser import ICalendarParser\n",
      "\n",
      "VERSION = \"2.0.0\"\n",
      "\n",
      "_LOGGER = logging.getLogger(__name__)\n",
      "\n",
      "CONF_DEVICE_ID = \"device_id\"\n",
      "CONF_CALENDARS = \"calendars\"\n",
      "CONF_CALENDAR = \"calendar\"\n",
      "CONF_INCLUDE_ALL_DAY = \"includeAllDay\"\n",
      "CONF_PARSER = \"parser\"\n",
      "\n",
      "OFFSET = \"!!\"\n",
      "\n",
      "PLATFORM_SCHEMA = PLATFORM_SCHEMA.extend(\n",
      "    {\n",
      "        # pylint: disable=no-value-for-parameter\n",
      "        vol.Optional(CONF_CALENDARS, default=[]): vol.All(\n",
      "            cv.ensure_list,\n",
      "            vol.Schema(\n",
      "                [\n",
      "                    vol.Schema(\n",
      "                        {\n",
      "                            vol.Required(CONF_URL): vol.Url(),\n",
      "                            vol.Required(CONF_NAME): cv.string,\n",
      "                            vol.Optional(\n",
      "                                CONF_INCLUDE_ALL_DAY, default=False\n",
      "                            ): cv.boolean,\n",
      "                            vol.Optional(CONF_USERNAME, default=\"\"): cv.string,\n",
      "                            vol.Optional(CONF_PASSWORD, default=\"\"): cv.string,\n",
      "                            vol.Optional(CONF_PARSER, default=\"icalevents\"): cv.string,\n",
      "                        }\n",
      "                    )\n",
      "                ]\n",
      "            ),\n",
      "        )\n",
      "    }\n",
      ")\n",
      "\n",
      "MIN_TIME_BETWEEN_UPDATES = timedelta(minutes=15)\n",
      "# MIN_TIME_BETWEEN_DOWNLOADS is smaller than MIN_TIME_BETWEEN_UPDATES so that\n",
      "# it won't be skipped if an explicit update is called.  Eventually, if these are\n",
      "# configurable, we'll let end users worry about if they mean to have it happen\n",
      "# that way.\n",
      "MIN_TIME_BETWEEN_DOWNLOADS = timedelta(minutes=10)\n",
      "\n",
      "\n",
      "def setup_platform(hass, config, add_entities, _=None):\n",
      "    \"\"\"Set up the ICS Calendar platform\"\"\"\n",
      "    _LOGGER.debug(\"Setting up ics calendars\")\n",
      "    calendar_devices = []\n",
      "    for calendar in config.get(CONF_CALENDARS):\n",
      "        device_data = {\n",
      "            CONF_NAME: calendar.get(CONF_NAME),\n",
      "            CONF_URL: calendar.get(CONF_URL),\n",
      "            CONF_INCLUDE_ALL_DAY: calendar.get(CONF_INCLUDE_ALL_DAY),\n",
      "            CONF_USERNAME: calendar.get(CONF_USERNAME),\n",
      "            CONF_PASSWORD: calendar.get(CONF_PASSWORD),\n",
      "            CONF_PARSER: calendar.get(CONF_PARSER),\n",
      "        }\n",
      "        device_id = \"{}\".format(device_data[CONF_NAME])\n",
      "        entity_id = generate_entity_id(ENTITY_ID_FORMAT, device_id, hass=hass)\n",
      "        calendar_devices.append(ICSCalendarEventDevice(entity_id, device_data))\n",
      "\n",
      "    add_entities(calendar_devices)\n",
      "\n",
      "\n",
      "class ICSCalendarEventDevice(CalendarEventDevice):\n",
      "    \"\"\"A device for getting the next Task from an ICS Calendar\"\"\"\n",
      "\n",
      "    def __init__(self, entity_id, device_data):\n",
      "        _LOGGER.debug(\"Initializing calendar: %s\", device_data[CONF_NAME])\n",
      "        self.data = ICSCalendarData(device_data)\n",
      "        self.entity_id = entity_id\n",
      "        self._event = None\n",
      "        self._name = device_data[CONF_NAME]\n",
      "        self._offset_reached = False\n",
      "        self._last_call = None\n",
      "        self._last_event_list = None\n",
      "\n",
      "    @property\n",
      "    def device_state_attributes(self):\n",
      "        \"\"\"Return the calendar entity's state attributes.\"\"\"\n",
      "        return {\"offset_reached\": self._offset_reached}\n",
      "\n",
      "    @property\n",
      "    def event(self):\n",
      "        \"\"\"Returns the current event for the calendar entity or None\"\"\"\n",
      "        return self._event\n",
      "\n",
      "    @property\n",
      "    def name(self):\n",
      "        \"\"\"Returns the name of the calendar entity\"\"\"\n",
      "        return self._name\n",
      "\n",
      "    async def async_get_events(self, hass, start_date, end_date):\n",
      "        \"\"\"Get all events in a specific time frame.\"\"\"\n",
      "        if (\n",
      "            self._last_event_list is None\n",
      "            or self._last_call is None\n",
      "            or (datetime.now() - self._last_call) > MIN_TIME_BETWEEN_UPDATES\n",
      "        ):\n",
      "            self._last_call = datetime.now()\n",
      "            self._last_event_list = await self.data.async_get_events(\n",
      "                hass, start_date, end_date\n",
      "            )\n",
      "        return self._last_event_list\n",
      "\n",
      "    def update(self):\n",
      "        \"\"\"Update event data.\"\"\"\n",
      "        self.data.update()\n",
      "        event = copy.deepcopy(self.data.event)\n",
      "        if event is None:\n",
      "            self._event = event\n",
      "            return\n",
      "        event = calculate_offset(event, OFFSET)\n",
      "        self._offset_reached = is_offset_reached(event)\n",
      "        self._event = event\n",
      "\n",
      "\n",
      "class ICSCalendarData:\n",
      "    \"\"\"Calss to use the calendar ICS client object to get next event.\"\"\"\n",
      "\n",
      "    def __init__(self, device_data):\n",
      "        \"\"\"Set up how we are going to connect to the ICS Calendar\"\"\"\n",
      "        self.name = device_data[CONF_NAME]\n",
      "        self.url = device_data[CONF_URL]\n",
      "        self.include_all_day = device_data[CONF_INCLUDE_ALL_DAY]\n",
      "        self.parser = ICalendarParser.get_instance(device_data[CONF_PARSER])\n",
      "        self.event = None\n",
      "        self._calendar_data = None\n",
      "        self._last_download = None\n",
      "\n",
      "        if device_data[CONF_USERNAME] != \"\" and device_data[CONF_PASSWORD] != \"\":\n",
      "            passman = HTTPPasswordMgrWithDefaultRealm()\n",
      "            passman.add_password(\n",
      "                None, self.url, device_data[CONF_USERNAME], device_data[CONF_PASSWORD]\n",
      "            )\n",
      "            basic_auth_handler = HTTPBasicAuthHandler(passman)\n",
      "            digest_auth_handler = HTTPDigestAuthHandler(passman)\n",
      "            opener = build_opener(digest_auth_handler, basic_auth_handler)\n",
      "            install_opener(opener)\n",
      "\n",
      "    def _download_calendar(self):\n",
      "        if (\n",
      "            self._calendar_data is None\n",
      "            or self._last_download is None\n",
      "            or (datetime.now() - self._last_download) > MIN_TIME_BETWEEN_DOWNLOADS\n",
      "        ):\n",
      "            self._last_download = datetime.now()\n",
      "            self._calendar_data = None\n",
      "            try:\n",
      "                with urlopen(self.url) as conn:\n",
      "                    self._calendar_data = conn.read().decode().replace(\"\\0\", \"\")\n",
      "            except HTTPError as http_error:\n",
      "                _LOGGER.error(f\"{self.name}: Failed to open url: {http_error.reason}\")\n",
      "            except ContentTooShortError as content_too_short_error:\n",
      "                _LOGGER.error(\n",
      "                    f\"{self.name}: Could not download calendar data: {content_too_short_error.reason}\"\n",
      "                )\n",
      "            except URLError as url_error:\n",
      "                _LOGGER.error(f\"{self.name}: Failed to open url: {url_error.reason}\")\n",
      "            except:\n",
      "                _LOGGER.error(f\"{self.name}: Failed to open url!\")\n",
      "        return\n",
      "\n",
      "    async def async_get_events(self, hass, start_date, end_date):\n",
      "        \"\"\"Get all events in a specific time frame.\"\"\"\n",
      "        event_list = []\n",
      "        await hass.async_add_job(self._download_calendar)\n",
      "        try:\n",
      "            events = self.parser.get_event_list(\n",
      "                content=self._calendar_data,\n",
      "                start=start_date,\n",
      "                end=end_date,\n",
      "                include_all_day=self.include_all_day,\n",
      "            )\n",
      "            event_list = list(map(self.format_dates, events))\n",
      "        except:\n",
      "            _LOGGER.error(f\"{self.name}: Failed to parse ICS!\")\n",
      "            event_list = []\n",
      "\n",
      "        return event_list\n",
      "\n",
      "    @Throttle(MIN_TIME_BETWEEN_UPDATES)\n",
      "    def update(self):\n",
      "        \"\"\"Get the latest data.\"\"\"\n",
      "        self._download_calendar()\n",
      "        try:\n",
      "            self.event = self.parser.get_current_event(\n",
      "                content=self._calendar_data, include_all_day=self.include_all_day\n",
      "            )\n",
      "            self.event[\"start\"] = self.get_hass_date(\n",
      "                self.event[\"start\"], self.event[\"all_day\"]\n",
      "            )\n",
      "            self.event[\"end\"] = self.get_hass_date(\n",
      "                self.event[\"end\"], self.event[\"all_day\"]\n",
      "            )\n",
      "            return True\n",
      "        except:\n",
      "            _LOGGER.error(f\"{self.name}: Failed to parse ICS!\")\n",
      "\n",
      "        return False\n",
      "\n",
      "    def format_dates(self, event):\n",
      "        event[\"start\"] = self.get_date_formatted(event[\"start\"], event[\"all_day\"])\n",
      "        event[\"end\"] = self.get_date_formatted(event[\"end\"], event[\"all_day\"])\n",
      "        return event\n",
      "\n",
      "    def get_date_formatted(self, dt, is_all_day):\n",
      "        \"\"\"Return the formatted date\"\"\"\n",
      "        # Note that all day events should have a time of 0, and the timezone\n",
      "        # must be local.\n",
      "        if is_all_day:\n",
      "            return dt.strftime(\"%Y-%m-%d\")\n",
      "\n",
      "        return dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
      "\n",
      "    def get_hass_date(self, dt, is_all_day):\n",
      "        \"\"\"Return the wrapped and formatted date\"\"\"\n",
      "        if is_all_day:\n",
      "            return {\"date\": self.parser.get_date_formatted(dt, is_all_day)}\n",
      "        return {\"dateTime\": self.parser.get_date_formatted(dt, is_all_day)}\n",
      "\n",
      "# Copyright 2020 Huawei Technologies Co., Ltd\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "# ============================================================================\n",
      "\n",
      "\"\"\"SSD dataset\"\"\"\n",
      "\n",
      "from __future__ import division\n",
      "\n",
      "import os\n",
      "import json\n",
      "import xml.etree.ElementTree as et\n",
      "import numpy as np\n",
      "import cv2\n",
      "\n",
      "import mindspore.dataset as de\n",
      "import mindspore.dataset.vision.c_transforms as C\n",
      "from mindspore.mindrecord import FileWriter\n",
      "from .config import config\n",
      "from .box_utils import jaccard_numpy, ssd_bboxes_encode\n",
      "\n",
      "\n",
      "def _rand(a=0., b=1.):\n",
      "    \"\"\"Generate random.\"\"\"\n",
      "    return np.random.rand() * (b - a) + a\n",
      "\n",
      "\n",
      "def get_imageId_from_fileName(filename, id_iter):\n",
      "    \"\"\"Get imageID from fileName if fileName is int, else return id_iter.\"\"\"\n",
      "    filename = os.path.splitext(filename)[0]\n",
      "    if filename.isdigit():\n",
      "        return int(filename)\n",
      "    return id_iter\n",
      "\n",
      "\n",
      "def random_sample_crop(image, boxes):\n",
      "    \"\"\"Random Crop the image and boxes\"\"\"\n",
      "    height, width, _ = image.shape\n",
      "    min_iou = np.random.choice([None, 0.1, 0.3, 0.5, 0.7, 0.9])\n",
      "\n",
      "    if min_iou is None:\n",
      "        return image, boxes\n",
      "\n",
      "    # max trails (50)\n",
      "    for _ in range(50):\n",
      "        image_t = image\n",
      "\n",
      "        w = _rand(0.3, 1.0) * width\n",
      "        h = _rand(0.3, 1.0) * height\n",
      "\n",
      "        # aspect ratio constraint b/t .5 & 2\n",
      "        if h / w < 0.5 or h / w > 2:\n",
      "            continue\n",
      "\n",
      "        left = _rand() * (width - w)\n",
      "        top = _rand() * (height - h)\n",
      "\n",
      "        rect = np.array([int(top), int(left), int(top + h), int(left + w)])\n",
      "        overlap = jaccard_numpy(boxes, rect)\n",
      "\n",
      "        # dropout some boxes\n",
      "        drop_mask = overlap > 0\n",
      "        if not drop_mask.any():\n",
      "            continue\n",
      "\n",
      "        if overlap[drop_mask].min() < min_iou and overlap[drop_mask].max() > (min_iou + 0.2):\n",
      "            continue\n",
      "\n",
      "        image_t = image_t[rect[0]:rect[2], rect[1]:rect[3], :]\n",
      "\n",
      "        centers = (boxes[:, :2] + boxes[:, 2:4]) / 2.0\n",
      "\n",
      "        m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n",
      "        m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n",
      "\n",
      "        # mask in that both m1 and m2 are true\n",
      "        mask = m1 * m2 * drop_mask\n",
      "\n",
      "        # have any valid boxes? try again if not\n",
      "        if not mask.any():\n",
      "            continue\n",
      "\n",
      "        # take only matching gt boxes\n",
      "        boxes_t = boxes[mask, :].copy()\n",
      "\n",
      "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], rect[:2])\n",
      "        boxes_t[:, :2] -= rect[:2]\n",
      "        boxes_t[:, 2:4] = np.minimum(boxes_t[:, 2:4], rect[2:4])\n",
      "        boxes_t[:, 2:4] -= rect[:2]\n",
      "\n",
      "        return image_t, boxes_t\n",
      "    return image, boxes\n",
      "\n",
      "\n",
      "def preprocess_fn(img_id, image, box, is_training):\n",
      "    \"\"\"Preprocess function for dataset.\"\"\"\n",
      "    cv2.setNumThreads(2)\n",
      "\n",
      "    def _infer_data(image, input_shape):\n",
      "        img_h, img_w, _ = image.shape\n",
      "        input_h, input_w = input_shape\n",
      "\n",
      "        image = cv2.resize(image, (input_w, input_h))\n",
      "\n",
      "        # When the channels of image is 1\n",
      "        if len(image.shape) == 2:\n",
      "            image = np.expand_dims(image, axis=-1)\n",
      "            image = np.concatenate([image, image, image], axis=-1)\n",
      "\n",
      "        return img_id, image, np.array((img_h, img_w), np.float32)\n",
      "\n",
      "    def _data_aug(image, box, is_training, image_size=(300, 300)):\n",
      "        \"\"\"Data augmentation function.\"\"\"\n",
      "        ih, iw, _ = image.shape\n",
      "        w, h = image_size\n",
      "\n",
      "        if not is_training:\n",
      "            return _infer_data(image, image_size)\n",
      "\n",
      "        # Random crop\n",
      "        box = box.astype(np.float32)\n",
      "        image, box = random_sample_crop(image, box)\n",
      "        ih, iw, _ = image.shape\n",
      "\n",
      "        # Resize image\n",
      "        image = cv2.resize(image, (w, h))\n",
      "\n",
      "        # Flip image or not\n",
      "        flip = _rand() < .5\n",
      "        if flip:\n",
      "            image = cv2.flip(image, 1, dst=None)\n",
      "\n",
      "        # When the channels of image is 1\n",
      "        if len(image.shape) == 2:\n",
      "            image = np.expand_dims(image, axis=-1)\n",
      "            image = np.concatenate([image, image, image], axis=-1)\n",
      "\n",
      "        box[:, [0, 2]] = box[:, [0, 2]] / ih\n",
      "        box[:, [1, 3]] = box[:, [1, 3]] / iw\n",
      "\n",
      "        if flip:\n",
      "            box[:, [1, 3]] = 1 - box[:, [3, 1]]\n",
      "\n",
      "        box, label, num_match = ssd_bboxes_encode(box)\n",
      "        return image, box, label, num_match\n",
      "\n",
      "    return _data_aug(image, box, is_training, image_size=config.img_shape)\n",
      "\n",
      "\n",
      "def create_voc_label(is_training):\n",
      "    \"\"\"Get image path and annotation from VOC.\"\"\"\n",
      "    voc_root = config.voc_root\n",
      "    cls_map = {name: i for i, name in enumerate(config.classes)}\n",
      "    sub_dir = 'train' if is_training else 'eval'\n",
      "    voc_dir = os.path.join(voc_root, sub_dir)\n",
      "    if not os.path.isdir(voc_dir):\n",
      "        raise ValueError(f'Cannot find {sub_dir} dataset path.')\n",
      "\n",
      "    image_dir = anno_dir = voc_dir\n",
      "    if os.path.isdir(os.path.join(voc_dir, 'Images')):\n",
      "        image_dir = os.path.join(voc_dir, 'Images')\n",
      "    if os.path.isdir(os.path.join(voc_dir, 'Annotations')):\n",
      "        anno_dir = os.path.join(voc_dir, 'Annotations')\n",
      "\n",
      "    if not is_training:\n",
      "        json_file = os.path.join(config.voc_root, config.voc_json)\n",
      "        file_dir = os.path.split(json_file)[0]\n",
      "        if not os.path.isdir(file_dir):\n",
      "            os.makedirs(file_dir)\n",
      "        json_dict = {\"images\": [], \"type\": \"instances\", \"annotations\": [],\n",
      "                     \"categories\": []}\n",
      "        bnd_id = 1\n",
      "\n",
      "    image_files_dict = {}\n",
      "    image_anno_dict = {}\n",
      "    images = []\n",
      "    id_iter = 0\n",
      "    for anno_file in os.listdir(anno_dir):\n",
      "        print(anno_file)\n",
      "        if not anno_file.endswith('xml'):\n",
      "            continue\n",
      "        tree = et.parse(os.path.join(anno_dir, anno_file))\n",
      "        root_node = tree.getroot()\n",
      "        file_name = root_node.find('filename').text\n",
      "        img_id = get_imageId_from_fileName(file_name, id_iter)\n",
      "        id_iter += 1\n",
      "        image_path = os.path.join(image_dir, file_name)\n",
      "        print(image_path)\n",
      "        if not os.path.isfile(image_path):\n",
      "            print(f'Cannot find image {file_name} according to annotations.')\n",
      "            continue\n",
      "\n",
      "        labels = []\n",
      "        for obj in root_node.iter('object'):\n",
      "            cls_name = obj.find('name').text\n",
      "            if cls_name not in cls_map:\n",
      "                print(f'Label \"{cls_name}\" not in \"{config.classes}\"')\n",
      "                continue\n",
      "            bnd_box = obj.find('bndbox')\n",
      "            x_min = int(bnd_box.find('xmin').text) - 1\n",
      "            y_min = int(bnd_box.find('ymin').text) - 1\n",
      "            x_max = int(bnd_box.find('xmax').text) - 1\n",
      "            y_max = int(bnd_box.find('ymax').text) - 1\n",
      "            labels.append([y_min, x_min, y_max, x_max, cls_map[cls_name]])\n",
      "\n",
      "            if not is_training:\n",
      "                o_width = abs(x_max - x_min)\n",
      "                o_height = abs(y_max - y_min)\n",
      "                ann = {'area': o_width * o_height, 'iscrowd': 0, 'image_id': \\\n",
      "                    img_id, 'bbox': [x_min, y_min, o_width, o_height], \\\n",
      "                       'category_id': cls_map[cls_name], 'id': bnd_id, \\\n",
      "                       'ignore': 0, \\\n",
      "                       'segmentation': []}\n",
      "                json_dict['annotations'].append(ann)\n",
      "                bnd_id = bnd_id + 1\n",
      "\n",
      "        if labels:\n",
      "            images.append(img_id)\n",
      "            image_files_dict[img_id] = image_path\n",
      "            image_anno_dict[img_id] = np.array(labels)\n",
      "\n",
      "        if not is_training:\n",
      "            size = root_node.find(\"size\")\n",
      "            width = int(size.find('width').text)\n",
      "            height = int(size.find('height').text)\n",
      "            image = {'file_name': file_name, 'height': height, 'width': width,\n",
      "                     'id': img_id}\n",
      "            json_dict['images'].append(image)\n",
      "\n",
      "    if not is_training:\n",
      "        for cls_name, cid in cls_map.items():\n",
      "            cat = {'supercategory': 'none', 'id': cid, 'name': cls_name}\n",
      "            json_dict['categories'].append(cat)\n",
      "        json_fp = open(json_file, 'w')\n",
      "        json_str = json.dumps(json_dict)\n",
      "        json_fp.write(json_str)\n",
      "        json_fp.close()\n",
      "\n",
      "    return images, image_files_dict, image_anno_dict\n",
      "\n",
      "\n",
      "def create_coco_label(is_training):\n",
      "    \"\"\"Get image path and annotation from COCO.\"\"\"\n",
      "    from pycocotools.coco import COCO\n",
      "\n",
      "    coco_root = config.coco_root\n",
      "    data_type = config.val_data_type\n",
      "    if is_training:\n",
      "        data_type = config.train_data_type\n",
      "\n",
      "    # Classes need to train or test.\n",
      "    train_cls = config.classes\n",
      "    train_cls_dict = {}\n",
      "    for i, cls in enumerate(train_cls):\n",
      "        train_cls_dict[cls] = i\n",
      "\n",
      "    anno_json = os.path.join(coco_root, config.instances_set.format(data_type))\n",
      "\n",
      "    coco = COCO(anno_json)\n",
      "    classs_dict = {}\n",
      "    cat_ids = coco.loadCats(coco.getCatIds())\n",
      "    for cat in cat_ids:\n",
      "        classs_dict[cat[\"id\"]] = cat[\"name\"]\n",
      "\n",
      "    image_ids = coco.getImgIds()\n",
      "    images = []\n",
      "    image_path_dict = {}\n",
      "    image_anno_dict = {}\n",
      "\n",
      "    for img_id in image_ids:\n",
      "        image_info = coco.loadImgs(img_id)\n",
      "        file_name = image_info[0][\"file_name\"]\n",
      "        anno_ids = coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
      "        anno = coco.loadAnns(anno_ids)\n",
      "        image_path = os.path.join(coco_root, data_type, file_name)\n",
      "        annos = []\n",
      "        iscrowd = False\n",
      "        for label in anno:\n",
      "            bbox = label[\"bbox\"]\n",
      "            class_name = classs_dict[label[\"category_id\"]]\n",
      "            iscrowd = iscrowd or label[\"iscrowd\"]\n",
      "            if class_name in train_cls:\n",
      "                x_min, x_max = bbox[0], bbox[0] + bbox[2]\n",
      "                y_min, y_max = bbox[1], bbox[1] + bbox[3]\n",
      "                annos.append(list(map(round, [y_min, x_min, y_max, x_max])) + [train_cls_dict[class_name]])\n",
      "\n",
      "        if not is_training and iscrowd:\n",
      "            continue\n",
      "        if len(annos) >= 1:\n",
      "            images.append(img_id)\n",
      "            image_path_dict[img_id] = image_path\n",
      "            image_anno_dict[img_id] = np.array(annos)\n",
      "\n",
      "    return images, image_path_dict, image_anno_dict\n",
      "\n",
      "\n",
      "def anno_parser(annos_str):\n",
      "    \"\"\"Parse annotation from string to list.\"\"\"\n",
      "    annos = []\n",
      "    for anno_str in annos_str:\n",
      "        anno = list(map(int, anno_str.strip().split(',')))\n",
      "        annos.append(anno)\n",
      "    return annos\n",
      "\n",
      "\n",
      "def filter_valid_data(image_dir, anno_path):\n",
      "    \"\"\"Filter valid image file, which both in image_dir and anno_path.\"\"\"\n",
      "    images = []\n",
      "    image_path_dict = {}\n",
      "    image_anno_dict = {}\n",
      "    if not os.path.isdir(image_dir):\n",
      "        raise RuntimeError(\"Path given is not valid.\")\n",
      "    if not os.path.isfile(anno_path):\n",
      "        raise RuntimeError(\"Annotation file is not valid.\")\n",
      "\n",
      "    with open(anno_path, \"rb\") as f:\n",
      "        lines = f.readlines()\n",
      "    for img_id, line in enumerate(lines):\n",
      "        line_str = line.decode(\"utf-8\").strip()\n",
      "        line_split = str(line_str).split(' ')\n",
      "        file_name = line_split[0]\n",
      "        image_path = os.path.join(image_dir, file_name)\n",
      "        if os.path.isfile(image_path):\n",
      "            images.append(img_id)\n",
      "            image_path_dict[img_id] = image_path\n",
      "            image_anno_dict[img_id] = anno_parser(line_split[1:])\n",
      "\n",
      "    return images, image_path_dict, image_anno_dict\n",
      "\n",
      "\n",
      "def voc_data_to_mindrecord(mindrecord_dir, is_training, prefix=\"ssd.mindrecord\", file_num=8):\n",
      "    \"\"\"Create MindRecord file by image_dir and anno_path.\"\"\"\n",
      "    mindrecord_path = os.path.join(mindrecord_dir, prefix)\n",
      "    writer = FileWriter(mindrecord_path, file_num)\n",
      "    images, image_path_dict, image_anno_dict = create_voc_label(is_training)\n",
      "\n",
      "    ssd_json = {\n",
      "        \"img_id\": {\"type\": \"int32\", \"shape\": [1]},\n",
      "        \"image\": {\"type\": \"bytes\"},\n",
      "        \"annotation\": {\"type\": \"int32\", \"shape\": [-1, 5]},\n",
      "    }\n",
      "    writer.add_schema(ssd_json, \"ssd_json\")\n",
      "\n",
      "    for img_id in images:\n",
      "        image_path = image_path_dict[img_id]\n",
      "        with open(image_path, 'rb') as f:\n",
      "            img = f.read()\n",
      "        annos = np.array(image_anno_dict[img_id], dtype=np.int32)\n",
      "        img_id = np.array([img_id], dtype=np.int32)\n",
      "        row = {\"img_id\": img_id, \"image\": img, \"annotation\": annos}\n",
      "        writer.write_raw_data([row])\n",
      "    writer.commit()\n",
      "\n",
      "\n",
      "def data_to_mindrecord_byte_image(dataset=\"coco\", is_training=True, prefix=\"ssd.mindrecord\", file_num=8):\n",
      "    \"\"\"Create MindRecord file.\"\"\"\n",
      "    mindrecord_dir = config.mindrecord_dir\n",
      "    mindrecord_path = os.path.join(mindrecord_dir, prefix)\n",
      "    writer = FileWriter(mindrecord_path, file_num)\n",
      "    if dataset == \"coco\":\n",
      "        images, image_path_dict, image_anno_dict = create_coco_label(is_training)\n",
      "    else:\n",
      "        images, image_path_dict, image_anno_dict = filter_valid_data(config.image_dir, config.anno_path)\n",
      "\n",
      "    ssd_json = {\n",
      "        \"img_id\": {\"type\": \"int32\", \"shape\": [1]},\n",
      "        \"image\": {\"type\": \"bytes\"},\n",
      "        \"annotation\": {\"type\": \"int32\", \"shape\": [-1, 5]},\n",
      "    }\n",
      "    writer.add_schema(ssd_json, \"ssd_json\")\n",
      "\n",
      "    for img_id in images:\n",
      "        image_path = image_path_dict[img_id]\n",
      "        with open(image_path, 'rb') as f:\n",
      "            img = f.read()\n",
      "        annos = np.array(image_anno_dict[img_id], dtype=np.int32)\n",
      "        img_id = np.array([img_id], dtype=np.int32)\n",
      "        row = {\"img_id\": img_id, \"image\": img, \"annotation\": annos}\n",
      "        writer.write_raw_data([row])\n",
      "    writer.commit()\n",
      "\n",
      "\n",
      "def create_ssd_dataset(mindrecord_file, batch_size=32, repeat_num=10, device_num=1, rank=0,\n",
      "                       is_training=True, num_parallel_workers=4, use_multiprocessing=True):\n",
      "    \"\"\"Create SSD dataset with MindDataset.\"\"\"\n",
      "    ds = de.MindDataset(mindrecord_file, columns_list=[\"img_id\", \"image\", \"annotation\"], num_shards=device_num,\n",
      "                        shard_id=rank, num_parallel_workers=num_parallel_workers, shuffle=is_training)\n",
      "    decode = C.Decode()\n",
      "    ds = ds.map(operations=decode, input_columns=[\"image\"])\n",
      "    change_swap_op = C.HWC2CHW()\n",
      "    normalize_op = C.Normalize(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n",
      "                               std=[0.229 * 255, 0.224 * 255, 0.225 * 255])\n",
      "    color_adjust_op = C.RandomColorAdjust(brightness=0.4, contrast=0.4, saturation=0.4)\n",
      "    compose_map_func = (lambda img_id, image, annotation: preprocess_fn(img_id, image, annotation, is_training))\n",
      "    if is_training:\n",
      "        output_columns = [\"image\", \"box\", \"label\", \"num_match\"]\n",
      "        trans = [color_adjust_op, normalize_op, change_swap_op]\n",
      "    else:\n",
      "        output_columns = [\"img_id\", \"image\", \"image_shape\"]\n",
      "        trans = [normalize_op, change_swap_op]\n",
      "    ds = ds.map(operations=compose_map_func, input_columns=[\"img_id\", \"image\", \"annotation\"],\n",
      "                output_columns=output_columns, column_order=output_columns,\n",
      "                python_multiprocessing=use_multiprocessing,\n",
      "                num_parallel_workers=num_parallel_workers)\n",
      "    ds = ds.map(operations=trans, input_columns=[\"image\"], python_multiprocessing=use_multiprocessing,\n",
      "                num_parallel_workers=num_parallel_workers)\n",
      "    ds = ds.batch(batch_size, drop_remainder=True)\n",
      "    ds = ds.repeat(repeat_num)\n",
      "    return ds\n",
      "\n",
      "\n",
      "def create_mindrecord(dataset=\"coco\", prefix=\"ssd.mindrecord\", is_training=True):\n",
      "    print(\"Start create dataset!\")\n",
      "\n",
      "    # It will generate mindrecord file in config.mindrecord_dir,\n",
      "    # and the file name is ssd.mindrecord0, 1, ... file_num.\n",
      "\n",
      "    mindrecord_dir = config.mindrecord_dir\n",
      "    mindrecord_file = os.path.join(mindrecord_dir, prefix + \"0\")\n",
      "    if not os.path.exists(mindrecord_file):\n",
      "        if not os.path.isdir(mindrecord_dir):\n",
      "            os.makedirs(mindrecord_dir)\n",
      "        if dataset == \"coco\":\n",
      "            if os.path.isdir(config.coco_root):\n",
      "                print(\"Create Mindrecord.\")\n",
      "                data_to_mindrecord_byte_image(\"coco\", is_training, prefix)\n",
      "                print(\"Create Mindrecord Done, at {}\".format(mindrecord_dir))\n",
      "            else:\n",
      "                print(\"coco_root not exits.\")\n",
      "        elif dataset == \"voc\":\n",
      "            if os.path.isdir(config.voc_root):\n",
      "                print(\"Create Mindrecord.\")\n",
      "                voc_data_to_mindrecord(mindrecord_dir, is_training, prefix)\n",
      "                print(\"Create Mindrecord Done, at {}\".format(mindrecord_dir))\n",
      "            else:\n",
      "                print(\"voc_root not exits.\")\n",
      "        else:\n",
      "            if os.path.isdir(config.image_dir) and os.path.exists(config.anno_path):\n",
      "                print(\"Create Mindrecord.\")\n",
      "                data_to_mindrecord_byte_image(\"other\", is_training, prefix)\n",
      "                print(\"Create Mindrecord Done, at {}\".format(mindrecord_dir))\n",
      "            else:\n",
      "                print(\"image_dir or anno_path not exits.\")\n",
      "    return mindrecord_file\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# PLEASE DO NOT EDIT THIS FILE, IT IS GENERATED AND WILL BE OVERWRITTEN:\n",
      "# https://github.com/ccxt/ccxt/blob/master/CONTRIBUTING.md#how-to-contribute-code\n",
      "\n",
      "from ccxt.base.exchange import Exchange\n",
      "import json\n",
      "from ccxt.base.errors import ExchangeError\n",
      "from ccxt.base.errors import AuthenticationError\n",
      "from ccxt.base.errors import PermissionDenied\n",
      "from ccxt.base.errors import AccountSuspended\n",
      "from ccxt.base.errors import ArgumentsRequired\n",
      "from ccxt.base.errors import BadRequest\n",
      "from ccxt.base.errors import BadSymbol\n",
      "from ccxt.base.errors import InsufficientFunds\n",
      "from ccxt.base.errors import InvalidOrder\n",
      "from ccxt.base.errors import OrderNotFound\n",
      "from ccxt.base.errors import OrderImmediatelyFillable\n",
      "from ccxt.base.errors import NotSupported\n",
      "from ccxt.base.errors import DDoSProtection\n",
      "from ccxt.base.errors import RateLimitExceeded\n",
      "from ccxt.base.errors import ExchangeNotAvailable\n",
      "from ccxt.base.errors import OnMaintenance\n",
      "from ccxt.base.errors import InvalidNonce\n",
      "from ccxt.base.decimal_to_precision import TRUNCATE\n",
      "from ccxt.base.precise import Precise\n",
      "\n",
      "\n",
      "class binance(Exchange):\n",
      "\n",
      "    def describe(self):\n",
      "        return self.deep_extend(super(binance, self).describe(), {\n",
      "            'id': 'binance',\n",
      "            'name': 'Binance',\n",
      "            'countries': ['JP', 'MT'],  # Japan, Malta\n",
      "            'rateLimit': 50,\n",
      "            'certified': True,\n",
      "            'pro': True,\n",
      "            # new metainfo interface\n",
      "            'has': {\n",
      "                'cancelAllOrders': True,\n",
      "                'cancelOrder': True,\n",
      "                'CORS': None,\n",
      "                'createOrder': True,\n",
      "                'fetchBalance': True,\n",
      "                'fetchBorrowRate': True,\n",
      "                'fetchBorrowRates': False,\n",
      "                'fetchBidsAsks': True,\n",
      "                'fetchClosedOrders': 'emulated',\n",
      "                'fetchCurrencies': True,\n",
      "                'fetchDepositAddress': True,\n",
      "                'fetchDeposits': True,\n",
      "                'fetchFundingFees': True,\n",
      "                'fetchFundingHistory': True,\n",
      "                'fetchFundingRate': True,\n",
      "                'fetchFundingRateHistory': True,\n",
      "                'fetchFundingRates': True,\n",
      "                'fetchIndexOHLCV': True,\n",
      "                'fetchIsolatedPositions': True,\n",
      "                'fetchMarkets': True,\n",
      "                'fetchMarkOHLCV': True,\n",
      "                'fetchMyTrades': True,\n",
      "                'fetchOHLCV': True,\n",
      "                'fetchOpenOrders': True,\n",
      "                'fetchOrder': True,\n",
      "                'fetchOrderBook': True,\n",
      "                'fetchOrders': True,\n",
      "                'fetchPositions': True,\n",
      "                'fetchPremiumIndexOHLCV': False,\n",
      "                'fetchStatus': True,\n",
      "                'fetchTicker': True,\n",
      "                'fetchTickers': True,\n",
      "                'fetchTime': True,\n",
      "                'fetchTrades': True,\n",
      "                'fetchTradingFee': True,\n",
      "                'fetchTradingFees': True,\n",
      "                'fetchTransactions': False,\n",
      "                'fetchTransfers': True,\n",
      "                'fetchWithdrawals': True,\n",
      "                'setLeverage': True,\n",
      "                'setMarginMode': True,\n",
      "                'setPositionMode': True,\n",
      "                'addMargin': True,\n",
      "                'reduceMargin': True,\n",
      "                'transfer': True,\n",
      "                'withdraw': True,\n",
      "            },\n",
      "            'timeframes': {\n",
      "                '1m': '1m',\n",
      "                '3m': '3m',\n",
      "                '5m': '5m',\n",
      "                '15m': '15m',\n",
      "                '30m': '30m',\n",
      "                '1h': '1h',\n",
      "                '2h': '2h',\n",
      "                '4h': '4h',\n",
      "                '6h': '6h',\n",
      "                '8h': '8h',\n",
      "                '12h': '12h',\n",
      "                '1d': '1d',\n",
      "                '3d': '3d',\n",
      "                '1w': '1w',\n",
      "                '1M': '1M',\n",
      "            },\n",
      "            'urls': {\n",
      "                'logo': 'https://user-images.githubusercontent.com/1294454/29604020-d5483cdc-87ee-11e7-94c7-d1a8d9169293.jpg',\n",
      "                'test': {\n",
      "                    'dapiPublic': 'https://testnet.binancefuture.com/dapi/v1',\n",
      "                    'dapiPrivate': 'https://testnet.binancefuture.com/dapi/v1',\n",
      "                    'fapiPublic': 'https://testnet.binancefuture.com/fapi/v1',\n",
      "                    'fapiPrivate': 'https://testnet.binancefuture.com/fapi/v1',\n",
      "                    'fapiPrivateV2': 'https://testnet.binancefuture.com/fapi/v2',\n",
      "                    'public': 'https://testnet.binance.vision/api/v3',\n",
      "                    'private': 'https://testnet.binance.vision/api/v3',\n",
      "                    'v1': 'https://testnet.binance.vision/api/v1',\n",
      "                },\n",
      "                'api': {\n",
      "                    'wapi': 'https://api.binance.com/wapi/v3',\n",
      "                    'sapi': 'https://api.binance.com/sapi/v1',\n",
      "                    'dapiPublic': 'https://dapi.binance.com/dapi/v1',\n",
      "                    'dapiPrivate': 'https://dapi.binance.com/dapi/v1',\n",
      "                    'dapiPrivateV2': 'https://dapi.binance.com/dapi/v2',\n",
      "                    'dapiData': 'https://dapi.binance.com/futures/data',\n",
      "                    'fapiPublic': 'https://fapi.binance.com/fapi/v1',\n",
      "                    'fapiPrivate': 'https://fapi.binance.com/fapi/v1',\n",
      "                    'fapiData': 'https://fapi.binance.com/futures/data',\n",
      "                    'fapiPrivateV2': 'https://fapi.binance.com/fapi/v2',\n",
      "                    'public': 'https://api.binance.com/api/v3',\n",
      "                    'private': 'https://api.binance.com/api/v3',\n",
      "                    'v1': 'https://api.binance.com/api/v1',\n",
      "                },\n",
      "                'www': 'https://www.binance.com',\n",
      "                # 'referral': {\n",
      "                #     'url': 'https://www.binance.com/en/register?ref=BLEJC98C',\n",
      "                #     'discount': 0.2,\n",
      "                # },\n",
      "                'doc': [\n",
      "                    'https://binance-docs.github.io/apidocs/spot/en',\n",
      "                ],\n",
      "                'api_management': 'https://www.binance.com/en/usercenter/settings/api-management',\n",
      "                'fees': 'https://www.binance.com/en/fee/schedule',\n",
      "            },\n",
      "            'depth': 1,\n",
      "            'api': {\n",
      "                # the API structure below will need 3-layer apidefs\n",
      "                'sapi': {\n",
      "                    'get': {\n",
      "                        'accountSnapshot': 1,\n",
      "                        'system/status': 1,\n",
      "                        # these endpoints require self.apiKey\n",
      "                        'margin/asset': 1,\n",
      "                        'margin/pair': 1,\n",
      "                        'margin/allAssets': 1,\n",
      "                        'margin/allPairs': 1,\n",
      "                        'margin/priceIndex': 1,\n",
      "                        # these endpoints require self.apiKey + self.secret\n",
      "                        'asset/assetDividend': 1,\n",
      "                        'asset/dribblet': 1,\n",
      "                        'asset/transfer': 1,\n",
      "                        'asset/assetDetail': 1,\n",
      "                        'asset/tradeFee': 1,\n",
      "                        'asset/get-funding-asset': 1,\n",
      "                        'margin/loan': 1,\n",
      "                        'margin/repay': 1,\n",
      "                        'margin/account': 1,\n",
      "                        'margin/transfer': 1,\n",
      "                        'margin/interestHistory': 1,\n",
      "                        'margin/forceLiquidationRec': 1,\n",
      "                        'margin/order': 1,\n",
      "                        'margin/openOrders': 1,\n",
      "                        'margin/allOrders': 1,\n",
      "                        'margin/myTrades': 1,\n",
      "                        'margin/maxBorrowable': 5,\n",
      "                        'margin/maxTransferable': 5,\n",
      "                        'margin/isolated/transfer': 1,\n",
      "                        'margin/isolated/account': 1,\n",
      "                        'margin/isolated/pair': 1,\n",
      "                        'margin/isolated/allPairs': 1,\n",
      "                        'margin/isolated/accountLimit': 1,\n",
      "                        'margin/interestRateHistory': 1,\n",
      "                        'margin/orderList': 2,\n",
      "                        'margin/allOrderList': 10,\n",
      "                        'margin/openOrderList': 3,\n",
      "                        'loan/income': 1,\n",
      "                        'fiat/orders': 1,\n",
      "                        'fiat/payments': 1,\n",
      "                        'futures/transfer': 5,\n",
      "                        'futures/loan/borrow/history': 1,\n",
      "                        'futures/loan/repay/history': 1,\n",
      "                        'futures/loan/wallet': 1,\n",
      "                        'futures/loan/configs': 1,\n",
      "                        'futures/loan/calcAdjustLevel': 1,\n",
      "                        'futures/loan/calcMaxAdjustAmount': 1,\n",
      "                        'futures/loan/adjustCollateral/history': 1,\n",
      "                        'futures/loan/liquidationHistory': 1,\n",
      "                        # https://binance-docs.github.io/apidocs/spot/en/#withdraw-sapi\n",
      "                        'capital/config/getall': 1,  # get networks for withdrawing USDT ERC20 vs USDT Omni\n",
      "                        'capital/deposit/address': 1,\n",
      "                        'capital/deposit/hisrec': 1,\n",
      "                        'capital/deposit/subAddress': 1,\n",
      "                        'capital/deposit/subHisrec': 1,\n",
      "                        'capital/withdraw/history': 1,\n",
      "                        'account/status': 1,\n",
      "                        'account/apiTradingStatus': 1,\n",
      "                        'account/apiRestrictions/ipRestriction': 1,\n",
      "                        'bnbBurn': 1,\n",
      "                        'sub-account/assets': 1,\n",
      "                        'sub-account/futures/account': 1,\n",
      "                        'sub-account/futures/accountSummary': 1,\n",
      "                        'sub-account/futures/positionRisk': 1,\n",
      "                        'sub-account/futures/internalTransfer': 1,\n",
      "                        'sub-account/list': 1,\n",
      "                        'sub-account/margin/account': 1,\n",
      "                        'sub-account/margin/accountSummary': 1,\n",
      "                        'sub-account/spotSummary': 5,\n",
      "                        'sub-account/status': 1,\n",
      "                        'sub-account/sub/transfer/history': 1,\n",
      "                        'sub-account/transfer/subUserHistory': 1,\n",
      "                        'sub-account/universalTransfer': 1,\n",
      "                        # lending endpoints\n",
      "                        'lending/daily/product/list': 1,\n",
      "                        'lending/daily/userLeftQuota': 1,\n",
      "                        'lending/daily/userRedemptionQuota': 1,\n",
      "                        'lending/daily/token/position': 1,\n",
      "                        'lending/union/account': 1,\n",
      "                        'lending/union/purchaseRecord': 1,\n",
      "                        'lending/union/redemptionRecord': 1,\n",
      "                        'lending/union/interestHistory': 1,\n",
      "                        'lending/project/list': 1,\n",
      "                        'lending/project/position/list': 1,\n",
      "                        # mining endpoints\n",
      "                        'mining/pub/algoList': 1,\n",
      "                        'mining/pub/coinList': 1,\n",
      "                        'mining/worker/detail': 5,\n",
      "                        'mining/worker/list': 5,\n",
      "                        'mining/payment/list': 5,\n",
      "                        'mining/statistics/user/status': 5,\n",
      "                        'mining/statistics/user/list': 5,\n",
      "                        # liquid swap endpoints\n",
      "                        'bswap/pools': 1,\n",
      "                        'bswap/liquidity': {'cost': 1, 'noPoolId': 10},\n",
      "                        'bswap/liquidityOps': 2,\n",
      "                        'bswap/quote': 2,\n",
      "                        'bswap/swap': 1,\n",
      "                        'bswap/poolConfigure': 1,\n",
      "                        'bswap/addLiquidityPreview': 1,\n",
      "                        'bswap/removeLiquidityPreview': 1,\n",
      "                        # leveraged token endpoints\n",
      "                        'blvt/tokenInfo': 1,\n",
      "                        'blvt/subscribe/record': 1,\n",
      "                        'blvt/redeem/record': 1,\n",
      "                        'blvt/userLimit': 1,\n",
      "                        # broker api\n",
      "                        'apiReferral/ifNewUser': 1,\n",
      "                        'apiReferral/customization': 1,\n",
      "                        'apiReferral/userCustomization': 1,\n",
      "                        'apiReferral/rebate/recentRecord': 1,\n",
      "                        'apiReferral/rebate/historicalRecord': 1,\n",
      "                        'apiReferral/kickback/recentRecord': 1,\n",
      "                        'apiReferral/kickback/historicalRecord': 1,\n",
      "                        # brokerage API\n",
      "                        'broker/subAccountApi': 1,\n",
      "                        'broker/subAccount': 1,\n",
      "                        'broker/subAccountApi/commission/futures': 1,\n",
      "                        'broker/subAccountApi/commission/coinFutures': 1,\n",
      "                        'broker/info': 1,\n",
      "                        'broker/transfer': 1,\n",
      "                        'broker/transfer/futures': 1,\n",
      "                        'broker/rebate/recentRecord': 1,\n",
      "                        'broker/rebate/historicalRecord': 1,\n",
      "                        'broker/subAccount/bnbBurn/status': 1,\n",
      "                        'broker/subAccount/depositHist': 1,\n",
      "                        'broker/subAccount/spotSummary': 1,\n",
      "                        'broker/subAccount/marginSummary': 1,\n",
      "                        'broker/subAccount/futuresSummary': 1,\n",
      "                        'broker/rebate/futures/recentRecord': 1,\n",
      "                        'broker/subAccountApi/ipRestriction': 1,\n",
      "                        'broker/universalTransfer': 1,\n",
      "                        # v2 not supported yet\n",
      "                        # GET /sapi/v2/broker/subAccount/futuresSummary\n",
      "                        'account/apiRestrictions': 1,\n",
      "                        # subaccounts\n",
      "                        'managed-subaccount/asset': 1,\n",
      "                        # c2c / p2p\n",
      "                        'c2c/orderMatch/listUserOrderHistory': 1,\n",
      "                    },\n",
      "                    'post': {\n",
      "                        'asset/dust': 1,\n",
      "                        'asset/transfer': 1,\n",
      "                        'asset/get-funding-asset': 1,\n",
      "                        'account/disableFastWithdrawSwitch': 1,\n",
      "                        'account/enableFastWithdrawSwitch': 1,\n",
      "                        'account/apiRestrictions/ipRestriction': 1,\n",
      "                        'account/apiRestrictions/ipRestriction/ipList': 1,\n",
      "                        'capital/withdraw/apply': 1,\n",
      "                        'margin/transfer': 1,\n",
      "                        'margin/loan': 1,\n",
      "                        'margin/repay': 1,\n",
      "                        'margin/order': 4,\n",
      "                        'margin/order/oco': 1,\n",
      "                        'margin/isolated/create': 1,\n",
      "                        'margin/isolated/transfer': 1,\n",
      "                        'margin/isolated/account': 1,\n",
      "                        'bnbBurn': 1,\n",
      "                        'sub-account/margin/transfer': 1,\n",
      "                        'sub-account/margin/enable': 1,\n",
      "                        # 'sub-account/margin/enable': 1,\n",
      "                        'sub-account/futures/enable': 1,\n",
      "                        'sub-account/futures/transfer': 1,\n",
      "                        'sub-account/futures/internalTransfer': 1,\n",
      "                        'sub-account/transfer/subToSub': 1,\n",
      "                        'sub-account/transfer/subToMaster': 1,\n",
      "                        'sub-account/universalTransfer': 1,\n",
      "                        'managed-subaccount/deposit': 1,\n",
      "                        'managed-subaccount/withdraw': 1,\n",
      "                        'userDataStream': 1,\n",
      "                        'userDataStream/isolated': 1,\n",
      "                        'futures/transfer': 1,\n",
      "                        'futures/loan/borrow': 20,\n",
      "                        'futures/loan/repay': 20,\n",
      "                        'futures/loan/adjustCollateral': 20,\n",
      "                        # lending\n",
      "                        'lending/customizedFixed/purchase': 1,\n",
      "                        'lending/daily/purchase': 1,\n",
      "                        'lending/daily/redeem': 1,\n",
      "                        # liquid swap endpoints\n",
      "                        'bswap/liquidityAdd': 2,\n",
      "                        'bswap/liquidityRemove': 2,\n",
      "                        'bswap/swap': 2,\n",
      "                        # leveraged token endpoints\n",
      "                        'blvt/subscribe': 1,\n",
      "                        'blvt/redeem': 1,\n",
      "                        # brokerage API\n",
      "                        'apiReferral/customization': 1,\n",
      "                        'apiReferral/userCustomization': 1,\n",
      "                        'apiReferral/rebate/historicalRecord': 1,\n",
      "                        'apiReferral/kickback/historicalRecord': 1,\n",
      "                        'broker/subAccount': 1,\n",
      "                        'broker/subAccount/margin': 1,\n",
      "                        'broker/subAccount/futures': 1,\n",
      "                        'broker/subAccountApi': 1,\n",
      "                        'broker/subAccountApi/permission': 1,\n",
      "                        'broker/subAccountApi/commission': 1,\n",
      "                        'broker/subAccountApi/commission/futures': 1,\n",
      "                        'broker/subAccountApi/commission/coinFutures': 1,\n",
      "                        'broker/transfer': 1,\n",
      "                        'broker/transfer/futures': 1,\n",
      "                        'broker/rebate/historicalRecord': 1,\n",
      "                        'broker/subAccount/bnbBurn/spot': 1,\n",
      "                        'broker/subAccount/bnbBurn/marginInterest': 1,\n",
      "                        'broker/subAccount/blvt': 1,\n",
      "                        'broker/subAccountApi/ipRestriction': 1,\n",
      "                        'broker/subAccountApi/ipRestriction/ipList': 1,\n",
      "                        'broker/universalTransfer': 1,\n",
      "                        'broker/subAccountApi/permission/universalTransfer': 1,\n",
      "                        'broker/subAccountApi/permission/vanillaOptions': 1,\n",
      "                    },\n",
      "                    'put': {\n",
      "                        'userDataStream': 1,\n",
      "                        'userDataStream/isolated': 1,\n",
      "                    },\n",
      "                    'delete': {\n",
      "                        'account/apiRestrictions/ipRestriction/ipList': 1,\n",
      "                        'margin/openOrders': 1,\n",
      "                        'margin/order': 1,\n",
      "                        'margin/orderList': 1,\n",
      "                        'margin/isolated/account': 1,\n",
      "                        'userDataStream': 1,\n",
      "                        'userDataStream/isolated': 1,\n",
      "                        # brokerage API\n",
      "                        'broker/subAccountApi': 1,\n",
      "                        'broker/subAccountApi/ipRestriction/ipList': 1,\n",
      "                    },\n",
      "                },\n",
      "                # deprecated\n",
      "                'wapi': {\n",
      "                    'post': {\n",
      "                        'withdraw': 1,\n",
      "                        'sub-account/transfer': 1,\n",
      "                    },\n",
      "                    'get': {\n",
      "                        'depositHistory': 1,\n",
      "                        'withdrawHistory': 1,\n",
      "                        'depositAddress': 1,\n",
      "                        'accountStatus': 1,\n",
      "                        'systemStatus': 1,\n",
      "                        'apiTradingStatus': 1,\n",
      "                        'userAssetDribbletLog': 1,\n",
      "                        'tradeFee': 1,\n",
      "                        'assetDetail': 1,\n",
      "                        'sub-account/list': 1,\n",
      "                        'sub-account/transfer/history': 1,\n",
      "                        'sub-account/assets': 1,\n",
      "                    },\n",
      "                },\n",
      "                'dapiPublic': {\n",
      "                    'get': {\n",
      "                        'ping': 1,\n",
      "                        'time': 1,\n",
      "                        'exchangeInfo': 1,\n",
      "                        'depth': {'cost': 2, 'byLimit': [[50, 2], [100, 5], [500, 10], [1000, 20]]},\n",
      "                        'trades': 1,\n",
      "                        'historicalTrades': 20,\n",
      "                        'aggTrades': 20,\n",
      "                        'premiumIndex': 10,\n",
      "                        'fundingRate': 1,\n",
      "                        'klines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'continuousKlines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'indexPriceKlines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'markPriceKlines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'ticker/24hr': {'cost': 1, 'noSymbol': 40},\n",
      "                        'ticker/price': {'cost': 1, 'noSymbol': 2},\n",
      "                        'ticker/bookTicker': {'cost': 1, 'noSymbol': 2},\n",
      "                        'openInterest': 1,\n",
      "                    },\n",
      "                },\n",
      "                'dapiData': {\n",
      "                    'get': {\n",
      "                        'openInterestHist': 1,\n",
      "                        'topLongShortAccountRatio': 1,\n",
      "                        'topLongShortPositionRatio': 1,\n",
      "                        'globalLongShortAccountRatio': 1,\n",
      "                        'takerBuySellVol': 1,\n",
      "                        'basis': 1,\n",
      "                    },\n",
      "                },\n",
      "                'dapiPrivate': {\n",
      "                    'get': {\n",
      "                        'positionSide/dual': 30,\n",
      "                        'order': 1,\n",
      "                        'openOrder': 1,\n",
      "                        'openOrders': {'cost': 1, 'noSymbol': 5},\n",
      "                        'allOrders': {'cost': 20, 'noSymbol': 40},\n",
      "                        'balance': 1,\n",
      "                        'account': 5,\n",
      "                        'positionMargin/history': 1,\n",
      "                        'positionRisk': 1,\n",
      "                        'userTrades': {'cost': 20, 'noSymbol': 40},\n",
      "                        'income': 20,\n",
      "                        'leverageBracket': 1,\n",
      "                        'forceOrders': {'cost': 20, 'noSymbol': 50},\n",
      "                        'adlQuantile': 5,\n",
      "                    },\n",
      "                    'post': {\n",
      "                        'positionSide/dual': 1,\n",
      "                        'order': 4,\n",
      "                        'batchOrders': 5,\n",
      "                        'countdownCancelAll': 10,\n",
      "                        'leverage': 1,\n",
      "                        'marginType': 1,\n",
      "                        'positionMargin': 1,\n",
      "                        'listenKey': 1,\n",
      "                    },\n",
      "                    'put': {\n",
      "                        'listenKey': 1,\n",
      "                    },\n",
      "                    'delete': {\n",
      "                        'order': 1,\n",
      "                        'allOpenOrders': 1,\n",
      "                        'batchOrders': 5,\n",
      "                        'listenKey': 1,\n",
      "                    },\n",
      "                },\n",
      "                'dapiPrivateV2': {\n",
      "                    'get': {\n",
      "                        'leverageBracket': 1,\n",
      "                    },\n",
      "                },\n",
      "                'fapiPublic': {\n",
      "                    'get': {\n",
      "                        'ping': 1,\n",
      "                        'time': 1,\n",
      "                        'exchangeInfo': 1,\n",
      "                        'depth': {'cost': 2, 'byLimit': [[50, 2], [100, 5], [500, 10], [1000, 20]]},\n",
      "                        'trades': 1,\n",
      "                        'historicalTrades': 20,\n",
      "                        'aggTrades': 20,\n",
      "                        'klines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'continuousKlines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'markPriceKlines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'indexPriceKlines': {'cost': 1, 'byLimit': [[99, 1], [499, 2], [1000, 5], [10000, 10]]},\n",
      "                        'fundingRate': 1,\n",
      "                        'premiumIndex': 1,\n",
      "                        'ticker/24hr': {'cost': 1, 'noSymbol': 40},\n",
      "                        'ticker/price': {'cost': 1, 'noSymbol': 2},\n",
      "                        'ticker/bookTicker': {'cost': 1, 'noSymbol': 2},\n",
      "                        'openInterest': 1,\n",
      "                        'indexInfo': 1,\n",
      "                        'apiTradingStatus': {'cost': 1, 'noSymbol': 10},\n",
      "                        'lvtKlines': 1,\n",
      "                    },\n",
      "                },\n",
      "                'fapiData': {\n",
      "                    'get': {\n",
      "                        'openInterestHist': 1,\n",
      "                        'topLongShortAccountRatio': 1,\n",
      "                        'topLongShortPositionRatio': 1,\n",
      "                        'globalLongShortAccountRatio': 1,\n",
      "                        'takerlongshortRatio': 1,\n",
      "                    },\n",
      "                },\n",
      "                'fapiPrivate': {\n",
      "                    'get': {\n",
      "                        'forceOrders': {'cost': 20, 'noSymbol': 50},\n",
      "                        'allOrders': 5,\n",
      "                        'openOrder': 1,\n",
      "                        'openOrders': 1,\n",
      "                        'order': 1,\n",
      "                        'account': 5,\n",
      "                        'balance': 5,\n",
      "                        'leverageBracket': 1,\n",
      "                        'positionMargin/history': 1,\n",
      "                        'positionRisk': 5,\n",
      "                        'positionSide/dual': 30,\n",
      "                        'userTrades': 5,\n",
      "                        'income': 30,\n",
      "                        'commissionRate': 20,\n",
      "                        'apiTradingStatus': 1,\n",
      "                        'multiAssetsMargin': 30,\n",
      "                        # broker endpoints\n",
      "                        'apiReferral/ifNewUser': 1,\n",
      "                        'apiReferral/customization': 1,\n",
      "                        'apiReferral/userCustomization': 1,\n",
      "                        'apiReferral/traderNum': 1,\n",
      "                        'apiReferral/overview': 1,\n",
      "                        'apiReferral/tradeVol': 1,\n",
      "                        'apiReferral/rebateVol': 1,\n",
      "                        'apiReferral/traderSummary': 1,\n",
      "                        'adlQuantile': 5,\n",
      "                    },\n",
      "                    'post': {\n",
      "                        'batchOrders': 5,\n",
      "                        'positionSide/dual': 1,\n",
      "                        'positionMargin': 1,\n",
      "                        'marginType': 1,\n",
      "                        'order': 4,\n",
      "                        'leverage': 1,\n",
      "                        'listenKey': 1,\n",
      "                        'countdownCancelAll': 10,\n",
      "                        'multiAssetsMargin': 1,\n",
      "                        # broker endpoints\n",
      "                        'apiReferral/customization': 1,\n",
      "                        'apiReferral/userCustomization': 1,\n",
      "                    },\n",
      "                    'put': {\n",
      "                        'listenKey': 1,\n",
      "                    },\n",
      "                    'delete': {\n",
      "                        'batchOrders': 1,\n",
      "                        'order': 1,\n",
      "                        'allOpenOrders': 1,\n",
      "                        'listenKey': 1,\n",
      "                    },\n",
      "                },\n",
      "                'fapiPrivateV2': {\n",
      "                    'get': {\n",
      "                        'account': 1,\n",
      "                        'balance': 1,\n",
      "                        'positionRisk': 1,\n",
      "                    },\n",
      "                },\n",
      "                'public': {\n",
      "                    'get': {\n",
      "                        'ping': 1,\n",
      "                        'time': 1,\n",
      "                        'depth': {'cost': 1, 'byLimit': [[100, 1], [500, 5], [1000, 10], [5000, 50]]},\n",
      "                        'trades': 1,\n",
      "                        'aggTrades': 1,\n",
      "                        'historicalTrades': 5,\n",
      "                        'klines': 1,\n",
      "                        'ticker/24hr': {'cost': 1, 'noSymbol': 40},\n",
      "                        'ticker/price': {'cost': 1, 'noSymbol': 2},\n",
      "                        'ticker/bookTicker': {'cost': 1, 'noSymbol': 2},\n",
      "                        'exchangeInfo': 10,\n",
      "                    },\n",
      "                    'put': {\n",
      "                        'userDataStream': 1,\n",
      "                    },\n",
      "                    'post': {\n",
      "                        'userDataStream': 1,\n",
      "                    },\n",
      "                    'delete': {\n",
      "                        'userDataStream': 1,\n",
      "                    },\n",
      "                },\n",
      "                'private': {\n",
      "                    'get': {\n",
      "                        'allOrderList': 10,  # oco\n",
      "                        'openOrderList': 3,  # oco\n",
      "                        'orderList': 2,  # oco\n",
      "                        'order': 2,\n",
      "                        'openOrders': {'cost': 3, 'noSymbol': 40},\n",
      "                        'allOrders': 10,\n",
      "                        'account': 10,\n",
      "                        'myTrades': 10,\n",
      "                        'rateLimit/order': 20,\n",
      "                    },\n",
      "                    'post': {\n",
      "                        'order/oco': 1,\n",
      "                        'order': 4,\n",
      "                        'order/test': 1,\n",
      "                    },\n",
      "                    'delete': {\n",
      "                        'openOrders': 1,  # added on 2020-04-25 for canceling all open orders per symbol\n",
      "                        'orderList': 1,  # oco\n",
      "                        'order': 1,\n",
      "                    },\n",
      "                },\n",
      "            },\n",
      "            'fees': {\n",
      "                'trading': {\n",
      "                    'feeSide': 'get',\n",
      "                    'tierBased': False,\n",
      "                    'percentage': True,\n",
      "                    'taker': self.parse_number('0.001'),\n",
      "                    'maker': self.parse_number('0.001'),\n",
      "                },\n",
      "                'future': {\n",
      "                    'trading': {\n",
      "                        'feeSide': 'quote',\n",
      "                        'tierBased': True,\n",
      "                        'percentage': True,\n",
      "                        'taker': self.parse_number('0.000400'),\n",
      "                        'maker': self.parse_number('0.000200'),\n",
      "                        'tiers': {\n",
      "                            'taker': [\n",
      "                                [self.parse_number('0'), self.parse_number('0.000400')],\n",
      "                                [self.parse_number('250'), self.parse_number('0.000400')],\n",
      "                                [self.parse_number('2500'), self.parse_number('0.000350')],\n",
      "                                [self.parse_number('7500'), self.parse_number('0.000320')],\n",
      "                                [self.parse_number('22500'), self.parse_number('0.000300')],\n",
      "                                [self.parse_number('50000'), self.parse_number('0.000270')],\n",
      "                                [self.parse_number('100000'), self.parse_number('0.000250')],\n",
      "                                [self.parse_number('200000'), self.parse_number('0.000220')],\n",
      "                                [self.parse_number('400000'), self.parse_number('0.000200')],\n",
      "                                [self.parse_number('750000'), self.parse_number('0.000170')],\n",
      "                            ],\n",
      "                            'maker': [\n",
      "                                [self.parse_number('0'), self.parse_number('0.000200')],\n",
      "                                [self.parse_number('250'), self.parse_number('0.000160')],\n",
      "                                [self.parse_number('2500'), self.parse_number('0.000140')],\n",
      "                                [self.parse_number('7500'), self.parse_number('0.000120')],\n",
      "                                [self.parse_number('22500'), self.parse_number('0.000100')],\n",
      "                                [self.parse_number('50000'), self.parse_number('0.000080')],\n",
      "                                [self.parse_number('100000'), self.parse_number('0.000060')],\n",
      "                                [self.parse_number('200000'), self.parse_number('0.000040')],\n",
      "                                [self.parse_number('400000'), self.parse_number('0.000020')],\n",
      "                                [self.parse_number('750000'), self.parse_number('0')],\n",
      "                            ],\n",
      "                        },\n",
      "                    },\n",
      "                },\n",
      "                'delivery': {\n",
      "                    'trading': {\n",
      "                        'feeSide': 'base',\n",
      "                        'tierBased': True,\n",
      "                        'percentage': True,\n",
      "                        'taker': self.parse_number('0.000500'),\n",
      "                        'maker': self.parse_number('0.000100'),\n",
      "                        'tiers': {\n",
      "                            'taker': [\n",
      "                                [self.parse_number('0'), self.parse_number('0.000500')],\n",
      "                                [self.parse_number('250'), self.parse_number('0.000450')],\n",
      "                                [self.parse_number('2500'), self.parse_number('0.000400')],\n",
      "                                [self.parse_number('7500'), self.parse_number('0.000300')],\n",
      "                                [self.parse_number('22500'), self.parse_number('0.000250')],\n",
      "                                [self.parse_number('50000'), self.parse_number('0.000240')],\n",
      "                                [self.parse_number('100000'), self.parse_number('0.000240')],\n",
      "                                [self.parse_number('200000'), self.parse_number('0.000240')],\n",
      "                                [self.parse_number('400000'), self.parse_number('0.000240')],\n",
      "                                [self.parse_number('750000'), self.parse_number('0.000240')],\n",
      "                            ],\n",
      "                            'maker': [\n",
      "                                [self.parse_number('0'), self.parse_number('0.000100')],\n",
      "                                [self.parse_number('250'), self.parse_number('0.000080')],\n",
      "                                [self.parse_number('2500'), self.parse_number('0.000050')],\n",
      "                                [self.parse_number('7500'), self.parse_number('0.0000030')],\n",
      "                                [self.parse_number('22500'), self.parse_number('0')],\n",
      "                                [self.parse_number('50000'), self.parse_number('-0.000050')],\n",
      "                                [self.parse_number('100000'), self.parse_number('-0.000060')],\n",
      "                                [self.parse_number('200000'), self.parse_number('-0.000070')],\n",
      "                                [self.parse_number('400000'), self.parse_number('-0.000080')],\n",
      "                                [self.parse_number('750000'), self.parse_number('-0.000090')],\n",
      "                            ],\n",
      "                        },\n",
      "                    },\n",
      "                },\n",
      "            },\n",
      "            'commonCurrencies': {\n",
      "                'BCC': 'BCC',  # kept for backward-compatibility https://github.com/ccxt/ccxt/issues/4848\n",
      "                'YOYO': 'YOYOW',\n",
      "            },\n",
      "            # exchange-specific options\n",
      "            'options': {\n",
      "                'fetchCurrencies': True,  # self is a private call and it requires API keys\n",
      "                # 'fetchTradesMethod': 'publicGetAggTrades',  # publicGetTrades, publicGetHistoricalTrades\n",
      "                'defaultTimeInForce': 'GTC',  # 'GTC' = Good To Cancel(default), 'IOC' = Immediate Or Cancel\n",
      "                'defaultType': 'spot',  # 'spot', 'future', 'margin', 'delivery'\n",
      "                'hasAlreadyAuthenticatedSuccessfully': False,\n",
      "                'warnOnFetchOpenOrdersWithoutSymbol': True,\n",
      "                'fetchPositions': 'positionRisk',  # or 'account'\n",
      "                'recvWindow': 5 * 1000,  # 5 sec, binance default\n",
      "                'timeDifference': 0,  # the difference between system clock and Binance clock\n",
      "                'adjustForTimeDifference': False,  # controls the adjustment logic upon instantiation\n",
      "                'newOrderRespType': {\n",
      "                    'market': 'FULL',  # 'ACK' for order id, 'RESULT' for full order or 'FULL' for order with fills\n",
      "                    'limit': 'FULL',  # we change it from 'ACK' by default to 'FULL'(returns immediately if limit is not hit)\n",
      "                },\n",
      "                'quoteOrderQty': True,  # whether market orders support amounts in quote currency\n",
      "                'broker': {\n",
      "                    'spot': 'x-R4BD3S82',\n",
      "                    'margin': 'x-R4BD3S82',\n",
      "                    'future': 'x-xcKtGhcu',\n",
      "                    'delivery': 'x-xcKtGhcu',\n",
      "                },\n",
      "                'accountsByType': {\n",
      "                    'main': 'MAIN',\n",
      "                    'spot': 'MAIN',\n",
      "                    'funding': 'FUNDING',\n",
      "                    'margin': 'MARGIN',\n",
      "                    'future': 'UMFUTURE',\n",
      "                    'delivery': 'CMFUTURE',\n",
      "                    'mining': 'MINING',\n",
      "                },\n",
      "                'typesByAccount': {\n",
      "                    'MAIN': 'spot',\n",
      "                    'FUNDING': 'funding',\n",
      "                    'MARGIN': 'margin',\n",
      "                    'UMFUTURE': 'future',\n",
      "                    'CMFUTURE': 'delivery',\n",
      "                    'MINING': 'mining',\n",
      "                },\n",
      "                'networks': {\n",
      "                    'ERC20': 'ETH',\n",
      "                    'TRC20': 'TRX',\n",
      "                    'BEP2': 'BNB',\n",
      "                    'BEP20': 'BSC',\n",
      "                    'OMNI': 'OMNI',\n",
      "                    'EOS': 'EOS',\n",
      "                    'SPL': 'SOL',\n",
      "                },\n",
      "                'reverseNetworks': {\n",
      "                    'tronscan.org': 'TRC20',\n",
      "                    'etherscan.io': 'ERC20',\n",
      "                    'bscscan.com': 'BSC',\n",
      "                    'explorer.binance.org': 'BEP2',\n",
      "                    'bithomp.com': 'XRP',\n",
      "                    'bloks.io': 'EOS',\n",
      "                    'stellar.expert': 'XLM',\n",
      "                    'blockchair.com/bitcoin': 'BTC',\n",
      "                    'blockchair.com/bitcoin-cash': 'BCH',\n",
      "                    'blockchair.com/ecash': 'XEC',\n",
      "                    'explorer.litecoin.net': 'LTC',\n",
      "                    'explorer.avax.network': 'AVAX',\n",
      "                    'solscan.io': 'SOL',\n",
      "                    'polkadot.subscan.io': 'DOT',\n",
      "                    'dashboard.internetcomputer.org': 'ICP',\n",
      "                    'explorer.chiliz.com': 'CHZ',\n",
      "                    'cardanoscan.io': 'ADA',\n",
      "                    'mainnet.theoan.com': 'AION',\n",
      "                    'algoexplorer.io': 'ALGO',\n",
      "                    'explorer.ambrosus.com': 'AMB',\n",
      "                    'viewblock.io/zilliqa': 'ZIL',\n",
      "                    'viewblock.io/arweave': 'AR',\n",
      "                    'explorer.ark.io': 'ARK',\n",
      "                    'atomscan.com': 'ATOM',\n",
      "                    'www.mintscan.io': 'CTK',\n",
      "                    'explorer.bitcoindiamond.org': 'BCD',\n",
      "                    'btgexplorer.com': 'BTG',\n",
      "                    'bts.ai': 'BTS',\n",
      "                    'explorer.celo.org': 'CELO',\n",
      "                    'explorer.nervos.org': 'CKB',\n",
      "                    'cerebro.cortexlabs.ai': 'CTXC',\n",
      "                    'chainz.cryptoid.info': 'VIA',\n",
      "                    'explorer.dcrdata.org': 'DCR',\n",
      "                    'digiexplorer.info': 'DGB',\n",
      "                    'dock.subscan.io': 'DOCK',\n",
      "                    'dogechain.info': 'DOGE',\n",
      "                    'explorer.elrond.com': 'EGLD',\n",
      "                    'blockscout.com': 'ETC',\n",
      "                    'explore-fetchhub.fetch.ai': 'FET',\n",
      "                    'filfox.info': 'FIL',\n",
      "                    'fio.bloks.io': 'FIO',\n",
      "                    'explorer.firo.org': 'FIRO',\n",
      "                    'neoscan.io': 'NEO',\n",
      "                    'ftmscan.com': 'FTM',\n",
      "                    'explorer.gochain.io': 'GO',\n",
      "                    'block.gxb.io': 'GXS',\n",
      "                    'hash-hash.info': 'HBAR',\n",
      "                    'www.hiveblockexplorer.com': 'HIVE',\n",
      "                    'explorer.helium.com': 'HNT',\n",
      "                    'tracker.icon.foundation': 'ICX',\n",
      "                    'www.iostabc.com': 'IOST',\n",
      "                    'explorer.iota.org': 'IOTA',\n",
      "                    'iotexscan.io': 'IOTX',\n",
      "                    'irishub.iobscan.io': 'IRIS',\n",
      "                    'kava.mintscan.io': 'KAVA',\n",
      "                    'scope.klaytn.com': 'KLAY',\n",
      "                    'kmdexplorer.io': 'KMD',\n",
      "                    'kusama.subscan.io': 'KSM',\n",
      "                    'explorer.lto.network': 'LTO',\n",
      "                    'polygonscan.com': 'POLYGON',\n",
      "                    'explorer.ont.io': 'ONT',\n",
      "                    'minaexplorer.com': 'MINA',\n",
      "                    'nanolooker.com': 'NANO',\n",
      "                    'explorer.nebulas.io': 'NAS',\n",
      "                    'explorer.nbs.plus': 'NBS',\n",
      "                    'explorer.nebl.io': 'NEBL',\n",
      "                    'nulscan.io': 'NULS',\n",
      "                    'nxscan.com': 'NXS',\n",
      "                    'explorer.harmony.one': 'ONE',\n",
      "                    'explorer.poa.network': 'POA',\n",
      "                    'qtum.info': 'QTUM',\n",
      "                    'explorer.rsk.co': 'RSK',\n",
      "                    'www.oasisscan.com': 'ROSE',\n",
      "                    'ravencoin.network': 'RVN',\n",
      "                    'sc.tokenview.com': 'SC',\n",
      "                    'secretnodes.com': 'SCRT',\n",
      "                    'explorer.skycoin.com': 'SKY',\n",
      "                    'steemscan.com': 'STEEM',\n",
      "                    'explorer.stacks.co': 'STX',\n",
      "                    'www.thetascan.io': 'THETA',\n",
      "                    'scan.tomochain.com': 'TOMO',\n",
      "                    'explore.vechain.org': 'VET',\n",
      "                    'explorer.vite.net': 'VITE',\n",
      "                    'www.wanscan.org': 'WAN',\n",
      "                    'wavesexplorer.com': 'WAVES',\n",
      "                    'wax.eosx.io': 'WAXP',\n",
      "                    'waltonchain.pro': 'WTC',\n",
      "                    'chain.nem.ninja': 'XEM',\n",
      "                    'verge-blockchain.info': 'XVG',\n",
      "                    'explorer.yoyow.org': 'YOYOW',\n",
      "                    'explorer.zcha.in': 'ZEC',\n",
      "                    'explorer.zensystem.io': 'ZEN',\n",
      "                },\n",
      "                'impliedNetworks': {\n",
      "                    'ETH': {'ERC20': 'ETH'},\n",
      "                    'TRX': {'TRC20': 'TRX'},\n",
      "                },\n",
      "                'legalMoney': {\n",
      "                    'MXN': True,\n",
      "                    'UGX': True,\n",
      "                    'SEK': True,\n",
      "                    'CHF': True,\n",
      "                    'VND': True,\n",
      "                    'AED': True,\n",
      "                    'DKK': True,\n",
      "                    'KZT': True,\n",
      "                    'HUF': True,\n",
      "                    'PEN': True,\n",
      "                    'PHP': True,\n",
      "                    'USD': True,\n",
      "                    'TRY': True,\n",
      "                    'EUR': True,\n",
      "                    'NGN': True,\n",
      "                    'PLN': True,\n",
      "                    'BRL': True,\n",
      "                    'ZAR': True,\n",
      "                    'KES': True,\n",
      "                    'ARS': True,\n",
      "                    'RUB': True,\n",
      "                    'AUD': True,\n",
      "                    'NOK': True,\n",
      "                    'CZK': True,\n",
      "                    'GBP': True,\n",
      "                    'UAH': True,\n",
      "                    'GHS': True,\n",
      "                    'HKD': True,\n",
      "                    'CAD': True,\n",
      "                    'INR': True,\n",
      "                    'JPY': True,\n",
      "                    'NZD': True,\n",
      "                },\n",
      "            },\n",
      "            # https://binance-docs.github.io/apidocs/spot/en/#error-codes-2\n",
      "            'exceptions': {\n",
      "                'exact': {\n",
      "                    'System is under maintenance.': OnMaintenance,  # {\"code\":1,\"msg\":\"System is under maintenance.\"}\n",
      "                    'System abnormality': ExchangeError,  # {\"code\":-1000,\"msg\":\"System abnormality\"}\n",
      "                    'You are not authorized to execute self request.': PermissionDenied,  # {\"msg\":\"You are not authorized to execute self request.\"}\n",
      "                    'API key does not exist': AuthenticationError,\n",
      "                    'Order would trigger immediately.': OrderImmediatelyFillable,\n",
      "                    'Stop price would trigger immediately.': OrderImmediatelyFillable,  # {\"code\":-2010,\"msg\":\"Stop price would trigger immediately.\"}\n",
      "                    'Order would immediately match and take.': OrderImmediatelyFillable,  # {\"code\":-2010,\"msg\":\"Order would immediately match and take.\"}\n",
      "                    'Account has insufficient balance for requested action.': InsufficientFunds,\n",
      "                    'Rest API trading is not enabled.': ExchangeNotAvailable,\n",
      "                    \"You don't have permission.\": PermissionDenied,  # {\"msg\":\"You don't have permission.\",\"success\":false}\n",
      "                    'Market is closed.': ExchangeNotAvailable,  # {\"code\":-1013,\"msg\":\"Market is closed.\"}\n",
      "                    'Too many requests. Please try again later.': DDoSProtection,  # {\"msg\":\"Too many requests. Please try again later.\",\"success\":false}\n",
      "                    '-1000': ExchangeNotAvailable,  # {\"code\":-1000,\"msg\":\"An unknown error occured while processing the request.\"}\n",
      "                    '-1001': ExchangeNotAvailable,  # 'Internal error; unable to process your request. Please try again.'\n",
      "                    '-1002': AuthenticationError,  # 'You are not authorized to execute self request.'\n",
      "                    '-1003': RateLimitExceeded,  # {\"code\":-1003,\"msg\":\"Too much request weight used, current limit is 1200 request weight per 1 MINUTE. Please use the websocket for live updates to avoid polling the API.\"}\n",
      "                    '-1013': InvalidOrder,  # createOrder -> 'invalid quantity'/'invalid price'/MIN_NOTIONAL\n",
      "                    '-1015': RateLimitExceeded,  # 'Too many new orders; current limit is %s orders per %s.'\n",
      "                    '-1016': ExchangeNotAvailable,  # 'This service is no longer available.',\n",
      "                    '-1020': BadRequest,  # 'This operation is not supported.'\n",
      "                    '-1021': InvalidNonce,  # 'your time is ahead of server'\n",
      "                    '-1022': AuthenticationError,  # {\"code\":-1022,\"msg\":\"Signature for self request is not valid.\"}\n",
      "                    '-1100': BadRequest,  # createOrder(symbol, 1, asdf) -> 'Illegal characters found in parameter 'price'\n",
      "                    '-1101': BadRequest,  # Too many parameters; expected %s and received %s.\n",
      "                    '-1102': BadRequest,  # Param %s or %s must be sent, but both were empty\n",
      "                    '-1103': BadRequest,  # An unknown parameter was sent.\n",
      "                    '-1104': BadRequest,  # Not all sent parameters were read, read 8 parameters but was sent 9\n",
      "                    '-1105': BadRequest,  # Parameter %s was empty.\n",
      "                    '-1106': BadRequest,  # Parameter %s sent when not required.\n",
      "                    '-1111': BadRequest,  # Precision is over the maximum defined for self asset.\n",
      "                    '-1112': InvalidOrder,  # No orders on book for symbol.\n",
      "                    '-1114': BadRequest,  # TimeInForce parameter sent when not required.\n",
      "                    '-1115': BadRequest,  # Invalid timeInForce.\n",
      "                    '-1116': BadRequest,  # Invalid orderType.\n",
      "                    '-1117': BadRequest,  # Invalid side.\n",
      "                    '-1118': BadRequest,  # New client order ID was empty.\n",
      "                    '-1119': BadRequest,  # Original client order ID was empty.\n",
      "                    '-1120': BadRequest,  # Invalid interval.\n",
      "                    '-1121': BadSymbol,  # Invalid symbol.\n",
      "                    '-1125': AuthenticationError,  # This listenKey does not exist.\n",
      "                    '-1127': BadRequest,  # More than %s hours between startTime and endTime.\n",
      "                    '-1128': BadRequest,  # {\"code\":-1128,\"msg\":\"Combination of optional parameters invalid.\"}\n",
      "                    '-1130': BadRequest,  # Data sent for paramter %s is not valid.\n",
      "                    '-1131': BadRequest,  # recvWindow must be less than 60000\n",
      "                    '-2008': AuthenticationError,  # {\"code\":-2008,\"msg\":\"Invalid Api-Key ID.\"}\n",
      "                    '-2010': ExchangeError,  # generic error code for createOrder -> 'Account has insufficient balance for requested action.', {\"code\":-2010,\"msg\":\"Rest API trading is not enabled.\"}, etc...\n",
      "                    '-2011': OrderNotFound,  # cancelOrder(1, 'BTC/USDT') -> 'UNKNOWN_ORDER'\n",
      "                    '-2013': OrderNotFound,  # fetchOrder(1, 'BTC/USDT') -> 'Order does not exist'\n",
      "                    '-2014': AuthenticationError,  # {\"code\":-2014, \"msg\": \"API-key format invalid.\"}\n",
      "                    '-2015': AuthenticationError,  # \"Invalid API-key, IP, or permissions for action.\"\n",
      "                    '-2019': InsufficientFunds,  # {\"code\":-2019,\"msg\":\"Margin is insufficient.\"}\n",
      "                    '-3005': InsufficientFunds,  # {\"code\":-3005,\"msg\":\"Transferring out not allowed. Transfer out amount exceeds max amount.\"}\n",
      "                    '-3006': InsufficientFunds,  # {\"code\":-3006,\"msg\":\"Your borrow amount has exceed maximum borrow amount.\"}\n",
      "                    '-3008': InsufficientFunds,  # {\"code\":-3008,\"msg\":\"Borrow not allowed. Your borrow amount has exceed maximum borrow amount.\"}\n",
      "                    '-3010': ExchangeError,  # {\"code\":-3010,\"msg\":\"Repay not allowed. Repay amount exceeds borrow amount.\"}\n",
      "                    '-3015': ExchangeError,  # {\"code\":-3015,\"msg\":\"Repay amount exceeds borrow amount.\"}\n",
      "                    '-3022': AccountSuspended,  # You account's trading is banned.\n",
      "                    '-4028': BadRequest,  # {\"code\":-4028,\"msg\":\"Leverage 100 is not valid\"}\n",
      "                    '-3020': InsufficientFunds,  # {\"code\":-3020,\"msg\":\"Transfer out amount exceeds max amount.\"}\n",
      "                    '-3041': InsufficientFunds,  # {\"code\":-3041,\"msg\":\"Balance is not enough\"}\n",
      "                    '-5013': InsufficientFunds,  # Asset transfer failed: insufficient balance\"\n",
      "                    '-11008': InsufficientFunds,  # {\"code\":-11008,\"msg\":\"Exceeding the account's maximum borrowable limit.\"}\n",
      "                    '-4051': InsufficientFunds,  # {\"code\":-4051,\"msg\":\"Isolated balance insufficient.\"}\n",
      "                },\n",
      "                'broad': {\n",
      "                    'has no operation privilege': PermissionDenied,\n",
      "                    'MAX_POSITION': InvalidOrder,  # {\"code\":-2010,\"msg\":\"Filter failure: MAX_POSITION\"}\n",
      "                },\n",
      "            },\n",
      "        })\n",
      "\n",
      "    def cost_to_precision(self, symbol, cost):\n",
      "        return self.decimal_to_precision(cost, TRUNCATE, self.markets[symbol]['precision']['quote'], self.precisionMode, self.paddingMode)\n",
      "\n",
      "    def currency_to_precision(self, currency, fee):\n",
      "        # info is available in currencies only if the user has configured his api keys\n",
      "        if self.safe_value(self.currencies[currency], 'precision') is not None:\n",
      "            return self.decimal_to_precision(fee, TRUNCATE, self.currencies[currency]['precision'], self.precisionMode, self.paddingMode)\n",
      "        else:\n",
      "            return self.number_to_string(fee)\n",
      "\n",
      "    def nonce(self):\n",
      "        return self.milliseconds() - self.options['timeDifference']\n",
      "\n",
      "    def fetch_time(self, params={}):\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchTime', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        method = 'publicGetTime'\n",
      "        if type == 'future':\n",
      "            method = 'fapiPublicGetTime'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPublicGetTime'\n",
      "        response = getattr(self, method)(query)\n",
      "        return self.safe_integer(response, 'serverTime')\n",
      "\n",
      "    def load_time_difference(self, params={}):\n",
      "        serverTime = self.fetch_time(params)\n",
      "        after = self.milliseconds()\n",
      "        self.options['timeDifference'] = after - serverTime\n",
      "        return self.options['timeDifference']\n",
      "\n",
      "    def fetch_currencies(self, params={}):\n",
      "        fetchCurrenciesEnabled = self.safe_value(self.options, 'fetchCurrencies')\n",
      "        if not fetchCurrenciesEnabled:\n",
      "            return None\n",
      "        # self endpoint requires authentication\n",
      "        # while fetchCurrencies is a public API method by design\n",
      "        # therefore we check the keys here\n",
      "        # and fallback to generating the currencies from the markets\n",
      "        if not self.check_required_credentials(False):\n",
      "            return None\n",
      "        # sandbox/testnet does not support sapi endpoints\n",
      "        apiBackup = self.safe_string(self.urls, 'apiBackup')\n",
      "        if apiBackup is not None:\n",
      "            return None\n",
      "        response = self.sapiGetCapitalConfigGetall(params)\n",
      "        result = {}\n",
      "        for i in range(0, len(response)):\n",
      "            #\n",
      "            #     {\n",
      "            #         coin: 'LINK',\n",
      "            #         depositAllEnable: True,\n",
      "            #         withdrawAllEnable: True,\n",
      "            #         name: 'ChainLink',\n",
      "            #         free: '0.06168',\n",
      "            #         locked: '0',\n",
      "            #         freeze: '0',\n",
      "            #         withdrawing: '0',\n",
      "            #         ipoing: '0',\n",
      "            #         ipoable: '0',\n",
      "            #         storage: '0',\n",
      "            #         isLegalMoney: False,\n",
      "            #         trading: True,\n",
      "            #         networkList: [\n",
      "            #             {\n",
      "            #                 network: 'BNB',\n",
      "            #                 coin: 'LINK',\n",
      "            #                 withdrawIntegerMultiple: '0',\n",
      "            #                 isDefault: False,\n",
      "            #                 depositEnable: True,\n",
      "            #                 withdrawEnable: True,\n",
      "            #                 depositDesc: '',\n",
      "            #                 withdrawDesc: '',\n",
      "            #                 specialTips: 'Both a MEMO and an Address are required to successfully deposit your LINK BEP2 tokens to Binance.',\n",
      "            #                 name: 'BEP2',\n",
      "            #                 resetAddressStatus: False,\n",
      "            #                 addressRegex: '^(bnb1)[0-9a-z]{38}$',\n",
      "            #                 memoRegex: '^[0-9A-Za-z\\\\-_]{1,120}$',\n",
      "            #                 withdrawFee: '0.002',\n",
      "            #                 withdrawMin: '0.01',\n",
      "            #                 withdrawMax: '9999999',\n",
      "            #                 minConfirm: 1,\n",
      "            #                 unLockConfirm: 0\n",
      "            #             },\n",
      "            #             {\n",
      "            #                 network: 'BSC',\n",
      "            #                 coin: 'LINK',\n",
      "            #                 withdrawIntegerMultiple: '0.00000001',\n",
      "            #                 isDefault: False,\n",
      "            #                 depositEnable: True,\n",
      "            #                 withdrawEnable: True,\n",
      "            #                 depositDesc: '',\n",
      "            #                 withdrawDesc: '',\n",
      "            #                 specialTips: '',\n",
      "            #                 name: 'BEP20(BSC)',\n",
      "            #                 resetAddressStatus: False,\n",
      "            #                 addressRegex: '^(0x)[0-9A-Fa-f]{40}$',\n",
      "            #                 memoRegex: '',\n",
      "            #                 withdrawFee: '0.005',\n",
      "            #                 withdrawMin: '0.01',\n",
      "            #                 withdrawMax: '9999999',\n",
      "            #                 minConfirm: 15,\n",
      "            #                 unLockConfirm: 0\n",
      "            #             },\n",
      "            #             {\n",
      "            #                 network: 'ETH',\n",
      "            #                 coin: 'LINK',\n",
      "            #                 withdrawIntegerMultiple: '0.00000001',\n",
      "            #                 isDefault: True,\n",
      "            #                 depositEnable: True,\n",
      "            #                 withdrawEnable: True,\n",
      "            #                 depositDesc: '',\n",
      "            #                 withdrawDesc: '',\n",
      "            #                 name: 'ERC20',\n",
      "            #                 resetAddressStatus: False,\n",
      "            #                 addressRegex: '^(0x)[0-9A-Fa-f]{40}$',\n",
      "            #                 memoRegex: '',\n",
      "            #                 withdrawFee: '0.34',\n",
      "            #                 withdrawMin: '0.68',\n",
      "            #                 withdrawMax: '0',\n",
      "            #                 minConfirm: 12,\n",
      "            #                 unLockConfirm: 0\n",
      "            #             }\n",
      "            #         ]\n",
      "            #     }\n",
      "            #\n",
      "            entry = response[i]\n",
      "            id = self.safe_string(entry, 'coin')\n",
      "            name = self.safe_string(entry, 'name')\n",
      "            code = self.safe_currency_code(id)\n",
      "            precision = None\n",
      "            isWithdrawEnabled = True\n",
      "            isDepositEnabled = True\n",
      "            networkList = self.safe_value(entry, 'networkList', [])\n",
      "            fees = {}\n",
      "            fee = None\n",
      "            for j in range(0, len(networkList)):\n",
      "                networkItem = networkList[j]\n",
      "                network = self.safe_string(networkItem, 'network')\n",
      "                # name = self.safe_string(networkItem, 'name')\n",
      "                withdrawFee = self.safe_number(networkItem, 'withdrawFee')\n",
      "                depositEnable = self.safe_value(networkItem, 'depositEnable')\n",
      "                withdrawEnable = self.safe_value(networkItem, 'withdrawEnable')\n",
      "                isDepositEnabled = isDepositEnabled or depositEnable\n",
      "                isWithdrawEnabled = isWithdrawEnabled or withdrawEnable\n",
      "                fees[network] = withdrawFee\n",
      "                isDefault = self.safe_value(networkItem, 'isDefault')\n",
      "                if isDefault or fee is None:\n",
      "                    fee = withdrawFee\n",
      "            trading = self.safe_value(entry, 'trading')\n",
      "            active = (isWithdrawEnabled and isDepositEnabled and trading)\n",
      "            result[code] = {\n",
      "                'id': id,\n",
      "                'name': name,\n",
      "                'code': code,\n",
      "                'precision': precision,\n",
      "                'info': entry,\n",
      "                'active': active,\n",
      "                'networks': networkList,\n",
      "                'fee': fee,\n",
      "                'fees': fees,\n",
      "                'limits': self.limits,\n",
      "            }\n",
      "        return result\n",
      "\n",
      "    def fetch_markets(self, params={}):\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchMarkets', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        if (type != 'spot') and (type != 'future') and (type != 'margin') and (type != 'delivery'):\n",
      "            raise ExchangeError(self.id + \" does not support '\" + type + \"' type, set exchange.options['defaultType'] to 'spot', 'margin', 'delivery' or 'future'\")  # eslint-disable-line quotes\n",
      "        method = 'publicGetExchangeInfo'\n",
      "        if type == 'future':\n",
      "            method = 'fapiPublicGetExchangeInfo'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPublicGetExchangeInfo'\n",
      "        response = getattr(self, method)(query)\n",
      "        #\n",
      "        # spot / margin\n",
      "        #\n",
      "        #     {\n",
      "        #         \"timezone\":\"UTC\",\n",
      "        #         \"serverTime\":1575416692969,\n",
      "        #         \"rateLimits\":[\n",
      "        #             {\"rateLimitType\":\"REQUEST_WEIGHT\",\"interval\":\"MINUTE\",\"intervalNum\":1,\"limit\":1200},\n",
      "        #             {\"rateLimitType\":\"ORDERS\",\"interval\":\"SECOND\",\"intervalNum\":10,\"limit\":100},\n",
      "        #             {\"rateLimitType\":\"ORDERS\",\"interval\":\"DAY\",\"intervalNum\":1,\"limit\":200000}\n",
      "        #         ],\n",
      "        #         \"exchangeFilters\":[],\n",
      "        #         \"symbols\":[\n",
      "        #             {\n",
      "        #                 \"symbol\":\"ETHBTC\",\n",
      "        #                 \"status\":\"TRADING\",\n",
      "        #                 \"baseAsset\":\"ETH\",\n",
      "        #                 \"baseAssetPrecision\":8,\n",
      "        #                 \"quoteAsset\":\"BTC\",\n",
      "        #                 \"quotePrecision\":8,\n",
      "        #                 \"baseCommissionPrecision\":8,\n",
      "        #                 \"quoteCommissionPrecision\":8,\n",
      "        #                 \"orderTypes\":[\"LIMIT\",\"LIMIT_MAKER\",\"MARKET\",\"STOP_LOSS_LIMIT\",\"TAKE_PROFIT_LIMIT\"],\n",
      "        #                 \"icebergAllowed\":true,\n",
      "        #                 \"ocoAllowed\":true,\n",
      "        #                 \"quoteOrderQtyMarketAllowed\":true,\n",
      "        #                 \"isSpotTradingAllowed\":true,\n",
      "        #                 \"isMarginTradingAllowed\":true,\n",
      "        #                 \"filters\":[\n",
      "        #                     {\"filterType\":\"PRICE_FILTER\",\"minPrice\":\"0.00000100\",\"maxPrice\":\"100000.00000000\",\"tickSize\":\"0.00000100\"},\n",
      "        #                     {\"filterType\":\"PERCENT_PRICE\",\"multiplierUp\":\"5\",\"multiplierDown\":\"0.2\",\"avgPriceMins\":5},\n",
      "        #                     {\"filterType\":\"LOT_SIZE\",\"minQty\":\"0.00100000\",\"maxQty\":\"100000.00000000\",\"stepSize\":\"0.00100000\"},\n",
      "        #                     {\"filterType\":\"MIN_NOTIONAL\",\"minNotional\":\"0.00010000\",\"applyToMarket\":true,\"avgPriceMins\":5},\n",
      "        #                     {\"filterType\":\"ICEBERG_PARTS\",\"limit\":10},\n",
      "        #                     {\"filterType\":\"MARKET_LOT_SIZE\",\"minQty\":\"0.00000000\",\"maxQty\":\"63100.00000000\",\"stepSize\":\"0.00000000\"},\n",
      "        #                     {\"filterType\":\"MAX_NUM_ALGO_ORDERS\",\"maxNumAlgoOrders\":5}\n",
      "        #                 ]\n",
      "        #             },\n",
      "        #         ],\n",
      "        #     }\n",
      "        #\n",
      "        # futures/usdt-margined(fapi)\n",
      "        #\n",
      "        #     {\n",
      "        #         \"timezone\":\"UTC\",\n",
      "        #         \"serverTime\":1575417244353,\n",
      "        #         \"rateLimits\":[\n",
      "        #             {\"rateLimitType\":\"REQUEST_WEIGHT\",\"interval\":\"MINUTE\",\"intervalNum\":1,\"limit\":1200},\n",
      "        #             {\"rateLimitType\":\"ORDERS\",\"interval\":\"MINUTE\",\"intervalNum\":1,\"limit\":1200}\n",
      "        #         ],\n",
      "        #         \"exchangeFilters\":[],\n",
      "        #         \"symbols\":[\n",
      "        #             {\n",
      "        #                 \"symbol\":\"BTCUSDT\",\n",
      "        #                 \"status\":\"TRADING\",\n",
      "        #                 \"maintMarginPercent\":\"2.5000\",\n",
      "        #                 \"requiredMarginPercent\":\"5.0000\",\n",
      "        #                 \"baseAsset\":\"BTC\",\n",
      "        #                 \"quoteAsset\":\"USDT\",\n",
      "        #                 \"pricePrecision\":2,\n",
      "        #                 \"quantityPrecision\":3,\n",
      "        #                 \"baseAssetPrecision\":8,\n",
      "        #                 \"quotePrecision\":8,\n",
      "        #                 \"filters\":[\n",
      "        #                     {\"minPrice\":\"0.01\",\"maxPrice\":\"100000\",\"filterType\":\"PRICE_FILTER\",\"tickSize\":\"0.01\"},\n",
      "        #                     {\"stepSize\":\"0.001\",\"filterType\":\"LOT_SIZE\",\"maxQty\":\"1000\",\"minQty\":\"0.001\"},\n",
      "        #                     {\"stepSize\":\"0.001\",\"filterType\":\"MARKET_LOT_SIZE\",\"maxQty\":\"1000\",\"minQty\":\"0.001\"},\n",
      "        #                     {\"limit\":200,\"filterType\":\"MAX_NUM_ORDERS\"},\n",
      "        #                     {\"multiplierDown\":\"0.8500\",\"multiplierUp\":\"1.1500\",\"multiplierDecimal\":\"4\",\"filterType\":\"PERCENT_PRICE\"}\n",
      "        #                 ],\n",
      "        #                 \"orderTypes\":[\"LIMIT\",\"MARKET\",\"STOP\"],\n",
      "        #                 \"timeInForce\":[\"GTC\",\"IOC\",\"FOK\",\"GTX\"]\n",
      "        #             }\n",
      "        #         ]\n",
      "        #     }\n",
      "        #\n",
      "        # delivery/coin-margined(dapi)\n",
      "        #\n",
      "        #     {\n",
      "        #         \"timezone\": \"UTC\",\n",
      "        #         \"serverTime\": 1597667052958,\n",
      "        #         \"rateLimits\": [\n",
      "        #             {\"rateLimitType\":\"REQUEST_WEIGHT\",\"interval\":\"MINUTE\",\"intervalNum\":1,\"limit\":6000},\n",
      "        #             {\"rateLimitType\":\"ORDERS\",\"interval\":\"MINUTE\",\"intervalNum\":1,\"limit\":6000}\n",
      "        #         ],\n",
      "        #         \"exchangeFilters\": [],\n",
      "        #         \"symbols\": [\n",
      "        #             {\n",
      "        #                 \"symbol\": \"BTCUSD_200925\",\n",
      "        #                 \"pair\": \"BTCUSD\",\n",
      "        #                 \"contractType\": \"CURRENT_QUARTER\",\n",
      "        #                 \"deliveryDate\": 1601020800000,\n",
      "        #                 \"onboardDate\": 1590739200000,\n",
      "        #                 \"contractStatus\": \"TRADING\",\n",
      "        #                 \"contractSize\": 100,\n",
      "        #                 \"marginAsset\": \"BTC\",\n",
      "        #                 \"maintMarginPercent\": \"2.5000\",\n",
      "        #                 \"requiredMarginPercent\": \"5.0000\",\n",
      "        #                 \"baseAsset\": \"BTC\",\n",
      "        #                 \"quoteAsset\": \"USD\",\n",
      "        #                 \"pricePrecision\": 1,\n",
      "        #                 \"quantityPrecision\": 0,\n",
      "        #                 \"baseAssetPrecision\": 8,\n",
      "        #                 \"quotePrecision\": 8,\n",
      "        #                 \"equalQtyPrecision\": 4,\n",
      "        #                 \"filters\": [\n",
      "        #                     {\"minPrice\":\"0.1\",\"maxPrice\":\"100000\",\"filterType\":\"PRICE_FILTER\",\"tickSize\":\"0.1\"},\n",
      "        #                     {\"stepSize\":\"1\",\"filterType\":\"LOT_SIZE\",\"maxQty\":\"100000\",\"minQty\":\"1\"},\n",
      "        #                     {\"stepSize\":\"0\",\"filterType\":\"MARKET_LOT_SIZE\",\"maxQty\":\"100000\",\"minQty\":\"1\"},\n",
      "        #                     {\"limit\":200,\"filterType\":\"MAX_NUM_ORDERS\"},\n",
      "        #                     {\"multiplierDown\":\"0.9500\",\"multiplierUp\":\"1.0500\",\"multiplierDecimal\":\"4\",\"filterType\":\"PERCENT_PRICE\"}\n",
      "        #                 ],\n",
      "        #                 \"orderTypes\": [\"LIMIT\",\"MARKET\",\"STOP\",\"STOP_MARKET\",\"TAKE_PROFIT\",\"TAKE_PROFIT_MARKET\",\"TRAILING_STOP_MARKET\"],\n",
      "        #                 \"timeInForce\": [\"GTC\",\"IOC\",\"FOK\",\"GTX\"]\n",
      "        #             },\n",
      "        #             {\n",
      "        #                 \"symbol\": \"BTCUSD_PERP\",\n",
      "        #                 \"pair\": \"BTCUSD\",\n",
      "        #                 \"contractType\": \"PERPETUAL\",\n",
      "        #                 \"deliveryDate\": 4133404800000,\n",
      "        #                 \"onboardDate\": 1596006000000,\n",
      "        #                 \"contractStatus\": \"TRADING\",\n",
      "        #                 \"contractSize\": 100,\n",
      "        #                 \"marginAsset\": \"BTC\",\n",
      "        #                 \"maintMarginPercent\": \"2.5000\",\n",
      "        #                 \"requiredMarginPercent\": \"5.0000\",\n",
      "        #                 \"baseAsset\": \"BTC\",\n",
      "        #                 \"quoteAsset\": \"USD\",\n",
      "        #                 \"pricePrecision\": 1,\n",
      "        #                 \"quantityPrecision\": 0,\n",
      "        #                 \"baseAssetPrecision\": 8,\n",
      "        #                 \"quotePrecision\": 8,\n",
      "        #                 \"equalQtyPrecision\": 4,\n",
      "        #                 \"filters\": [\n",
      "        #                     {\"minPrice\":\"0.1\",\"maxPrice\":\"100000\",\"filterType\":\"PRICE_FILTER\",\"tickSize\":\"0.1\"},\n",
      "        #                     {\"stepSize\":\"1\",\"filterType\":\"LOT_SIZE\",\"maxQty\":\"100000\",\"minQty\":\"1\"},\n",
      "        #                     {\"stepSize\":\"1\",\"filterType\":\"MARKET_LOT_SIZE\",\"maxQty\":\"100000\",\"minQty\":\"1\"},\n",
      "        #                     {\"limit\":200,\"filterType\":\"MAX_NUM_ORDERS\"},\n",
      "        #                     {\"multiplierDown\":\"0.8500\",\"multiplierUp\":\"1.1500\",\"multiplierDecimal\":\"4\",\"filterType\":\"PERCENT_PRICE\"}\n",
      "        #                 ],\n",
      "        #                 \"orderTypes\": [\"LIMIT\",\"MARKET\",\"STOP\",\"STOP_MARKET\",\"TAKE_PROFIT\",\"TAKE_PROFIT_MARKET\",\"TRAILING_STOP_MARKET\"],\n",
      "        #                 \"timeInForce\": [\"GTC\",\"IOC\",\"FOK\",\"GTX\"]\n",
      "        #             }\n",
      "        #         ]\n",
      "        #     }\n",
      "        #\n",
      "        if self.options['adjustForTimeDifference']:\n",
      "            self.load_time_difference()\n",
      "        markets = self.safe_value(response, 'symbols', [])\n",
      "        result = []\n",
      "        for i in range(0, len(markets)):\n",
      "            market = markets[i]\n",
      "            spot = (type == 'spot')\n",
      "            future = (type == 'future')\n",
      "            delivery = (type == 'delivery')\n",
      "            id = self.safe_string(market, 'symbol')\n",
      "            lowercaseId = self.safe_string_lower(market, 'symbol')\n",
      "            baseId = self.safe_string(market, 'baseAsset')\n",
      "            quoteId = self.safe_string(market, 'quoteAsset')\n",
      "            base = self.safe_currency_code(baseId)\n",
      "            quote = self.safe_currency_code(quoteId)\n",
      "            contractType = self.safe_string(market, 'contractType')\n",
      "            idSymbol = (future or delivery) and (contractType != 'PERPETUAL')\n",
      "            symbol = None\n",
      "            expiry = None\n",
      "            if idSymbol:\n",
      "                symbol = id\n",
      "                expiry = self.safe_integer(market, 'deliveryDate')\n",
      "            else:\n",
      "                symbol = base + '/' + quote\n",
      "            filters = self.safe_value(market, 'filters', [])\n",
      "            filtersByType = self.index_by(filters, 'filterType')\n",
      "            precision = {\n",
      "                'base': self.safe_integer(market, 'baseAssetPrecision'),\n",
      "                'quote': self.safe_integer(market, 'quotePrecision'),\n",
      "                'amount': self.safe_integer(market, 'quantityPrecision'),\n",
      "                'price': self.safe_integer(market, 'pricePrecision'),\n",
      "            }\n",
      "            status = self.safe_string_2(market, 'status', 'contractStatus')\n",
      "            active = (status == 'TRADING')\n",
      "            margin = self.safe_value(market, 'isMarginTradingAllowed', False)\n",
      "            contractSize = None\n",
      "            fees = self.fees\n",
      "            if future or delivery:\n",
      "                contractSize = self.safe_string(market, 'contractSize', '1')\n",
      "                fees = self.fees[type]\n",
      "            maker = fees['trading']['maker']\n",
      "            taker = fees['trading']['taker']\n",
      "            settleId = self.safe_string(market, 'marginAsset')\n",
      "            settle = self.safe_currency_code(settleId)\n",
      "            entry = {\n",
      "                'id': id,\n",
      "                'lowercaseId': lowercaseId,\n",
      "                'symbol': symbol,\n",
      "                'base': base,\n",
      "                'quote': quote,\n",
      "                'baseId': baseId,\n",
      "                'quoteId': quoteId,\n",
      "                'info': market,\n",
      "                'spot': spot,\n",
      "                'type': type,\n",
      "                'margin': margin,\n",
      "                'future': future,\n",
      "                'delivery': delivery,\n",
      "                'linear': future,\n",
      "                'inverse': delivery,\n",
      "                'expiry': expiry,\n",
      "                'expiryDatetime': self.iso8601(expiry),\n",
      "                'settleId': settleId,\n",
      "                'settle': settle,\n",
      "                'active': active,\n",
      "                'precision': precision,\n",
      "                'contractSize': contractSize,\n",
      "                'maker': maker,\n",
      "                'taker': taker,\n",
      "                'limits': {\n",
      "                    'amount': {\n",
      "                        'min': None,\n",
      "                        'max': None,\n",
      "                    },\n",
      "                    'price': {\n",
      "                        'min': None,\n",
      "                        'max': None,\n",
      "                    },\n",
      "                    'cost': {\n",
      "                        'min': None,\n",
      "                        'max': None,\n",
      "                    },\n",
      "                },\n",
      "            }\n",
      "            if 'PRICE_FILTER' in filtersByType:\n",
      "                filter = self.safe_value(filtersByType, 'PRICE_FILTER', {})\n",
      "                tickSize = self.safe_string(filter, 'tickSize')\n",
      "                entry['precision']['price'] = self.precision_from_string(tickSize)\n",
      "                # PRICE_FILTER reports zero values for maxPrice\n",
      "                # since they updated filter types in November 2018\n",
      "                # https://github.com/ccxt/ccxt/issues/4286\n",
      "                # therefore limits['price']['max'] doesn't have any meaningful value except None\n",
      "                entry['limits']['price'] = {\n",
      "                    'min': self.safe_number(filter, 'minPrice'),\n",
      "                    'max': self.safe_number(filter, 'maxPrice'),\n",
      "                }\n",
      "                entry['precision']['price'] = self.precision_from_string(filter['tickSize'])\n",
      "            if 'LOT_SIZE' in filtersByType:\n",
      "                filter = self.safe_value(filtersByType, 'LOT_SIZE', {})\n",
      "                stepSize = self.safe_string(filter, 'stepSize')\n",
      "                entry['precision']['amount'] = self.precision_from_string(stepSize)\n",
      "                entry['limits']['amount'] = {\n",
      "                    'min': self.safe_number(filter, 'minQty'),\n",
      "                    'max': self.safe_number(filter, 'maxQty'),\n",
      "                }\n",
      "            if 'MARKET_LOT_SIZE' in filtersByType:\n",
      "                filter = self.safe_value(filtersByType, 'MARKET_LOT_SIZE', {})\n",
      "                entry['limits']['market'] = {\n",
      "                    'min': self.safe_number(filter, 'minQty'),\n",
      "                    'max': self.safe_number(filter, 'maxQty'),\n",
      "                }\n",
      "            if 'MIN_NOTIONAL' in filtersByType:\n",
      "                filter = self.safe_value(filtersByType, 'MIN_NOTIONAL', {})\n",
      "                entry['limits']['cost']['min'] = self.safe_number_2(filter, 'minNotional', 'notional')\n",
      "            result.append(entry)\n",
      "        return result\n",
      "\n",
      "    def fetch_balance(self, params={}):\n",
      "        self.load_markets()\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchBalance', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        method = 'privateGetAccount'\n",
      "        if type == 'future':\n",
      "            options = self.safe_value(self.options, type, {})\n",
      "            fetchBalanceOptions = self.safe_value(options, 'fetchBalance', {})\n",
      "            method = self.safe_string(fetchBalanceOptions, 'method', 'fapiPrivateV2GetAccount')\n",
      "        elif type == 'delivery':\n",
      "            options = self.safe_value(self.options, type, {})\n",
      "            fetchBalanceOptions = self.safe_value(options, 'fetchBalance', {})\n",
      "            method = self.safe_string(fetchBalanceOptions, 'method', 'dapiPrivateGetAccount')\n",
      "        elif type == 'margin':\n",
      "            method = 'sapiGetMarginAccount'\n",
      "        elif type == 'savings':\n",
      "            method = 'sapiGetLendingUnionAccount'\n",
      "        elif type == 'funding':\n",
      "            method = 'sapiPostAssetGetFundingAsset'\n",
      "        query = self.omit(params, 'type')\n",
      "        response = getattr(self, method)(query)\n",
      "        #\n",
      "        # spot\n",
      "        #\n",
      "        #     {\n",
      "        #         makerCommission: 10,\n",
      "        #         takerCommission: 10,\n",
      "        #         buyerCommission: 0,\n",
      "        #         sellerCommission: 0,\n",
      "        #         canTrade: True,\n",
      "        #         canWithdraw: True,\n",
      "        #         canDeposit: True,\n",
      "        #         updateTime: 1575357359602,\n",
      "        #         accountType: \"MARGIN\",\n",
      "        #         balances: [\n",
      "        #             {asset: \"BTC\", free: \"0.00219821\", locked: \"0.00000000\"  },\n",
      "        #         ]\n",
      "        #     }\n",
      "        #\n",
      "        # margin\n",
      "        #\n",
      "        #     {\n",
      "        #         \"borrowEnabled\":true,\n",
      "        #         \"marginLevel\":\"999.00000000\",\n",
      "        #         \"totalAssetOfBtc\":\"0.00000000\",\n",
      "        #         \"totalLiabilityOfBtc\":\"0.00000000\",\n",
      "        #         \"totalNetAssetOfBtc\":\"0.00000000\",\n",
      "        #         \"tradeEnabled\":true,\n",
      "        #         \"transferEnabled\":true,\n",
      "        #         \"userAssets\":[\n",
      "        #             {\"asset\":\"MATIC\",\"borrowed\":\"0.00000000\",\"free\":\"0.00000000\",\"interest\":\"0.00000000\",\"locked\":\"0.00000000\",\"netAsset\":\"0.00000000\"},\n",
      "        #             {\"asset\":\"VET\",\"borrowed\":\"0.00000000\",\"free\":\"0.00000000\",\"interest\":\"0.00000000\",\"locked\":\"0.00000000\",\"netAsset\":\"0.00000000\"},\n",
      "        #             {\"asset\":\"USDT\",\"borrowed\":\"0.00000000\",\"free\":\"0.00000000\",\"interest\":\"0.00000000\",\"locked\":\"0.00000000\",\"netAsset\":\"0.00000000\"}\n",
      "        #         ],\n",
      "        #     }\n",
      "        #\n",
      "        # futures(fapi)\n",
      "        #\n",
      "        #     fapiPrivateGetAccount\n",
      "        #\n",
      "        #     {\n",
      "        #         \"feeTier\":0,\n",
      "        #         \"canTrade\":true,\n",
      "        #         \"canDeposit\":true,\n",
      "        #         \"canWithdraw\":true,\n",
      "        #         \"updateTime\":0,\n",
      "        #         \"totalInitialMargin\":\"0.00000000\",\n",
      "        #         \"totalMaintMargin\":\"0.00000000\",\n",
      "        #         \"totalWalletBalance\":\"4.54000000\",\n",
      "        #         \"totalUnrealizedProfit\":\"0.00000000\",\n",
      "        #         \"totalMarginBalance\":\"4.54000000\",\n",
      "        #         \"totalPositionInitialMargin\":\"0.00000000\",\n",
      "        #         \"totalOpenOrderInitialMargin\":\"0.00000000\",\n",
      "        #         \"maxWithdrawAmount\":\"4.54000000\",\n",
      "        #         \"assets\":[\n",
      "        #             {\n",
      "        #                 \"asset\":\"USDT\",\n",
      "        #                 \"walletBalance\":\"4.54000000\",\n",
      "        #                 \"unrealizedProfit\":\"0.00000000\",\n",
      "        #                 \"marginBalance\":\"4.54000000\",\n",
      "        #                 \"maintMargin\":\"0.00000000\",\n",
      "        #                 \"initialMargin\":\"0.00000000\",\n",
      "        #                 \"positionInitialMargin\":\"0.00000000\",\n",
      "        #                 \"openOrderInitialMargin\":\"0.00000000\",\n",
      "        #                 \"maxWithdrawAmount\":\"4.54000000\"\n",
      "        #             }\n",
      "        #         ],\n",
      "        #         \"positions\":[\n",
      "        #             {\n",
      "        #                 \"symbol\":\"BTCUSDT\",\n",
      "        #                 \"initialMargin\":\"0.00000\",\n",
      "        #                 \"maintMargin\":\"0.00000\",\n",
      "        #                 \"unrealizedProfit\":\"0.00000000\",\n",
      "        #                 \"positionInitialMargin\":\"0.00000\",\n",
      "        #                 \"openOrderInitialMargin\":\"0.00000\"\n",
      "        #             }\n",
      "        #         ]\n",
      "        #     }\n",
      "        #\n",
      "        #     fapiPrivateV2GetAccount\n",
      "        #\n",
      "        #     {\n",
      "        #         \"feeTier\":0,\n",
      "        #         \"canTrade\":true,\n",
      "        #         \"canDeposit\":true,\n",
      "        #         \"canWithdraw\":true,\n",
      "        #         \"updateTime\":0,\n",
      "        #         \"totalInitialMargin\":\"0.00000000\",\n",
      "        #         \"totalMaintMargin\":\"0.00000000\",\n",
      "        #         \"totalWalletBalance\":\"0.00000000\",\n",
      "        #         \"totalUnrealizedProfit\":\"0.00000000\",\n",
      "        #         \"totalMarginBalance\":\"0.00000000\",\n",
      "        #         \"totalPositionInitialMargin\":\"0.00000000\",\n",
      "        #         \"totalOpenOrderInitialMargin\":\"0.00000000\",\n",
      "        #         \"totalCrossWalletBalance\":\"0.00000000\",\n",
      "        #         \"totalCrossUnPnl\":\"0.00000000\",\n",
      "        #         \"availableBalance\":\"0.00000000\",\n",
      "        #         \"maxWithdrawAmount\":\"0.00000000\",\n",
      "        #         \"assets\":[\n",
      "        #             {\n",
      "        #                 \"asset\":\"BNB\",\n",
      "        #                 \"walletBalance\":\"0.01000000\",\n",
      "        #                 \"unrealizedProfit\":\"0.00000000\",\n",
      "        #                 \"marginBalance\":\"0.01000000\",\n",
      "        #                 \"maintMargin\":\"0.00000000\",\n",
      "        #                 \"initialMargin\":\"0.00000000\",\n",
      "        #                 \"positionInitialMargin\":\"0.00000000\",\n",
      "        #                 \"openOrderInitialMargin\":\"0.00000000\",\n",
      "        #                 \"maxWithdrawAmount\":\"0.01000000\",\n",
      "        #                 \"crossWalletBalance\":\"0.01000000\",\n",
      "        #                 \"crossUnPnl\":\"0.00000000\",\n",
      "        #                 \"availableBalance\":\"0.01000000\"\n",
      "        #             }\n",
      "        #         ],\n",
      "        #         \"positions\":[\n",
      "        #             {\n",
      "        #                 \"symbol\":\"BTCUSDT\",\n",
      "        #                 \"initialMargin\":\"0\",\n",
      "        #                 \"maintMargin\":\"0\",\n",
      "        #                 \"unrealizedProfit\":\"0.00000000\",\n",
      "        #                 \"positionInitialMargin\":\"0\",\n",
      "        #                 \"openOrderInitialMargin\":\"0\",\n",
      "        #                 \"leverage\":\"20\",\n",
      "        #                 \"isolated\":false,\n",
      "        #                 \"entryPrice\":\"0.00000\",\n",
      "        #                 \"maxNotional\":\"5000000\",\n",
      "        #                 \"positionSide\":\"BOTH\"\n",
      "        #             },\n",
      "        #         ]\n",
      "        #     }\n",
      "        #\n",
      "        #     fapiPrivateV2GetBalance\n",
      "        #\n",
      "        #     [\n",
      "        #         {\n",
      "        #             \"accountAlias\":\"FzFzXquXXqoC\",\n",
      "        #             \"asset\":\"BNB\",\n",
      "        #             \"balance\":\"0.01000000\",\n",
      "        #             \"crossWalletBalance\":\"0.01000000\",\n",
      "        #             \"crossUnPnl\":\"0.00000000\",\n",
      "        #             \"availableBalance\":\"0.01000000\",\n",
      "        #             \"maxWithdrawAmount\":\"0.01000000\"\n",
      "        #         }\n",
      "        #     ]\n",
      "        #\n",
      "        # savings\n",
      "        #\n",
      "        #     {\n",
      "        #       \"totalAmountInBTC\": \"0.3172\",\n",
      "        #       \"totalAmountInUSDT\": \"10000\",\n",
      "        #       \"totalFixedAmountInBTC\": \"0.3172\",\n",
      "        #       \"totalFixedAmountInUSDT\": \"10000\",\n",
      "        #       \"totalFlexibleInBTC\": \"0\",\n",
      "        #       \"totalFlexibleInUSDT\": \"0\",\n",
      "        #       \"positionAmountVos\": [\n",
      "        #         {\n",
      "        #           \"asset\": \"USDT\",\n",
      "        #           \"amount\": \"10000\",\n",
      "        #           \"amountInBTC\": \"0.3172\",\n",
      "        #           \"amountInUSDT\": \"10000\"\n",
      "        #         },\n",
      "        #         {\n",
      "        #           \"asset\": \"BUSD\",\n",
      "        #           \"amount\": \"0\",\n",
      "        #           \"amountInBTC\": \"0\",\n",
      "        #           \"amountInUSDT\": \"0\"\n",
      "        #         }\n",
      "        #       ]\n",
      "        #     }\n",
      "        #\n",
      "        # binance pay\n",
      "        #\n",
      "        #     [\n",
      "        #       {\n",
      "        #         \"asset\": \"BUSD\",\n",
      "        #         \"free\": \"1129.83\",\n",
      "        #         \"locked\": \"0\",\n",
      "        #         \"freeze\": \"0\",\n",
      "        #         \"withdrawing\": \"0\"\n",
      "        #       }\n",
      "        #     ]\n",
      "        #\n",
      "        result = {\n",
      "            'info': response,\n",
      "        }\n",
      "        timestamp = None\n",
      "        if (type == 'spot') or (type == 'margin'):\n",
      "            timestamp = self.safe_integer(response, 'updateTime')\n",
      "            balances = self.safe_value_2(response, 'balances', 'userAssets', [])\n",
      "            for i in range(0, len(balances)):\n",
      "                balance = balances[i]\n",
      "                currencyId = self.safe_string(balance, 'asset')\n",
      "                code = self.safe_currency_code(currencyId)\n",
      "                account = self.account()\n",
      "                account['free'] = self.safe_string(balance, 'free')\n",
      "                account['used'] = self.safe_string(balance, 'locked')\n",
      "                result[code] = account\n",
      "        elif type == 'savings':\n",
      "            positionAmountVos = self.safe_value(response, 'positionAmountVos')\n",
      "            for i in range(0, len(positionAmountVos)):\n",
      "                entry = positionAmountVos[i]\n",
      "                currencyId = self.safe_string(entry, 'asset')\n",
      "                code = self.safe_currency_code(currencyId)\n",
      "                account = self.account()\n",
      "                usedAndTotal = self.safe_string(entry, 'amount')\n",
      "                account['total'] = usedAndTotal\n",
      "                account['used'] = usedAndTotal\n",
      "                result[code] = account\n",
      "        elif type == 'funding':\n",
      "            for i in range(0, len(response)):\n",
      "                entry = response[i]\n",
      "                account = self.account()\n",
      "                currencyId = self.safe_string(entry, 'asset')\n",
      "                code = self.safe_currency_code(currencyId)\n",
      "                account['free'] = self.safe_string(entry, 'free')\n",
      "                frozen = self.safe_string(entry, 'freeze')\n",
      "                withdrawing = self.safe_string(entry, 'withdrawing')\n",
      "                locked = self.safe_string(entry, 'locked')\n",
      "                account['used'] = Precise.string_add(frozen, Precise.string_add(locked, withdrawing))\n",
      "                result[code] = account\n",
      "        else:\n",
      "            balances = response\n",
      "            if not isinstance(response, list):\n",
      "                balances = self.safe_value(response, 'assets', [])\n",
      "            for i in range(0, len(balances)):\n",
      "                balance = balances[i]\n",
      "                currencyId = self.safe_string(balance, 'asset')\n",
      "                code = self.safe_currency_code(currencyId)\n",
      "                account = self.account()\n",
      "                account['free'] = self.safe_string(balance, 'availableBalance')\n",
      "                account['used'] = self.safe_string(balance, 'initialMargin')\n",
      "                account['total'] = self.safe_string_2(balance, 'marginBalance', 'balance')\n",
      "                result[code] = account\n",
      "        result['timestamp'] = timestamp\n",
      "        result['datetime'] = self.iso8601(timestamp)\n",
      "        return self.parse_balance(result)\n",
      "\n",
      "    def fetch_order_book(self, symbol, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        if limit is not None:\n",
      "            request['limit'] = limit  # default 100, max 5000, see https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#order-book\n",
      "        method = 'publicGetDepth'\n",
      "        if market['linear']:\n",
      "            method = 'fapiPublicGetDepth'\n",
      "        elif market['inverse']:\n",
      "            method = 'dapiPublicGetDepth'\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        #\n",
      "        # future\n",
      "        #\n",
      "        #     {\n",
      "        #         \"lastUpdateId\":333598053905,\n",
      "        #         \"E\":1618631511986,\n",
      "        #         \"T\":1618631511964,\n",
      "        #         \"bids\":[\n",
      "        #             [\"2493.56\",\"20.189\"],\n",
      "        #             [\"2493.54\",\"1.000\"],\n",
      "        #             [\"2493.51\",\"0.005\"],[\"2493.37\",\"0.280\"],[\"2493.31\",\"0.865\"],[\"2493.30\",\"0.514\"],[\"2493.29\",\"2.309\"],[\"2493.25\",\"1.500\"],[\"2493.23\",\"0.012\"],[\"2493.22\",\"7.240\"],[\"2493.21\",\"3.349\"],[\"2493.20\",\"2.030\"],[\"2493.19\",\"58.118\"],[\"2493.18\",\"174.836\"],[\"2493.17\",\"14.436\"],[\"2493.12\",\"2.000\"],[\"2493.09\",\"3.232\"],[\"2493.08\",\"2.010\"],[\"2493.07\",\"2.000\"],[\"2493.06\",\"2.000\"],[\"2493.05\",\"2.684\"],[\"2493.04\",\"2.000\"],[\"2493.03\",\"2.000\"],[\"2493.02\",\"5.000\"],[\"2493.01\",\"2.000\"],[\"2493.00\",\"1.035\"],[\"2492.99\",\"8.546\"],[\"2492.98\",\"4.012\"],[\"2492.96\",\"40.937\"],[\"2492.95\",\"40.595\"],[\"2492.94\",\"21.051\"],[\"2492.92\",\"4.012\"],[\"2492.91\",\"0.200\"],[\"2492.85\",\"2.000\"],[\"2492.83\",\"24.929\"],[\"2492.81\",\"50.000\"],[\"2492.80\",\"0.030\"],[\"2492.76\",\"0.264\"],[\"2492.73\",\"32.098\"],[\"2492.71\",\"32.664\"],[\"2492.70\",\"4.228\"],[\"2492.65\",\"1.230\"],[\"2492.61\",\"5.598\"],[\"2492.60\",\"34.786\"],[\"2492.58\",\"10.393\"],[\"2492.54\",\"4.543\"],[\"2492.50\",\"0.400\"],[\"2492.49\",\"0.600\"],[\"2492.48\",\"4.941\"],[\"2492.45\",\"1.207\"],[\"2492.43\",\"4.878\"],[\"2492.40\",\"4.762\"],[\"2492.39\",\"36.489\"],[\"2492.37\",\"3.000\"],[\"2492.36\",\"4.882\"],[\"2492.33\",\"28.117\"],[\"2492.29\",\"0.490\"],[\"2492.28\",\"76.365\"],[\"2492.27\",\"0.200\"],[\"2492.23\",\"3.804\"],[\"2492.22\",\"1.000\"],[\"2492.19\",\"20.011\"],[\"2492.17\",\"13.500\"],[\"2492.16\",\"4.058\"],[\"2492.14\",\"35.673\"],[\"2492.13\",\"1.915\"],[\"2492.12\",\"76.896\"],[\"2492.10\",\"8.050\"],[\"2492.01\",\"16.615\"],[\"2492.00\",\"10.335\"],[\"2491.95\",\"5.880\"],[\"2491.93\",\"10.000\"],[\"2491.92\",\"3.916\"],[\"2491.90\",\"0.795\"],[\"2491.87\",\"22.000\"],[\"2491.85\",\"1.260\"],[\"2491.84\",\"4.014\"],[\"2491.83\",\"6.668\"],[\"2491.73\",\"0.855\"],[\"2491.72\",\"7.572\"],[\"2491.71\",\"7.000\"],[\"2491.68\",\"3.916\"],[\"2491.66\",\"2.500\"],[\"2491.64\",\"4.945\"],[\"2491.63\",\"2.302\"],[\"2491.62\",\"4.012\"],[\"2491.61\",\"16.170\"],[\"2491.60\",\"0.793\"],[\"2491.59\",\"0.403\"],[\"2491.57\",\"17.445\"],[\"2491.56\",\"88.177\"],[\"2491.53\",\"10.000\"],[\"2491.47\",\"0.013\"],[\"2491.45\",\"0.157\"],[\"2491.44\",\"11.733\"],[\"2491.39\",\"3.593\"],[\"2491.38\",\"3.570\"],[\"2491.36\",\"28.077\"],[\"2491.35\",\"0.808\"],[\"2491.30\",\"0.065\"],[\"2491.29\",\"4.880\"],[\"2491.27\",\"22.000\"],[\"2491.24\",\"9.021\"],[\"2491.23\",\"68.393\"],[\"2491.22\",\"0.050\"],[\"2491.21\",\"1.316\"],[\"2491.20\",\"4.000\"],[\"2491.19\",\"0.108\"],[\"2491.18\",\"0.498\"],[\"2491.17\",\"5.000\"],[\"2491.14\",\"10.000\"],[\"2491.13\",\"0.383\"],[\"2491.12\",\"125.959\"],[\"2491.10\",\"0.870\"],[\"2491.08\",\"10.518\"],[\"2491.05\",\"54.743\"],[\"2491.01\",\"7.980\"],[\"2490.96\",\"3.916\"],[\"2490.95\",\"0.135\"],[\"2490.91\",\"0.140\"],[\"2490.89\",\"8.424\"],[\"2490.88\",\"5.930\"],[\"2490.84\",\"1.208\"],[\"2490.83\",\"2.005\"],[\"2490.82\",\"5.517\"],[\"2490.81\",\"73.707\"],[\"2490.80\",\"1.042\"],[\"2490.79\",\"9.626\"],[\"2490.72\",\"3.916\"],[\"2490.70\",\"0.148\"],[\"2490.69\",\"0.403\"],[\"2490.68\",\"0.012\"],[\"2490.67\",\"21.887\"],[\"2490.66\",\"0.008\"],[\"2490.64\",\"11.500\"],[\"2490.61\",\"0.005\"],[\"2490.58\",\"68.175\"],[\"2490.55\",\"0.218\"],[\"2490.54\",\"14.132\"],[\"2490.53\",\"5.157\"],[\"2490.50\",\"0.018\"],[\"2490.49\",\"9.216\"],[\"2490.48\",\"3.979\"],[\"2490.47\",\"1.884\"],[\"2490.44\",\"0.003\"],[\"2490.36\",\"14.132\"],[\"2490.35\",\"2.008\"],[\"2490.34\",\"0.200\"],[\"2490.33\",\"0.015\"],[\"2490.30\",\"0.065\"],[\"2490.29\",\"5.500\"],[\"2490.28\",\"24.203\"],[\"2490.26\",\"4.373\"],[\"2490.25\",\"0.026\"],[\"2490.24\",\"4.000\"],[\"2490.23\",\"177.628\"],[\"2490.22\",\"14.132\"],[\"2490.21\",\"0.181\"],[\"2490.20\",\"0.645\"],[\"2490.19\",\"9.024\"],[\"2490.18\",\"0.108\"],[\"2490.17\",\"0.085\"],[\"2490.16\",\"0.077\"],[\"2490.14\",\"0.275\"],[\"2490.10\",\"0.080\"],[\"2490.07\",\"0.015\"],[\"2490.04\",\"6.056\"],[\"2490.00\",\"6.796\"],[\"2489.98\",\"0.005\"],[\"2489.97\",\"0.258\"],[\"2489.96\",\"10.084\"],[\"2489.95\",\"1.202\"],[\"2489.91\",\"10.121\"],[\"2489.90\",\"10.084\"],[\"2489.88\",\"0.040\"],[\"2489.87\",\"0.004\"],[\"2489.85\",\"0.003\"],[\"2489.76\",\"3.916\"],[\"2489.73\",\"10.084\"],[\"2489.71\",\"0.272\"],[\"2489.70\",\"12.834\"],[\"2489.67\",\"0.403\"],[\"2489.66\",\"0.362\"],[\"2489.64\",\"0.738\"],[\"2489.63\",\"193.236\"],[\"2489.62\",\"14.152\"],[\"2489.61\",\"0.157\"],[\"2489.59\",\"4.011\"],[\"2489.57\",\"0.015\"],[\"2489.55\",\"0.046\"],[\"2489.52\",\"3.921\"],[\"2489.51\",\"0.005\"],[\"2489.45\",\"80.000\"],[\"2489.44\",\"0.649\"],[\"2489.43\",\"10.088\"],[\"2489.39\",\"0.009\"],[\"2489.37\",\"14.132\"],[\"2489.35\",\"72.262\"],[\"2489.34\",\"10.084\"],[\"2489.33\",\"14.136\"],[\"2489.32\",\"23.953\"],[\"2489.30\",\"0.065\"],[\"2489.28\",\"8.136\"],[\"2489.24\",\"8.022\"],[\"2489.19\",\"14.132\"],[\"2489.18\",\"0.085\"],[\"2489.17\",\"0.108\"],[\"2489.14\",\"10.084\"],[\"2489.13\",\"3.142\"],[\"2489.12\",\"77.827\"],[\"2489.11\",\"10.084\"],[\"2489.10\",\"0.080\"],[\"2489.09\",\"50.024\"],[\"2489.04\",\"3.916\"],[\"2489.03\",\"0.008\"],[\"2489.01\",\"10.084\"],[\"2488.99\",\"0.135\"],[\"2488.98\",\"0.187\"],[\"2488.96\",\"0.324\"],[\"2488.92\",\"0.064\"],[\"2488.85\",\"16.056\"],[\"2488.83\",\"14.132\"],[\"2488.80\",\"3.916\"],[\"2488.79\",\"10.084\"],[\"2488.77\",\"4.414\"],[\"2488.76\",\"0.005\"],[\"2488.75\",\"13.685\"],[\"2488.73\",\"0.020\"],[\"2488.69\",\"0.157\"],[\"2488.60\",\"80.000\"],[\"2488.58\",\"10.164\"],[\"2488.57\",\"0.004\"],[\"2488.56\",\"3.933\"],[\"2488.54\",\"3.311\"],[\"2488.51\",\"12.814\"],[\"2488.50\",\"80.099\"],[\"2488.48\",\"0.684\"],[\"2488.44\",\"0.024\"],[\"2488.42\",\"68.180\"],[\"2488.39\",\"4.412\"],[\"2488.38\",\"26.138\"],[\"2488.34\",\"44.134\"],[\"2488.32\",\"8.014\"],[\"2488.30\",\"0.065\"],[\"2488.29\",\"0.009\"],[\"2488.27\",\"4.513\"],[\"2488.26\",\"4.222\"],[\"2488.25\",\"80.000\"],[\"2488.23\",\"0.007\"],[\"2488.22\",\"0.281\"],[\"2488.19\",\"0.100\"],[\"2488.18\",\"80.100\"],[\"2488.17\",\"80.000\"],[\"2488.16\",\"8.197\"],[\"2488.15\",\"79.184\"],[\"2488.13\",\"0.025\"],[\"2488.11\",\"0.050\"],[\"2488.10\",\"0.080\"],[\"2488.08\",\"3.919\"],[\"2488.04\",\"40.103\"],[\"2488.03\",\"0.120\"],[\"2488.02\",\"0.008\"],[\"2488.01\",\"0.140\"],[\"2488.00\",\"0.406\"],[\"2487.99\",\"0.384\"],[\"2487.98\",\"0.060\"],[\"2487.96\",\"8.010\"],[\"2487.94\",\"0.246\"],[\"2487.93\",\"0.020\"],[\"2487.91\",\"0.136\"],[\"2487.87\",\"0.403\"],[\"2487.84\",\"17.910\"],[\"2487.81\",\"0.005\"],[\"2487.80\",\"0.073\"],[\"2487.74\",\"36.000\"],[\"2487.73\",\"3.225\"],[\"2487.72\",\"0.018\"],[\"2487.71\",\"0.319\"],[\"2487.70\",\"0.006\"],[\"2487.66\",\"0.003\"],[\"2487.64\",\"0.003\"],[\"2487.63\",\"0.008\"],[\"2487.62\",\"0.040\"],[\"2487.60\",\"3.916\"],[\"2487.54\",\"0.805\"],[\"2487.52\",\"0.022\"],[\"2487.51\",\"0.003\"],[\"2487.50\",\"0.051\"],[\"2487.49\",\"6.081\"],[\"2487.47\",\"80.015\"],[\"2487.46\",\"4.735\"],[\"2487.45\",\"30.000\"],[\"2487.41\",\"0.096\"],[\"2487.40\",\"0.078\"],[\"2487.39\",\"0.103\"],[\"2487.37\",\"2.279\"],[\"2487.36\",\"8.152\"],[\"2487.35\",\"2.145\"],[\"2487.32\",\"12.816\"],[\"2487.31\",\"10.023\"],[\"2487.30\",\"0.157\"],[\"2487.27\",\"0.005\"],[\"2487.26\",\"4.010\"],[\"2487.25\",\"0.008\"],[\"2487.24\",\"0.003\"],[\"2487.23\",\"0.014\"],[\"2487.20\",\"0.085\"],[\"2487.17\",\"0.011\"],[\"2487.14\",\"3.217\"],[\"2487.12\",\"3.916\"],[\"2487.11\",\"0.300\"],[\"2487.10\",\"0.088\"],[\"2487.08\",\"10.097\"],[\"2487.07\",\"1.467\"],[\"2487.04\",\"0.600\"],[\"2487.01\",\"18.363\"],[\"2487.00\",\"0.292\"],[\"2486.99\",\"0.014\"],[\"2486.98\",\"0.144\"],[\"2486.97\",\"0.443\"],[\"2486.92\",\"0.005\"],[\"2486.91\",\"0.016\"],[\"2486.89\",\"3.364\"],[\"2486.88\",\"4.166\"],[\"2486.84\",\"24.306\"],[\"2486.83\",\"0.181\"],[\"2486.81\",\"0.015\"],[\"2486.80\",\"0.082\"],[\"2486.79\",\"0.007\"],[\"2486.76\",\"0.011\"],[\"2486.74\",\"0.050\"],[\"2486.73\",\"0.782\"],[\"2486.72\",\"0.004\"],[\"2486.69\",\"0.003\"],[\"2486.68\",\"8.018\"],[\"2486.66\",\"10.004\"],[\"2486.65\",\"40.391\"],[\"2486.64\",\"3.916\"],[\"2486.61\",\"0.489\"],[\"2486.60\",\"0.196\"],[\"2486.57\",\"0.396\"],[\"2486.55\",\"4.015\"],[\"2486.51\",\"3.000\"],[\"2486.50\",\"0.003\"],[\"2486.48\",\"0.005\"],[\"2486.47\",\"0.010\"],[\"2486.45\",\"4.011\"],[\"2486.44\",\"0.602\"],[\"2486.43\",\"0.566\"],[\"2486.42\",\"3.140\"],[\"2486.40\",\"3.958\"],[\"2486.39\",\"0.003\"],[\"2486.34\",\"0.010\"],[\"2486.31\",\"6.281\"],[\"2486.27\",\"0.005\"],[\"2486.26\",\"0.004\"],[\"2486.23\",\"10.088\"],[\"2486.22\",\"0.015\"],[\"2486.17\",\"0.030\"],[\"2486.16\",\"3.916\"],[\"2486.15\",\"0.020\"],[\"2486.13\",\"13.130\"],[\"2486.12\",\"82.414\"],[\"2486.11\",\"0.244\"],[\"2486.10\",\"0.132\"],[\"2486.08\",\"0.720\"],[\"2486.06\",\"0.385\"],[\"2486.01\",\"0.004\"],[\"2486.00\",\"2.359\"],[\"2485.99\",\"154.159\"],[\"2485.98\",\"20.054\"],[\"2485.96\",\"1.000\"],[\"2485.95\",\"0.190\"],[\"2485.92\",\"4.463\"],[\"2485.90\",\"1.557\"],[\"2485.87\",\"0.402\"],[\"2485.85\",\"0.114\"],[\"2485.81\",\"0.900\"],[\"2485.76\",\"4.700\"],[\"2485.75\",\"0.300\"],[\"2485.74\",\"0.196\"],[\"2485.73\",\"4.010\"],[\"2485.72\",\"0.323\"],[\"2485.70\",\"0.263\"],[\"2485.69\",\"0.261\"],[\"2485.68\",\"3.688\"],[\"2485.67\",\"0.005\"],[\"2485.64\",\"1.216\"],[\"2485.63\",\"0.005\"],[\"2485.62\",\"0.015\"],[\"2485.61\",\"0.033\"],[\"2485.60\",\"0.004\"],[\"2485.58\",\"2.012\"],[\"2485.56\",\"0.020\"],[\"2485.54\",\"0.699\"],[\"2485.52\",\"0.003\"],[\"2485.51\",\"1.830\"],[\"2485.48\",\"5.964\"],[\"2485.47\",\"0.015\"],[\"2485.44\",\"7.251\"],[\"2485.43\",\"0.006\"],[\"2485.42\",\"0.644\"],[\"2485.40\",\"8.026\"],[\"2485.38\",\"0.489\"],[\"2485.36\",\"0.014\"],[\"2485.35\",\"0.005\"],[\"2485.31\",\"1.507\"],[\"2485.30\",\"2.107\"],[\"2485.29\",\"0.039\"],[\"2485.28\",\"0.642\"],[\"2485.26\",\"1.990\"],[\"2485.25\",\"4.996\"],[\"2485.23\",\"0.003\"],[\"2485.22\",\"0.277\"],[\"2485.21\",\"0.121\"],[\"2485.20\",\"3.952\"],[\"2485.18\",\"0.006\"],[\"2485.17\",\"0.043\"],[\"2485.15\",\"4.008\"],[\"2485.14\",\"4.434\"],[\"2485.13\",\"1.003\"],[\"2485.05\",\"0.204\"],[\"2485.04\",\"0.254\"],[\"2485.02\",\"5.000\"],[\"2485.01\",\"0.050\"],[\"2485.00\",\"80.821\"],[\"2484.96\",\"3.941\"],[\"2484.95\",\"10.023\"],[\"2484.94\",\"13.935\"],[\"2484.92\",\"0.059\"],[\"2484.90\",\"150.000\"],[\"2484.89\",\"0.004\"],[\"2484.88\",\"150.127\"],[\"2484.87\",\"0.004\"],[\"2484.85\",\"0.100\"],[\"2484.83\",\"0.006\"],[\"2484.82\",\"0.030\"],[\"2484.81\",\"1.246\"],[\"2484.80\",\"0.003\"],[\"2484.79\",\"0.045\"],[\"2484.77\",\"0.003\"],[\"2484.74\",\"0.036\"],[\"2484.72\",\"3.919\"],[\"2484.70\",\"0.134\"],[\"2484.68\",\"1.111\"],[\"2484.66\",\"76.955\"],[\"2484.60\",\"2.580\"],[\"2484.59\",\"31.432\"],[\"2484.58\",\"1.468\"],[\"2484.55\",\"1.153\"],[\"2484.54\",\"0.265\"],[\"2484.53\",\"20.024\"],[\"2484.51\",\"1.047\"],[\"2484.50\",\"0.818\"],[\"2484.49\",\"0.022\"],[\"2484.48\",\"3.887\"],[\"2484.46\",\"0.048\"],[\"2484.45\",\"0.224\"],[\"2484.44\",\"0.174\"],[\"2484.43\",\"223.079\"],[\"2484.42\",\"0.014\"],[\"2484.41\",\"1.115\"],[\"2484.39\",\"26.090\"],[\"2484.38\",\"0.066\"],[\"2484.37\",\"0.121\"],[\"2484.34\",\"0.255\"],[\"2484.33\",\"23.968\"],[\"2484.29\",\"0.085\"],[\"2484.27\",\"1.128\"],[\"2484.26\",\"1.456\"],[\"2484.24\",\"3.916\"],[\"2484.23\",\"28.126\"],[\"2484.22\",\"1.329\"],[\"2484.19\",\"2.015\"],[\"2484.18\",\"0.263\"],[\"2484.15\",\"15.489\"],[\"2484.14\",\"1.135\"],[\"2484.13\",\"0.572\"],[\"2484.12\",\"8.032\"],[\"2484.11\",\"0.021\"],[\"2484.09\",\"0.059\"],[\"2484.08\",\"0.038\"],[\"2484.07\",\"0.147\"],[\"2484.05\",\"24.156\"],[\"2484.04\",\"0.008\"],[\"2484.01\",\"1.184\"],[\"2484.00\",\"4.641\"],[\"2483.99\",\"0.006\"],[\"2483.97\",\"0.294\"],[\"2483.96\",\"0.424\"],[\"2483.94\",\"3.660\"],[\"2483.93\",\"2.067\"],[\"2483.92\",\"0.008\"],[\"2483.89\",\"0.141\"],[\"2483.88\",\"1.089\"],\n",
      "        #             [\"2483.87\",\"110.000\"],[\"2483.85\",\"4.018\"],[\"2483.81\",\"150.077\"],[\"2483.80\",\"0.003\"],[\"2483.77\",\"0.020\"]\n",
      "        #         ],\n",
      "        #         \"asks\":[\n",
      "        #             [\"2493.57\",\"0.877\"],\n",
      "        #             [\"2493.62\",\"0.063\"],\n",
      "        #             [\"2493.71\",\"12.054\"],\n",
      "        #         ]\n",
      "        #     }\n",
      "        timestamp = self.safe_integer(response, 'T')\n",
      "        orderbook = self.parse_order_book(response, symbol, timestamp)\n",
      "        orderbook['nonce'] = self.safe_integer(response, 'lastUpdateId')\n",
      "        return orderbook\n",
      "\n",
      "    def parse_ticker(self, ticker, market=None):\n",
      "        #\n",
      "        #     {\n",
      "        #         symbol: 'ETHBTC',\n",
      "        #         priceChange: '0.00068700',\n",
      "        #         priceChangePercent: '2.075',\n",
      "        #         weightedAvgPrice: '0.03342681',\n",
      "        #         prevClosePrice: '0.03310300',\n",
      "        #         lastPrice: '0.03378900',\n",
      "        #         lastQty: '0.07700000',\n",
      "        #         bidPrice: '0.03378900',\n",
      "        #         bidQty: '7.16800000',\n",
      "        #         askPrice: '0.03379000',\n",
      "        #         askQty: '24.00000000',\n",
      "        #         openPrice: '0.03310200',\n",
      "        #         highPrice: '0.03388900',\n",
      "        #         lowPrice: '0.03306900',\n",
      "        #         volume: '205478.41000000',\n",
      "        #         quoteVolume: '6868.48826294',\n",
      "        #         openTime: 1601469986932,\n",
      "        #         closeTime: 1601556386932,\n",
      "        #         firstId: 196098772,\n",
      "        #         lastId: 196186315,\n",
      "        #         count: 87544\n",
      "        #     }\n",
      "        #\n",
      "        # coinm\n",
      "        #     {\n",
      "        #         baseVolume: '214549.95171161',\n",
      "        #         closeTime: '1621965286847',\n",
      "        #         count: '1283779',\n",
      "        #         firstId: '152560106',\n",
      "        #         highPrice: '39938.3',\n",
      "        #         lastId: '153843955',\n",
      "        #         lastPrice: '37993.4',\n",
      "        #         lastQty: '1',\n",
      "        #         lowPrice: '36457.2',\n",
      "        #         openPrice: '37783.4',\n",
      "        #         openTime: '1621878840000',\n",
      "        #         pair: 'BTCUSD',\n",
      "        #         priceChange: '210.0',\n",
      "        #         priceChangePercent: '0.556',\n",
      "        #         symbol: 'BTCUSD_PERP',\n",
      "        #         volume: '81990451',\n",
      "        #         weightedAvgPrice: '38215.08713747'\n",
      "        #     }\n",
      "        #\n",
      "        timestamp = self.safe_integer(ticker, 'closeTime')\n",
      "        marketId = self.safe_string(ticker, 'symbol')\n",
      "        symbol = self.safe_symbol(marketId, market)\n",
      "        last = self.safe_number(ticker, 'lastPrice')\n",
      "        isCoinm = ('baseVolume' in ticker)\n",
      "        baseVolume = None\n",
      "        quoteVolume = None\n",
      "        if isCoinm:\n",
      "            baseVolume = self.safe_number(ticker, 'baseVolume')\n",
      "            quoteVolume = self.safe_number(ticker, 'volume')\n",
      "        else:\n",
      "            baseVolume = self.safe_number(ticker, 'volume')\n",
      "            quoteVolume = self.safe_number(ticker, 'quoteVolume')\n",
      "        return self.safe_ticker({\n",
      "            'symbol': symbol,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'high': self.safe_number(ticker, 'highPrice'),\n",
      "            'low': self.safe_number(ticker, 'lowPrice'),\n",
      "            'bid': self.safe_number(ticker, 'bidPrice'),\n",
      "            'bidVolume': self.safe_number(ticker, 'bidQty'),\n",
      "            'ask': self.safe_number(ticker, 'askPrice'),\n",
      "            'askVolume': self.safe_number(ticker, 'askQty'),\n",
      "            'vwap': self.safe_number(ticker, 'weightedAvgPrice'),\n",
      "            'open': self.safe_number(ticker, 'openPrice'),\n",
      "            'close': last,\n",
      "            'last': last,\n",
      "            'previousClose': self.safe_number(ticker, 'prevClosePrice'),  # previous day close\n",
      "            'change': self.safe_number(ticker, 'priceChange'),\n",
      "            'percentage': self.safe_number(ticker, 'priceChangePercent'),\n",
      "            'average': None,\n",
      "            'baseVolume': baseVolume,\n",
      "            'quoteVolume': quoteVolume,\n",
      "            'info': ticker,\n",
      "        }, market)\n",
      "\n",
      "    def fetch_status(self, params={}):\n",
      "        response = self.sapiGetSystemStatus(params)\n",
      "        status = self.safe_string(response, 'status')\n",
      "        if status is not None:\n",
      "            status = 'ok' if (status == '0') else 'maintenance'\n",
      "            self.status = self.extend(self.status, {\n",
      "                'status': status,\n",
      "                'updated': self.milliseconds(),\n",
      "            })\n",
      "        return self.status\n",
      "\n",
      "    def fetch_ticker(self, symbol, params={}):\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        method = 'publicGetTicker24hr'\n",
      "        if market['linear']:\n",
      "            method = 'fapiPublicGetTicker24hr'\n",
      "        elif market['inverse']:\n",
      "            method = 'dapiPublicGetTicker24hr'\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        if isinstance(response, list):\n",
      "            firstTicker = self.safe_value(response, 0, {})\n",
      "            return self.parse_ticker(firstTicker, market)\n",
      "        return self.parse_ticker(response, market)\n",
      "\n",
      "    def fetch_bids_asks(self, symbols=None, params={}):\n",
      "        self.load_markets()\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchBidsAsks', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        method = None\n",
      "        if type == 'future':\n",
      "            method = 'fapiPublicGetTickerBookTicker'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPublicGetTickerBookTicker'\n",
      "        else:\n",
      "            method = 'publicGetTickerBookTicker'\n",
      "        response = getattr(self, method)(query)\n",
      "        return self.parse_tickers(response, symbols)\n",
      "\n",
      "    def fetch_tickers(self, symbols=None, params={}):\n",
      "        self.load_markets()\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchTickers', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        defaultMethod = None\n",
      "        if type == 'future':\n",
      "            defaultMethod = 'fapiPublicGetTicker24hr'\n",
      "        elif type == 'delivery':\n",
      "            defaultMethod = 'dapiPublicGetTicker24hr'\n",
      "        else:\n",
      "            defaultMethod = 'publicGetTicker24hr'\n",
      "        method = self.safe_string(self.options, 'fetchTickersMethod', defaultMethod)\n",
      "        response = getattr(self, method)(query)\n",
      "        return self.parse_tickers(response, symbols)\n",
      "\n",
      "    def parse_ohlcv(self, ohlcv, market=None):\n",
      "        # when api method = publicGetKlines or fapiPublicGetKlines or dapiPublicGetKlines\n",
      "        #     [\n",
      "        #         1591478520000,  # open time\n",
      "        #         \"0.02501300\",  # open\n",
      "        #         \"0.02501800\",  # high\n",
      "        #         \"0.02500000\",  # low\n",
      "        #         \"0.02500000\",  # close\n",
      "        #         \"22.19000000\",  # volume\n",
      "        #         1591478579999,  # close time\n",
      "        #         \"0.55490906\",  # quote asset volume\n",
      "        #         40,            # number of trades\n",
      "        #         \"10.92900000\",  # taker buy base asset volume\n",
      "        #         \"0.27336462\",  # taker buy quote asset volume\n",
      "        #         \"0\"            # ignore\n",
      "        #     ]\n",
      "        #\n",
      "        #  when api method = fapiPublicGetMarkPriceKlines or fapiPublicGetIndexPriceKlines\n",
      "        #     [\n",
      "        #         [\n",
      "        #         1591256460000,          # Open time\n",
      "        #         \"9653.29201333\",        # Open\n",
      "        #         \"9654.56401333\",        # High\n",
      "        #         \"9653.07367333\",        # Low\n",
      "        #         \"9653.07367333\",        # Close(or latest price)\n",
      "        #         \"0\",                    # Ignore\n",
      "        #         1591256519999,          # Close time\n",
      "        #         \"0\",                    # Ignore\n",
      "        #         60,                     # Number of bisic data\n",
      "        #         \"0\",                    # Ignore\n",
      "        #         \"0\",                    # Ignore\n",
      "        #         \"0\"                     # Ignore\n",
      "        #         ]\n",
      "        #     ]\n",
      "        #\n",
      "        return [\n",
      "            self.safe_integer(ohlcv, 0),\n",
      "            self.safe_number(ohlcv, 1),\n",
      "            self.safe_number(ohlcv, 2),\n",
      "            self.safe_number(ohlcv, 3),\n",
      "            self.safe_number(ohlcv, 4),\n",
      "            self.safe_number(ohlcv, 5),\n",
      "        ]\n",
      "\n",
      "    def fetch_ohlcv(self, symbol, timeframe='1m', since=None, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        # binance docs say that the default limit 500, max 1500 for futures, max 1000 for spot markets\n",
      "        # the reality is that the time range wider than 500 candles won't work right\n",
      "        defaultLimit = 500\n",
      "        maxLimit = 1500\n",
      "        price = self.safe_string(params, 'price')\n",
      "        params = self.omit(params, 'price')\n",
      "        limit = defaultLimit if (limit is None) else min(limit, maxLimit)\n",
      "        request = {\n",
      "            'interval': self.timeframes[timeframe],\n",
      "            'limit': limit,\n",
      "        }\n",
      "        if price == 'index':\n",
      "            request['pair'] = market['id']   # Index price takes self argument instead of symbol\n",
      "        else:\n",
      "            request['symbol'] = market['id']\n",
      "        # duration = self.parse_timeframe(timeframe)\n",
      "        if since is not None:\n",
      "            request['startTime'] = since\n",
      "            #\n",
      "            # It didn't work before without the endTime\n",
      "            # https://github.com/ccxt/ccxt/issues/8454\n",
      "            #\n",
      "            # if since > 0:\n",
      "            #     endTime = self.sum(since, limit * duration * 1000 - 1)\n",
      "            #     now = self.milliseconds()\n",
      "            #     request['endTime'] = min(now, endTime)\n",
      "            # }\n",
      "        method = 'publicGetKlines'\n",
      "        if price == 'mark':\n",
      "            if market['inverse']:\n",
      "                method = 'dapiPublicGetMarkPriceKlines'\n",
      "            else:\n",
      "                method = 'fapiPublicGetMarkPriceKlines'\n",
      "        elif price == 'index':\n",
      "            if market['inverse']:\n",
      "                method = 'dapiPublicGetIndexPriceKlines'\n",
      "            else:\n",
      "                method = 'fapiPublicGetIndexPriceKlines'\n",
      "        elif market['linear']:\n",
      "            method = 'fapiPublicGetKlines'\n",
      "        elif market['inverse']:\n",
      "            method = 'dapiPublicGetKlines'\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        #\n",
      "        #     [\n",
      "        #         [1591478520000,\"0.02501300\",\"0.02501800\",\"0.02500000\",\"0.02500000\",\"22.19000000\",1591478579999,\"0.55490906\",40,\"10.92900000\",\"0.27336462\",\"0\"],\n",
      "        #         [1591478580000,\"0.02499600\",\"0.02500900\",\"0.02499400\",\"0.02500300\",\"21.34700000\",1591478639999,\"0.53370468\",24,\"7.53800000\",\"0.18850725\",\"0\"],\n",
      "        #         [1591478640000,\"0.02500800\",\"0.02501100\",\"0.02500300\",\"0.02500800\",\"154.14200000\",1591478699999,\"3.85405839\",97,\"5.32300000\",\"0.13312641\",\"0\"],\n",
      "        #     ]\n",
      "        #\n",
      "        return self.parse_ohlcvs(response, market, timeframe, since, limit)\n",
      "\n",
      "    def fetch_mark_ohlcv(self, symbol, timeframe='1m', since=None, limit=None, params={}):\n",
      "        request = {\n",
      "            'price': 'mark',\n",
      "        }\n",
      "        return self.fetch_ohlcv(symbol, timeframe, since, limit, self.extend(request, params))\n",
      "\n",
      "    def fetch_index_ohlcv(self, symbol, timeframe='1m', since=None, limit=None, params={}):\n",
      "        request = {\n",
      "            'price': 'index',\n",
      "        }\n",
      "        return self.fetch_ohlcv(symbol, timeframe, since, limit, self.extend(request, params))\n",
      "\n",
      "    def parse_trade(self, trade, market=None):\n",
      "        if 'isDustTrade' in trade:\n",
      "            return self.parse_dust_trade(trade, market)\n",
      "        #\n",
      "        # aggregate trades\n",
      "        # https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#compressedaggregate-trades-list\n",
      "        #\n",
      "        #     {\n",
      "        #         \"a\": 26129,         # Aggregate tradeId\n",
      "        #         \"p\": \"0.01633102\",  # Price\n",
      "        #         \"q\": \"4.70443515\",  # Quantity\n",
      "        #         \"f\": 27781,         # First tradeId\n",
      "        #         \"l\": 27781,         # Last tradeId\n",
      "        #         \"T\": 1498793709153,  # Timestamp\n",
      "        #         \"m\": True,          # Was the buyer the maker?\n",
      "        #         \"M\": True           # Was the trade the best price match?\n",
      "        #     }\n",
      "        #\n",
      "        # recent public trades and old public trades\n",
      "        # https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#recent-trades-list\n",
      "        # https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#old-trade-lookup-market_data\n",
      "        #\n",
      "        #     {\n",
      "        #         \"id\": 28457,\n",
      "        #         \"price\": \"4.00000100\",\n",
      "        #         \"qty\": \"12.00000000\",\n",
      "        #         \"time\": 1499865549590,\n",
      "        #         \"isBuyerMaker\": True,\n",
      "        #         \"isBestMatch\": True\n",
      "        #     }\n",
      "        #\n",
      "        # private trades\n",
      "        # https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#account-trade-list-user_data\n",
      "        #\n",
      "        #     {\n",
      "        #         \"symbol\": \"BNBBTC\",\n",
      "        #         \"id\": 28457,\n",
      "        #         \"orderId\": 100234,\n",
      "        #         \"price\": \"4.00000100\",\n",
      "        #         \"qty\": \"12.00000000\",\n",
      "        #         \"commission\": \"10.10000000\",\n",
      "        #         \"commissionAsset\": \"BNB\",\n",
      "        #         \"time\": 1499865549590,\n",
      "        #         \"isBuyer\": True,\n",
      "        #         \"isMaker\": False,\n",
      "        #         \"isBestMatch\": True\n",
      "        #     }\n",
      "        #\n",
      "        # futures trades\n",
      "        # https://binance-docs.github.io/apidocs/futures/en/#account-trade-list-user_data\n",
      "        #\n",
      "        #     {\n",
      "        #       \"accountId\": 20,\n",
      "        #       \"buyer\": False,\n",
      "        #       \"commission\": \"-0.07819010\",\n",
      "        #       \"commissionAsset\": \"USDT\",\n",
      "        #       \"counterPartyId\": 653,\n",
      "        #       \"id\": 698759,\n",
      "        #       \"maker\": False,\n",
      "        #       \"orderId\": 25851813,\n",
      "        #       \"price\": \"7819.01\",\n",
      "        #       \"qty\": \"0.002\",\n",
      "        #       \"quoteQty\": \"0.01563\",\n",
      "        #       \"realizedPnl\": \"-0.91539999\",\n",
      "        #       \"side\": \"SELL\",\n",
      "        #       \"symbol\": \"BTCUSDT\",\n",
      "        #       \"time\": 1569514978020\n",
      "        #     }\n",
      "        #     {\n",
      "        #       \"symbol\": \"BTCUSDT\",\n",
      "        #       \"id\": 477128891,\n",
      "        #       \"orderId\": 13809777875,\n",
      "        #       \"side\": \"SELL\",\n",
      "        #       \"price\": \"38479.55\",\n",
      "        #       \"qty\": \"0.001\",\n",
      "        #       \"realizedPnl\": \"-0.00009534\",\n",
      "        #       \"marginAsset\": \"USDT\",\n",
      "        #       \"quoteQty\": \"38.47955\",\n",
      "        #       \"commission\": \"-0.00076959\",\n",
      "        #       \"commissionAsset\": \"USDT\",\n",
      "        #       \"time\": 1612733566708,\n",
      "        #       \"positionSide\": \"BOTH\",\n",
      "        #       \"maker\": True,\n",
      "        #       \"buyer\": False\n",
      "        #     }\n",
      "        #\n",
      "        # {respType: FULL}\n",
      "        #\n",
      "        #     {\n",
      "        #       \"price\": \"4000.00000000\",\n",
      "        #       \"qty\": \"1.00000000\",\n",
      "        #       \"commission\": \"4.00000000\",\n",
      "        #       \"commissionAsset\": \"USDT\",\n",
      "        #       \"tradeId\": \"1234\",\n",
      "        #     }\n",
      "        #\n",
      "        timestamp = self.safe_integer_2(trade, 'T', 'time')\n",
      "        price = self.safe_string_2(trade, 'p', 'price')\n",
      "        amount = self.safe_string_2(trade, 'q', 'qty')\n",
      "        cost = self.safe_string_2(trade, 'quoteQty', 'baseQty')  # inverse futures\n",
      "        marketId = self.safe_string(trade, 'symbol')\n",
      "        symbol = self.safe_symbol(marketId, market)\n",
      "        id = self.safe_string_2(trade, 't', 'a')\n",
      "        id = self.safe_string_2(trade, 'id', 'tradeId', id)\n",
      "        side = None\n",
      "        orderId = self.safe_string(trade, 'orderId')\n",
      "        if 'm' in trade:\n",
      "            side = 'sell' if trade['m'] else 'buy'  # self is reversed intentionally\n",
      "        elif 'isBuyerMaker' in trade:\n",
      "            side = 'sell' if trade['isBuyerMaker'] else 'buy'\n",
      "        elif 'side' in trade:\n",
      "            side = self.safe_string_lower(trade, 'side')\n",
      "        else:\n",
      "            if 'isBuyer' in trade:\n",
      "                side = 'buy' if trade['isBuyer'] else 'sell'  # self is a True side\n",
      "        fee = None\n",
      "        if 'commission' in trade:\n",
      "            fee = {\n",
      "                'cost': self.safe_string(trade, 'commission'),\n",
      "                'currency': self.safe_currency_code(self.safe_string(trade, 'commissionAsset')),\n",
      "            }\n",
      "        takerOrMaker = None\n",
      "        if 'isMaker' in trade:\n",
      "            takerOrMaker = 'maker' if trade['isMaker'] else 'taker'\n",
      "        if 'maker' in trade:\n",
      "            takerOrMaker = 'maker' if trade['maker'] else 'taker'\n",
      "        return self.safe_trade({\n",
      "            'info': trade,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'symbol': symbol,\n",
      "            'id': id,\n",
      "            'order': orderId,\n",
      "            'type': None,\n",
      "            'side': side,\n",
      "            'takerOrMaker': takerOrMaker,\n",
      "            'price': price,\n",
      "            'amount': amount,\n",
      "            'cost': cost,\n",
      "            'fee': fee,\n",
      "        }, market)\n",
      "\n",
      "    def fetch_trades(self, symbol, since=None, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "            # 'fromId': 123,    # ID to get aggregate trades from INCLUSIVE.\n",
      "            # 'startTime': 456,  # Timestamp in ms to get aggregate trades from INCLUSIVE.\n",
      "            # 'endTime': 789,   # Timestamp in ms to get aggregate trades until INCLUSIVE.\n",
      "            # 'limit': 500,     # default = 500, maximum = 1000\n",
      "        }\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchTrades', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        defaultMethod = None\n",
      "        if type == 'future':\n",
      "            defaultMethod = 'fapiPublicGetAggTrades'\n",
      "        elif type == 'delivery':\n",
      "            defaultMethod = 'dapiPublicGetAggTrades'\n",
      "        else:\n",
      "            defaultMethod = 'publicGetAggTrades'\n",
      "        method = self.safe_string(self.options, 'fetchTradesMethod', defaultMethod)\n",
      "        if method == 'publicGetAggTrades':\n",
      "            if since is not None:\n",
      "                request['startTime'] = since\n",
      "                # https://github.com/ccxt/ccxt/issues/6400\n",
      "                # https://github.com/binance-exchange/binance-official-api-docs/blob/master/rest-api.md#compressedaggregate-trades-list\n",
      "                request['endTime'] = self.sum(since, 3600000)\n",
      "            if type == 'future':\n",
      "                method = 'fapiPublicGetAggTrades'\n",
      "            elif type == 'delivery':\n",
      "                method = 'dapiPublicGetAggTrades'\n",
      "        elif method == 'publicGetHistoricalTrades':\n",
      "            if type == 'future':\n",
      "                method = 'fapiPublicGetHistoricalTrades'\n",
      "            elif type == 'delivery':\n",
      "                method = 'dapiPublicGetHistoricalTrades'\n",
      "        if limit is not None:\n",
      "            request['limit'] = limit  # default = 500, maximum = 1000\n",
      "        #\n",
      "        # Caveats:\n",
      "        # - default limit(500) applies only if no other parameters set, trades up\n",
      "        #   to the maximum limit may be returned to satisfy other parameters\n",
      "        # - if both limit and time window is set and time window contains more\n",
      "        #   trades than the limit then the last trades from the window are returned\n",
      "        # - 'tradeId' accepted and returned by self method is \"aggregate\" trade id\n",
      "        #   which is different from actual trade id\n",
      "        # - setting both fromId and time window results in error\n",
      "        response = getattr(self, method)(self.extend(request, query))\n",
      "        #\n",
      "        # aggregate trades\n",
      "        #\n",
      "        #     [\n",
      "        #         {\n",
      "        #             \"a\": 26129,         # Aggregate tradeId\n",
      "        #             \"p\": \"0.01633102\",  # Price\n",
      "        #             \"q\": \"4.70443515\",  # Quantity\n",
      "        #             \"f\": 27781,         # First tradeId\n",
      "        #             \"l\": 27781,         # Last tradeId\n",
      "        #             \"T\": 1498793709153,  # Timestamp\n",
      "        #             \"m\": True,          # Was the buyer the maker?\n",
      "        #             \"M\": True           # Was the trade the best price match?\n",
      "        #         }\n",
      "        #     ]\n",
      "        #\n",
      "        # recent public trades and historical public trades\n",
      "        #\n",
      "        #     [\n",
      "        #         {\n",
      "        #             \"id\": 28457,\n",
      "        #             \"price\": \"4.00000100\",\n",
      "        #             \"qty\": \"12.00000000\",\n",
      "        #             \"time\": 1499865549590,\n",
      "        #             \"isBuyerMaker\": True,\n",
      "        #             \"isBestMatch\": True\n",
      "        #         }\n",
      "        #     ]\n",
      "        #\n",
      "        return self.parse_trades(response, market, since, limit)\n",
      "\n",
      "    def parse_order_status(self, status):\n",
      "        statuses = {\n",
      "            'NEW': 'open',\n",
      "            'PARTIALLY_FILLED': 'open',\n",
      "            'FILLED': 'closed',\n",
      "            'CANCELED': 'canceled',\n",
      "            'PENDING_CANCEL': 'canceling',  # currently unused\n",
      "            'REJECTED': 'rejected',\n",
      "            'EXPIRED': 'expired',\n",
      "        }\n",
      "        return self.safe_string(statuses, status, status)\n",
      "\n",
      "    def parse_order(self, order, market=None):\n",
      "        #\n",
      "        # spot\n",
      "        #\n",
      "        #     {\n",
      "        #         \"symbol\": \"LTCBTC\",\n",
      "        #         \"orderId\": 1,\n",
      "        #         \"clientOrderId\": \"myOrder1\",\n",
      "        #         \"price\": \"0.1\",\n",
      "        #         \"origQty\": \"1.0\",\n",
      "        #         \"executedQty\": \"0.0\",\n",
      "        #         \"cummulativeQuoteQty\": \"0.0\",\n",
      "        #         \"status\": \"NEW\",\n",
      "        #         \"timeInForce\": \"GTC\",\n",
      "        #         \"type\": \"LIMIT\",\n",
      "        #         \"side\": \"BUY\",\n",
      "        #         \"stopPrice\": \"0.0\",\n",
      "        #         \"icebergQty\": \"0.0\",\n",
      "        #         \"time\": 1499827319559,\n",
      "        #         \"updateTime\": 1499827319559,\n",
      "        #         \"isWorking\": True\n",
      "        #     }\n",
      "        #\n",
      "        # futures\n",
      "        #\n",
      "        #     {\n",
      "        #         \"symbol\": \"BTCUSDT\",\n",
      "        #         \"orderId\": 1,\n",
      "        #         \"clientOrderId\": \"myOrder1\",\n",
      "        #         \"price\": \"0.1\",\n",
      "        #         \"origQty\": \"1.0\",\n",
      "        #         \"executedQty\": \"1.0\",\n",
      "        #         \"cumQuote\": \"10.0\",\n",
      "        #         \"status\": \"NEW\",\n",
      "        #         \"timeInForce\": \"GTC\",\n",
      "        #         \"type\": \"LIMIT\",\n",
      "        #         \"side\": \"BUY\",\n",
      "        #         \"stopPrice\": \"0.0\",\n",
      "        #         \"updateTime\": 1499827319559\n",
      "        #     }\n",
      "        #\n",
      "        # createOrder with {\"newOrderRespType\": \"FULL\"}\n",
      "        #\n",
      "        #     {\n",
      "        #       \"symbol\": \"BTCUSDT\",\n",
      "        #       \"orderId\": 5403233939,\n",
      "        #       \"orderListId\": -1,\n",
      "        #       \"clientOrderId\": \"x-R4BD3S825e669e75b6c14f69a2c43e\",\n",
      "        #       \"transactTime\": 1617151923742,\n",
      "        #       \"price\": \"0.00000000\",\n",
      "        #       \"origQty\": \"0.00050000\",\n",
      "        #       \"executedQty\": \"0.00050000\",\n",
      "        #       \"cummulativeQuoteQty\": \"29.47081500\",\n",
      "        #       \"status\": \"FILLED\",\n",
      "        #       \"timeInForce\": \"GTC\",\n",
      "        #       \"type\": \"MARKET\",\n",
      "        #       \"side\": \"BUY\",\n",
      "        #       \"fills\": [\n",
      "        #         {\n",
      "        #           \"price\": \"58941.63000000\",\n",
      "        #           \"qty\": \"0.00050000\",\n",
      "        #           \"commission\": \"0.00007050\",\n",
      "        #           \"commissionAsset\": \"BNB\",\n",
      "        #           \"tradeId\": 737466631\n",
      "        #         }\n",
      "        #       ]\n",
      "        #     }\n",
      "        #\n",
      "        # delivery\n",
      "        #\n",
      "        #     {\n",
      "        #       \"orderId\": \"18742727411\",\n",
      "        #       \"symbol\": \"ETHUSD_PERP\",\n",
      "        #       \"pair\": \"ETHUSD\",\n",
      "        #       \"status\": \"FILLED\",\n",
      "        #       \"clientOrderId\": \"x-xcKtGhcu3e2d1503fdd543b3b02419\",\n",
      "        #       \"price\": \"0\",\n",
      "        #       \"avgPrice\": \"4522.14\",\n",
      "        #       \"origQty\": \"1\",\n",
      "        #       \"executedQty\": \"1\",\n",
      "        #       \"cumBase\": \"0.00221134\",\n",
      "        #       \"timeInForce\": \"GTC\",\n",
      "        #       \"type\": \"MARKET\",\n",
      "        #       \"reduceOnly\": False,\n",
      "        #       \"closePosition\": False,\n",
      "        #       \"side\": \"SELL\",\n",
      "        #       \"positionSide\": \"BOTH\",\n",
      "        #       \"stopPrice\": \"0\",\n",
      "        #       \"workingType\": \"CONTRACT_PRICE\",\n",
      "        #       \"priceProtect\": False,\n",
      "        #       \"origType\": \"MARKET\",\n",
      "        #       \"time\": \"1636061952660\",\n",
      "        #       \"updateTime\": \"1636061952660\"\n",
      "        #     }\n",
      "        #\n",
      "        status = self.parse_order_status(self.safe_string(order, 'status'))\n",
      "        marketId = self.safe_string(order, 'symbol')\n",
      "        symbol = self.safe_symbol(marketId, market)\n",
      "        filled = self.safe_string(order, 'executedQty', '0')\n",
      "        timestamp = None\n",
      "        lastTradeTimestamp = None\n",
      "        if 'time' in order:\n",
      "            timestamp = self.safe_integer(order, 'time')\n",
      "        elif 'transactTime' in order:\n",
      "            timestamp = self.safe_integer(order, 'transactTime')\n",
      "        elif 'updateTime' in order:\n",
      "            if status == 'open':\n",
      "                if Precise.string_gt(filled, '0'):\n",
      "                    lastTradeTimestamp = self.safe_integer(order, 'updateTime')\n",
      "                else:\n",
      "                    timestamp = self.safe_integer(order, 'updateTime')\n",
      "        average = self.safe_string(order, 'avgPrice')\n",
      "        price = self.safe_string(order, 'price')\n",
      "        amount = self.safe_string(order, 'origQty')\n",
      "        # - Spot/Margin market: cummulativeQuoteQty\n",
      "        # - Futures market: cumQuote.\n",
      "        #   Note self is not the actual cost, since Binance futures uses leverage to calculate margins.\n",
      "        cost = self.safe_string_2(order, 'cummulativeQuoteQty', 'cumQuote')\n",
      "        cost = self.safe_string(order, 'cumBase', cost)\n",
      "        id = self.safe_string(order, 'orderId')\n",
      "        type = self.safe_string_lower(order, 'type')\n",
      "        side = self.safe_string_lower(order, 'side')\n",
      "        fills = self.safe_value(order, 'fills', [])\n",
      "        clientOrderId = self.safe_string(order, 'clientOrderId')\n",
      "        timeInForce = self.safe_string(order, 'timeInForce')\n",
      "        postOnly = (type == 'limit_maker') or (timeInForce == 'GTX')\n",
      "        if type == 'limit_maker':\n",
      "            type = 'limit'\n",
      "        stopPriceString = self.safe_string(order, 'stopPrice')\n",
      "        stopPrice = self.parse_number(self.omit_zero(stopPriceString))\n",
      "        return self.safe_order2({\n",
      "            'info': order,\n",
      "            'id': id,\n",
      "            'clientOrderId': clientOrderId,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'lastTradeTimestamp': lastTradeTimestamp,\n",
      "            'symbol': symbol,\n",
      "            'type': type,\n",
      "            'timeInForce': timeInForce,\n",
      "            'postOnly': postOnly,\n",
      "            'side': side,\n",
      "            'price': price,\n",
      "            'stopPrice': stopPrice,\n",
      "            'amount': amount,\n",
      "            'cost': cost,\n",
      "            'average': average,\n",
      "            'filled': filled,\n",
      "            'remaining': None,\n",
      "            'status': status,\n",
      "            'fee': None,\n",
      "            'trades': fills,\n",
      "        }, market)\n",
      "\n",
      "    def create_reduce_only_order(self, symbol, type, side, amount, price=None, params={}):\n",
      "        request = {\n",
      "            'reduceOnly': True,\n",
      "        }\n",
      "        return self.create_order(symbol, type, side, amount, price, self.extend(request, params))\n",
      "\n",
      "    def create_order(self, symbol, type, side, amount, price=None, params={}):\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        defaultType = self.safe_string_2(self.options, 'createOrder', 'defaultType', 'spot')\n",
      "        orderType = self.safe_string(params, 'type', defaultType)\n",
      "        clientOrderId = self.safe_string_2(params, 'newClientOrderId', 'clientOrderId')\n",
      "        postOnly = self.safe_value(params, 'postOnly', False)\n",
      "        params = self.omit(params, ['type', 'newClientOrderId', 'clientOrderId', 'postOnly'])\n",
      "        reduceOnly = self.safe_value(params, 'reduceOnly')\n",
      "        if reduceOnly is not None:\n",
      "            if (orderType != 'future') and (orderType != 'delivery'):\n",
      "                raise InvalidOrder(self.id + ' createOrder() does not support reduceOnly for ' + orderType + ' orders, reduceOnly orders are supported for futures and perpetuals only')\n",
      "        method = 'privatePostOrder'\n",
      "        if orderType == 'future':\n",
      "            method = 'fapiPrivatePostOrder'\n",
      "        elif orderType == 'delivery':\n",
      "            method = 'dapiPrivatePostOrder'\n",
      "        elif orderType == 'margin':\n",
      "            method = 'sapiPostMarginOrder'\n",
      "        # the next 5 lines are added to support for testing orders\n",
      "        if market['spot']:\n",
      "            test = self.safe_value(params, 'test', False)\n",
      "            if test:\n",
      "                method += 'Test'\n",
      "            params = self.omit(params, 'test')\n",
      "            # only supported for spot/margin api(all margin markets are spot markets)\n",
      "            if postOnly:\n",
      "                type = 'LIMIT_MAKER'\n",
      "        uppercaseType = type.upper()\n",
      "        validOrderTypes = self.safe_value(market['info'], 'orderTypes')\n",
      "        if not self.in_array(uppercaseType, validOrderTypes):\n",
      "            raise InvalidOrder(self.id + ' ' + type + ' is not a valid order type in market ' + symbol)\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "            'type': uppercaseType,\n",
      "            'side': side.upper(),\n",
      "        }\n",
      "        if clientOrderId is None:\n",
      "            broker = self.safe_value(self.options, 'broker')\n",
      "            if broker is not None:\n",
      "                brokerId = self.safe_string(broker, orderType)\n",
      "                if brokerId is not None:\n",
      "                    request['newClientOrderId'] = brokerId + self.uuid22()\n",
      "        else:\n",
      "            request['newClientOrderId'] = clientOrderId\n",
      "        if (orderType == 'spot') or (orderType == 'margin'):\n",
      "            request['newOrderRespType'] = self.safe_value(self.options['newOrderRespType'], type, 'RESULT')  # 'ACK' for order id, 'RESULT' for full order or 'FULL' for order with fills\n",
      "        else:\n",
      "            # delivery and future\n",
      "            request['newOrderRespType'] = 'RESULT'  # \"ACK\", \"RESULT\", default \"ACK\"\n",
      "        # additional required fields depending on the order type\n",
      "        timeInForceIsRequired = False\n",
      "        priceIsRequired = False\n",
      "        stopPriceIsRequired = False\n",
      "        quantityIsRequired = False\n",
      "        #\n",
      "        # spot/margin\n",
      "        #\n",
      "        #     LIMIT                timeInForce, quantity, price\n",
      "        #     MARKET               quantity or quoteOrderQty\n",
      "        #     STOP_LOSS            quantity, stopPrice\n",
      "        #     STOP_LOSS_LIMIT      timeInForce, quantity, price, stopPrice\n",
      "        #     TAKE_PROFIT          quantity, stopPrice\n",
      "        #     TAKE_PROFIT_LIMIT    timeInForce, quantity, price, stopPrice\n",
      "        #     LIMIT_MAKER          quantity, price\n",
      "        #\n",
      "        # futures\n",
      "        #\n",
      "        #     LIMIT                timeInForce, quantity, price\n",
      "        #     MARKET               quantity\n",
      "        #     STOP/TAKE_PROFIT     quantity, price, stopPrice\n",
      "        #     STOP_MARKET          stopPrice\n",
      "        #     TAKE_PROFIT_MARKET   stopPrice\n",
      "        #     TRAILING_STOP_MARKET callbackRate\n",
      "        #\n",
      "        if uppercaseType == 'MARKET':\n",
      "            quoteOrderQty = self.safe_value(self.options, 'quoteOrderQty', False)\n",
      "            if quoteOrderQty:\n",
      "                quoteOrderQty = self.safe_number(params, 'quoteOrderQty')\n",
      "                precision = market['precision']['price']\n",
      "                if quoteOrderQty is not None:\n",
      "                    request['quoteOrderQty'] = self.decimal_to_precision(quoteOrderQty, TRUNCATE, precision, self.precisionMode)\n",
      "                    params = self.omit(params, 'quoteOrderQty')\n",
      "                elif price is not None:\n",
      "                    request['quoteOrderQty'] = self.decimal_to_precision(amount * price, TRUNCATE, precision, self.precisionMode)\n",
      "                else:\n",
      "                    quantityIsRequired = True\n",
      "            else:\n",
      "                quantityIsRequired = True\n",
      "        elif uppercaseType == 'LIMIT':\n",
      "            priceIsRequired = True\n",
      "            timeInForceIsRequired = True\n",
      "            quantityIsRequired = True\n",
      "        elif (uppercaseType == 'STOP_LOSS') or (uppercaseType == 'TAKE_PROFIT'):\n",
      "            stopPriceIsRequired = True\n",
      "            quantityIsRequired = True\n",
      "            if market['linear'] or market['inverse']:\n",
      "                priceIsRequired = True\n",
      "        elif (uppercaseType == 'STOP_LOSS_LIMIT') or (uppercaseType == 'TAKE_PROFIT_LIMIT'):\n",
      "            quantityIsRequired = True\n",
      "            stopPriceIsRequired = True\n",
      "            priceIsRequired = True\n",
      "            timeInForceIsRequired = True\n",
      "        elif uppercaseType == 'LIMIT_MAKER':\n",
      "            priceIsRequired = True\n",
      "            quantityIsRequired = True\n",
      "        elif uppercaseType == 'STOP':\n",
      "            quantityIsRequired = True\n",
      "            stopPriceIsRequired = True\n",
      "            priceIsRequired = True\n",
      "        elif (uppercaseType == 'STOP_MARKET') or (uppercaseType == 'TAKE_PROFIT_MARKET'):\n",
      "            closePosition = self.safe_value(params, 'closePosition')\n",
      "            if closePosition is None:\n",
      "                quantityIsRequired = True\n",
      "            stopPriceIsRequired = True\n",
      "        elif uppercaseType == 'TRAILING_STOP_MARKET':\n",
      "            quantityIsRequired = True\n",
      "            callbackRate = self.safe_number(params, 'callbackRate')\n",
      "            if callbackRate is None:\n",
      "                raise InvalidOrder(self.id + ' createOrder() requires a callbackRate extra param for a ' + type + ' order')\n",
      "        if quantityIsRequired:\n",
      "            request['quantity'] = self.amount_to_precision(symbol, amount)\n",
      "        if priceIsRequired:\n",
      "            if price is None:\n",
      "                raise InvalidOrder(self.id + ' createOrder() requires a price argument for a ' + type + ' order')\n",
      "            request['price'] = self.price_to_precision(symbol, price)\n",
      "        if timeInForceIsRequired:\n",
      "            request['timeInForce'] = self.options['defaultTimeInForce']  # 'GTC' = Good To Cancel(default), 'IOC' = Immediate Or Cancel\n",
      "        if stopPriceIsRequired:\n",
      "            stopPrice = self.safe_number(params, 'stopPrice')\n",
      "            if stopPrice is None:\n",
      "                raise InvalidOrder(self.id + ' createOrder() requires a stopPrice extra param for a ' + type + ' order')\n",
      "            else:\n",
      "                params = self.omit(params, 'stopPrice')\n",
      "                request['stopPrice'] = self.price_to_precision(symbol, stopPrice)\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        return self.parse_order(response, market)\n",
      "\n",
      "    def fetch_order(self, id, symbol=None, params={}):\n",
      "        if symbol is None:\n",
      "            raise ArgumentsRequired(self.id + ' fetchOrder() requires a symbol argument')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchOrder', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        method = 'privateGetOrder'\n",
      "        if type == 'future':\n",
      "            method = 'fapiPrivateGetOrder'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateGetOrder'\n",
      "        elif type == 'margin':\n",
      "            method = 'sapiGetMarginOrder'\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        clientOrderId = self.safe_value_2(params, 'origClientOrderId', 'clientOrderId')\n",
      "        if clientOrderId is not None:\n",
      "            request['origClientOrderId'] = clientOrderId\n",
      "        else:\n",
      "            request['orderId'] = id\n",
      "        query = self.omit(params, ['type', 'clientOrderId', 'origClientOrderId'])\n",
      "        response = getattr(self, method)(self.extend(request, query))\n",
      "        return self.parse_order(response, market)\n",
      "\n",
      "    def fetch_orders(self, symbol=None, since=None, limit=None, params={}):\n",
      "        if symbol is None:\n",
      "            raise ArgumentsRequired(self.id + ' fetchOrders() requires a symbol argument')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchOrders', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        method = 'privateGetAllOrders'\n",
      "        if type == 'future':\n",
      "            method = 'fapiPrivateGetAllOrders'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateGetAllOrders'\n",
      "        elif type == 'margin':\n",
      "            method = 'sapiGetMarginAllOrders'\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        if since is not None:\n",
      "            request['startTime'] = since\n",
      "        if limit is not None:\n",
      "            request['limit'] = limit\n",
      "        query = self.omit(params, 'type')\n",
      "        response = getattr(self, method)(self.extend(request, query))\n",
      "        #\n",
      "        #  spot\n",
      "        #\n",
      "        #     [\n",
      "        #         {\n",
      "        #             \"symbol\": \"LTCBTC\",\n",
      "        #             \"orderId\": 1,\n",
      "        #             \"clientOrderId\": \"myOrder1\",\n",
      "        #             \"price\": \"0.1\",\n",
      "        #             \"origQty\": \"1.0\",\n",
      "        #             \"executedQty\": \"0.0\",\n",
      "        #             \"cummulativeQuoteQty\": \"0.0\",\n",
      "        #             \"status\": \"NEW\",\n",
      "        #             \"timeInForce\": \"GTC\",\n",
      "        #             \"type\": \"LIMIT\",\n",
      "        #             \"side\": \"BUY\",\n",
      "        #             \"stopPrice\": \"0.0\",\n",
      "        #             \"icebergQty\": \"0.0\",\n",
      "        #             \"time\": 1499827319559,\n",
      "        #             \"updateTime\": 1499827319559,\n",
      "        #             \"isWorking\": True\n",
      "        #         }\n",
      "        #     ]\n",
      "        #\n",
      "        #  futures\n",
      "        #\n",
      "        #     [\n",
      "        #         {\n",
      "        #             \"symbol\": \"BTCUSDT\",\n",
      "        #             \"orderId\": 1,\n",
      "        #             \"clientOrderId\": \"myOrder1\",\n",
      "        #             \"price\": \"0.1\",\n",
      "        #             \"origQty\": \"1.0\",\n",
      "        #             \"executedQty\": \"1.0\",\n",
      "        #             \"cumQuote\": \"10.0\",\n",
      "        #             \"status\": \"NEW\",\n",
      "        #             \"timeInForce\": \"GTC\",\n",
      "        #             \"type\": \"LIMIT\",\n",
      "        #             \"side\": \"BUY\",\n",
      "        #             \"stopPrice\": \"0.0\",\n",
      "        #             \"updateTime\": 1499827319559\n",
      "        #         }\n",
      "        #     ]\n",
      "        #\n",
      "        return self.parse_orders(response, market, since, limit)\n",
      "\n",
      "    def fetch_open_orders(self, symbol=None, since=None, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        market = None\n",
      "        query = None\n",
      "        type = None\n",
      "        request = {}\n",
      "        if symbol is not None:\n",
      "            market = self.market(symbol)\n",
      "            request['symbol'] = market['id']\n",
      "            defaultType = self.safe_string_2(self.options, 'fetchOpenOrders', 'defaultType', 'spot')\n",
      "            type = self.safe_string(params, 'type', defaultType)\n",
      "            query = self.omit(params, 'type')\n",
      "        elif self.options['warnOnFetchOpenOrdersWithoutSymbol']:\n",
      "            symbols = self.symbols\n",
      "            numSymbols = len(symbols)\n",
      "            fetchOpenOrdersRateLimit = int(numSymbols / 2)\n",
      "            raise ExchangeError(self.id + ' fetchOpenOrders WARNING: fetching open orders without specifying a symbol is rate-limited to one call per ' + str(fetchOpenOrdersRateLimit) + ' seconds. Do not call self method frequently to avoid ban. Set ' + self.id + '.options[\"warnOnFetchOpenOrdersWithoutSymbol\"] = False to suppress self warning message.')\n",
      "        else:\n",
      "            defaultType = self.safe_string_2(self.options, 'fetchOpenOrders', 'defaultType', 'spot')\n",
      "            type = self.safe_string(params, 'type', defaultType)\n",
      "            query = self.omit(params, 'type')\n",
      "        method = 'privateGetOpenOrders'\n",
      "        if type == 'future':\n",
      "            method = 'fapiPrivateGetOpenOrders'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateGetOpenOrders'\n",
      "        elif type == 'margin':\n",
      "            method = 'sapiGetMarginOpenOrders'\n",
      "        response = getattr(self, method)(self.extend(request, query))\n",
      "        return self.parse_orders(response, market, since, limit)\n",
      "\n",
      "    def fetch_closed_orders(self, symbol=None, since=None, limit=None, params={}):\n",
      "        orders = self.fetch_orders(symbol, since, limit, params)\n",
      "        return self.filter_by(orders, 'status', 'closed')\n",
      "\n",
      "    def cancel_order(self, id, symbol=None, params={}):\n",
      "        if symbol is None:\n",
      "            raise ArgumentsRequired(self.id + ' cancelOrder() requires a symbol argument')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchOpenOrders', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        # https://github.com/ccxt/ccxt/issues/6507\n",
      "        origClientOrderId = self.safe_value_2(params, 'origClientOrderId', 'clientOrderId')\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "            # 'orderId': id,\n",
      "            # 'origClientOrderId': id,\n",
      "        }\n",
      "        if origClientOrderId is None:\n",
      "            request['orderId'] = id\n",
      "        else:\n",
      "            request['origClientOrderId'] = origClientOrderId\n",
      "        method = 'privateDeleteOrder'\n",
      "        if type == 'future':\n",
      "            method = 'fapiPrivateDeleteOrder'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateDeleteOrder'\n",
      "        elif type == 'margin':\n",
      "            method = 'sapiDeleteMarginOrder'\n",
      "        query = self.omit(params, ['type', 'origClientOrderId', 'clientOrderId'])\n",
      "        response = getattr(self, method)(self.extend(request, query))\n",
      "        return self.parse_order(response, market)\n",
      "\n",
      "    def cancel_all_orders(self, symbol=None, params={}):\n",
      "        if symbol is None:\n",
      "            raise ArgumentsRequired(self.id + ' cancelAllOrders() requires a symbol argument')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        defaultType = self.safe_string_2(self.options, 'cancelAllOrders', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        method = 'privateDeleteOpenOrders'\n",
      "        if type == 'margin':\n",
      "            method = 'sapiDeleteMarginOpenOrders'\n",
      "        elif type == 'future':\n",
      "            method = 'fapiPrivateDeleteAllOpenOrders'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateDeleteAllOpenOrders'\n",
      "        response = getattr(self, method)(self.extend(request, query))\n",
      "        if isinstance(response, list):\n",
      "            return self.parse_orders(response, market)\n",
      "        else:\n",
      "            return response\n",
      "\n",
      "    def fetch_my_trades(self, symbol=None, since=None, limit=None, params={}):\n",
      "        if symbol is None:\n",
      "            raise ArgumentsRequired(self.id + ' fetchMyTrades() requires a symbol argument')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchMyTrades', 'defaultType', 'spot')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        params = self.omit(params, 'type')\n",
      "        method = None\n",
      "        if type == 'spot':\n",
      "            method = 'privateGetMyTrades'\n",
      "        elif type == 'margin':\n",
      "            method = 'sapiGetMarginMyTrades'\n",
      "        elif type == 'future':\n",
      "            method = 'fapiPrivateGetUserTrades'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateGetUserTrades'\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        if since is not None:\n",
      "            request['startTime'] = since\n",
      "        if limit is not None:\n",
      "            request['limit'] = limit\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        #\n",
      "        # spot trade\n",
      "        #\n",
      "        #     [\n",
      "        #         {\n",
      "        #             \"symbol\": \"BNBBTC\",\n",
      "        #             \"id\": 28457,\n",
      "        #             \"orderId\": 100234,\n",
      "        #             \"price\": \"4.00000100\",\n",
      "        #             \"qty\": \"12.00000000\",\n",
      "        #             \"commission\": \"10.10000000\",\n",
      "        #             \"commissionAsset\": \"BNB\",\n",
      "        #             \"time\": 1499865549590,\n",
      "        #             \"isBuyer\": True,\n",
      "        #             \"isMaker\": False,\n",
      "        #             \"isBestMatch\": True,\n",
      "        #         }\n",
      "        #     ]\n",
      "        #\n",
      "        # futures trade\n",
      "        #\n",
      "        #     [\n",
      "        #         {\n",
      "        #             \"accountId\": 20,\n",
      "        #             \"buyer\": False,\n",
      "        #             \"commission\": \"-0.07819010\",\n",
      "        #             \"commissionAsset\": \"USDT\",\n",
      "        #             \"counterPartyId\": 653,\n",
      "        #             \"id\": 698759,\n",
      "        #             \"maker\": False,\n",
      "        #             \"orderId\": 25851813,\n",
      "        #             \"price\": \"7819.01\",\n",
      "        #             \"qty\": \"0.002\",\n",
      "        #             \"quoteQty\": \"0.01563\",\n",
      "        #             \"realizedPnl\": \"-0.91539999\",\n",
      "        #             \"side\": \"SELL\",\n",
      "        #             \"symbol\": \"BTCUSDT\",\n",
      "        #             \"time\": 1569514978020\n",
      "        #         }\n",
      "        #     ]\n",
      "        #\n",
      "        return self.parse_trades(response, market, since, limit)\n",
      "\n",
      "    def fetch_my_dust_trades(self, symbol=None, since=None, limit=None, params={}):\n",
      "        #\n",
      "        # Binance provides an opportunity to trade insignificant(i.e. non-tradable and non-withdrawable)\n",
      "        # token leftovers(of any asset) into `BNB` coin which in turn can be used to pay trading fees with it.\n",
      "        # The corresponding trades history is called the `Dust Log` and can be requested via the following end-point:\n",
      "        # https://github.com/binance-exchange/binance-official-api-docs/blob/master/wapi-api.md#dustlog-user_data\n",
      "        #\n",
      "        self.load_markets()\n",
      "        request = {}\n",
      "        if since is not None:\n",
      "            request['startTime'] = since\n",
      "            request['endTime'] = self.sum(since, 7776000000)\n",
      "        response = self.sapiGetAssetDribblet(self.extend(request, params))\n",
      "        #     {\n",
      "        #       \"total\": \"4\",\n",
      "        #       \"userAssetDribblets\": [\n",
      "        #         {\n",
      "        #           \"operateTime\": \"1627575731000\",\n",
      "        #           \"totalServiceChargeAmount\": \"0.00001453\",\n",
      "        #           \"totalTransferedAmount\": \"0.00072693\",\n",
      "        #           \"transId\": \"70899815863\",\n",
      "        #           \"userAssetDribbletDetails\": [\n",
      "        #             {\n",
      "        #               \"fromAsset\": \"LTC\",\n",
      "        #               \"amount\": \"0.000006\",\n",
      "        #               \"transferedAmount\": \"0.00000267\",\n",
      "        #               \"serviceChargeAmount\": \"0.00000005\",\n",
      "        #               \"operateTime\": \"1627575731000\",\n",
      "        #               \"transId\": \"70899815863\"\n",
      "        #             },\n",
      "        #             {\n",
      "        #               \"fromAsset\": \"GBP\",\n",
      "        #               \"amount\": \"0.15949157\",\n",
      "        #               \"transferedAmount\": \"0.00072426\",\n",
      "        #               \"serviceChargeAmount\": \"0.00001448\",\n",
      "        #               \"operateTime\": \"1627575731000\",\n",
      "        #               \"transId\": \"70899815863\"\n",
      "        #             }\n",
      "        #           ]\n",
      "        #         },\n",
      "        #       ]\n",
      "        #     }\n",
      "        results = self.safe_value(response, 'userAssetDribblets', [])\n",
      "        rows = self.safe_integer(response, 'total', 0)\n",
      "        data = []\n",
      "        for i in range(0, rows):\n",
      "            logs = self.safe_value(results[i], 'userAssetDribbletDetails', [])\n",
      "            for j in range(0, len(logs)):\n",
      "                logs[j]['isDustTrade'] = True\n",
      "                data.append(logs[j])\n",
      "        trades = self.parse_trades(data, None, since, limit)\n",
      "        return self.filter_by_since_limit(trades, since, limit)\n",
      "\n",
      "    def parse_dust_trade(self, trade, market=None):\n",
      "        #\n",
      "        #     {\n",
      "        #       \"fromAsset\": \"USDT\",\n",
      "        #       \"amount\": \"0.009669\",\n",
      "        #       \"transferedAmount\": \"0.00002992\",\n",
      "        #       \"serviceChargeAmount\": \"0.00000059\",\n",
      "        #       \"operateTime\": \"1628076010000\",\n",
      "        #       \"transId\": \"71416578712\",\n",
      "        #       \"isDustTrade\": True\n",
      "        #     }\n",
      "        #\n",
      "        orderId = self.safe_string(trade, 'transId')\n",
      "        timestamp = self.safe_integer(trade, 'operateTime')\n",
      "        currencyId = self.safe_string(trade, 'fromAsset')\n",
      "        tradedCurrency = self.safe_currency_code(currencyId)\n",
      "        bnb = self.currency('BNB')\n",
      "        earnedCurrency = bnb['code']\n",
      "        applicantSymbol = earnedCurrency + '/' + tradedCurrency\n",
      "        tradedCurrencyIsQuote = False\n",
      "        if applicantSymbol in self.markets:\n",
      "            tradedCurrencyIsQuote = True\n",
      "        feeCostString = self.safe_string(trade, 'serviceChargeAmount')\n",
      "        fee = {\n",
      "            'currency': earnedCurrency,\n",
      "            'cost': self.parse_number(feeCostString),\n",
      "        }\n",
      "        symbol = None\n",
      "        amountString = None\n",
      "        costString = None\n",
      "        side = None\n",
      "        if tradedCurrencyIsQuote:\n",
      "            symbol = applicantSymbol\n",
      "            amountString = self.safe_string(trade, 'transferedAmount')\n",
      "            costString = self.safe_string(trade, 'amount')\n",
      "            side = 'buy'\n",
      "        else:\n",
      "            symbol = tradedCurrency + '/' + earnedCurrency\n",
      "            amountString = self.safe_string(trade, 'amount')\n",
      "            costString = self.safe_string(trade, 'transferedAmount')\n",
      "            side = 'sell'\n",
      "        priceString = None\n",
      "        if costString is not None:\n",
      "            if amountString:\n",
      "                priceString = Precise.string_div(costString, amountString)\n",
      "        id = None\n",
      "        amount = self.parse_number(amountString)\n",
      "        price = self.parse_number(priceString)\n",
      "        cost = self.parse_number(costString)\n",
      "        type = None\n",
      "        takerOrMaker = None\n",
      "        return {\n",
      "            'id': id,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'symbol': symbol,\n",
      "            'order': orderId,\n",
      "            'type': type,\n",
      "            'takerOrMaker': takerOrMaker,\n",
      "            'side': side,\n",
      "            'amount': amount,\n",
      "            'price': price,\n",
      "            'cost': cost,\n",
      "            'fee': fee,\n",
      "            'info': trade,\n",
      "        }\n",
      "\n",
      "    def fetch_deposits(self, code=None, since=None, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        currency = None\n",
      "        response = None\n",
      "        request = {}\n",
      "        legalMoney = self.safe_value(self.options, 'legalMoney', {})\n",
      "        if code in legalMoney:\n",
      "            if code is not None:\n",
      "                currency = self.currency(code)\n",
      "            request['transactionType'] = 0\n",
      "            if since is not None:\n",
      "                request['beginTime'] = since\n",
      "            raw = self.sapiGetFiatOrders(self.extend(request, params))\n",
      "            response = self.safe_value(raw, 'data')\n",
      "            #     {\n",
      "            #       \"code\": \"000000\",\n",
      "            #       \"message\": \"success\",\n",
      "            #       \"data\": [\n",
      "            #         {\n",
      "            #           \"orderNo\": \"25ced37075c1470ba8939d0df2316e23\",\n",
      "            #           \"fiatCurrency\": \"EUR\",\n",
      "            #           \"indicatedAmount\": \"15.00\",\n",
      "            #           \"amount\": \"15.00\",\n",
      "            #           \"totalFee\": \"0.00\",\n",
      "            #           \"method\": \"card\",\n",
      "            #           \"status\": \"Failed\",\n",
      "            #           \"createTime\": 1627501026000,\n",
      "            #           \"updateTime\": 1627501027000\n",
      "            #         }\n",
      "            #       ],\n",
      "            #       \"total\": 1,\n",
      "            #       \"success\": True\n",
      "            #     }\n",
      "        else:\n",
      "            if code is not None:\n",
      "                currency = self.currency(code)\n",
      "                request['coin'] = currency['id']\n",
      "            if since is not None:\n",
      "                request['startTime'] = since\n",
      "                # max 3 months range https://github.com/ccxt/ccxt/issues/6495\n",
      "                request['endTime'] = self.sum(since, 7776000000)\n",
      "            if limit is not None:\n",
      "                request['limit'] = limit\n",
      "            response = self.sapiGetCapitalDepositHisrec(self.extend(request, params))\n",
      "            #     [\n",
      "            #       {\n",
      "            #         \"amount\": \"0.01844487\",\n",
      "            #         \"coin\": \"BCH\",\n",
      "            #         \"network\": \"BCH\",\n",
      "            #         \"status\": 1,\n",
      "            #         \"address\": \"1NYxAJhW2281HK1KtJeaENBqHeygA88FzR\",\n",
      "            #         \"addressTag\": \"\",\n",
      "            #         \"txId\": \"bafc5902504d6504a00b7d0306a41154cbf1d1b767ab70f3bc226327362588af\",\n",
      "            #         \"insertTime\": 1610784980000,\n",
      "            #         \"transferType\": 0,\n",
      "            #         \"confirmTimes\": \"2/2\"\n",
      "            #       },\n",
      "            #       {\n",
      "            #         \"amount\": \"4500\",\n",
      "            #         \"coin\": \"USDT\",\n",
      "            #         \"network\": \"BSC\",\n",
      "            #         \"status\": 1,\n",
      "            #         \"address\": \"0xc9c923c87347ca0f3451d6d308ce84f691b9f501\",\n",
      "            #         \"addressTag\": \"\",\n",
      "            #         \"txId\": \"Internal transfer 51376627901\",\n",
      "            #         \"insertTime\": 1618394381000,\n",
      "            #         \"transferType\": 1,\n",
      "            #         \"confirmTimes\": \"1/15\"\n",
      "            #     }\n",
      "            #   ]\n",
      "        return self.parse_transactions(response, currency, since, limit)\n",
      "\n",
      "    def fetch_withdrawals(self, code=None, since=None, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        legalMoney = self.safe_value(self.options, 'legalMoney', {})\n",
      "        request = {}\n",
      "        response = None\n",
      "        currency = None\n",
      "        if code in legalMoney:\n",
      "            if code is not None:\n",
      "                currency = self.currency(code)\n",
      "            request['transactionType'] = 1\n",
      "            if since is not None:\n",
      "                request['beginTime'] = since\n",
      "            raw = self.sapiGetFiatOrders(self.extend(request, params))\n",
      "            response = self.safe_value(raw, 'data')\n",
      "            #     {\n",
      "            #       \"code\": \"000000\",\n",
      "            #       \"message\": \"success\",\n",
      "            #       \"data\": [\n",
      "            #         {\n",
      "            #           \"orderNo\": \"CJW706452266115170304\",\n",
      "            #           \"fiatCurrency\": \"GBP\",\n",
      "            #           \"indicatedAmount\": \"10001.50\",\n",
      "            #           \"amount\": \"100.00\",\n",
      "            #           \"totalFee\": \"1.50\",\n",
      "            #           \"method\": \"bank transfer\",\n",
      "            #           \"status\": \"Successful\",\n",
      "            #           \"createTime\": 1620037745000,\n",
      "            #           \"updateTime\": 1620038480000\n",
      "            #         },\n",
      "            #         {\n",
      "            #           \"orderNo\": \"CJW706287492781891584\",\n",
      "            #           \"fiatCurrency\": \"GBP\",\n",
      "            #           \"indicatedAmount\": \"10001.50\",\n",
      "            #           \"amount\": \"100.00\",\n",
      "            #           \"totalFee\": \"1.50\",\n",
      "            #           \"method\": \"bank transfer\",\n",
      "            #           \"status\": \"Successful\",\n",
      "            #           \"createTime\": 1619998460000,\n",
      "            #           \"updateTime\": 1619998823000\n",
      "            #         }\n",
      "            #       ],\n",
      "            #       \"total\": 39,\n",
      "            #       \"success\": True\n",
      "            #     }\n",
      "        else:\n",
      "            if code is not None:\n",
      "                currency = self.currency(code)\n",
      "                request['coin'] = currency['id']\n",
      "            if since is not None:\n",
      "                request['startTime'] = since\n",
      "                # max 3 months range https://github.com/ccxt/ccxt/issues/6495\n",
      "                request['endTime'] = self.sum(since, 7776000000)\n",
      "            if limit is not None:\n",
      "                request['limit'] = limit\n",
      "            response = self.sapiGetCapitalWithdrawHistory(self.extend(request, params))\n",
      "            #     [\n",
      "            #       {\n",
      "            #         \"id\": \"69e53ad305124b96b43668ceab158a18\",\n",
      "            #         \"amount\": \"28.75\",\n",
      "            #         \"transactionFee\": \"0.25\",\n",
      "            #         \"coin\": \"XRP\",\n",
      "            #         \"status\": 6,\n",
      "            #         \"address\": \"r3T75fuLjX51mmfb5Sk1kMNuhBgBPJsjza\",\n",
      "            #         \"addressTag\": \"101286922\",\n",
      "            #         \"txId\": \"19A5B24ED0B697E4F0E9CD09FCB007170A605BC93C9280B9E6379C5E6EF0F65A\",\n",
      "            #         \"applyTime\": \"2021-04-15 12:09:16\",\n",
      "            #         \"network\": \"XRP\",\n",
      "            #         \"transferType\": 0\n",
      "            #       },\n",
      "            #       {\n",
      "            #         \"id\": \"9a67628b16ba4988ae20d329333f16bc\",\n",
      "            #         \"amount\": \"20\",\n",
      "            #         \"transactionFee\": \"20\",\n",
      "            #         \"coin\": \"USDT\",\n",
      "            #         \"status\": 6,\n",
      "            #         \"address\": \"0x0AB991497116f7F5532a4c2f4f7B1784488628e1\",\n",
      "            #         \"txId\": \"0x77fbf2cf2c85b552f0fd31fd2e56dc95c08adae031d96f3717d8b17e1aea3e46\",\n",
      "            #         \"applyTime\": \"2021-04-15 12:06:53\",\n",
      "            #         \"network\": \"ETH\",\n",
      "            #         \"transferType\": 0\n",
      "            #       },\n",
      "            #       {\n",
      "            #         \"id\": \"a7cdc0afbfa44a48bd225c9ece958fe2\",\n",
      "            #         \"amount\": \"51\",\n",
      "            #         \"transactionFee\": \"1\",\n",
      "            #         \"coin\": \"USDT\",\n",
      "            #         \"status\": 6,\n",
      "            #         \"address\": \"TYDmtuWL8bsyjvcauUTerpfYyVhFtBjqyo\",\n",
      "            #         \"txId\": \"168a75112bce6ceb4823c66726ad47620ad332e69fe92d9cb8ceb76023f9a028\",\n",
      "            #         \"applyTime\": \"2021-04-13 12:46:59\",\n",
      "            #         \"network\": \"TRX\",\n",
      "            #         \"transferType\": 0\n",
      "            #       }\n",
      "            #     ]\n",
      "        return self.parse_transactions(response, currency, since, limit)\n",
      "\n",
      "    def parse_transaction_status_by_type(self, status, type=None):\n",
      "        statusesByType = {\n",
      "            'deposit': {\n",
      "                '0': 'pending',\n",
      "                '1': 'ok',\n",
      "                # Fiat\n",
      "                # Processing, Failed, Successful, Finished, Refunding, Refunded, Refund Failed, Order Partial credit Stopped\n",
      "                'Processing': 'pending',\n",
      "                'Failed': 'failed',\n",
      "                'Successful': 'ok',\n",
      "                'Refunding': 'canceled',\n",
      "                'Refunded': 'canceled',\n",
      "                'Refund Failed': 'failed',\n",
      "            },\n",
      "            'withdrawal': {\n",
      "                '0': 'pending',  # Email Sent\n",
      "                '1': 'canceled',  # Cancelled(different from 1 = ok in deposits)\n",
      "                '2': 'pending',  # Awaiting Approval\n",
      "                '3': 'failed',  # Rejected\n",
      "                '4': 'pending',  # Processing\n",
      "                '5': 'failed',  # Failure\n",
      "                '6': 'ok',  # Completed\n",
      "                # Fiat\n",
      "                # Processing, Failed, Successful, Finished, Refunding, Refunded, Refund Failed, Order Partial credit Stopped\n",
      "                'Processing': 'pending',\n",
      "                'Failed': 'failed',\n",
      "                'Successful': 'ok',\n",
      "                'Refunding': 'canceled',\n",
      "                'Refunded': 'canceled',\n",
      "                'Refund Failed': 'failed',\n",
      "            },\n",
      "        }\n",
      "        statuses = self.safe_value(statusesByType, type, {})\n",
      "        return self.safe_string(statuses, status, status)\n",
      "\n",
      "    def parse_transaction(self, transaction, currency=None):\n",
      "        #\n",
      "        # fetchDeposits\n",
      "        #\n",
      "        #     {\n",
      "        #       \"amount\": \"4500\",\n",
      "        #       \"coin\": \"USDT\",\n",
      "        #       \"network\": \"BSC\",\n",
      "        #       \"status\": 1,\n",
      "        #       \"address\": \"0xc9c923c87347ca0f3451d6d308ce84f691b9f501\",\n",
      "        #       \"addressTag\": \"\",\n",
      "        #       \"txId\": \"Internal transfer 51376627901\",\n",
      "        #       \"insertTime\": 1618394381000,\n",
      "        #       \"transferType\": 1,\n",
      "        #       \"confirmTimes\": \"1/15\"\n",
      "        #     }\n",
      "        #\n",
      "        # fetchWithdrawals\n",
      "        #\n",
      "        #     {\n",
      "        #       \"id\": \"69e53ad305124b96b43668ceab158a18\",\n",
      "        #       \"amount\": \"28.75\",\n",
      "        #       \"transactionFee\": \"0.25\",\n",
      "        #       \"coin\": \"XRP\",\n",
      "        #       \"status\": 6,\n",
      "        #       \"address\": \"r3T75fuLjX51mmfb5Sk1kMNuhBgBPJsjza\",\n",
      "        #       \"addressTag\": \"101286922\",\n",
      "        #       \"txId\": \"19A5B24ED0B697E4F0E9CD09FCB007170A605BC93C9280B9E6379C5E6EF0F65A\",\n",
      "        #       \"applyTime\": \"2021-04-15 12:09:16\",\n",
      "        #       \"network\": \"XRP\",\n",
      "        #       \"transferType\": 0\n",
      "        #     }\n",
      "        #\n",
      "        # fiat transaction\n",
      "        # withdraw\n",
      "        #     {\n",
      "        #       \"orderNo\": \"CJW684897551397171200\",\n",
      "        #       \"fiatCurrency\": \"GBP\",\n",
      "        #       \"indicatedAmount\": \"29.99\",\n",
      "        #       \"amount\": \"28.49\",\n",
      "        #       \"totalFee\": \"1.50\",\n",
      "        #       \"method\": \"bank transfer\",\n",
      "        #       \"status\": \"Successful\",\n",
      "        #       \"createTime\": 1614898701000,\n",
      "        #       \"updateTime\": 1614898820000\n",
      "        #     }\n",
      "        #\n",
      "        # deposit\n",
      "        #     {\n",
      "        #       \"orderNo\": \"25ced37075c1470ba8939d0df2316e23\",\n",
      "        #       \"fiatCurrency\": \"EUR\",\n",
      "        #       \"indicatedAmount\": \"15.00\",\n",
      "        #       \"amount\": \"15.00\",\n",
      "        #       \"totalFee\": \"0.00\",\n",
      "        #       \"method\": \"card\",\n",
      "        #       \"status\": \"Failed\",\n",
      "        #       \"createTime\": \"1627501026000\",\n",
      "        #       \"updateTime\": \"1627501027000\"\n",
      "        #     }\n",
      "        #\n",
      "        id = self.safe_string_2(transaction, 'id', 'orderNo')\n",
      "        address = self.safe_string(transaction, 'address')\n",
      "        tag = self.safe_string(transaction, 'addressTag')  # set but unused\n",
      "        if tag is not None:\n",
      "            if len(tag) < 1:\n",
      "                tag = None\n",
      "        txid = self.safe_string(transaction, 'txId')\n",
      "        if (txid is not None) and (txid.find('Internal transfer ') >= 0):\n",
      "            txid = txid[18:]\n",
      "        currencyId = self.safe_string_2(transaction, 'coin', 'fiatCurrency')\n",
      "        code = self.safe_currency_code(currencyId, currency)\n",
      "        timestamp = None\n",
      "        insertTime = self.safe_integer_2(transaction, 'insertTime', 'createTime')\n",
      "        applyTime = self.parse8601(self.safe_string(transaction, 'applyTime'))\n",
      "        type = self.safe_string(transaction, 'type')\n",
      "        if type is None:\n",
      "            if (insertTime is not None) and (applyTime is None):\n",
      "                type = 'deposit'\n",
      "                timestamp = insertTime\n",
      "            elif (insertTime is None) and (applyTime is not None):\n",
      "                type = 'withdrawal'\n",
      "                timestamp = applyTime\n",
      "        status = self.parse_transaction_status_by_type(self.safe_string(transaction, 'status'), type)\n",
      "        amount = self.safe_number(transaction, 'amount')\n",
      "        feeCost = self.safe_number_2(transaction, 'transactionFee', 'totalFee')\n",
      "        fee = None\n",
      "        if feeCost is not None:\n",
      "            fee = {'currency': code, 'cost': feeCost}\n",
      "        updated = self.safe_integer_2(transaction, 'successTime', 'updateTime')\n",
      "        internal = self.safe_integer(transaction, 'transferType', False)\n",
      "        internal = True if internal else False\n",
      "        return {\n",
      "            'info': transaction,\n",
      "            'id': id,\n",
      "            'txid': txid,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'address': address,\n",
      "            'addressTo': address,\n",
      "            'addressFrom': None,\n",
      "            'tag': tag,\n",
      "            'tagTo': tag,\n",
      "            'tagFrom': None,\n",
      "            'type': type,\n",
      "            'amount': amount,\n",
      "            'currency': code,\n",
      "            'status': status,\n",
      "            'updated': updated,\n",
      "            'internal': internal,\n",
      "            'fee': fee,\n",
      "        }\n",
      "\n",
      "    def parse_transfer_status(self, status):\n",
      "        statuses = {\n",
      "            'CONFIRMED': 'ok',\n",
      "        }\n",
      "        return self.safe_string(statuses, status, status)\n",
      "\n",
      "    def parse_transfer(self, transfer, currency=None):\n",
      "        #\n",
      "        # transfer\n",
      "        #\n",
      "        #     {\n",
      "        #         \"tranId\":13526853623\n",
      "        #     }\n",
      "        #\n",
      "        # fetchTransfers\n",
      "        #\n",
      "        #     {\n",
      "        #         timestamp: 1614640878000,\n",
      "        #         asset: 'USDT',\n",
      "        #         amount: '25',\n",
      "        #         type: 'MAIN_UMFUTURE',\n",
      "        #         status: 'CONFIRMED',\n",
      "        #         tranId: 43000126248\n",
      "        #     }\n",
      "        #\n",
      "        id = self.safe_string(transfer, 'tranId')\n",
      "        currencyId = self.safe_string(transfer, 'asset')\n",
      "        code = self.safe_currency_code(currencyId, currency)\n",
      "        amount = self.safe_number(transfer, 'amount')\n",
      "        type = self.safe_string(transfer, 'type')\n",
      "        fromAccount = None\n",
      "        toAccount = None\n",
      "        typesByAccount = self.safe_value(self.options, 'typesByAccount', {})\n",
      "        if type is not None:\n",
      "            parts = type.split('_')\n",
      "            fromAccount = self.safe_value(parts, 0)\n",
      "            toAccount = self.safe_value(parts, 1)\n",
      "            fromAccount = self.safe_string(typesByAccount, fromAccount, fromAccount)\n",
      "            toAccount = self.safe_string(typesByAccount, toAccount, toAccount)\n",
      "        timestamp = self.safe_integer(transfer, 'timestamp')\n",
      "        status = self.parse_transfer_status(self.safe_string(transfer, 'status'))\n",
      "        return {\n",
      "            'info': transfer,\n",
      "            'id': id,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'currency': code,\n",
      "            'amount': amount,\n",
      "            'fromAccount': fromAccount,\n",
      "            'toAccount': toAccount,\n",
      "            'status': status,\n",
      "        }\n",
      "\n",
      "    def parse_income(self, income, market=None):\n",
      "        #\n",
      "        #     {\n",
      "        #       \"symbol\": \"ETHUSDT\",\n",
      "        #       \"incomeType\": \"FUNDING_FEE\",\n",
      "        #       \"income\": \"0.00134317\",\n",
      "        #       \"asset\": \"USDT\",\n",
      "        #       \"time\": \"1621584000000\",\n",
      "        #       \"info\": \"FUNDING_FEE\",\n",
      "        #       \"tranId\": \"4480321991774044580\",\n",
      "        #       \"tradeId\": \"\"\n",
      "        #     }\n",
      "        #\n",
      "        marketId = self.safe_string(income, 'symbol')\n",
      "        symbol = self.safe_symbol(marketId, market)\n",
      "        amount = self.safe_number(income, 'income')\n",
      "        currencyId = self.safe_string(income, 'asset')\n",
      "        code = self.safe_currency_code(currencyId)\n",
      "        id = self.safe_string(income, 'tranId')\n",
      "        timestamp = self.safe_integer(income, 'time')\n",
      "        return {\n",
      "            'info': income,\n",
      "            'symbol': symbol,\n",
      "            'code': code,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'id': id,\n",
      "            'amount': amount,\n",
      "        }\n",
      "\n",
      "    def parse_incomes(self, incomes, market=None, since=None, limit=None):\n",
      "        result = []\n",
      "        for i in range(0, len(incomes)):\n",
      "            entry = incomes[i]\n",
      "            parsed = self.parse_income(entry, market)\n",
      "            result.append(parsed)\n",
      "        sorted = self.sort_by(result, 'timestamp')\n",
      "        return self.filter_by_since_limit(sorted, since, limit)\n",
      "\n",
      "    def transfer(self, code, amount, fromAccount, toAccount, params={}):\n",
      "        self.load_markets()\n",
      "        currency = self.currency(code)\n",
      "        type = self.safe_string(params, 'type')\n",
      "        if type is None:\n",
      "            accountsByType = self.safe_value(self.options, 'accountsByType', {})\n",
      "            fromAccount = fromAccount.lower()\n",
      "            toAccount = toAccount.lower()\n",
      "            fromId = self.safe_string(accountsByType, fromAccount)\n",
      "            toId = self.safe_string(accountsByType, toAccount)\n",
      "            if fromId is None:\n",
      "                keys = list(accountsByType.keys())\n",
      "                raise ExchangeError(self.id + ' fromAccount must be one of ' + ', '.join(keys))\n",
      "            if toId is None:\n",
      "                keys = list(accountsByType.keys())\n",
      "                raise ExchangeError(self.id + ' toAccount must be one of ' + ', '.join(keys))\n",
      "            type = fromId + '_' + toId\n",
      "        request = {\n",
      "            'asset': currency['id'],\n",
      "            'amount': self.currency_to_precision(code, amount),\n",
      "            'type': type,\n",
      "        }\n",
      "        response = self.sapiPostAssetTransfer(self.extend(request, params))\n",
      "        #\n",
      "        #     {\n",
      "        #         \"tranId\":13526853623\n",
      "        #     }\n",
      "        #\n",
      "        transfer = self.parse_transfer(response, currency)\n",
      "        return self.extend(transfer, {\n",
      "            'amount': amount,\n",
      "            'currency': code,\n",
      "            'fromAccount': fromAccount,\n",
      "            'toAccount': toAccount,\n",
      "        })\n",
      "\n",
      "    def fetch_transfers(self, code=None, since=None, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        currency = None\n",
      "        if code is not None:\n",
      "            currency = self.currency(code)\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchTransfers', 'defaultType', 'spot')\n",
      "        fromAccount = self.safe_string(params, 'fromAccount', defaultType)\n",
      "        defaultTo = 'spot' if (fromAccount == 'future') else 'future'\n",
      "        toAccount = self.safe_string(params, 'toAccount', defaultTo)\n",
      "        type = self.safe_string(params, 'type')\n",
      "        accountsByType = self.safe_value(self.options, 'accountsByType', {})\n",
      "        fromId = self.safe_string(accountsByType, fromAccount)\n",
      "        toId = self.safe_string(accountsByType, toAccount)\n",
      "        if type is None:\n",
      "            if fromId is None:\n",
      "                keys = list(accountsByType.keys())\n",
      "                raise ExchangeError(self.id + ' fromAccount parameter must be one of ' + ', '.join(keys))\n",
      "            if toId is None:\n",
      "                keys = list(accountsByType.keys())\n",
      "                raise ExchangeError(self.id + ' toAccount parameter must be one of ' + ', '.join(keys))\n",
      "            type = fromId + '_' + toId\n",
      "        request = {\n",
      "            'type': type,\n",
      "        }\n",
      "        if since is not None:\n",
      "            request['startTime'] = since\n",
      "        if limit is not None:\n",
      "            request['size'] = limit\n",
      "        response = self.sapiGetAssetTransfer(self.extend(request, params))\n",
      "        #\n",
      "        #     {\n",
      "        #         total: 3,\n",
      "        #         rows: [\n",
      "        #             {\n",
      "        #                 timestamp: 1614640878000,\n",
      "        #                 asset: 'USDT',\n",
      "        #                 amount: '25',\n",
      "        #                 type: 'MAIN_UMFUTURE',\n",
      "        #                 status: 'CONFIRMED',\n",
      "        #                 tranId: 43000126248\n",
      "        #             },\n",
      "        #         ]\n",
      "        #     }\n",
      "        #\n",
      "        rows = self.safe_value(response, 'rows', [])\n",
      "        return self.parse_transfers(rows, currency, since, limit)\n",
      "\n",
      "    def fetch_deposit_address(self, code, params={}):\n",
      "        self.load_markets()\n",
      "        currency = self.currency(code)\n",
      "        request = {\n",
      "            'coin': currency['id'],\n",
      "            # 'network': 'ETH',  # 'BSC', 'XMR', you can get network and isDefault in networkList in the response of sapiGetCapitalConfigDetail\n",
      "        }\n",
      "        networks = self.safe_value(self.options, 'networks', {})\n",
      "        network = self.safe_string_upper(params, 'network')  # self line allows the user to specify either ERC20 or ETH\n",
      "        network = self.safe_string(networks, network, network)  # handle ERC20>ETH alias\n",
      "        if network is not None:\n",
      "            request['network'] = network\n",
      "            params = self.omit(params, 'network')\n",
      "        # has support for the 'network' parameter\n",
      "        # https://binance-docs.github.io/apidocs/spot/en/#deposit-address-supporting-network-user_data\n",
      "        response = self.sapiGetCapitalDepositAddress(self.extend(request, params))\n",
      "        #\n",
      "        #     {\n",
      "        #         currency: 'XRP',\n",
      "        #         address: 'rEb8TK3gBgk5auZkwc6sHnwrGVJH8DuaLh',\n",
      "        #         tag: '108618262',\n",
      "        #         info: {\n",
      "        #             coin: 'XRP',\n",
      "        #             address: 'rEb8TK3gBgk5auZkwc6sHnwrGVJH8DuaLh',\n",
      "        #             tag: '108618262',\n",
      "        #             url: 'https://bithomp.com/explorer/rEb8TK3gBgk5auZkwc6sHnwrGVJH8DuaLh'\n",
      "        #         }\n",
      "        #     }\n",
      "        #\n",
      "        address = self.safe_string(response, 'address')\n",
      "        url = self.safe_string(response, 'url')\n",
      "        impliedNetwork = None\n",
      "        if url is not None:\n",
      "            reverseNetworks = self.safe_value(self.options, 'reverseNetworks', {})\n",
      "            parts = url.split('/')\n",
      "            topLevel = self.safe_string(parts, 2)\n",
      "            if (topLevel == 'blockchair.com') or (topLevel == 'viewblock.io'):\n",
      "                subLevel = self.safe_string(parts, 3)\n",
      "                if subLevel is not None:\n",
      "                    topLevel = topLevel + '/' + subLevel\n",
      "            impliedNetwork = self.safe_string(reverseNetworks, topLevel)\n",
      "            impliedNetworks = self.safe_value(self.options, 'impliedNetworks', {\n",
      "                'ETH': {'ERC20': 'ETH'},\n",
      "                'TRX': {'TRC20': 'TRX'},\n",
      "            })\n",
      "            if code in impliedNetworks:\n",
      "                conversion = self.safe_value(impliedNetworks, code, {})\n",
      "                impliedNetwork = self.safe_string(conversion, impliedNetwork, impliedNetwork)\n",
      "        tag = self.safe_string(response, 'tag', '')\n",
      "        if len(tag) == 0:\n",
      "            tag = None\n",
      "        self.check_address(address)\n",
      "        return {\n",
      "            'currency': code,\n",
      "            'address': address,\n",
      "            'tag': tag,\n",
      "            'network': impliedNetwork,\n",
      "            'info': response,\n",
      "        }\n",
      "\n",
      "    def fetch_funding_fees(self, codes=None, params={}):\n",
      "        self.load_markets()\n",
      "        response = self.sapiGetCapitalConfigGetall(params)\n",
      "        #\n",
      "        #  [\n",
      "        #     {\n",
      "        #       coin: 'BAT',\n",
      "        #       depositAllEnable: True,\n",
      "        #       withdrawAllEnable: True,\n",
      "        #       name: 'Basic Attention Token',\n",
      "        #       free: '0',\n",
      "        #       locked: '0',\n",
      "        #       freeze: '0',\n",
      "        #       withdrawing: '0',\n",
      "        #       ipoing: '0',\n",
      "        #       ipoable: '0',\n",
      "        #       storage: '0',\n",
      "        #       isLegalMoney: False,\n",
      "        #       trading: True,\n",
      "        #       networkList: [\n",
      "        #         {\n",
      "        #           network: 'BNB',\n",
      "        #           coin: 'BAT',\n",
      "        #           withdrawIntegerMultiple: '0.00000001',\n",
      "        #           isDefault: False,\n",
      "        #           depositEnable: True,\n",
      "        #           withdrawEnable: True,\n",
      "        #           depositDesc: '',\n",
      "        #           withdrawDesc: '',\n",
      "        #           specialTips: 'The name of self asset is Basic Attention Token(BAT). Both a MEMO and an Address are required to successfully deposit your BEP2 tokens to Binance.',\n",
      "        #           name: 'BEP2',\n",
      "        #           resetAddressStatus: False,\n",
      "        #           addressRegex: '^(bnb1)[0-9a-z]{38}$',\n",
      "        #           memoRegex: '^[0-9A-Za-z\\\\-_]{1,120}$',\n",
      "        #           withdrawFee: '0.27',\n",
      "        #           withdrawMin: '0.54',\n",
      "        #           withdrawMax: '10000000000',\n",
      "        #           minConfirm: '1',\n",
      "        #           unLockConfirm: '0'\n",
      "        #         },\n",
      "        #         {\n",
      "        #           network: 'BSC',\n",
      "        #           coin: 'BAT',\n",
      "        #           withdrawIntegerMultiple: '0.00000001',\n",
      "        #           isDefault: False,\n",
      "        #           depositEnable: True,\n",
      "        #           withdrawEnable: True,\n",
      "        #           depositDesc: '',\n",
      "        #           withdrawDesc: '',\n",
      "        #           specialTips: 'The name of self asset is Basic Attention Token. Please ensure you are depositing Basic Attention Token(BAT) tokens under the contract address ending in 9766e.',\n",
      "        #           name: 'BEP20(BSC)',\n",
      "        #           resetAddressStatus: False,\n",
      "        #           addressRegex: '^(0x)[0-9A-Fa-f]{40}$',\n",
      "        #           memoRegex: '',\n",
      "        #           withdrawFee: '0.27',\n",
      "        #           withdrawMin: '0.54',\n",
      "        #           withdrawMax: '10000000000',\n",
      "        #           minConfirm: '15',\n",
      "        #           unLockConfirm: '0'\n",
      "        #         },\n",
      "        #         {\n",
      "        #           network: 'ETH',\n",
      "        #           coin: 'BAT',\n",
      "        #           withdrawIntegerMultiple: '0.00000001',\n",
      "        #           isDefault: True,\n",
      "        #           depositEnable: True,\n",
      "        #           withdrawEnable: True,\n",
      "        #           depositDesc: '',\n",
      "        #           withdrawDesc: '',\n",
      "        #           specialTips: 'The name of self asset is Basic Attention Token. Please ensure you are depositing Basic Attention Token(BAT) tokens under the contract address ending in 887ef.',\n",
      "        #           name: 'ERC20',\n",
      "        #           resetAddressStatus: False,\n",
      "        #           addressRegex: '^(0x)[0-9A-Fa-f]{40}$',\n",
      "        #           memoRegex: '',\n",
      "        #           withdrawFee: '27',\n",
      "        #           withdrawMin: '54',\n",
      "        #           withdrawMax: '10000000000',\n",
      "        #           minConfirm: '12',\n",
      "        #           unLockConfirm: '0'\n",
      "        #         }\n",
      "        #       ]\n",
      "        #     }\n",
      "        #  ]\n",
      "        #\n",
      "        withdrawFees = {}\n",
      "        for i in range(0, len(response)):\n",
      "            entry = response[i]\n",
      "            currencyId = self.safe_string(entry, 'coin')\n",
      "            code = self.safe_currency_code(currencyId)\n",
      "            networkList = self.safe_value(entry, 'networkList')\n",
      "            withdrawFees[code] = {}\n",
      "            for j in range(0, len(networkList)):\n",
      "                networkEntry = networkList[j]\n",
      "                networkId = self.safe_string(networkEntry, 'network')\n",
      "                networkCode = self.safe_currency_code(networkId)\n",
      "                fee = self.safe_number(networkEntry, 'withdrawFee')\n",
      "                withdrawFees[code][networkCode] = fee\n",
      "        return {\n",
      "            'withdraw': withdrawFees,\n",
      "            'deposit': {},\n",
      "            'info': response,\n",
      "        }\n",
      "\n",
      "    def withdraw(self, code, amount, address, tag=None, params={}):\n",
      "        tag, params = self.handle_withdraw_tag_and_params(tag, params)\n",
      "        self.check_address(address)\n",
      "        self.load_markets()\n",
      "        currency = self.currency(code)\n",
      "        request = {\n",
      "            'coin': currency['id'],\n",
      "            'address': address,\n",
      "            'amount': amount,\n",
      "            # https://binance-docs.github.io/apidocs/spot/en/#withdraw-sapi\n",
      "            # issue sapiGetCapitalConfigGetall() to get networks for withdrawing USDT ERC20 vs USDT Omni\n",
      "            # 'network': 'ETH',  # 'BTC', 'TRX', etc, optional\n",
      "        }\n",
      "        if tag is not None:\n",
      "            request['addressTag'] = tag\n",
      "        networks = self.safe_value(self.options, 'networks', {})\n",
      "        network = self.safe_string_upper(params, 'network')  # self line allows the user to specify either ERC20 or ETH\n",
      "        network = self.safe_string(networks, network, network)  # handle ERC20>ETH alias\n",
      "        if network is not None:\n",
      "            request['network'] = network\n",
      "            params = self.omit(params, 'network')\n",
      "        response = self.sapiPostCapitalWithdrawApply(self.extend(request, params))\n",
      "        #     {id: '9a67628b16ba4988ae20d329333f16bc'}\n",
      "        return {\n",
      "            'info': response,\n",
      "            'id': self.safe_string(response, 'id'),\n",
      "        }\n",
      "\n",
      "    def parse_trading_fee(self, fee, market=None):\n",
      "        #\n",
      "        #     {\n",
      "        #         \"symbol\": \"ADABNB\",\n",
      "        #         \"makerCommission\": 0.001,\n",
      "        #         \"takerCommission\": 0.001\n",
      "        #     }\n",
      "        #\n",
      "        marketId = self.safe_string(fee, 'symbol')\n",
      "        symbol = self.safe_symbol(marketId)\n",
      "        return {\n",
      "            'info': fee,\n",
      "            'symbol': symbol,\n",
      "            'maker': self.safe_number(fee, 'makerCommission'),\n",
      "            'taker': self.safe_number(fee, 'takerCommission'),\n",
      "        }\n",
      "\n",
      "    def fetch_trading_fee(self, symbol, params={}):\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        response = self.sapiGetAssetTradeFee(self.extend(request, params))\n",
      "        #\n",
      "        #     [\n",
      "        #       {\n",
      "        #         \"symbol\": \"BTCUSDT\",\n",
      "        #         \"makerCommission\": \"0.001\",\n",
      "        #         \"takerCommission\": \"0.001\"\n",
      "        #       }\n",
      "        #     ]\n",
      "        #\n",
      "        first = self.safe_value(response, 0, {})\n",
      "        return self.parse_trading_fee(first)\n",
      "\n",
      "    def fetch_trading_fees(self, params={}):\n",
      "        self.load_markets()\n",
      "        method = None\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchFundingRates', 'defaultType', 'future')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        if (type == 'spot') or (type == 'margin'):\n",
      "            method = 'sapiGetAssetTradeFee'\n",
      "        elif type == 'future':\n",
      "            method = 'fapiPrivateGetAccount'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateGetAccount'\n",
      "        response = getattr(self, method)(query)\n",
      "        #\n",
      "        # sapi / spot\n",
      "        #\n",
      "        #    [\n",
      "        #       {\n",
      "        #         \"symbol\": \"ZRXBNB\",\n",
      "        #         \"makerCommission\": \"0.001\",\n",
      "        #         \"takerCommission\": \"0.001\"\n",
      "        #       },\n",
      "        #       {\n",
      "        #         \"symbol\": \"ZRXBTC\",\n",
      "        #         \"makerCommission\": \"0.001\",\n",
      "        #         \"takerCommission\": \"0.001\"\n",
      "        #       },\n",
      "        #    ]\n",
      "        #\n",
      "        # fapi / future / linear\n",
      "        #\n",
      "        #     {\n",
      "        #         \"feeTier\": 0,       # account commisssion tier\n",
      "        #         \"canTrade\": True,   # if can trade\n",
      "        #         \"canDeposit\": True,     # if can transfer in asset\n",
      "        #         \"canWithdraw\": True,    # if can transfer out asset\n",
      "        #         \"updateTime\": 0,\n",
      "        #         \"totalInitialMargin\": \"0.00000000\",    # total initial margin required with current mark price(useless with isolated positions), only for USDT asset\n",
      "        #         \"totalMaintMargin\": \"0.00000000\",     # total maintenance margin required, only for USDT asset\n",
      "        #         \"totalWalletBalance\": \"23.72469206\",     # total wallet balance, only for USDT asset\n",
      "        #         \"totalUnrealizedProfit\": \"0.00000000\",   # total unrealized profit, only for USDT asset\n",
      "        #         \"totalMarginBalance\": \"23.72469206\",     # total margin balance, only for USDT asset\n",
      "        #         \"totalPositionInitialMargin\": \"0.00000000\",    # initial margin required for positions with current mark price, only for USDT asset\n",
      "        #         \"totalOpenOrderInitialMargin\": \"0.00000000\",   # initial margin required for open orders with current mark price, only for USDT asset\n",
      "        #         \"totalCrossWalletBalance\": \"23.72469206\",      # crossed wallet balance, only for USDT asset\n",
      "        #         \"totalCrossUnPnl\": \"0.00000000\",      # unrealized profit of crossed positions, only for USDT asset\n",
      "        #         \"availableBalance\": \"23.72469206\",       # available balance, only for USDT asset\n",
      "        #         \"maxWithdrawAmount\": \"23.72469206\"     # maximum amount for transfer out, only for USDT asset\n",
      "        #         ...\n",
      "        #     }\n",
      "        #\n",
      "        # dapi / delivery / inverse\n",
      "        #\n",
      "        #     {\n",
      "        #         \"canDeposit\": True,\n",
      "        #         \"canTrade\": True,\n",
      "        #         \"canWithdraw\": True,\n",
      "        #         \"feeTier\": 2,\n",
      "        #         \"updateTime\": 0\n",
      "        #     }\n",
      "        #\n",
      "        if (type == 'spot') or (type == 'margin'):\n",
      "            #\n",
      "            #    [\n",
      "            #       {\n",
      "            #         \"symbol\": \"ZRXBNB\",\n",
      "            #         \"makerCommission\": \"0.001\",\n",
      "            #         \"takerCommission\": \"0.001\"\n",
      "            #       },\n",
      "            #       {\n",
      "            #         \"symbol\": \"ZRXBTC\",\n",
      "            #         \"makerCommission\": \"0.001\",\n",
      "            #         \"takerCommission\": \"0.001\"\n",
      "            #       },\n",
      "            #    ]\n",
      "            #\n",
      "            result = {}\n",
      "            for i in range(0, len(response)):\n",
      "                fee = self.parse_trading_fee(response[i])\n",
      "                symbol = fee['symbol']\n",
      "                result[symbol] = fee\n",
      "            return result\n",
      "        elif type == 'future':\n",
      "            #\n",
      "            #     {\n",
      "            #         \"feeTier\": 0,       # account commisssion tier\n",
      "            #         \"canTrade\": True,   # if can trade\n",
      "            #         \"canDeposit\": True,     # if can transfer in asset\n",
      "            #         \"canWithdraw\": True,    # if can transfer out asset\n",
      "            #         \"updateTime\": 0,\n",
      "            #         \"totalInitialMargin\": \"0.00000000\",    # total initial margin required with current mark price(useless with isolated positions), only for USDT asset\n",
      "            #         \"totalMaintMargin\": \"0.00000000\",     # total maintenance margin required, only for USDT asset\n",
      "            #         \"totalWalletBalance\": \"23.72469206\",     # total wallet balance, only for USDT asset\n",
      "            #         \"totalUnrealizedProfit\": \"0.00000000\",   # total unrealized profit, only for USDT asset\n",
      "            #         \"totalMarginBalance\": \"23.72469206\",     # total margin balance, only for USDT asset\n",
      "            #         \"totalPositionInitialMargin\": \"0.00000000\",    # initial margin required for positions with current mark price, only for USDT asset\n",
      "            #         \"totalOpenOrderInitialMargin\": \"0.00000000\",   # initial margin required for open orders with current mark price, only for USDT asset\n",
      "            #         \"totalCrossWalletBalance\": \"23.72469206\",      # crossed wallet balance, only for USDT asset\n",
      "            #         \"totalCrossUnPnl\": \"0.00000000\",      # unrealized profit of crossed positions, only for USDT asset\n",
      "            #         \"availableBalance\": \"23.72469206\",       # available balance, only for USDT asset\n",
      "            #         \"maxWithdrawAmount\": \"23.72469206\"     # maximum amount for transfer out, only for USDT asset\n",
      "            #         ...\n",
      "            #     }\n",
      "            #\n",
      "            symbols = list(self.markets.keys())\n",
      "            result = {}\n",
      "            feeTier = self.safe_integer(response, 'feeTier')\n",
      "            feeTiers = self.fees[type]['trading']['tiers']\n",
      "            maker = feeTiers['maker'][feeTier][1]\n",
      "            taker = feeTiers['taker'][feeTier][1]\n",
      "            for i in range(0, len(symbols)):\n",
      "                symbol = symbols[i]\n",
      "                result[symbol] = {\n",
      "                    'info': {\n",
      "                        'feeTier': feeTier,\n",
      "                    },\n",
      "                    'symbol': symbol,\n",
      "                    'maker': maker,\n",
      "                    'taker': taker,\n",
      "                }\n",
      "            return result\n",
      "        elif type == 'delivery':\n",
      "            #\n",
      "            #     {\n",
      "            #         \"canDeposit\": True,\n",
      "            #         \"canTrade\": True,\n",
      "            #         \"canWithdraw\": True,\n",
      "            #         \"feeTier\": 2,\n",
      "            #         \"updateTime\": 0\n",
      "            #     }\n",
      "            #\n",
      "            symbols = list(self.markets.keys())\n",
      "            result = {}\n",
      "            feeTier = self.safe_integer(response, 'feeTier')\n",
      "            feeTiers = self.fees[type]['trading']['tiers']\n",
      "            maker = feeTiers['maker'][feeTier][1]\n",
      "            taker = feeTiers['taker'][feeTier][1]\n",
      "            for i in range(0, len(symbols)):\n",
      "                symbol = symbols[i]\n",
      "                result[symbol] = {\n",
      "                    'info': {\n",
      "                        'feeTier': feeTier,\n",
      "                    },\n",
      "                    'symbol': symbol,\n",
      "                    'maker': maker,\n",
      "                    'taker': taker,\n",
      "                }\n",
      "            return result\n",
      "\n",
      "    def futures_transfer(self, code, amount, type, params={}):\n",
      "        if (type < 1) or (type > 4):\n",
      "            raise ArgumentsRequired(self.id + ' type must be between 1 and 4')\n",
      "        self.load_markets()\n",
      "        currency = self.currency(code)\n",
      "        request = {\n",
      "            'asset': currency['id'],\n",
      "            'amount': amount,\n",
      "            'type': type,\n",
      "        }\n",
      "        response = self.sapiPostFuturesTransfer(self.extend(request, params))\n",
      "        #\n",
      "        #   {\n",
      "        #       \"tranId\": 100000001\n",
      "        #   }\n",
      "        #\n",
      "        return self.parse_transfer(response, currency)\n",
      "\n",
      "    def fetch_funding_rate(self, symbol, params={}):\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "        }\n",
      "        method = None\n",
      "        if market['linear']:\n",
      "            method = 'fapiPublicGetPremiumIndex'\n",
      "        elif market['inverse']:\n",
      "            method = 'dapiPublicGetPremiumIndex'\n",
      "        else:\n",
      "            raise NotSupported(self.id + ' fetchFundingRate() supports linear and inverse contracts only')\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        if market['inverse']:\n",
      "            response = response[0]\n",
      "        #\n",
      "        #     {\n",
      "        #         \"symbol\": \"BTCUSDT\",\n",
      "        #         \"markPrice\": \"45802.81129892\",\n",
      "        #         \"indexPrice\": \"45745.47701915\",\n",
      "        #         \"estimatedSettlePrice\": \"45133.91753671\",\n",
      "        #         \"lastFundingRate\": \"0.00063521\",\n",
      "        #         \"interestRate\": \"0.00010000\",\n",
      "        #         \"nextFundingTime\": \"1621267200000\",\n",
      "        #         \"time\": \"1621252344001\"\n",
      "        #     }\n",
      "        #\n",
      "        return self.parse_funding_rate(response, market)\n",
      "\n",
      "    def fetch_funding_rate_history(self, symbol=None, since=None, limit=None, params={}):\n",
      "        #\n",
      "        # Gets a history of funding rates with their timestamps\n",
      "        #  (param) symbol: Future currency pair(e.g. \"BTC/USDT\")\n",
      "        #  (param) limit: maximum number of data points returned\n",
      "        #  (param) since: Unix timestamp in miliseconds for the time of the earliest requested funding rate\n",
      "        #  (param) params: Object containing more params for the request\n",
      "        #          - until: Unix timestamp in miliseconds for the time of the earliest requested funding rate\n",
      "        #  return: [{symbol, fundingRate, timestamp}]\n",
      "        #\n",
      "        self.load_markets()\n",
      "        request = {}\n",
      "        method = None\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchFundingRateHistory', 'defaultType', 'future')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        params = self.omit(params, 'type')\n",
      "        if type == 'future':\n",
      "            method = 'fapiPublicGetFundingRate'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPublicGetFundingRate'\n",
      "        if symbol is not None:\n",
      "            market = self.market(symbol)\n",
      "            request['symbol'] = market['id']\n",
      "            if market['linear']:\n",
      "                method = 'fapiPublicGetFundingRate'\n",
      "            elif market['inverse']:\n",
      "                method = 'dapiPublicGetFundingRate'\n",
      "        if method is None:\n",
      "            raise NotSupported(self.id + ' fetchFundingRateHistory() not supported for ' + type + ' markets')\n",
      "        if since is not None:\n",
      "            request['startTime'] = since\n",
      "        till = self.safe_integer(params, 'till')  # unified in milliseconds\n",
      "        endTime = self.safe_string(params, 'endTime', till)  # exchange-specific in milliseconds\n",
      "        params = self.omit(params, ['endTime', 'till'])\n",
      "        if endTime is not None:\n",
      "            request['endTime'] = endTime\n",
      "        if limit is not None:\n",
      "            request['limit'] = limit\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        #\n",
      "        #     {\n",
      "        #         \"symbol\": \"BTCUSDT\",\n",
      "        #         \"fundingRate\": \"0.00063521\",\n",
      "        #         \"fundingTime\": \"1621267200000\",\n",
      "        #     }\n",
      "        #\n",
      "        rates = []\n",
      "        for i in range(0, len(response)):\n",
      "            entry = response[i]\n",
      "            timestamp = self.safe_integer(entry, 'fundingTime')\n",
      "            rates.append({\n",
      "                'info': entry,\n",
      "                'symbol': self.safe_symbol(self.safe_string(entry, 'symbol')),\n",
      "                'fundingRate': self.safe_number(entry, 'fundingRate'),\n",
      "                'timestamp': timestamp,\n",
      "                'datetime': self.iso8601(timestamp),\n",
      "            })\n",
      "        sorted = self.sort_by(rates, 'timestamp')\n",
      "        return self.filter_by_symbol_since_limit(sorted, symbol, since, limit)\n",
      "\n",
      "    def fetch_funding_rates(self, symbols=None, params={}):\n",
      "        self.load_markets()\n",
      "        method = None\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchFundingRates', 'defaultType', 'future')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        if type == 'future':\n",
      "            method = 'fapiPublicGetPremiumIndex'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPublicGetPremiumIndex'\n",
      "        else:\n",
      "            raise NotSupported(self.id + ' fetchFundingRates() supports linear and inverse contracts only')\n",
      "        response = getattr(self, method)(query)\n",
      "        result = []\n",
      "        for i in range(0, len(response)):\n",
      "            entry = response[i]\n",
      "            parsed = self.parse_funding_rate(entry)\n",
      "            result.append(parsed)\n",
      "        return self.filter_by_array(result, 'symbol', symbols)\n",
      "\n",
      "    def parse_funding_rate(self, premiumIndex, market=None):\n",
      "        # ensure it matches with https://www.binance.com/en/futures/funding-history/0\n",
      "        #\n",
      "        #   {\n",
      "        #     \"symbol\": \"BTCUSDT\",\n",
      "        #     \"markPrice\": \"45802.81129892\",\n",
      "        #     \"indexPrice\": \"45745.47701915\",\n",
      "        #     \"estimatedSettlePrice\": \"45133.91753671\",\n",
      "        #     \"lastFundingRate\": \"0.00063521\",\n",
      "        #     \"interestRate\": \"0.00010000\",\n",
      "        #     \"nextFundingTime\": \"1621267200000\",\n",
      "        #     \"time\": \"1621252344001\"\n",
      "        #  }\n",
      "        #\n",
      "        timestamp = self.safe_integer(premiumIndex, 'time')\n",
      "        marketId = self.safe_string(premiumIndex, 'symbol')\n",
      "        symbol = self.safe_symbol(marketId, market)\n",
      "        markPrice = self.safe_number(premiumIndex, 'markPrice')\n",
      "        indexPrice = self.safe_number(premiumIndex, 'indexPrice')\n",
      "        interestRate = self.safe_number(premiumIndex, 'interestRate')\n",
      "        estimatedSettlePrice = self.safe_number(premiumIndex, 'estimatedSettlePrice')\n",
      "        nextFundingRate = self.safe_number(premiumIndex, 'lastFundingRate')\n",
      "        nextFundingTime = self.safe_integer(premiumIndex, 'nextFundingTime')\n",
      "        previousFundingTime = nextFundingTime - (8 * 3600000)\n",
      "        return {\n",
      "            'info': premiumIndex,\n",
      "            'symbol': symbol,\n",
      "            'markPrice': markPrice,\n",
      "            'indexPrice': indexPrice,\n",
      "            'interestRate': interestRate,\n",
      "            'estimatedSettlePrice': estimatedSettlePrice,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'previousFundingRate': None,\n",
      "            'nextFundingRate': nextFundingRate,\n",
      "            'previousFundingTimestamp': previousFundingTime,  # subtract 8 hours\n",
      "            'nextFundingTimestamp': nextFundingTime,\n",
      "            'previousFundingDatetime': self.iso8601(previousFundingTime),\n",
      "            'nextFundingDatetime': self.iso8601(nextFundingTime),\n",
      "        }\n",
      "\n",
      "    def parse_account_positions(self, account):\n",
      "        positions = self.safe_value(account, 'positions')\n",
      "        assets = self.safe_value(account, 'assets')\n",
      "        balances = {}\n",
      "        for i in range(0, len(assets)):\n",
      "            entry = assets[i]\n",
      "            currencyId = self.safe_string(entry, 'asset')\n",
      "            code = self.safe_currency_code(currencyId)\n",
      "            crossWalletBalance = self.safe_string(entry, 'crossWalletBalance')\n",
      "            crossUnPnl = self.safe_string(entry, 'crossUnPnl')\n",
      "            balances[code] = {\n",
      "                'crossMargin': Precise.string_add(crossWalletBalance, crossUnPnl),\n",
      "                'crossWalletBalance': crossWalletBalance,\n",
      "            }\n",
      "        result = []\n",
      "        for i in range(0, len(positions)):\n",
      "            position = positions[i]\n",
      "            marketId = self.safe_string(position, 'symbol')\n",
      "            market = self.safe_market(marketId)\n",
      "            code = market['quote'] if (self.options['defaultType'] == 'future') else market['base']\n",
      "            # sometimes not all the codes are correctly returned...\n",
      "            if code in balances:\n",
      "                parsed = self.parse_account_position(self.extend(position, {\n",
      "                    'crossMargin': balances[code]['crossMargin'],\n",
      "                    'crossWalletBalance': balances[code]['crossWalletBalance'],\n",
      "                }), market)\n",
      "                result.append(parsed)\n",
      "        return result\n",
      "\n",
      "    def parse_account_position(self, position, market=None):\n",
      "        #\n",
      "        # usdm\n",
      "        #    {\n",
      "        #       \"symbol\": \"BTCBUSD\",\n",
      "        #       \"initialMargin\": \"0\",\n",
      "        #       \"maintMargin\": \"0\",\n",
      "        #       \"unrealizedProfit\": \"0.00000000\",\n",
      "        #       \"positionInitialMargin\": \"0\",\n",
      "        #       \"openOrderInitialMargin\": \"0\",\n",
      "        #       \"leverage\": \"20\",\n",
      "        #       \"isolated\": False,\n",
      "        #       \"entryPrice\": \"0.0000\",\n",
      "        #       \"maxNotional\": \"100000\",\n",
      "        #       \"positionSide\": \"BOTH\",\n",
      "        #       \"positionAmt\": \"0.000\",\n",
      "        #       \"notional\": \"0\",\n",
      "        #       \"isolatedWallet\": \"0\",\n",
      "        #       \"updateTime\": \"0\",\n",
      "        #       \"crossMargin\": \"100.93634809\",\n",
      "        #     }\n",
      "        #\n",
      "        # coinm\n",
      "        #     {\n",
      "        #       \"symbol\": \"BTCUSD_210625\",\n",
      "        #       \"initialMargin\": \"0.00024393\",\n",
      "        #       \"maintMargin\": \"0.00002439\",\n",
      "        #       \"unrealizedProfit\": \"-0.00000163\",\n",
      "        #       \"positionInitialMargin\": \"0.00024393\",\n",
      "        #       \"openOrderInitialMargin\": \"0\",\n",
      "        #       \"leverage\": \"10\",\n",
      "        #       \"isolated\": False,\n",
      "        #       \"positionSide\": \"BOTH\",\n",
      "        #       \"entryPrice\": \"41021.20000069\",\n",
      "        #       \"maxQty\": \"100\",\n",
      "        #       \"notionalValue\": \"0.00243939\",\n",
      "        #       \"isolatedWallet\": \"0\",\n",
      "        #       \"crossMargin\": \"0.314\"\n",
      "        #       \"crossWalletBalance\": \"34\",\n",
      "        #     }\n",
      "        #\n",
      "        marketId = self.safe_string(position, 'symbol')\n",
      "        market = self.safe_market(marketId, market)\n",
      "        symbol = market['symbol']\n",
      "        leverageString = self.safe_string(position, 'leverage')\n",
      "        leverage = int(leverageString)\n",
      "        initialMarginString = self.safe_string(position, 'initialMargin')\n",
      "        initialMargin = self.parse_number(initialMarginString)\n",
      "        initialMarginPercentageString = Precise.string_div('1', leverageString, 8)\n",
      "        rational = (1000 % leverage) == 0\n",
      "        if not rational:\n",
      "            initialMarginPercentageString = Precise.string_div(Precise.string_add(initialMarginPercentageString, '1e-8'), '1', 8)\n",
      "        usdm = ('notional' in position)\n",
      "        maintenanceMarginString = self.safe_string(position, 'maintMargin')\n",
      "        maintenanceMargin = self.parse_number(maintenanceMarginString)\n",
      "        entryPriceString = self.safe_string(position, 'entryPrice')\n",
      "        entryPrice = self.parse_number(entryPriceString)\n",
      "        notionalString = self.safe_string_2(position, 'notional', 'notionalValue')\n",
      "        notionalStringAbs = Precise.string_abs(notionalString)\n",
      "        notionalFloat = float(notionalString)\n",
      "        notionalFloatAbs = float(notionalStringAbs)\n",
      "        notional = self.parse_number(Precise.string_abs(notionalString))\n",
      "        contractsString = self.safe_string(position, 'positionAmt')\n",
      "        contractsStringAbs = Precise.string_abs(contractsString)\n",
      "        if contractsString is None:\n",
      "            entryNotional = Precise.string_mul(Precise.string_mul(leverageString, initialMarginString), entryPriceString)\n",
      "            contractsString = Precise.string_div(entryNotional, market['contractSize'])\n",
      "            contractsStringAbs = Precise.string_div(Precise.string_add(contractsString, '0.5'), '1', 0)\n",
      "        contracts = self.parse_number(contractsStringAbs)\n",
      "        leverageBrackets = self.safe_value(self.options, 'leverageBrackets', {})\n",
      "        leverageBracket = self.safe_value(leverageBrackets, symbol, [])\n",
      "        maintenanceMarginPercentageString = None\n",
      "        for i in range(0, len(leverageBracket)):\n",
      "            bracket = leverageBracket[i]\n",
      "            if notionalFloatAbs < bracket[0]:\n",
      "                break\n",
      "            maintenanceMarginPercentageString = bracket[1]\n",
      "        maintenanceMarginPercentage = self.parse_number(maintenanceMarginPercentageString)\n",
      "        unrealizedPnlString = self.safe_string(position, 'unrealizedProfit')\n",
      "        unrealizedPnl = self.parse_number(unrealizedPnlString)\n",
      "        timestamp = self.safe_integer(position, 'updateTime')\n",
      "        if timestamp == 0:\n",
      "            timestamp = None\n",
      "        isolated = self.safe_value(position, 'isolated')\n",
      "        marginType = None\n",
      "        collateralString = None\n",
      "        walletBalance = None\n",
      "        if isolated:\n",
      "            marginType = 'isolated'\n",
      "            walletBalance = self.safe_string(position, 'isolatedWallet')\n",
      "            collateralString = Precise.string_add(walletBalance, unrealizedPnlString)\n",
      "        else:\n",
      "            marginType = 'cross'\n",
      "            walletBalance = self.safe_string(position, 'crossWalletBalance')\n",
      "            collateralString = self.safe_string(position, 'crossMargin')\n",
      "        collateral = self.parse_number(collateralString)\n",
      "        marginRatio = None\n",
      "        side = None\n",
      "        percentage = None\n",
      "        liquidationPriceStringRaw = None\n",
      "        liquidationPrice = None\n",
      "        if notionalFloat == 0.0:\n",
      "            entryPrice = None\n",
      "        else:\n",
      "            side = 'short' if (notionalFloat < 0) else 'long'\n",
      "            marginRatio = self.parse_number(Precise.string_div(Precise.string_add(Precise.string_div(maintenanceMarginString, collateralString), '5e-5'), '1', 4))\n",
      "            percentage = self.parse_number(Precise.string_mul(Precise.string_div(unrealizedPnlString, initialMarginString, 4), '100'))\n",
      "            if usdm:\n",
      "                # calculate liquidation price\n",
      "                #\n",
      "                # liquidationPrice = (walletBalance / (contracts * (±1 + mmp))) + (±entryPrice / (±1 + mmp))\n",
      "                #\n",
      "                # mmp = maintenanceMarginPercentage\n",
      "                # where ± is negative for long and positive for short\n",
      "                # TODO: calculate liquidation price for coinm contracts\n",
      "                onePlusMaintenanceMarginPercentageString = None\n",
      "                entryPriceSignString = entryPriceString\n",
      "                if side == 'short':\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_add('1', maintenanceMarginPercentageString)\n",
      "                else:\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_add('-1', maintenanceMarginPercentageString)\n",
      "                    entryPriceSignString = Precise.string_mul('-1', entryPriceSignString)\n",
      "                leftSide = Precise.string_div(walletBalance, Precise.string_mul(contractsStringAbs, onePlusMaintenanceMarginPercentageString))\n",
      "                rightSide = Precise.string_div(entryPriceSignString, onePlusMaintenanceMarginPercentageString)\n",
      "                liquidationPriceStringRaw = Precise.string_add(leftSide, rightSide)\n",
      "            else:\n",
      "                # calculate liquidation price\n",
      "                #\n",
      "                # liquidationPrice = (contracts * contractSize(±1 - mmp)) / (±1/entryPrice * contracts * contractSize - walletBalance)\n",
      "                #\n",
      "                onePlusMaintenanceMarginPercentageString = None\n",
      "                entryPriceSignString = entryPriceString\n",
      "                if side == 'short':\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_sub('1', maintenanceMarginPercentageString)\n",
      "                else:\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_sub('-1', maintenanceMarginPercentageString)\n",
      "                    entryPriceSignString = Precise.string_mul('-1', entryPriceSignString)\n",
      "                size = Precise.string_mul(contractsStringAbs, market['contractSize'])\n",
      "                leftSide = Precise.string_mul(size, onePlusMaintenanceMarginPercentageString)\n",
      "                rightSide = Precise.string_sub(Precise.string_mul(Precise.string_div('1', entryPriceSignString), size), walletBalance)\n",
      "                liquidationPriceStringRaw = Precise.string_div(leftSide, rightSide)\n",
      "            pricePrecision = market['precision']['price']\n",
      "            pricePrecisionPlusOne = pricePrecision + 1\n",
      "            pricePrecisionPlusOneString = str(pricePrecisionPlusOne)\n",
      "            # round half up\n",
      "            rounder = Precise('5e-' + pricePrecisionPlusOneString)\n",
      "            rounderString = str(rounder)\n",
      "            liquidationPriceRoundedString = Precise.string_add(rounderString, liquidationPriceStringRaw)\n",
      "            truncatedLiquidationPrice = Precise.string_div(liquidationPriceRoundedString, '1', pricePrecision)\n",
      "            if truncatedLiquidationPrice[0] == '-':\n",
      "                # user cannot be liquidated\n",
      "                # since he has more collateral than the size of the position\n",
      "                truncatedLiquidationPrice = None\n",
      "            liquidationPrice = self.parse_number(truncatedLiquidationPrice)\n",
      "        positionSide = self.safe_string(position, 'positionSide')\n",
      "        hedged = positionSide != 'BOTH'\n",
      "        return {\n",
      "            'info': position,\n",
      "            'symbol': symbol,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'initialMargin': initialMargin,\n",
      "            'initialMarginPercentage': self.parse_number(initialMarginPercentageString),\n",
      "            'maintenanceMargin': maintenanceMargin,\n",
      "            'maintenanceMarginPercentage': maintenanceMarginPercentage,\n",
      "            'entryPrice': entryPrice,\n",
      "            'notional': notional,\n",
      "            'leverage': self.parse_number(leverageString),\n",
      "            'unrealizedPnl': unrealizedPnl,\n",
      "            'contracts': contracts,\n",
      "            'contractSize': self.parse_number(market['contractSize']),\n",
      "            'marginRatio': marginRatio,\n",
      "            'liquidationPrice': liquidationPrice,\n",
      "            'markPrice': None,\n",
      "            'collateral': collateral,\n",
      "            'marginType': marginType,\n",
      "            'side': side,\n",
      "            'hedged': hedged,\n",
      "            'percentage': percentage,\n",
      "        }\n",
      "\n",
      "    def parse_position_risk(self, position, market=None):\n",
      "        #\n",
      "        # usdm\n",
      "        #     {\n",
      "        #       \"symbol\": \"BTCUSDT\",\n",
      "        #       \"positionAmt\": \"0.001\",\n",
      "        #       \"entryPrice\": \"43578.07000\",\n",
      "        #       \"markPrice\": \"43532.30000000\",\n",
      "        #       \"unRealizedProfit\": \"-0.04577000\",\n",
      "        #       \"liquidationPrice\": \"21841.24993976\",\n",
      "        #       \"leverage\": \"2\",\n",
      "        #       \"maxNotionalValue\": \"300000000\",\n",
      "        #       \"marginType\": \"isolated\",\n",
      "        #       \"isolatedMargin\": \"21.77841506\",\n",
      "        #       \"isAutoAddMargin\": \"false\",\n",
      "        #       \"positionSide\": \"BOTH\",\n",
      "        #       \"notional\": \"43.53230000\",\n",
      "        #       \"isolatedWallet\": \"21.82418506\",\n",
      "        #       \"updateTime\": \"1621358023886\"\n",
      "        #     }\n",
      "        #\n",
      "        # coinm\n",
      "        #     {\n",
      "        #       \"symbol\": \"BTCUSD_PERP\",\n",
      "        #       \"positionAmt\": \"2\",\n",
      "        #       \"entryPrice\": \"37643.10000021\",\n",
      "        #       \"markPrice\": \"38103.05510455\",\n",
      "        #       \"unRealizedProfit\": \"0.00006413\",\n",
      "        #       \"liquidationPrice\": \"25119.97445760\",\n",
      "        #       \"leverage\": \"2\",\n",
      "        #       \"maxQty\": \"1500\",\n",
      "        #       \"marginType\": \"isolated\",\n",
      "        #       \"isolatedMargin\": \"0.00274471\",\n",
      "        #       \"isAutoAddMargin\": \"false\",\n",
      "        #       \"positionSide\": \"BOTH\",\n",
      "        #       \"notionalValue\": \"0.00524892\",\n",
      "        #       \"isolatedWallet\": \"0.00268058\"\n",
      "        #     }\n",
      "        #\n",
      "        marketId = self.safe_string(position, 'symbol')\n",
      "        market = self.safe_market(marketId, market)\n",
      "        symbol = market['symbol']\n",
      "        leverageBrackets = self.safe_value(self.options, 'leverageBrackets', {})\n",
      "        leverageBracket = self.safe_value(leverageBrackets, symbol, [])\n",
      "        notionalString = self.safe_string_2(position, 'notional', 'notionalValue')\n",
      "        notionalStringAbs = Precise.string_abs(notionalString)\n",
      "        notionalFloatAbs = float(notionalStringAbs)\n",
      "        notionalFloat = float(notionalString)\n",
      "        maintenanceMarginPercentageString = None\n",
      "        for i in range(0, len(leverageBracket)):\n",
      "            bracket = leverageBracket[i]\n",
      "            if notionalFloatAbs < bracket[0]:\n",
      "                break\n",
      "            maintenanceMarginPercentageString = bracket[1]\n",
      "        notional = self.parse_number(notionalStringAbs)\n",
      "        contractsAbs = Precise.string_abs(self.safe_string(position, 'positionAmt'))\n",
      "        contracts = self.parse_number(contractsAbs)\n",
      "        unrealizedPnlString = self.safe_string(position, 'unRealizedProfit')\n",
      "        unrealizedPnl = self.parse_number(unrealizedPnlString)\n",
      "        leverageString = self.safe_string(position, 'leverage')\n",
      "        leverage = int(leverageString)\n",
      "        liquidationPriceString = self.omit_zero(self.safe_string(position, 'liquidationPrice'))\n",
      "        liquidationPrice = self.parse_number(liquidationPriceString)\n",
      "        collateralString = None\n",
      "        marginType = self.safe_string(position, 'marginType')\n",
      "        side = None\n",
      "        if notionalFloat > 0:\n",
      "            side = 'long'\n",
      "        elif notionalFloat < 0:\n",
      "            side = 'short'\n",
      "        entryPriceString = self.safe_string(position, 'entryPrice')\n",
      "        entryPrice = self.parse_number(entryPriceString)\n",
      "        if marginType == 'cross':\n",
      "            # calculate collateral\n",
      "            if market['linear']:\n",
      "                # walletBalance = (liquidationPrice * (±1 + mmp) ± entryPrice) * contracts\n",
      "                onePlusMaintenanceMarginPercentageString = None\n",
      "                entryPriceSignString = entryPriceString\n",
      "                if side == 'short':\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_add('1', maintenanceMarginPercentageString)\n",
      "                    entryPriceSignString = Precise.string_mul('-1', entryPriceSignString)\n",
      "                else:\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_add('-1', maintenanceMarginPercentageString)\n",
      "                inner = Precise.string_mul(liquidationPriceString, onePlusMaintenanceMarginPercentageString)\n",
      "                leftSide = Precise.string_add(inner, entryPriceSignString)\n",
      "                collateralString = Precise.string_div(Precise.string_mul(leftSide, contractsAbs), '1', market['precision']['quote'])\n",
      "            else:\n",
      "                # walletBalance = (contracts * contractSize) * (±1/entryPrice - (±1 - mmp) / liquidationPrice)\n",
      "                onePlusMaintenanceMarginPercentageString = None\n",
      "                entryPriceSignString = entryPriceString\n",
      "                if side == 'short':\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_sub('1', maintenanceMarginPercentageString)\n",
      "                else:\n",
      "                    onePlusMaintenanceMarginPercentageString = Precise.string_sub('-1', maintenanceMarginPercentageString)\n",
      "                    entryPriceSignString = Precise.string_mul('-1', entryPriceSignString)\n",
      "                leftSide = Precise.string_mul(contractsAbs, market['contractSize'])\n",
      "                rightSide = Precise.string_sub(Precise.string_div('1', entryPriceSignString), Precise.string_div(onePlusMaintenanceMarginPercentageString, liquidationPriceString))\n",
      "                collateralString = Precise.string_div(Precise.string_mul(leftSide, rightSide), '1', market['precision']['base'])\n",
      "        else:\n",
      "            collateralString = self.safe_string(position, 'isolatedMargin')\n",
      "        collateralString = '0' if (collateralString is None) else collateralString\n",
      "        collateralFloat = float(collateralString)\n",
      "        collateral = self.parse_number(collateralString)\n",
      "        markPrice = self.parse_number(self.omit_zero(self.safe_string(position, 'markPrice')))\n",
      "        timestamp = self.safe_integer(position, 'updateTime')\n",
      "        if timestamp == 0:\n",
      "            timestamp = None\n",
      "        maintenanceMarginPercentage = self.parse_number(maintenanceMarginPercentageString)\n",
      "        maintenanceMarginString = Precise.string_mul(maintenanceMarginPercentageString, notionalStringAbs)\n",
      "        maintenanceMargin = self.parse_number(maintenanceMarginString)\n",
      "        initialMarginPercentageString = Precise.string_div('1', leverageString, 8)\n",
      "        rational = (1000 % leverage) == 0\n",
      "        if not rational:\n",
      "            initialMarginPercentageString = Precise.string_add(initialMarginPercentageString, '1e-8')\n",
      "        initialMarginString = Precise.string_div(Precise.string_mul(notionalStringAbs, initialMarginPercentageString), '1', 8)\n",
      "        initialMargin = self.parse_number(initialMarginString)\n",
      "        marginRatio = None\n",
      "        percentage = None\n",
      "        if collateralFloat != 0.0:\n",
      "            marginRatio = self.parse_number(Precise.string_div(Precise.string_add(Precise.string_div(maintenanceMarginString, collateralString), '5e-5'), '1', 4))\n",
      "            percentage = self.parse_number(Precise.string_mul(Precise.string_div(unrealizedPnlString, initialMarginString, 4), '100'))\n",
      "        positionSide = self.safe_string(position, 'positionSide')\n",
      "        hedged = positionSide != 'BOTH'\n",
      "        return {\n",
      "            'info': position,\n",
      "            'symbol': symbol,\n",
      "            'contracts': contracts,\n",
      "            'contractSize': self.parse_number(market['contractSize']),\n",
      "            'unrealizedPnl': unrealizedPnl,\n",
      "            'leverage': self.parse_number(leverageString),\n",
      "            'liquidationPrice': liquidationPrice,\n",
      "            'collateral': collateral,\n",
      "            'notional': notional,\n",
      "            'markPrice': markPrice,\n",
      "            'entryPrice': entryPrice,\n",
      "            'timestamp': timestamp,\n",
      "            'initialMargin': initialMargin,\n",
      "            'initialMarginPercentage': self.parse_number(initialMarginPercentageString),\n",
      "            'maintenanceMargin': maintenanceMargin,\n",
      "            'maintenanceMarginPercentage': maintenanceMarginPercentage,\n",
      "            'marginRatio': marginRatio,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'marginType': marginType,\n",
      "            'side': side,\n",
      "            'hedged': hedged,\n",
      "            'percentage': percentage,\n",
      "        }\n",
      "\n",
      "    def load_leverage_brackets(self, reload=False, params={}):\n",
      "        self.load_markets()\n",
      "        # by default cache the leverage bracket\n",
      "        # it contains useful stuff like the maintenance margin and initial margin for positions\n",
      "        leverageBrackets = self.safe_value(self.options, 'leverageBrackets')\n",
      "        if (leverageBrackets is None) or (reload):\n",
      "            method = None\n",
      "            defaultType = self.safe_string(self.options, 'defaultType', 'future')\n",
      "            type = self.safe_string(params, 'type', defaultType)\n",
      "            query = self.omit(params, 'type')\n",
      "            if type == 'future':\n",
      "                method = 'fapiPrivateGetLeverageBracket'\n",
      "            elif type == 'delivery':\n",
      "                method = 'dapiPrivateV2GetLeverageBracket'\n",
      "            else:\n",
      "                raise NotSupported(self.id + ' loadLeverageBrackets() supports linear and inverse contracts only')\n",
      "            response = getattr(self, method)(query)\n",
      "            self.options['leverageBrackets'] = {}\n",
      "            for i in range(0, len(response)):\n",
      "                entry = response[i]\n",
      "                marketId = self.safe_string(entry, 'symbol')\n",
      "                symbol = self.safe_symbol(marketId)\n",
      "                brackets = self.safe_value(entry, 'brackets')\n",
      "                result = []\n",
      "                for j in range(0, len(brackets)):\n",
      "                    bracket = brackets[j]\n",
      "                    # we use floats here internally on purpose\n",
      "                    floorValue = self.safe_float_2(bracket, 'notionalFloor', 'qtyFloor')\n",
      "                    maintenanceMarginPercentage = self.safe_string(bracket, 'maintMarginRatio')\n",
      "                    result.append([floorValue, maintenanceMarginPercentage])\n",
      "                self.options['leverageBrackets'][symbol] = result\n",
      "        return self.options['leverageBrackets']\n",
      "\n",
      "    def fetch_positions(self, symbols=None, params={}):\n",
      "        defaultMethod = self.safe_string(self.options, 'fetchPositions', 'positionRisk')\n",
      "        if defaultMethod == 'positionRisk':\n",
      "            return self.fetch_positions_risk(symbols, params)\n",
      "        elif defaultMethod == 'account':\n",
      "            return self.fetch_account_positions(symbols, params)\n",
      "        else:\n",
      "            raise NotSupported(self.id + '.options[\"fetchPositions\"] = \"' + defaultMethod + '\" is invalid, please choose between \"account\" and \"positionRisk\"')\n",
      "\n",
      "    def fetch_account_positions(self, symbols=None, params={}):\n",
      "        if symbols is not None:\n",
      "            if not isinstance(symbols, list):\n",
      "                raise ArgumentsRequired(self.id + ' fetchPositions requires an array argument for symbols')\n",
      "        self.load_markets()\n",
      "        self.load_leverage_brackets()\n",
      "        method = None\n",
      "        defaultType = self.safe_string(self.options, 'defaultType', 'future')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        query = self.omit(params, 'type')\n",
      "        if type == 'future':\n",
      "            method = 'fapiPrivateGetAccount'\n",
      "        elif type == 'delivery':\n",
      "            method = 'dapiPrivateGetAccount'\n",
      "        else:\n",
      "            raise NotSupported(self.id + ' fetchPositions() supports linear and inverse contracts only')\n",
      "        account = getattr(self, method)(query)\n",
      "        result = self.parse_account_positions(account)\n",
      "        return self.filter_by_array(result, 'symbol', symbols, False)\n",
      "\n",
      "    def fetch_positions_risk(self, symbols=None, params={}):\n",
      "        if symbols is not None:\n",
      "            if not isinstance(symbols, list):\n",
      "                raise ArgumentsRequired(self.id + ' fetchPositions requires an array argument for symbols')\n",
      "        self.load_markets()\n",
      "        self.load_leverage_brackets()\n",
      "        request = {}\n",
      "        method = None\n",
      "        defaultType = 'future'\n",
      "        defaultType = self.safe_string(self.options, 'defaultType', defaultType)\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        params = self.omit(params, 'type')\n",
      "        if (type == 'future') or (type == 'linear'):\n",
      "            method = 'fapiPrivateGetPositionRisk'\n",
      "        elif (type == 'delivery') or (type == 'inverse'):\n",
      "            method = 'dapiPrivateGetPositionRisk'\n",
      "        else:\n",
      "            raise NotSupported(self.id + ' fetchIsolatedPositions() supports linear and inverse contracts only')\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        result = []\n",
      "        for i in range(0, len(response)):\n",
      "            parsed = self.parse_position_risk(response[i])\n",
      "            result.append(parsed)\n",
      "        return self.filter_by_array(result, 'symbol', symbols, False)\n",
      "\n",
      "    def fetch_funding_history(self, symbol=None, since=None, limit=None, params={}):\n",
      "        self.load_markets()\n",
      "        market = None\n",
      "        method = None\n",
      "        defaultType = 'future'\n",
      "        request = {\n",
      "            'incomeType': 'FUNDING_FEE',  # \"TRANSFER\"，\"WELCOME_BONUS\", \"REALIZED_PNL\"，\"FUNDING_FEE\", \"COMMISSION\" and \"INSURANCE_CLEAR\"\n",
      "        }\n",
      "        if symbol is not None:\n",
      "            market = self.market(symbol)\n",
      "            request['symbol'] = market['id']\n",
      "            if market['linear']:\n",
      "                defaultType = 'future'\n",
      "            elif market['inverse']:\n",
      "                defaultType = 'delivery'\n",
      "            else:\n",
      "                raise NotSupported(self.id + ' fetchFundingHistory() supports linear and inverse contracts only')\n",
      "        if since is not None:\n",
      "            request['startTime'] = since\n",
      "        if limit is not None:\n",
      "            request['limit'] = limit\n",
      "        defaultType = self.safe_string_2(self.options, 'fetchFundingHistory', 'defaultType', defaultType)\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        params = self.omit(params, 'type')\n",
      "        if (type == 'future') or (type == 'linear'):\n",
      "            method = 'fapiPrivateGetIncome'\n",
      "        elif (type == 'delivery') or (type == 'inverse'):\n",
      "            method = 'dapiPrivateGetIncome'\n",
      "        else:\n",
      "            raise NotSupported(self.id + ' fetchFundingHistory() supports linear and inverse contracts only')\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        return self.parse_incomes(response, market, since, limit)\n",
      "\n",
      "    def set_leverage(self, leverage, symbol=None, params={}):\n",
      "        if symbol is None:\n",
      "            raise ArgumentsRequired(self.id + ' setLeverage() requires a symbol argument')\n",
      "        # WARNING: THIS WILL INCREASE LIQUIDATION PRICE FOR OPEN ISOLATED LONG POSITIONS\n",
      "        # AND DECREASE LIQUIDATION PRICE FOR OPEN ISOLATED SHORT POSITIONS\n",
      "        if (leverage < 1) or (leverage > 125):\n",
      "            raise BadRequest(self.id + ' leverage should be between 1 and 125')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        method = None\n",
      "        if market['linear']:\n",
      "            method = 'fapiPrivatePostLeverage'\n",
      "        elif market['inverse']:\n",
      "            method = 'dapiPrivatePostLeverage'\n",
      "        else:\n",
      "            raise NotSupported(self.id + ' setLeverage() supports linear and inverse contracts only')\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "            'leverage': leverage,\n",
      "        }\n",
      "        return getattr(self, method)(self.extend(request, params))\n",
      "\n",
      "    def set_margin_mode(self, marginType, symbol=None, params={}):\n",
      "        #\n",
      "        # {\"code\": -4048 , \"msg\": \"Margin type cannot be changed if there exists position.\"}\n",
      "        #\n",
      "        # or\n",
      "        #\n",
      "        # {\"code\": 200, \"msg\": \"success\"}\n",
      "        #\n",
      "        marginType = marginType.upper()\n",
      "        if (marginType != 'ISOLATED') and (marginType != 'CROSSED'):\n",
      "            raise BadRequest(self.id + ' marginType must be either isolated or crossed')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        method = None\n",
      "        if market['linear']:\n",
      "            method = 'fapiPrivatePostMarginType'\n",
      "        elif market['inverse']:\n",
      "            method = 'dapiPrivatePostMarginType'\n",
      "        else:\n",
      "            raise NotSupported(self.id + ' setMarginMode() supports linear and inverse contracts only')\n",
      "        request = {\n",
      "            'symbol': market['id'],\n",
      "            'marginType': marginType,\n",
      "        }\n",
      "        return getattr(self, method)(self.extend(request, params))\n",
      "\n",
      "    def set_position_mode(self, hedged, symbol=None, params={}):\n",
      "        defaultType = self.safe_string(self.options, 'defaultType', 'future')\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        params = self.omit(params, ['type'])\n",
      "        dualSidePosition = None\n",
      "        if hedged:\n",
      "            dualSidePosition = 'true'\n",
      "        else:\n",
      "            dualSidePosition = 'false'\n",
      "        request = {\n",
      "            'dualSidePosition': dualSidePosition,\n",
      "        }\n",
      "        method = None\n",
      "        if type == 'delivery':\n",
      "            method = 'dapiPrivatePostPositionSideDual'\n",
      "        else:\n",
      "            # default to future\n",
      "            method = 'fapiPrivatePostPositionSideDual'\n",
      "        #\n",
      "        #     {\n",
      "        #       \"code\": 200,\n",
      "        #       \"msg\": \"success\"\n",
      "        #     }\n",
      "        #\n",
      "        return getattr(self, method)(self.extend(request, params))\n",
      "\n",
      "    def sign(self, path, api='public', method='GET', params={}, headers=None, body=None):\n",
      "        if not (api in self.urls['api']):\n",
      "            raise NotSupported(self.id + ' does not have a testnet/sandbox URL for ' + api + ' endpoints')\n",
      "        url = self.urls['api'][api]\n",
      "        url += '/' + path\n",
      "        if api == 'wapi':\n",
      "            url += '.html'\n",
      "        if path == 'historicalTrades':\n",
      "            if self.apiKey:\n",
      "                headers = {\n",
      "                    'X-MBX-APIKEY': self.apiKey,\n",
      "                }\n",
      "            else:\n",
      "                raise AuthenticationError(self.id + ' historicalTrades endpoint requires `apiKey` credential')\n",
      "        userDataStream = (path == 'userDataStream') or (path == 'listenKey')\n",
      "        if userDataStream:\n",
      "            if self.apiKey:\n",
      "                # v1 special case for userDataStream\n",
      "                headers = {\n",
      "                    'X-MBX-APIKEY': self.apiKey,\n",
      "                    'Content-Type': 'application/x-www-form-urlencoded',\n",
      "                }\n",
      "                if method != 'GET':\n",
      "                    body = self.urlencode(params)\n",
      "            else:\n",
      "                raise AuthenticationError(self.id + ' userDataStream endpoint requires `apiKey` credential')\n",
      "        elif (api == 'private') or (api == 'sapi') or (api == 'wapi' and path != 'systemStatus') or (api == 'dapiPrivate') or (api == 'dapiPrivateV2') or (api == 'fapiPrivate') or (api == 'fapiPrivateV2'):\n",
      "            self.check_required_credentials()\n",
      "            query = None\n",
      "            recvWindow = self.safe_integer(self.options, 'recvWindow', 5000)\n",
      "            if (api == 'sapi') and (path == 'asset/dust'):\n",
      "                query = self.urlencode_with_array_repeat(self.extend({\n",
      "                    'timestamp': self.nonce(),\n",
      "                    'recvWindow': recvWindow,\n",
      "                }, params))\n",
      "            elif (path == 'batchOrders') or (path.find('sub-account') >= 0):\n",
      "                query = self.rawencode(self.extend({\n",
      "                    'timestamp': self.nonce(),\n",
      "                    'recvWindow': recvWindow,\n",
      "                }, params))\n",
      "            else:\n",
      "                query = self.urlencode(self.extend({\n",
      "                    'timestamp': self.nonce(),\n",
      "                    'recvWindow': recvWindow,\n",
      "                }, params))\n",
      "            signature = self.hmac(self.encode(query), self.encode(self.secret))\n",
      "            query += '&' + 'signature=' + signature\n",
      "            headers = {\n",
      "                'X-MBX-APIKEY': self.apiKey,\n",
      "            }\n",
      "            if (method == 'GET') or (method == 'DELETE') or (api == 'wapi'):\n",
      "                url += '?' + query\n",
      "            else:\n",
      "                body = query\n",
      "                headers['Content-Type'] = 'application/x-www-form-urlencoded'\n",
      "        else:\n",
      "            if params:\n",
      "                url += '?' + self.urlencode(params)\n",
      "        return {'url': url, 'method': method, 'body': body, 'headers': headers}\n",
      "\n",
      "    def handle_errors(self, code, reason, url, method, headers, body, response, requestHeaders, requestBody):\n",
      "        if (code == 418) or (code == 429):\n",
      "            raise DDoSProtection(self.id + ' ' + str(code) + ' ' + reason + ' ' + body)\n",
      "        # error response in a form: {\"code\": -1013, \"msg\": \"Invalid quantity.\"}\n",
      "        # following block cointains legacy checks against message patterns in \"msg\" property\n",
      "        # will switch \"code\" checks eventually, when we know all of them\n",
      "        if code >= 400:\n",
      "            if body.find('Price * QTY is zero or less') >= 0:\n",
      "                raise InvalidOrder(self.id + ' order cost = amount * price is zero or less ' + body)\n",
      "            if body.find('LOT_SIZE') >= 0:\n",
      "                raise InvalidOrder(self.id + ' order amount should be evenly divisible by lot size ' + body)\n",
      "            if body.find('PRICE_FILTER') >= 0:\n",
      "                raise InvalidOrder(self.id + ' order price is invalid, i.e. exceeds allowed price precision, exceeds min price or max price limits or is invalid float value in general, use self.price_to_precision(symbol, amount) ' + body)\n",
      "        if response is None:\n",
      "            return  # fallback to default error handler\n",
      "        # check success value for wapi endpoints\n",
      "        # response in format {'msg': 'The coin does not exist.', 'success': True/false}\n",
      "        success = self.safe_value(response, 'success', True)\n",
      "        if not success:\n",
      "            message = self.safe_string(response, 'msg')\n",
      "            parsedMessage = None\n",
      "            if message is not None:\n",
      "                try:\n",
      "                    parsedMessage = json.loads(message)\n",
      "                except Exception as e:\n",
      "                    # do nothing\n",
      "                    parsedMessage = None\n",
      "                if parsedMessage is not None:\n",
      "                    response = parsedMessage\n",
      "        message = self.safe_string(response, 'msg')\n",
      "        if message is not None:\n",
      "            self.throw_exactly_matched_exception(self.exceptions['exact'], message, self.id + ' ' + message)\n",
      "            self.throw_broadly_matched_exception(self.exceptions['broad'], message, self.id + ' ' + message)\n",
      "        # checks against error codes\n",
      "        error = self.safe_string(response, 'code')\n",
      "        if error is not None:\n",
      "            # https://github.com/ccxt/ccxt/issues/6501\n",
      "            # https://github.com/ccxt/ccxt/issues/7742\n",
      "            if (error == '200') or Precise.string_equals(error, '0'):\n",
      "                return\n",
      "            # a workaround for {\"code\":-2015,\"msg\":\"Invalid API-key, IP, or permissions for action.\"}\n",
      "            # despite that their message is very confusing, it is raised by Binance\n",
      "            # on a temporary ban, the API key is valid, but disabled for a while\n",
      "            if (error == '-2015') and self.options['hasAlreadyAuthenticatedSuccessfully']:\n",
      "                raise DDoSProtection(self.id + ' temporary banned: ' + body)\n",
      "            feedback = self.id + ' ' + body\n",
      "            self.throw_exactly_matched_exception(self.exceptions['exact'], error, feedback)\n",
      "            raise ExchangeError(feedback)\n",
      "        if not success:\n",
      "            raise ExchangeError(self.id + ' ' + body)\n",
      "\n",
      "    def calculate_rate_limiter_cost(self, api, method, path, params, config={}, context={}):\n",
      "        if ('noSymbol' in config) and not ('symbol' in params):\n",
      "            return config['noSymbol']\n",
      "        elif ('noPoolId' in config) and not ('poolId' in params):\n",
      "            return config['noPoolId']\n",
      "        elif ('byLimit' in config) and ('limit' in params):\n",
      "            limit = params['limit']\n",
      "            byLimit = config['byLimit']\n",
      "            for i in range(0, len(byLimit)):\n",
      "                entry = byLimit[i]\n",
      "                if limit <= entry[0]:\n",
      "                    return entry[1]\n",
      "        return self.safe_integer(config, 'cost', 1)\n",
      "\n",
      "    def request(self, path, api='public', method='GET', params={}, headers=None, body=None, config={}, context={}):\n",
      "        response = self.fetch2(path, api, method, params, headers, body, config, context)\n",
      "        # a workaround for {\"code\":-2015,\"msg\":\"Invalid API-key, IP, or permissions for action.\"}\n",
      "        if (api == 'private') or (api == 'wapi'):\n",
      "            self.options['hasAlreadyAuthenticatedSuccessfully'] = True\n",
      "        return response\n",
      "\n",
      "    def modify_margin_helper(self, symbol, amount, addOrReduce, params={}):\n",
      "        # used to modify isolated positions\n",
      "        defaultType = self.safe_string(self.options, 'defaultType', 'future')\n",
      "        if defaultType == 'spot':\n",
      "            defaultType = 'future'\n",
      "        type = self.safe_string(params, 'type', defaultType)\n",
      "        if (type == 'margin') or (type == 'spot'):\n",
      "            raise NotSupported(self.id + ' add / reduce margin only supported with type future or delivery')\n",
      "        self.load_markets()\n",
      "        market = self.market(symbol)\n",
      "        request = {\n",
      "            'type': addOrReduce,\n",
      "            'symbol': market['id'],\n",
      "            'amount': amount,\n",
      "        }\n",
      "        method = None\n",
      "        code = None\n",
      "        if type == 'future':\n",
      "            method = 'fapiPrivatePostPositionMargin'\n",
      "            code = market['quote']\n",
      "        else:\n",
      "            method = 'dapiPrivatePostPositionMargin'\n",
      "            code = market['base']\n",
      "        response = getattr(self, method)(self.extend(request, params))\n",
      "        #\n",
      "        #     {\n",
      "        #       \"code\": 200,\n",
      "        #       \"msg\": \"Successfully modify position margin.\",\n",
      "        #       \"amount\": 0.001,\n",
      "        #       \"type\": 1\n",
      "        #     }\n",
      "        #\n",
      "        rawType = self.safe_integer(response, 'type')\n",
      "        resultType = 'add' if (rawType == 1) else 'reduce'\n",
      "        resultAmount = self.safe_number(response, 'amount')\n",
      "        errorCode = self.safe_string(response, 'code')\n",
      "        status = 'ok' if (errorCode == '200') else 'failed'\n",
      "        return {\n",
      "            'info': response,\n",
      "            'type': resultType,\n",
      "            'amount': resultAmount,\n",
      "            'code': code,\n",
      "            'symbol': market['symbol'],\n",
      "            'status': status,\n",
      "        }\n",
      "\n",
      "    def reduce_margin(self, symbol, amount, params={}):\n",
      "        return self.modify_margin_helper(symbol, amount, 2, params)\n",
      "\n",
      "    def add_margin(self, symbol, amount, params={}):\n",
      "        return self.modify_margin_helper(symbol, amount, 1, params)\n",
      "\n",
      "    def fetch_borrow_rate(self, code, params={}):\n",
      "        self.load_markets()\n",
      "        currency = self.currency(code)\n",
      "        request = {\n",
      "            'asset': currency['id'],\n",
      "            # 'vipLevel': self.safe_integer(params, 'vipLevel'),\n",
      "        }\n",
      "        response = self.sapiGetMarginInterestRateHistory(self.extend(request, params))\n",
      "        #\n",
      "        # [\n",
      "        #     {\n",
      "        #         \"asset\": \"USDT\",\n",
      "        #         \"timestamp\": 1638230400000,\n",
      "        #         \"dailyInterestRate\": \"0.0006\",\n",
      "        #         \"vipLevel\": 0\n",
      "        #     },\n",
      "        #     ...\n",
      "        # ]\n",
      "        #\n",
      "        rate = self.safe_value(response, 0)\n",
      "        timestamp = self.safe_number(rate, 'timestamp')\n",
      "        return {\n",
      "            'currency': code,\n",
      "            'rate': self.safe_number(rate, 'dailyInterestRate'),\n",
      "            'period': 86400000,\n",
      "            'timestamp': timestamp,\n",
      "            'datetime': self.iso8601(timestamp),\n",
      "            'info': response,\n",
      "        }\n",
      "\n",
      "from arm.logicnode.arm_nodes import *\n",
      "\n",
      "class SetTransformNode(ArmLogicTreeNode):\n",
      "    \"\"\"Use to set the transform of an object.\"\"\"\n",
      "    bl_idname = 'LNSetTransformNode'\n",
      "    bl_label = 'Set Object Transform'\n",
      "    arm_version = 1\n",
      "\n",
      "    def init(self, context):\n",
      "        super(SetTransformNode, self).init(context)\n",
      "        self.add_input('ArmNodeSocketAction', 'In')\n",
      "        self.add_input('ArmNodeSocketObject', 'Object')\n",
      "        self.add_input('NodeSocketShader', 'Transform')\n",
      "        self.add_output('ArmNodeSocketAction', 'Out')\n",
      "\n",
      "add_node(SetTransformNode, category=PKG_AS_CATEGORY)\n",
      "\n",
      "# SPDX-FileCopyrightText: 2017 Scott Shawcroft, written for Adafruit Industries\n",
      "# SPDX-FileCopyrightText: Copyright (c) 2021 Jose David M. for circuitpython\n",
      "#\n",
      "# SPDX-License-Identifier: MIT\n",
      "\n",
      "\"\"\"A setuptools based setup module.\n",
      "\n",
      "See:\n",
      "https://packaging.python.org/en/latest/distributing.html\n",
      "https://github.com/pypa/sampleproject\n",
      "\"\"\"\n",
      "\n",
      "from setuptools import setup, find_packages\n",
      "\n",
      "# To use a consistent encoding\n",
      "from codecs import open\n",
      "from os import path\n",
      "\n",
      "here = path.abspath(path.dirname(__file__))\n",
      "\n",
      "# Get the long description from the README file\n",
      "with open(path.join(here, \"README.rst\"), encoding=\"utf-8\") as f:\n",
      "    long_description = f.read()\n",
      "\n",
      "setup(\n",
      "    # Community Bundle Information\n",
      "    name=\"circuitpython-displayio-cartesian\",\n",
      "    use_scm_version=True,\n",
      "    setup_requires=[\"setuptools_scm\"],\n",
      "    description=\"A cartesian plane widget for displaying graphical information.\",\n",
      "    long_description=long_description,\n",
      "    long_description_content_type=\"text/x-rst\",\n",
      "    # The project's main homepage.\n",
      "    url=\"https://github.com/circuitpython/CircuitPython_Org_DisplayIO_Cartesian.git\",\n",
      "    # Author details\n",
      "    author=\"Jose David M.\",\n",
      "    author_email=\"\",\n",
      "    install_requires=[\n",
      "        \"Adafruit-Blinka\",\n",
      "        \"adafruit-circuitpython-display-text\",\n",
      "        \"adafruit-circuitpython-displayio-layout\",\n",
      "    ],\n",
      "    # Choose your license\n",
      "    license=\"MIT\",\n",
      "    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n",
      "    classifiers=[\n",
      "        \"Development Status :: 3 - Alpha\",\n",
      "        \"Intended Audience :: Developers\",\n",
      "        \"Topic :: Software Development :: Libraries\",\n",
      "        \"Topic :: System :: Hardware\",\n",
      "        \"License :: OSI Approved :: MIT License\",\n",
      "        \"Programming Language :: Python :: 3\",\n",
      "        \"Programming Language :: Python :: 3.4\",\n",
      "        \"Programming Language :: Python :: 3.5\",\n",
      "    ],\n",
      "    # What does your project relate to?\n",
      "    keywords=\"adafruit blinka circuitpython micropython displayio_cartesian displayio widget \"\n",
      "    \"graphics gui graph chart graphic\",\n",
      "    # You can just specify the packages manually here if your project is\n",
      "    # simple. Or you can use find_packages().\n",
      "    # TODO: IF LIBRARY FILES ARE A PACKAGE FOLDER,\n",
      "    #       CHANGE `py_modules=['...']` TO `packages=['...']`\n",
      "    py_modules=[\"displayio_cartesian\"],\n",
      ")\n",
      "\n",
      "from ssmpfwd.helpers import verify_plugin_version, verbose_debug_quiet, time_decorator\n",
      "from unittest.mock import MagicMock, patch\n",
      "import unittest\n",
      "\n",
      "\n",
      "class TestVerifyPluginVersion(unittest.TestCase):\n",
      "    @patch(\"ssmpfwd.helpers.subprocess\")\n",
      "    def test_verify_plugin_version_success(self, mock_subprocess):\n",
      "        result = mock_subprocess.run()\n",
      "        result.stdout = b\"9.8.3\"\n",
      "        self.assertTrue(verify_plugin_version(\"9.8.3\"))\n",
      "\n",
      "    @patch(\"ssmpfwd.helpers.subprocess\")\n",
      "    def test_verify_plugin_version_fail(self, mock_subprocess):\n",
      "        with self.assertLogs(\"ssmpfwd.helpers\", level=\"INFO\") as cm:\n",
      "            result = mock_subprocess.run()\n",
      "            result.stdout = b\"1.8.1\"\n",
      "            self.assertFalse(verify_plugin_version(\"9.2.3\"))\n",
      "            self.assertEqual(cm.output[0], \"ERROR:ssmpfwd.helpers:session-manager-plugin version 1.8.1 is installed, 9.2.3 is required\")\n",
      "\n",
      "\n",
      "class TestVerboseDebugQuiet(unittest.TestCase):\n",
      "    import logging\n",
      "\n",
      "    def setUp(self):\n",
      "        @verbose_debug_quiet\n",
      "        def test_func():\n",
      "            pass\n",
      "\n",
      "        self.vdq = test_func\n",
      "        self.vdq()\n",
      "\n",
      "    def test_quiet(self):\n",
      "        option_name = \"quiet\"\n",
      "        self.assertTrue(any([p.name == option_name for p in self.vdq.__click_params__]), msg=f\"Can not find {option_name} in option parameters\")\n",
      "\n",
      "    def test_debug(self):\n",
      "        flag_value = self.logging.DEBUG\n",
      "        self.assertTrue(any([p.flag_value == flag_value for p in self.vdq.__click_params__]), msg=f\"Can not find {flag_value} in option flag values\")\n",
      "\n",
      "    def test_verbose(self):\n",
      "        flag_value = self.logging.INFO\n",
      "        self.assertTrue(any([p.flag_value == flag_value for p in self.vdq.__click_params__]), msg=f\"Can not find {flag_value} in option flag values\")\n",
      "\n",
      "    def test_default_loglevel(self):\n",
      "        flag_value = self.logging.WARN\n",
      "        self.assertTrue(any([p.flag_value == flag_value for p in self.vdq.__click_params__]), msg=f\"Can not find {flag_value} in option flag values\")\n",
      "\n",
      "\n",
      "class TestTimeDecorator(unittest.TestCase):\n",
      "    from time import sleep\n",
      "\n",
      "    def setUp(self):\n",
      "        @time_decorator\n",
      "        def test_func():\n",
      "            self.sleep(0.5)\n",
      "\n",
      "        self.time_decorated_method = test_func\n",
      "\n",
      "    def test_time_decorartor(self):\n",
      "        with self.assertLogs(\"ssmpfwd.helpers\", level=\"INFO\") as cm:\n",
      "            self.time_decorated_method()\n",
      "            self.assertEqual(cm.output[0], \"INFO:ssmpfwd.helpers:[*] starting test_func\")\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pint\n",
      "# Use the same registry\n",
      "from main import ureg\n",
      "ureg.setup_matplotlib(True)\n",
      "from uncertainties import ufloat, umath, unumpy\n",
      "import pandas as pd\n",
      "from scipy.signal import find_peaks\n",
      "from scipy.integrate import simpson\n",
      "from scipy.optimize import curve_fit\n",
      "plt.rcParams['text.usetex'] = True\n",
      "\n",
      "amp = 700*ureg.mV\n",
      "R=ufloat(0.82, 0.82*0.1)*ureg.ohm\n",
      "\n",
      "df = pd.read_csv(\"./ESRB.csv\")\n",
      "# The I0_modulation signal is horrible, the system was too noisy, so instead:\n",
      "#\n",
      "#  I0_modulation = (unumpy.uarray(\n",
      "    #  df['V_modulation_raw'].values,\n",
      "    #  df['V_modulation_err'].values\n",
      "#  )*ureg.mV/R).to('ampere')\n",
      "#\n",
      "# we regnerate it, assuming it should be linear, just as V_DC is.\n",
      "I0_modulation = (unumpy.uarray(np.linspace(\n",
      "    df['V_modulation_raw'].min(),\n",
      "    df['V_modulation_raw'].max(),\n",
      "    len(df)\n",
      "), df['V_modulation_err'].mean())*ureg.mV/R).to('ampere')\n",
      "\n",
      "ptp_Y = unumpy.uarray(\n",
      "    df['ptp_Y_raw'].values*df['phase_sign'].values,\n",
      "    df['ptp_Y_err'].values\n",
      ")*ureg.mV\n",
      "ptp_X_modulation = ufloat(3.09, 0.01)*ureg.mV\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "I0_modulation_err = np.array([val.m.s for val in I0_modulation])\n",
      "I0_modulation_raw = np.array([val.m.n for val in I0_modulation])\n",
      "ptp_ratio = ptp_Y/ptp_X_modulation\n",
      "absorption_deriviative = ptp_ratio/max(ptp_ratio)\n",
      "absorption_deriviative_raw = np.array([val.m.n for val in absorption_deriviative])\n",
      "absorption_deriviative_err = np.array([val.m.s for val in absorption_deriviative])\n",
      "ax.errorbar(\n",
      "    I0_modulation_raw*ureg.ampere,\n",
      "    absorption_deriviative_raw, # Dimensionless\n",
      "    fmt='.',\n",
      "    yerr=absorption_deriviative_err,\n",
      "    # TODO: Mention in report that error is too big to be drafted\n",
      "    #xerr=I_modulation_err,\n",
      "    # TODO: Is this the correct label?\n",
      "    label='Absorption Deriviative'\n",
      ")\n",
      "\n",
      "def lorentzian_dif_fit(I, I0, gamma, amplitude):\n",
      "    return amplitude*(-2*(gamma**2)*(I - I0))/ \\\n",
      "            (gamma**2 + (I - I0)**2)**2\n",
      "def lorentzian_fit(I, I0, gamma, amplitude):\n",
      "    return amplitude*gamma**2/\\\n",
      "            (gamma**2 + (I - I0)**2)**2\n",
      "##### By MATLAB:\n",
      "#  Goodness of fit:\n",
      "#  SSE: 0.197\n",
      "#  R-square: 0.9845\n",
      "#  Adjusted R-square: 0.9838\n",
      "#  RMSE: 0.06769\n",
      "#               I0      gamma    amplitude\n",
      "matlab_p0    = [0.5479, 0.03847, 0.05554]\n",
      "matlab_bounds=((0.547,  0.03672,  0.05304),\n",
      "               (0.5488, 0.04021,   0.05805))\n",
      "I_rf = ufloat(matlab_p0[0], abs(matlab_bounds[0][0] - matlab_p0[0]))*ureg.ampere\n",
      "I_hwhm = ufloat(matlab_p0[1], abs(matlab_bounds[0][1] - matlab_p0[1]))*ureg.ampere\n",
      "\n",
      "from main import g_times_bohr\n",
      "# TODO: Take this value from Itamar & Tomer\n",
      "H_RF = ufloat(34.914, 0.009)*ureg.gauss\n",
      "k = H_RF/I_rf\n",
      "# Converts current I To frequency f using all of the constants\n",
      "def I2f(I):\n",
      "    return (I*k*g_times_bohr/ureg.planck_constant).to('megahertz')\n",
      "\n",
      "f0_modulation = I2f(I0_modulation)\n",
      "f_rf = I2f(I_rf)\n",
      "f_hwhm = I2f(I_hwhm)\n",
      "T2 = (1/f_hwhm).to('nanosecond')\n",
      "\n",
      "##### A failing Python fit attempt - I consider it as a failure because it hits\n",
      "##### the bounds :/\n",
      "#  popt, pcov = curve_fit(\n",
      "    #  lorentzian_dif_fit, absorption_deriviative_raw, I0_modulation_raw,\n",
      "    #  p0=matlab_p0, bounds=matlab_bounds\n",
      "#  )\n",
      "#  lorentzian_dif_fit_points = lorentzian_dif_fit(I0_modulation_raw, *popt)\n",
      "#  ax.plot(\n",
      "    #  I0_modulation_raw*ureg.ampere,\n",
      "    #  lorentzian_dif_fit_points,\n",
      "    #  label=\"Python fit\"\n",
      "#  )\n",
      "\n",
      "I0_modulation_seq = np.linspace(\n",
      "    I0_modulation.min().m.n,\n",
      "    I0_modulation.max().m.n,\n",
      "    len(I0_modulation)*100\n",
      ")\n",
      "ax.plot(\n",
      "    I0_modulation_seq*ureg.ampere,\n",
      "    lorentzian_dif_fit(I0_modulation_seq, I_rf.m.n, I_hwhm.m.n, matlab_p0[2]),\n",
      "    label=\"Matlab fit\"\n",
      ")\n",
      "ax.set_yticks([])\n",
      "axt = ax.twiny()\n",
      "axt.grid(linestyle='--')\n",
      "axt.set_yticks([])\n",
      "f0_modulation_seq = np.linspace(\n",
      "    f0_modulation.min().m.n,\n",
      "    f0_modulation.max().m.n,\n",
      "    len(f0_modulation)*100\n",
      ")\n",
      "def lorentzian_wrapper(f0):\n",
      "    # From some reason this need to be amplified by a factor of 800 so it will\n",
      "    # look good.\n",
      "    return lorentzian_fit(f0, f_rf.m.n, f_hwhm.m.n, matlab_p0[2]*800)\n",
      "axt.plot(\n",
      "    f0_modulation_seq*ureg.megahertz,\n",
      "    lorentzian_wrapper(f0_modulation_seq),\n",
      "    label = \"Lorenzian fit\", color='green'\n",
      ")\n",
      "axt.set_xticks(\n",
      "    [(f_rf - f_hwhm).m.n, f_rf.m.n, (f_rf + f_hwhm).m.n],\n",
      "    ['', '$f_{rf}$', '']\n",
      ")\n",
      "axt.set_xlabel('')\n",
      "axt.arrow(\n",
      "    length_includes_head = True,\n",
      "    x = (f_rf - f_hwhm).m.n*ureg.megahertz,\n",
      "    y = lorentzian_wrapper((f_rf - f_hwhm).m.n),\n",
      "    dx = 2*f_hwhm.m.n*ureg.megahertz,\n",
      "    dy = 0,\n",
      "    head_length = f_hwhm.m.n/10,\n",
      "    head_width = matlab_p0[2],\n",
      "    label=\"Full Width Half Max\",\n",
      ")\n",
      "axt.arrow(\n",
      "    length_includes_head = True,\n",
      "    x = (f_rf + f_hwhm).m.n*ureg.megahertz,\n",
      "    y = lorentzian_wrapper((f_rf + f_hwhm).m.n),\n",
      "    dx = -2*f_hwhm.m.n*ureg.megahertz,\n",
      "    head_length = f_hwhm.m.n/10,\n",
      "    head_width = matlab_p0[2],\n",
      "    dy = 0,\n",
      ")\n",
      "axt.text(\n",
      "    0.5, 0.63,\n",
      "    #  (f_hwhm.m.n/10),\n",
      "    #  lorentzian_wrapper((f0 - f_hwhm).m.n)*2,\n",
      "    \"FWHM\",\n",
      "    transform=ax.transAxes,\n",
      "    #  fontsize=00\n",
      ")\n",
      "ax.legend(loc='upper right')\n",
      "#  axt.legend(loc='upper left')\n",
      "plt.show()\n",
      "fig.savefig(\"ESRB.pgf\")\n",
      "fig.savefig(\"ESRB.png\")\n",
      "\n",
      "# TODO: Integrate numerically / or fit to a laurenzian's differentiation\n",
      "\n",
      "# TODO: Scale the x axis to frequency and find the width of the laurenzian in\n",
      "# frequency scale\n",
      "\n",
      "from flask import Flask, request, redirect\n",
      "from twilio.twiml.messaging_response import MessagingResponse\n",
      "from get_secrets import *\n",
      "\n",
      "def main():\n",
      "    resp = MessagingResponse()\n",
      "\n",
      "    resp.message (\"You have reached the DogBot. Thanks for contacting us :)\")\n",
      "\n",
      "    return str(resp)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "\n",
      "from __future__ import annotations\n",
      "\n",
      "from enum import IntEnum\n",
      "\n",
      "\n",
      "class Algorithm(IntEnum):\n",
      "    \"\"\"\n",
      "    https://developers.yubico.com/YubiHSM2/Concepts/Algorithms.html\n",
      "    \"\"\"\n",
      "\n",
      "    RSA_PKCS1_SHA1 = 1\n",
      "    RSA_PKCS1_SHA256 = 2\n",
      "    RSA_PKCS1_SHA384 = 3\n",
      "    RSA_PKCS1_SHA512 = 4\n",
      "    RSA_PSS_SHA1 = 5\n",
      "    RSA_PSS_SHA256 = 6\n",
      "    RSA_PSS_SHA384 = 7\n",
      "    RSA_PSS_SHA512 = 8\n",
      "    RSA_2048 = 9\n",
      "    RSA_3072 = 10\n",
      "    RSA_4096 = 11\n",
      "    EC_P256 = 12\n",
      "    EC_P384 = 13\n",
      "    EC_P521 = 14\n",
      "    EC_K256 = 15\n",
      "    EC_BP256 = 16\n",
      "    EC_BP384 = 17\n",
      "    EC_BP512 = 18\n",
      "    HMAC_SHA1 = 19\n",
      "    HMAC_SHA256 = 20\n",
      "    HMAC_SHA384 = 21\n",
      "    HMAC_SHA512 = 22\n",
      "    ECDSA_SHA1 = 23\n",
      "    EC_ECDH = 24\n",
      "    RSA_OAEP_SHA1 = 25\n",
      "    RSA_OAEP_SHA256 = 26\n",
      "    RSA_OAEP_SHA384 = 27\n",
      "    RSA_OAEP_SHA512 = 28\n",
      "    AES128_CCM_WRAP = 29\n",
      "    Opaque_Data = 30\n",
      "    Opaque_X509_Certificate = 31\n",
      "    MGF1_SHA1 = 32\n",
      "    MGF1_SHA256 = 33\n",
      "    MGF1_SHA384 = 34\n",
      "    MGF1_SHA512 = 35\n",
      "    SSH_Template = 36\n",
      "    Yubico_OTP_AES128 = 37\n",
      "    Yubico_AES_Authentication = 38\n",
      "    Yubico_OTP_AES192 = 39\n",
      "    Yubico_OTP_AES256 = 40\n",
      "    AES192_CCM_WRAP = 41\n",
      "    AES256_CCM_WRAP = 42\n",
      "    ECDSA_SHA256 = 43\n",
      "    ECDSA_SHA384 = 44\n",
      "    ECDSA_SHA512 = 45\n",
      "    ED25519 = 46\n",
      "    EC_P224 = 47\n",
      "\n",
      "\n",
      "class Capability(IntEnum):\n",
      "    \"\"\"\n",
      "    https://developers.yubico.com/YubiHSM2/Concepts/Capability.html\n",
      "    \"\"\"\n",
      "\n",
      "    GetOpaque = 0\n",
      "    PutOpaque = 1\n",
      "    PutAuthenticationKey = 2\n",
      "    PutAsymmetricKey = 3\n",
      "    GenerateAsymmetricKey = 4\n",
      "    SignPkcs = 5\n",
      "    SignPss = 6\n",
      "    SignEcdsa = 7\n",
      "    SignEddsa = 8\n",
      "    DecryptPkcs = 9\n",
      "    DecryptOaep = 10\n",
      "    DeriveEcdh = 11\n",
      "    ExportWrapped = 12\n",
      "    ImportWrapped = 13\n",
      "    PutWrapKey = 14\n",
      "    GenerateWrapKey = 15\n",
      "    ExportableUnderWrap = 16\n",
      "    SetOption = 17\n",
      "    GetOption = 18\n",
      "    GetPseudoRandom = 19\n",
      "    PutMacKey = 20\n",
      "    GenerateHmacKey = 21\n",
      "    SignHmac = 22\n",
      "    VerifyHmac = 23\n",
      "    GetLogEntries = 24\n",
      "    SignSshCertificate = 25\n",
      "    GetTemplate = 26\n",
      "    PutTemplate = 27\n",
      "    ResetDevice = 28\n",
      "    DecryptOtp = 29\n",
      "    CreateOtpAead = 30\n",
      "    RandomizeOtpAead = 31\n",
      "    RewrapFromOtpAeadKey = 32\n",
      "    RewrapToOtpAeadKey = 33\n",
      "    SignAttestationCertificate = 34\n",
      "    PutOtpAeadKey = 35\n",
      "    GenerateOtpAeadKey = 36\n",
      "    WrapData = 37\n",
      "    UnwrapData = 38\n",
      "    DeleteOpaque = 39\n",
      "    DeleteAuthenticationKey = 40\n",
      "    DeleteAsymmetricKey = 41\n",
      "    DeleteWrapKey = 42\n",
      "    DeleteHmacKey = 43\n",
      "    DeleteTemplate = 44\n",
      "    DeleteOtpAeadKey = 45\n",
      "    ChangeAuthenticationKey = 46\n",
      "\n",
      "\n",
      "class Command(IntEnum):\n",
      "    \"\"\"\n",
      "    https://developers.yubico.com/YubiHSM2/Commands/\n",
      "    \"\"\"\n",
      "\n",
      "    Echo = 0x01\n",
      "    CreateSession = 0x03\n",
      "    AuthenticateSession = 0x04\n",
      "    SessionMessage = 0x05\n",
      "    GetDeviceInfo = 0x06\n",
      "    ResetDevice = 0x08\n",
      "    CloseSession = 0x40\n",
      "    GetStorageInfo = 0x41\n",
      "    PutOpaque = 0x42\n",
      "    GetOpaque = 0x43\n",
      "    PutAuthenticationKey = 0x44\n",
      "    PutAsymmetricKey = 0x45\n",
      "    GenerateAsymmetricKey = 0x46\n",
      "    SignPkcs1 = 0x47\n",
      "    ListObjects = 0x48\n",
      "    DecryptPkcs1 = 0x49\n",
      "    ExportWrapped = 0x4A\n",
      "    ImportWrapped = 0x4B\n",
      "    PutWrapKey = 0x4C\n",
      "    GetLogEntries = 0x4D\n",
      "    GetObjectInfo = 0x4E\n",
      "    SetOption = 0x4F\n",
      "    GetOption = 0x50\n",
      "    GetPseudoRandom = 0x51\n",
      "    PutHmacKey = 0x52\n",
      "    SignHmac = 0x53\n",
      "    GetPublicKey = 0x54\n",
      "    SignPss = 0x55\n",
      "    SignEcdsa = 0x56\n",
      "    DeriveEcdh = 0x57\n",
      "    DeleteObject = 0x58\n",
      "    DecryptOaep = 0x59\n",
      "    GenerateHmacKey = 0x5A\n",
      "    GenerateWrapKey = 0x5B\n",
      "    VerifyHmac = 0x5C\n",
      "    SignSshCertificate = 0x5D\n",
      "    PutTemplate = 0x5E\n",
      "    GetTemplate = 0x5F\n",
      "    DecryptOtp = 0x60\n",
      "    CreateOtpAead = 0x61\n",
      "    RandomizeOtpAead = 0x62\n",
      "    RewrapOtpAead = 0x63\n",
      "    SignAttestationCertificate = 0x64\n",
      "    PutOtpAeadKey = 0x65\n",
      "    GenerateOtpAeadKey = 0x66\n",
      "    SetLogIndex = 0x67\n",
      "    WrapData = 0x68\n",
      "    UnwrapData = 0x69\n",
      "    SignEddsa = 0x6A\n",
      "    BlinkDevice = 0x6B\n",
      "    ChangeAuthenticationKey = 0x6C\n",
      "    Error = 0x7F\n",
      "\n",
      "\n",
      "class Error(IntEnum):\n",
      "    \"\"\"\n",
      "    https://developers.yubico.com/YubiHSM2/Concepts/Errors.html\n",
      "    \"\"\"\n",
      "\n",
      "    OK = 0x00\n",
      "    INVALID_COMMAND = 0x01\n",
      "    INVALID_DATA = 0x02\n",
      "    INVALID_SESSION = 0x03\n",
      "    AUTHENTICATION_FAILED = 0x04\n",
      "    SESSIONS_FULL = 0x05\n",
      "    SESSION_FAILED = 0x06\n",
      "    STORAGE_FAILED = 0x07\n",
      "    WRONG_LENGTH = 0x08\n",
      "    INSUFFICIENT_PERMISSIONS = 0x09\n",
      "    LOG_FULL = 0x0A\n",
      "    OBJECT_NOT_FOUND = 0x0B\n",
      "    INVALID_ID = 0x0C\n",
      "    SSH_CA_CONSTRAINT_VIOLATION = 0x0E\n",
      "    INVALID_OTP = 0x0F\n",
      "    DEMO_MODE = 0x10\n",
      "    OBJECT_EXISTS = 0x11\n",
      "\n",
      "\n",
      "class ObjectType(IntEnum):\n",
      "    \"\"\"\n",
      "    https://developers.yubico.com/YubiHSM2/Concepts/Object.html\n",
      "    \"\"\"\n",
      "\n",
      "    Opaque = 0x01\n",
      "    AuthenticationKey = 0x02\n",
      "    AsymmetricKey = 0x03\n",
      "    WrapKey = 0x04\n",
      "    HmacKey = 0x05\n",
      "    Template = 0x06\n",
      "    OtpAeadKey = 0x07\n",
      "\n",
      "\n",
      "class Option(IntEnum):\n",
      "    \"\"\"\n",
      "    https://developers.yubico.com/YubiHSM2/Concepts/Options.html\n",
      "    \"\"\"\n",
      "\n",
      "    ForceAudit = 0x01\n",
      "    CommandAudit = 0x03\n",
      "\n",
      "import unittest.mock\n",
      "\n",
      "from functools import partial\n",
      "\n",
      "import bokeh.core.properties as bp\n",
      "import param\n",
      "import pytest\n",
      "\n",
      "from bokeh.document import Document\n",
      "from bokeh.io.doc import patch_curdoc\n",
      "from bokeh.models import Div\n",
      "\n",
      "from panel.layout import Tabs, WidgetBox\n",
      "from panel.reactive import Reactive, ReactiveHTML\n",
      "from panel.viewable import Viewable\n",
      "from panel.widgets import (\n",
      "    Checkbox, IntInput, StaticText, TextInput,\n",
      ")\n",
      "\n",
      "\n",
      "def test_reactive_default_title():\n",
      "    doc = ReactiveHTML().server_doc()\n",
      "\n",
      "    assert doc.title == 'Panel Application'\n",
      "\n",
      "\n",
      "def test_reactive_servable_title():\n",
      "    doc = Document()\n",
      "\n",
      "    session_context = unittest.mock.Mock()\n",
      "\n",
      "    with patch_curdoc(doc):\n",
      "        doc._session_context = lambda: session_context\n",
      "        ReactiveHTML().servable(title='A')\n",
      "        ReactiveHTML().servable(title='B')\n",
      "\n",
      "    assert doc.title == 'B'\n",
      "\n",
      "\n",
      "def test_link():\n",
      "    \"Link two Reactive objects\"\n",
      "\n",
      "    class ReactiveLink(Reactive):\n",
      "\n",
      "        a = param.Parameter()\n",
      "\n",
      "    obj = ReactiveLink()\n",
      "    obj2 = ReactiveLink()\n",
      "    obj.link(obj2, a='a')\n",
      "    obj.a = 1\n",
      "\n",
      "    assert obj.a == 1\n",
      "    assert obj2.a == 1\n",
      "\n",
      "\n",
      "def test_param_rename():\n",
      "    \"Test that Reactive renames params and properties\"\n",
      "\n",
      "    class ReactiveRename(Reactive):\n",
      "\n",
      "        a = param.Parameter()\n",
      "\n",
      "        _rename = {'a': 'b'}\n",
      "\n",
      "    obj = ReactiveRename()\n",
      "\n",
      "    params = obj._process_property_change({'b': 1})\n",
      "    assert params == {'a': 1}\n",
      "\n",
      "    properties = obj._process_param_change({'a': 1})\n",
      "    assert properties == {'b': 1}\n",
      "\n",
      "\n",
      "def test_link_properties_nb(document, comm):\n",
      "\n",
      "    class ReactiveLink(Reactive):\n",
      "\n",
      "        text = param.String(default='A')\n",
      "\n",
      "    obj = ReactiveLink()\n",
      "    div = Div()\n",
      "\n",
      "    # Link property and check bokeh js property callback is defined\n",
      "    obj._link_props(div, ['text'], document, div, comm)\n",
      "    assert 'text' in div._callbacks\n",
      "\n",
      "    # Assert callback is set up correctly\n",
      "    cb = div._callbacks['text'][0]\n",
      "    assert isinstance(cb, partial)\n",
      "    assert cb.args == (document, div.ref['id'], comm, None)\n",
      "    assert cb.func == obj._comm_change\n",
      "\n",
      "\n",
      "def test_link_properties_server(document):\n",
      "\n",
      "    class ReactiveLink(Reactive):\n",
      "\n",
      "        text = param.String(default='A')\n",
      "\n",
      "    obj = ReactiveLink()\n",
      "    div = Div()\n",
      "\n",
      "    # Link property and check bokeh callback is defined\n",
      "    obj._link_props(div, ['text'], document, div)\n",
      "    assert 'text' in div._callbacks\n",
      "\n",
      "    # Assert callback is set up correctly\n",
      "    cb = div._callbacks['text'][0]\n",
      "    assert isinstance(cb, partial)\n",
      "    assert cb.args == (document, div.ref['id'], None)\n",
      "    assert cb.func == obj._server_change\n",
      "\n",
      "\n",
      "def test_text_input_controls():\n",
      "    text_input = TextInput()\n",
      "\n",
      "    controls = text_input.controls()\n",
      "\n",
      "    assert isinstance(controls, Tabs)\n",
      "    assert len(controls) == 2\n",
      "    wb1, wb2 = controls\n",
      "    assert isinstance(wb1, WidgetBox)\n",
      "    assert len(wb1) == 6\n",
      "    name, disabled, *(ws) = wb1\n",
      "\n",
      "    assert isinstance(name, StaticText)\n",
      "    assert isinstance(disabled, Checkbox)\n",
      "\n",
      "    not_checked = []\n",
      "    for w in ws:\n",
      "        if w.name == 'Value':\n",
      "            assert isinstance(w, TextInput)\n",
      "            text_input.value = \"New value\"\n",
      "            assert w.value == \"New value\"\n",
      "        elif w.name == 'Value input':\n",
      "            assert isinstance(w, TextInput)\n",
      "        elif w.name == 'Placeholder':\n",
      "            assert isinstance(w, TextInput)\n",
      "            text_input.placeholder = \"Test placeholder...\"\n",
      "            assert w.value == \"Test placeholder...\"\n",
      "        elif w.name == 'Max length':\n",
      "            assert isinstance(w, IntInput)\n",
      "        else:\n",
      "            not_checked.append(w)\n",
      "\n",
      "    assert not not_checked\n",
      "\n",
      "    assert isinstance(wb2, WidgetBox)\n",
      "    assert len(wb2) == len(list(Viewable.param)) + 1\n",
      "\n",
      "\n",
      "\n",
      "def test_text_input_controls_explicit():\n",
      "    text_input = TextInput()\n",
      "\n",
      "    controls = text_input.controls(['placeholder', 'disabled'])\n",
      "\n",
      "    assert isinstance(controls, WidgetBox)\n",
      "    assert len(controls) == 3\n",
      "    name, disabled, placeholder = controls\n",
      "\n",
      "    assert isinstance(name, StaticText)\n",
      "    assert isinstance(disabled, Checkbox)\n",
      "    assert isinstance(placeholder, TextInput)\n",
      "\n",
      "    text_input.disabled = True\n",
      "    assert disabled.value\n",
      "\n",
      "    text_input.placeholder = \"Test placeholder...\"\n",
      "    assert placeholder.value == \"Test placeholder...\"\n",
      "\n",
      "\n",
      "def test_reactive_html_basic():\n",
      "\n",
      "    class Test(ReactiveHTML):\n",
      "\n",
      "        int = param.Integer(default=3, doc='An integer')\n",
      "\n",
      "        float = param.Number(default=3.14, doc='A float')\n",
      "\n",
      "        _template = '<div id=\"div\" width=${int}></div>'\n",
      "\n",
      "    data_model = Test._data_model\n",
      "    assert data_model.__name__ == 'Test1'\n",
      "\n",
      "    properties = data_model.properties()\n",
      "    assert 'int' in properties\n",
      "    assert 'float' in properties\n",
      "\n",
      "    int_prop = data_model.lookup('int')\n",
      "    assert isinstance(int_prop.property, bp.Int)\n",
      "    assert int_prop.class_default(data_model) == 3\n",
      "\n",
      "    float_prop = data_model.lookup('float')\n",
      "    assert isinstance(float_prop.property, bp.Float)\n",
      "    assert float_prop.class_default(data_model) == 3.14\n",
      "\n",
      "    assert Test._node_callbacks == {}\n",
      "\n",
      "    test = Test()\n",
      "    root = test.get_root()\n",
      "    assert test._attrs == {'div': [('width', ['int'], '{int}')]}\n",
      "    assert root.callbacks == {}\n",
      "    assert root.events == {}\n",
      "\n",
      "def test_reactive_html_no_id_param_error():\n",
      "\n",
      "    with pytest.raises(ValueError) as excinfo:\n",
      "        class Test(ReactiveHTML):\n",
      "            width = param.Number(default=200)\n",
      "\n",
      "            _template = '<div width=${width}></div>'\n",
      "\n",
      "    assert \"Found <div> node with the `width` attribute referencing the `width` parameter.\" in str(excinfo.value)\n",
      "\n",
      "def test_reactive_html_no_id_method_error():\n",
      "\n",
      "    with pytest.raises(ValueError) as excinfo:\n",
      "        class Test(ReactiveHTML):\n",
      "\n",
      "            _template = '<div onclick=${_onclick}></div>'\n",
      "\n",
      "            def _onclick(self):\n",
      "                pass\n",
      "    assert \"Found <div> node with the `onclick` callback referencing the `_onclick` method.\" in str(excinfo.value)\n",
      "\n",
      "def test_reactive_html_dom_events():\n",
      "\n",
      "    class TestDOMEvents(ReactiveHTML):\n",
      "\n",
      "        int = param.Integer(default=3, doc='An integer')\n",
      "\n",
      "        float = param.Number(default=3.14, doc='A float')\n",
      "\n",
      "        _template = '<div id=\"div\" width=${int}></div>'\n",
      "\n",
      "        _dom_events = {'div': ['change']}\n",
      "\n",
      "    data_model = TestDOMEvents._data_model\n",
      "    assert data_model.__name__ == 'TestDOMEvents1'\n",
      "\n",
      "    properties = data_model.properties()\n",
      "    assert 'int' in properties\n",
      "    assert 'float' in properties\n",
      "\n",
      "    int_prop = data_model.lookup('int')\n",
      "    assert isinstance(int_prop.property, bp.Int)\n",
      "    assert int_prop.class_default(data_model) == 3\n",
      "\n",
      "    float_prop = data_model.lookup('float')\n",
      "    assert isinstance(float_prop.property, bp.Float)\n",
      "    assert float_prop.class_default(data_model) == 3.14\n",
      "\n",
      "    assert TestDOMEvents._node_callbacks == {}\n",
      "\n",
      "    test = TestDOMEvents()\n",
      "    root = test.get_root()\n",
      "    assert test._attrs == {'div': [('width', ['int'], '{int}')]}\n",
      "    assert root.callbacks == {}\n",
      "    assert root.events == {'div': {'change': True}}\n",
      "\n",
      "\n",
      "def test_reactive_html_inline():\n",
      "    class TestInline(ReactiveHTML):\n",
      "\n",
      "        int = param.Integer(default=3, doc='An integer')\n",
      "\n",
      "        _template = '<div id=\"div\" onchange=${_div_change} width=${int}></div>'\n",
      "\n",
      "        def _div_change(self, event):\n",
      "            pass\n",
      "\n",
      "    data_model = TestInline._data_model\n",
      "    assert data_model.__name__ == 'TestInline1'\n",
      "\n",
      "    properties = data_model.properties()\n",
      "    assert 'int' in properties\n",
      "\n",
      "    int_prop = data_model.lookup('int')\n",
      "    assert isinstance(int_prop.property, bp.Int)\n",
      "    assert int_prop.class_default(data_model) == 3\n",
      "\n",
      "    assert TestInline._node_callbacks == {'div': [('onchange', '_div_change')]}\n",
      "    assert TestInline._inline_callbacks == [('div', 'onchange', '_div_change')]\n",
      "\n",
      "    test = TestInline()\n",
      "    root = test.get_root()\n",
      "    assert test._attrs == {\n",
      "        'div': [\n",
      "            ('onchange', [], '{_div_change}'),\n",
      "            ('width', ['int'], '{int}')\n",
      "        ]\n",
      "    }\n",
      "    assert root.callbacks == {'div': [('onchange', '_div_change')]}\n",
      "    assert root.events == {}\n",
      "\n",
      "    test.on_event('div', 'click', print)\n",
      "    assert root.events == {'div': {'click': False}}\n",
      "\n",
      "\n",
      "def test_reactive_html_children():\n",
      "\n",
      "    class TestChildren(ReactiveHTML):\n",
      "\n",
      "        children = param.List(default=[])\n",
      "\n",
      "        _template = '<div id=\"div\">${children}</div>'\n",
      "\n",
      "    assert TestChildren._node_callbacks == {}\n",
      "    assert TestChildren._inline_callbacks == []\n",
      "    assert TestChildren._parser.children == {'div': 'children'}\n",
      "\n",
      "    widget = TextInput()\n",
      "    test = TestChildren(children=[widget])\n",
      "    root = test.get_root()\n",
      "    assert test._attrs == {}\n",
      "    assert root.children == {'div': [widget._models[root.ref['id']][0]]}\n",
      "    assert len(widget._models) == 1\n",
      "    assert test._panes == {'children': [widget]}\n",
      "\n",
      "    widget_new = TextInput()\n",
      "    test.children = [widget_new]\n",
      "    assert len(widget._models) == 0\n",
      "    assert root.children == {'div': [widget_new._models[root.ref['id']][0]]}\n",
      "    assert test._panes == {'children': [widget_new]}\n",
      "\n",
      "    test._cleanup(root)\n",
      "    assert len(test._models) == 0\n",
      "    assert len(widget_new._models) == 0\n",
      "\n",
      "\n",
      "def test_reactive_html_templated_children():\n",
      "\n",
      "    class TestTemplatedChildren(ReactiveHTML):\n",
      "\n",
      "        children = param.List(default=[])\n",
      "\n",
      "        _template = \"\"\"\n",
      "        <select id=\"select\">\n",
      "        {% for option in children %}\n",
      "        <option id=\"option-{{ loop.index0 }}\">${children[{{ loop.index0 }}]}</option>\n",
      "        {% endfor %}\n",
      "        </div>\n",
      "        \"\"\"\n",
      "\n",
      "    assert TestTemplatedChildren._node_callbacks == {}\n",
      "    assert TestTemplatedChildren._inline_callbacks == []\n",
      "    assert TestTemplatedChildren._parser.children == {'option': 'children'}\n",
      "\n",
      "    widget = TextInput()\n",
      "    test = TestTemplatedChildren(children=[widget])\n",
      "    root = test.get_root()\n",
      "    assert test._attrs == {}\n",
      "    assert root.looped == ['option']\n",
      "    assert root.children == {'option': [widget._models[root.ref['id']][0]]}\n",
      "    assert test._panes == {'children': [widget]}\n",
      "\n",
      "    widget_new = TextInput()\n",
      "    test.children = [widget_new]\n",
      "    assert len(widget._models) == 0\n",
      "    assert root.children == {'option': [widget_new._models[root.ref['id']][0]]}\n",
      "    assert test._panes == {'children': [widget_new]}\n",
      "\n",
      "\n",
      "def test_reactive_html_templated_dict_children():\n",
      "\n",
      "    class TestTemplatedChildren(ReactiveHTML):\n",
      "\n",
      "        children = param.Dict(default={})\n",
      "\n",
      "        _template = \"\"\"\n",
      "        <select id=\"select\">\n",
      "        {% for key, option in children.items() %}\n",
      "        <option id=\"option-{{ loop.index0 }}\">${children[{{ key }}]}</option>\n",
      "        {% endfor %}\n",
      "        </div>\n",
      "        \"\"\"\n",
      "\n",
      "    assert TestTemplatedChildren._node_callbacks == {}\n",
      "    assert TestTemplatedChildren._inline_callbacks == []\n",
      "    assert TestTemplatedChildren._parser.children == {'option': 'children'}\n",
      "\n",
      "    widget = TextInput()\n",
      "    test = TestTemplatedChildren(children={'test': widget})\n",
      "    root = test.get_root()\n",
      "    assert test._attrs == {}\n",
      "    assert root.looped == ['option']\n",
      "    assert root.children == {'option': [widget._models[root.ref['id']][0]]}\n",
      "    assert test._panes == {'children': [widget]}\n",
      "    widget_model = widget._models[root.ref['id']][0]\n",
      "\n",
      "    widget_new = TextInput()\n",
      "    test.children = {'test': widget_new, 'test2': widget}\n",
      "    assert len(widget._models) == 1\n",
      "    assert root.children == {\n",
      "        'option': [\n",
      "            widget_new._models[root.ref['id']][0],\n",
      "            widget_model\n",
      "        ]\n",
      "    }\n",
      "    assert test._panes == {'children': [widget_new, widget]}\n",
      "\n",
      "\n",
      "def test_reactive_html_templated_children_add_loop_id():\n",
      "\n",
      "    class TestTemplatedChildren(ReactiveHTML):\n",
      "\n",
      "        children = param.List(default=[])\n",
      "\n",
      "        _template = \"\"\"\n",
      "        <select id=\"select\">\n",
      "        {%- for option in children %}\n",
      "          <option id=\"option\">${children[{{ loop.index0 }}]}</option>\n",
      "        {%- endfor %}\n",
      "        </select>\n",
      "        \"\"\"\n",
      "\n",
      "    assert TestTemplatedChildren._node_callbacks == {}\n",
      "    assert TestTemplatedChildren._inline_callbacks == []\n",
      "    assert TestTemplatedChildren._parser.children == {'option': 'children'}\n",
      "\n",
      "    test = TestTemplatedChildren(children=['A', 'B', 'C'])\n",
      "\n",
      "    assert test._get_template()[0] == \"\"\"\n",
      "        <select id=\"select-${id}\">\n",
      "          <option id=\"option-0-${id}\"></option>\n",
      "          <option id=\"option-1-${id}\"></option>\n",
      "          <option id=\"option-2-${id}\"></option>\n",
      "        </select>\n",
      "        \"\"\"\n",
      "\n",
      "    model = test.get_root()\n",
      "    assert test._attrs == {}\n",
      "    assert model.looped == ['option']\n",
      "\n",
      "\n",
      "def test_reactive_html_templated_children_add_loop_id_and_for_loop_var():\n",
      "\n",
      "    class TestTemplatedChildren(ReactiveHTML):\n",
      "\n",
      "        children = param.List(default=[])\n",
      "\n",
      "        _template = \"\"\"\n",
      "        <select id=\"select\">\n",
      "        {%- for option in children %}\n",
      "          <option id=\"option\">${option}</option>\n",
      "        {%- endfor %}\n",
      "        </select>\n",
      "        \"\"\"\n",
      "\n",
      "    assert TestTemplatedChildren._node_callbacks == {}\n",
      "    assert TestTemplatedChildren._inline_callbacks == []\n",
      "    assert TestTemplatedChildren._parser.children == {'option': 'children'}\n",
      "\n",
      "    test = TestTemplatedChildren(children=['A', 'B', 'C'])\n",
      "\n",
      "    assert test._get_template()[0] == \"\"\"\n",
      "        <select id=\"select-${id}\">\n",
      "          <option id=\"option-0-${id}\"></option>\n",
      "          <option id=\"option-1-${id}\"></option>\n",
      "          <option id=\"option-2-${id}\"></option>\n",
      "        </select>\n",
      "        \"\"\"\n",
      "    model = test.get_root()\n",
      "    assert test._attrs == {}\n",
      "    assert model.looped == ['option']\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize('operator', ['', '+', '-', '*', '\\\\', '%', '**', '>>', '<<', '>>>', '&', '^', '&&', '||', '??'])\n",
      "\n",
      "@pytest.mark.parametrize('sep', [' ', ''])\n",
      "def test_reactive_html_scripts_linked_properties_assignment_operator(operator, sep):\n",
      "\n",
      "    class TestScripts(ReactiveHTML):\n",
      "\n",
      "        clicks = param.Integer()\n",
      "\n",
      "        _template = \"<div id='test'></div>\"\n",
      "\n",
      "        _scripts = {'render': f'test.onclick = () => {{ data.clicks{sep}{operator}= 1 }}'}\n",
      "\n",
      "    assert TestScripts()._linked_properties() == ['clicks']\n",
      "\n",
      "# Copyright © 2019 Province of British Columbia\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\"\"\"This module holds data for ppr queue event tracking.\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from mhr_api.models import utils as model_utils\n",
      "from mhr_api.utils.base import BaseEnum\n",
      "\n",
      "from .db import db\n",
      "\n",
      "\n",
      "class EventTracking(db.Model):  # pylint: disable=too-many-instance-attributes\n",
      "    \"\"\"This class manages all of the event tracking information.\"\"\"\n",
      "\n",
      "    class EventTrackingTypes(BaseEnum):\n",
      "        \"\"\"Render an Enum of the event tracking types.\"\"\"\n",
      "\n",
      "        SEARCH_REPORT = 'SEARCH_REPORT'\n",
      "        API_NOTIFICATION = 'API_NOTIFICATION'\n",
      "        EMAIL = 'EMAIL'\n",
      "        SURFACE_MAIL = 'SURFACE_MAIL'\n",
      "        EMAIL_REPORT = 'EMAIL_REPORT'\n",
      "        REGISTRATION_REPORT = 'REGISTRATION_REPORT'\n",
      "\n",
      "    __tablename__ = 'event_tracking'\n",
      "\n",
      "    id = db.Column('id', db.Integer, db.Sequence('event_tracking_id_seq'), primary_key=True)\n",
      "    key_id = db.Column('key_id', db.Integer, nullable=False, index=True)\n",
      "    event_ts = db.Column('event_ts', db.DateTime, nullable=False, index=True)\n",
      "    event_tracking_type = db.Column('event_tracking_type', db.String(20),\n",
      "                                    db.ForeignKey('event_tracking_types.event_tracking_type'),\n",
      "                                    nullable=False, index=True)\n",
      "    status = db.Column('status', db.Integer, nullable=True)\n",
      "    message = db.Column('message', db.String(2000), nullable=True)\n",
      "    email_id = db.Column('email_address', db.String(250), nullable=True)\n",
      "\n",
      "    # Relationships - SerialType\n",
      "    tracking_type = db.relationship('EventTrackingType', foreign_keys=[event_tracking_type],\n",
      "                                    back_populates='event_tracking', cascade='all, delete', uselist=False)\n",
      "\n",
      "    def save(self):\n",
      "        \"\"\"Save the object to the database immediately.\"\"\"\n",
      "        db.session.add(self)\n",
      "        db.session.commit()\n",
      "\n",
      "    @property\n",
      "    def json(self) -> dict:\n",
      "        \"\"\"Return the event tracking record as a json object.\"\"\"\n",
      "        event_tracking = {\n",
      "            'eventTrackingId': self.id,\n",
      "            'keyId': self.key_id,\n",
      "            'type': self.event_tracking_type,\n",
      "            'createDateTime': model_utils.format_ts(self.event_ts)\n",
      "        }\n",
      "        if self.status:\n",
      "            event_tracking['status'] = self.status\n",
      "        if self.message:\n",
      "            event_tracking['message'] = self.message\n",
      "        if self.email_id:\n",
      "            event_tracking['emailAddress'] = self.email_id\n",
      "\n",
      "        return event_tracking\n",
      "\n",
      "    @classmethod\n",
      "    def find_by_id(cls, event_id: int):\n",
      "        \"\"\"Return a tracking object by ID.\"\"\"\n",
      "        if event_id:\n",
      "            return cls.query.get(event_id)\n",
      "\n",
      "        return None\n",
      "\n",
      "    @classmethod\n",
      "    def find_by_key_id(cls, key_id: int):\n",
      "        \"\"\"Return a list of event tracking objects by key id.\"\"\"\n",
      "        event_tracking = None\n",
      "        if key_id:\n",
      "            event_tracking = cls.query.filter(EventTracking.key_id == key_id) \\\n",
      "                                      .order_by(EventTracking.id).all()\n",
      "\n",
      "        return event_tracking\n",
      "\n",
      "    @classmethod\n",
      "    def find_by_key_id_type(cls, key_id: int, event_tracking_type: str, extra_key: str = None):\n",
      "        \"\"\"Return a list of event tracking objects by key id and event tracking type.\"\"\"\n",
      "        event_tracking = None\n",
      "        if key_id and event_tracking_type:\n",
      "            event_tracking = cls.query.filter(EventTracking.key_id == key_id,\n",
      "                                              EventTracking.event_tracking_type == event_tracking_type) \\\n",
      "                                      .order_by(EventTracking.id).all()\n",
      "\n",
      "            if event_tracking is not None and extra_key:\n",
      "                events = []\n",
      "                for event in event_tracking:\n",
      "                    if event.message and event.message.find(extra_key) > 0:\n",
      "                        events.append(event)\n",
      "                return events\n",
      "        return event_tracking\n",
      "\n",
      "    @staticmethod\n",
      "    def create(key_id: int, event_type: str, status: int = None, message: str = None):\n",
      "        \"\"\"Create an EventTracking record.\"\"\"\n",
      "        event_tracking = EventTracking(key_id=key_id, event_tracking_type=event_type, status=status, message=message)\n",
      "        event_tracking.event_ts = model_utils.now_ts()\n",
      "        event_tracking.save()\n",
      "\n",
      "        return event_tracking\n",
      "\n",
      "import unittest\n",
      "\n",
      "from hydrus.core import HydrusConstants as HC\n",
      "from hydrus.core import HydrusData\n",
      "from hydrus.core import HydrusSerialisable\n",
      "\n",
      "from hydrus.client import ClientApplicationCommand as CAC\n",
      "from hydrus.client import ClientConstants as CC\n",
      "from hydrus.client import ClientData\n",
      "from hydrus.client import ClientDefaults\n",
      "from hydrus.client import ClientDuplicates\n",
      "from hydrus.client import ClientSearch\n",
      "from hydrus.client.gui import ClientGUIShortcuts\n",
      "from hydrus.client.importing import ClientImportOptions\n",
      "from hydrus.client.importing import ClientImportSubscriptions\n",
      "from hydrus.client.importing import ClientImportSubscriptionQuery\n",
      "from hydrus.client.media import ClientMedia\n",
      "from hydrus.client.media import ClientMediaManagers\n",
      "from hydrus.client.media import ClientMediaResult\n",
      "from hydrus.client.metadata import ClientTags\n",
      "\n",
      "from hydrus.test import TestController as TC\n",
      "\n",
      "class TestSerialisables( unittest.TestCase ):\n",
      "    \n",
      "    def _dump_and_load_and_test( self, obj, test_func ):\n",
      "        \n",
      "        serialisable_tuple = obj.GetSerialisableTuple()\n",
      "        \n",
      "        self.assertIsInstance( serialisable_tuple, tuple )\n",
      "        \n",
      "        if isinstance( obj, HydrusSerialisable.SerialisableBaseNamed ):\n",
      "            \n",
      "            ( serialisable_type, name, version, serialisable_info ) = serialisable_tuple\n",
      "            \n",
      "        elif isinstance( obj, HydrusSerialisable.SerialisableBase ):\n",
      "            \n",
      "            ( serialisable_type, version, serialisable_info ) = serialisable_tuple\n",
      "            \n",
      "        \n",
      "        self.assertEqual( serialisable_type, obj.SERIALISABLE_TYPE )\n",
      "        self.assertEqual( version, obj.SERIALISABLE_VERSION )\n",
      "        \n",
      "        dupe_obj = HydrusSerialisable.CreateFromSerialisableTuple( serialisable_tuple )\n",
      "        \n",
      "        self.assertIsNot( obj, dupe_obj )\n",
      "        \n",
      "        test_func( obj, dupe_obj )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        json_string = obj.DumpToString()\n",
      "        \n",
      "        self.assertIsInstance( json_string, str )\n",
      "        \n",
      "        dupe_obj = HydrusSerialisable.CreateFromString( json_string )\n",
      "        \n",
      "        self.assertIsNot( obj, dupe_obj )\n",
      "        \n",
      "        test_func( obj, dupe_obj )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        network_bytes = obj.DumpToNetworkBytes()\n",
      "        \n",
      "        self.assertIsInstance( network_bytes, bytes )\n",
      "        \n",
      "        dupe_obj = HydrusSerialisable.CreateFromNetworkBytes( network_bytes )\n",
      "        \n",
      "        self.assertIsNot( obj, dupe_obj )\n",
      "        \n",
      "        test_func( obj, dupe_obj )\n",
      "        \n",
      "    \n",
      "    def test_basics( self ):\n",
      "        \n",
      "        def test( obj, dupe_obj ):\n",
      "            \n",
      "            self.assertEqual( len( list(obj.items()) ), len( list(dupe_obj.items()) ) )\n",
      "            \n",
      "            for ( key, value ) in list(obj.items()):\n",
      "                \n",
      "                self.assertEqual( value, dupe_obj[ key ] )\n",
      "                \n",
      "            \n",
      "        \n",
      "        #\n",
      "        \n",
      "        d = HydrusSerialisable.SerialisableDictionary()\n",
      "        \n",
      "        d[ 1 ] = 2\n",
      "        d[ 3 ] = 'test1'\n",
      "        \n",
      "        d[ 'test2' ] = 4\n",
      "        d[ 'test3' ] = 5\n",
      "        \n",
      "        d[ 6 ] = HydrusSerialisable.SerialisableDictionary( { i : 'test' + str( i ) for i in range( 20 ) } )\n",
      "        d[ ClientSearch.Predicate( ClientSearch.PREDICATE_TYPE_TAG, 'test pred 1' ) ] = 56\n",
      "        \n",
      "        d[ ClientSearch.Predicate( ClientSearch.PREDICATE_TYPE_TAG, 'test pred 2' ) ] = HydrusSerialisable.SerialisableList( [ ClientSearch.Predicate( ClientSearch.PREDICATE_TYPE_TAG, 'test' + str( i ) ) for i in range( 10 ) ] )\n",
      "        \n",
      "        self.assertEqual( len( list(d.keys()) ), 7 )\n",
      "        \n",
      "        for ( key, value ) in list(d.items()):\n",
      "            \n",
      "            self.assertEqual( d[ key ], value )\n",
      "            \n",
      "        \n",
      "        self._dump_and_load_and_test( d, test )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        db = HydrusSerialisable.SerialisableBytesDictionary()\n",
      "        \n",
      "        db[ HydrusData.GenerateKey() ] = HydrusData.GenerateKey()\n",
      "        db[ HydrusData.GenerateKey() ] = [ HydrusData.GenerateKey() for i in range( 10 ) ]\n",
      "        db[ 1 ] = HydrusData.GenerateKey()\n",
      "        db[ 2 ] = [ HydrusData.GenerateKey() for i in range( 10 ) ]\n",
      "        \n",
      "        self.assertEqual( len( list(db.keys()) ), 4 )\n",
      "        \n",
      "        for ( key, value ) in list(db.items()):\n",
      "            \n",
      "            self.assertEqual( db[ key ], value )\n",
      "            \n",
      "        \n",
      "        self._dump_and_load_and_test( db, test )\n",
      "        \n",
      "    \n",
      "    def test_SERIALISABLE_TYPE_APPLICATION_COMMAND( self ):\n",
      "        \n",
      "        def test( obj, dupe_obj ):\n",
      "            \n",
      "            self.assertEqual( obj.GetCommandType(), dupe_obj.GetCommandType() )\n",
      "            \n",
      "            self.assertEqual( obj.GetData(), dupe_obj.GetData() )\n",
      "            \n",
      "        \n",
      "        acs = []\n",
      "        \n",
      "        acs.append( ( CAC.ApplicationCommand( CAC.APPLICATION_COMMAND_TYPE_SIMPLE, CAC.SIMPLE_ARCHIVE_FILE ), 'archive file' ) )\n",
      "        acs.append( ( CAC.ApplicationCommand( CAC.APPLICATION_COMMAND_TYPE_CONTENT, ( HydrusData.GenerateKey(), HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_FLIP, 'test' ) ), 'flip on/off mappings \"test\" for unknown service!' ) )\n",
      "        acs.append( ( CAC.ApplicationCommand( CAC.APPLICATION_COMMAND_TYPE_CONTENT, ( CC.DEFAULT_LOCAL_TAG_SERVICE_KEY, HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_FLIP, 'test' ) ), 'flip on/off mappings \"test\" for my tags' ) )\n",
      "        acs.append( ( CAC.ApplicationCommand( CAC.APPLICATION_COMMAND_TYPE_CONTENT, ( HydrusData.GenerateKey(), HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_SET, 0.4 ) ), 'set ratings uncertain rating, \"0.4\" for unknown service!' ) )\n",
      "        \n",
      "        for ( ac, s ) in acs:\n",
      "            \n",
      "            self._dump_and_load_and_test( ac, test )\n",
      "            \n",
      "            self.assertEqual( ac.ToString(), s )\n",
      "            \n",
      "        \n",
      "    \n",
      "    def test_SERIALISABLE_TYPE_DUPLICATE_ACTION_OPTIONS( self ):\n",
      "        \n",
      "        def test( obj, dupe_obj ):\n",
      "            \n",
      "            self.assertEqual( obj.ToTuple(), dupe_obj.ToTuple() )\n",
      "            \n",
      "        \n",
      "        duplicate_action_options_delete_and_move = ClientDuplicates.DuplicateActionOptions( [ ( CC.DEFAULT_LOCAL_TAG_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_MOVE, ClientTags.TagFilter() ) ], [ ( TC.LOCAL_RATING_LIKE_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_MOVE ), ( TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_MOVE ) ] )\n",
      "        duplicate_action_options_copy = ClientDuplicates.DuplicateActionOptions( [ ( CC.DEFAULT_LOCAL_TAG_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_COPY, ClientTags.TagFilter() ) ], [ ( TC.LOCAL_RATING_LIKE_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_COPY ), ( TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_COPY ) ] )\n",
      "        duplicate_action_options_merge = ClientDuplicates.DuplicateActionOptions( [ ( CC.DEFAULT_LOCAL_TAG_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_TWO_WAY_MERGE, ClientTags.TagFilter() ) ], [ ( TC.LOCAL_RATING_LIKE_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_TWO_WAY_MERGE ), ( TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY, HC.CONTENT_MERGE_ACTION_TWO_WAY_MERGE ) ] )\n",
      "        \n",
      "        inbox = True\n",
      "        size = 40960\n",
      "        mime = HC.IMAGE_JPEG\n",
      "        width = 640\n",
      "        height = 480\n",
      "        duration = None\n",
      "        num_frames = None\n",
      "        has_audio = False\n",
      "        num_words = None\n",
      "        \n",
      "        local_locations_manager = ClientMediaManagers.LocationsManager( { CC.LOCAL_FILE_SERVICE_KEY, CC.COMBINED_LOCAL_FILE_SERVICE_KEY }, set(), set(), set(), inbox )\n",
      "        trash_locations_manager = ClientMediaManagers.LocationsManager( { CC.TRASH_SERVICE_KEY, CC.COMBINED_LOCAL_FILE_SERVICE_KEY }, set(), set(), set(), inbox )\n",
      "        deleted_locations_manager = ClientMediaManagers.LocationsManager( set(), { CC.COMBINED_LOCAL_FILE_SERVICE_KEY }, set(), set(), inbox )\n",
      "        \n",
      "        # duplicate to generate proper dicts\n",
      "        \n",
      "        one_tags_manager = ClientMediaManagers.TagsManager( { CC.DEFAULT_LOCAL_TAG_SERVICE_KEY : { HC.CONTENT_STATUS_CURRENT : { 'one' } } }, { CC.DEFAULT_LOCAL_TAG_SERVICE_KEY : { HC.CONTENT_STATUS_CURRENT : { 'one' } } } ).Duplicate()\n",
      "        two_tags_manager = ClientMediaManagers.TagsManager( { CC.DEFAULT_LOCAL_TAG_SERVICE_KEY : { HC.CONTENT_STATUS_CURRENT : { 'two' } } }, { CC.DEFAULT_LOCAL_TAG_SERVICE_KEY : { HC.CONTENT_STATUS_CURRENT : { 'two' } } } ).Duplicate()\n",
      "        substantial_tags_manager = ClientMediaManagers.TagsManager( { CC.DEFAULT_LOCAL_TAG_SERVICE_KEY : { HC.CONTENT_STATUS_CURRENT : { 'test tag', 'series:namespaced test tag' } } }, { CC.DEFAULT_LOCAL_TAG_SERVICE_KEY : { HC.CONTENT_STATUS_CURRENT : { 'test tag', 'series:namespaced test tag' } } } ).Duplicate()\n",
      "        empty_tags_manager = ClientMediaManagers.TagsManager( {}, {} ).Duplicate()\n",
      "        \n",
      "        one_ratings_manager = ClientMediaManagers.RatingsManager( { TC.LOCAL_RATING_LIKE_SERVICE_KEY : 1.0, TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY : 0.8 } )\n",
      "        two_ratings_manager = ClientMediaManagers.RatingsManager( { TC.LOCAL_RATING_LIKE_SERVICE_KEY : 0.0, TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY : 0.6 } )\n",
      "        substantial_ratings_manager = ClientMediaManagers.RatingsManager( { TC.LOCAL_RATING_LIKE_SERVICE_KEY : 1.0, TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY : 0.8 } )\n",
      "        empty_ratings_manager = ClientMediaManagers.RatingsManager( {} )\n",
      "        \n",
      "        notes_manager = ClientMediaManagers.NotesManager( {} )\n",
      "        \n",
      "        file_viewing_stats_manager = ClientMediaManagers.FileViewingStatsManager.STATICGenerateEmptyManager()\n",
      "        \n",
      "        #\n",
      "        \n",
      "        local_hash_has_values = HydrusData.GenerateKey()\n",
      "        \n",
      "        file_info_manager = ClientMediaManagers.FileInfoManager( 1, local_hash_has_values, size, mime, width, height, duration, num_frames, has_audio, num_words )\n",
      "        \n",
      "        media_result = ClientMediaResult.MediaResult( file_info_manager, substantial_tags_manager, local_locations_manager, substantial_ratings_manager, notes_manager, file_viewing_stats_manager )\n",
      "        \n",
      "        local_media_has_values = ClientMedia.MediaSingleton( media_result )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        other_local_hash_has_values = HydrusData.GenerateKey()\n",
      "        \n",
      "        file_info_manager = ClientMediaManagers.FileInfoManager( 2, other_local_hash_has_values, size, mime, width, height, duration, num_frames, has_audio, num_words )\n",
      "        \n",
      "        media_result = ClientMediaResult.MediaResult( file_info_manager, substantial_tags_manager, local_locations_manager, substantial_ratings_manager, notes_manager, file_viewing_stats_manager )\n",
      "        \n",
      "        other_local_media_has_values = ClientMedia.MediaSingleton( media_result )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        local_hash_empty = HydrusData.GenerateKey()\n",
      "        \n",
      "        file_info_manager = ClientMediaManagers.FileInfoManager( 3, local_hash_empty, size, mime, width, height, duration, num_frames, has_audio, num_words )\n",
      "        \n",
      "        media_result = ClientMediaResult.MediaResult( file_info_manager, empty_tags_manager, local_locations_manager, empty_ratings_manager, notes_manager, file_viewing_stats_manager )\n",
      "        \n",
      "        local_media_empty = ClientMedia.MediaSingleton( media_result )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        trashed_hash_empty = HydrusData.GenerateKey()\n",
      "        \n",
      "        file_info_manager = ClientMediaManagers.FileInfoManager( 4, trashed_hash_empty, size, mime, width, height, duration, num_frames, has_audio, num_words )\n",
      "        \n",
      "        media_result = ClientMediaResult.MediaResult( file_info_manager, empty_tags_manager, trash_locations_manager, empty_ratings_manager, notes_manager, file_viewing_stats_manager )\n",
      "        \n",
      "        trashed_media_empty = ClientMedia.MediaSingleton( media_result )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        deleted_hash_empty = HydrusData.GenerateKey()\n",
      "        \n",
      "        file_info_manager = ClientMediaManagers.FileInfoManager( 5, deleted_hash_empty, size, mime, width, height, duration, num_frames, has_audio, num_words )\n",
      "        \n",
      "        media_result = ClientMediaResult.MediaResult( file_info_manager, empty_tags_manager, deleted_locations_manager, empty_ratings_manager, notes_manager, file_viewing_stats_manager )\n",
      "        \n",
      "        deleted_media_empty = ClientMedia.MediaSingleton( media_result )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        one_hash = HydrusData.GenerateKey()\n",
      "        \n",
      "        file_info_manager = ClientMediaManagers.FileInfoManager( 6, one_hash, size, mime, width, height, duration, num_frames, has_audio, num_words )\n",
      "        \n",
      "        media_result = ClientMediaResult.MediaResult( file_info_manager, one_tags_manager, local_locations_manager, one_ratings_manager, notes_manager, file_viewing_stats_manager )\n",
      "        \n",
      "        one_media = ClientMedia.MediaSingleton( media_result )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        two_hash = HydrusData.GenerateKey()\n",
      "        \n",
      "        file_info_manager = ClientMediaManagers.FileInfoManager( 7, two_hash, size, mime, width, height, duration, num_frames, has_audio, num_words )\n",
      "        \n",
      "        media_result = ClientMediaResult.MediaResult( file_info_manager, two_tags_manager, local_locations_manager, two_ratings_manager, notes_manager, file_viewing_stats_manager )\n",
      "        \n",
      "        two_media = ClientMedia.MediaSingleton( media_result )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        self._dump_and_load_and_test( duplicate_action_options_delete_and_move, test )\n",
      "        self._dump_and_load_and_test( duplicate_action_options_copy, test )\n",
      "        self._dump_and_load_and_test( duplicate_action_options_merge, test )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        def assertSCUEqual( one, two ):\n",
      "            \n",
      "            self.assertEqual( TC.ConvertServiceKeysToContentUpdatesToComparable( one ), TC.ConvertServiceKeysToContentUpdatesToComparable( two ) )\n",
      "            \n",
      "        \n",
      "        file_deletion_reason = 'test delete'\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_delete_and_move.ProcessPairIntoContentUpdates( local_media_has_values, local_media_empty, delete_second = True, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.LOCAL_FILE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_FILES, HC.CONTENT_UPDATE_DELETE, { local_hash_empty }, reason = file_deletion_reason ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_delete_and_move.ProcessPairIntoContentUpdates( local_media_has_values, trashed_media_empty, delete_second = True, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.TRASH_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_FILES, HC.CONTENT_UPDATE_DELETE, { trashed_hash_empty }, reason = file_deletion_reason ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_delete_and_move.ProcessPairIntoContentUpdates( local_media_has_values, deleted_media_empty, delete_second = True, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        self.assertEqual( result, {} )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_delete_and_move.ProcessPairIntoContentUpdates( local_media_has_values, other_local_media_has_values, delete_second = True, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.DEFAULT_LOCAL_TAG_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_DELETE, ( 'test tag', { other_local_hash_has_values } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_DELETE, ( 'series:namespaced test tag', { other_local_hash_has_values } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_LIKE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( None, { other_local_hash_has_values } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( None, { other_local_hash_has_values } ) ) ]\n",
      "        scu[ CC.LOCAL_FILE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_FILES, HC.CONTENT_UPDATE_DELETE, { other_local_hash_has_values }, reason = file_deletion_reason ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_delete_and_move.ProcessPairIntoContentUpdates( local_media_empty, other_local_media_has_values, delete_second = True, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.DEFAULT_LOCAL_TAG_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'test tag', { local_hash_empty } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'series:namespaced test tag', { local_hash_empty } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_DELETE, ( 'test tag', { other_local_hash_has_values } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_DELETE, ( 'series:namespaced test tag', { other_local_hash_has_values } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_LIKE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 1.0, { local_hash_empty } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( None, { other_local_hash_has_values } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 0.8, { local_hash_empty } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( None, { other_local_hash_has_values } ) ) ]\n",
      "        scu[ CC.LOCAL_FILE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_FILES, HC.CONTENT_UPDATE_DELETE, { other_local_hash_has_values }, reason = file_deletion_reason ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "        #\n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_copy.ProcessPairIntoContentUpdates( local_media_has_values, local_media_empty, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        self.assertEqual( result, {} )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_copy.ProcessPairIntoContentUpdates( local_media_empty, other_local_media_has_values, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.DEFAULT_LOCAL_TAG_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'test tag', { local_hash_empty } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'series:namespaced test tag', { local_hash_empty } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_LIKE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 1.0, { local_hash_empty } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 0.8, { local_hash_empty } ) ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "        #\n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_merge.ProcessPairIntoContentUpdates( local_media_has_values, local_media_empty, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.DEFAULT_LOCAL_TAG_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'test tag', { local_hash_empty } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'series:namespaced test tag', { local_hash_empty } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_LIKE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 1.0, { local_hash_empty } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 0.8, { local_hash_empty } ) ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_merge.ProcessPairIntoContentUpdates( local_media_empty, other_local_media_has_values, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.DEFAULT_LOCAL_TAG_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'test tag', { local_hash_empty } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'series:namespaced test tag', { local_hash_empty } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_LIKE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 1.0, { local_hash_empty } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 0.8, { local_hash_empty } ) ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        result = duplicate_action_options_merge.ProcessPairIntoContentUpdates( one_media, two_media, file_deletion_reason = file_deletion_reason )\n",
      "        \n",
      "        scu = {}\n",
      "        \n",
      "        scu[ CC.DEFAULT_LOCAL_TAG_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'one', { two_hash } ) ), HydrusData.ContentUpdate( HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_ADD, ( 'two', { one_hash } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_LIKE_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 1.0, { two_hash } ) ) ]\n",
      "        scu[ TC.LOCAL_RATING_NUMERICAL_SERVICE_KEY ] = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_RATINGS, HC.CONTENT_UPDATE_ADD, ( 0.8, { two_hash } ) ) ]\n",
      "        \n",
      "        assertSCUEqual( result, scu )\n",
      "        \n",
      "    \n",
      "    def test_SERIALISABLE_TYPE_SHORTCUT( self ):\n",
      "        \n",
      "        def test( obj, dupe_obj ):\n",
      "            \n",
      "            self.assertEqual( dupe_obj.__hash__(), ( dupe_obj.shortcut_type, dupe_obj.shortcut_key, dupe_obj.shortcut_press_type, tuple( dupe_obj.modifiers ) ).__hash__() )\n",
      "            \n",
      "            self.assertEqual( obj, dupe_obj )\n",
      "            \n",
      "        \n",
      "        shortcuts = []\n",
      "        \n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut(), 'f7' ) )\n",
      "        \n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_SPECIAL, ClientGUIShortcuts.SHORTCUT_KEY_SPECIAL_SPACE, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [] ), 'space' ) )\n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_CHARACTER, ord( 'a' ), ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] ), 'ctrl+a' ) )\n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_CHARACTER, ord( 'A' ), ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] ), 'ctrl+a' ) )\n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_SPECIAL, ClientGUIShortcuts.SHORTCUT_KEY_SPECIAL_HOME, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_ALT, ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] ), 'ctrl+alt+home' ) )\n",
      "        \n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_MOUSE, ClientGUIShortcuts.SHORTCUT_MOUSE_LEFT, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [] ), 'left-click' ) )\n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_MOUSE, ClientGUIShortcuts.SHORTCUT_MOUSE_MIDDLE, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] ), 'ctrl+middle-click' ) )\n",
      "        shortcuts.append( ( ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_MOUSE, ClientGUIShortcuts.SHORTCUT_MOUSE_SCROLL_DOWN, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_ALT, ClientGUIShortcuts.SHORTCUT_MODIFIER_SHIFT ] ), 'alt+shift+scroll down' ) )\n",
      "        \n",
      "        for ( shortcut, s ) in shortcuts:\n",
      "            \n",
      "            self._dump_and_load_and_test( shortcut, test )\n",
      "            \n",
      "            self.assertEqual( shortcut.ToString(), s )\n",
      "            \n",
      "        \n",
      "    \n",
      "    def test_SERIALISABLE_TYPE_SHORTCUT_SET( self ):\n",
      "        \n",
      "        def test( obj, dupe_obj ):\n",
      "            \n",
      "            for ( shortcut, command ) in obj:\n",
      "                \n",
      "                self.assertEqual( dupe_obj.GetCommand( shortcut ).GetData(), command.GetData() )\n",
      "                \n",
      "            \n",
      "        \n",
      "        default_shortcuts = ClientDefaults.GetDefaultShortcuts()\n",
      "        \n",
      "        for shortcuts in default_shortcuts:\n",
      "            \n",
      "            self._dump_and_load_and_test( shortcuts, test )\n",
      "            \n",
      "        \n",
      "        command_1 = CAC.ApplicationCommand( CAC.APPLICATION_COMMAND_TYPE_SIMPLE, CAC.SIMPLE_ARCHIVE_FILE )\n",
      "        command_2 = CAC.ApplicationCommand( CAC.APPLICATION_COMMAND_TYPE_CONTENT, ( HydrusData.GenerateKey(), HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_FLIP, 'test' ) )\n",
      "        command_3 = CAC.ApplicationCommand( CAC.APPLICATION_COMMAND_TYPE_CONTENT, ( CC.DEFAULT_LOCAL_TAG_SERVICE_KEY, HC.CONTENT_TYPE_MAPPINGS, HC.CONTENT_UPDATE_FLIP, 'test' ) )\n",
      "        \n",
      "        k_shortcut_1 = ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_SPECIAL, ClientGUIShortcuts.SHORTCUT_KEY_SPECIAL_SPACE, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [] )\n",
      "        k_shortcut_2 = ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_CHARACTER, ord( 'a' ), ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] )\n",
      "        k_shortcut_3 = ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_CHARACTER, ord( 'A' ), ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] )\n",
      "        k_shortcut_4 = ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_KEYBOARD_SPECIAL, ClientGUIShortcuts.SHORTCUT_KEY_SPECIAL_HOME, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_ALT, ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] )\n",
      "        \n",
      "        m_shortcut_1 = ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_MOUSE, ClientGUIShortcuts.SHORTCUT_MOUSE_LEFT, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [] )\n",
      "        m_shortcut_2 = ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_MOUSE, ClientGUIShortcuts.SHORTCUT_MOUSE_MIDDLE, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_CTRL ] )\n",
      "        m_shortcut_3 = ClientGUIShortcuts.Shortcut( ClientGUIShortcuts.SHORTCUT_TYPE_MOUSE, ClientGUIShortcuts.SHORTCUT_MOUSE_SCROLL_DOWN, ClientGUIShortcuts.SHORTCUT_PRESS_TYPE_PRESS, [ ClientGUIShortcuts.SHORTCUT_MODIFIER_ALT, ClientGUIShortcuts.SHORTCUT_MODIFIER_SHIFT ] )\n",
      "        \n",
      "        shortcut_set = ClientGUIShortcuts.ShortcutSet( 'test' )\n",
      "        \n",
      "        shortcut_set.SetCommand( k_shortcut_1, command_1 )\n",
      "        shortcut_set.SetCommand( k_shortcut_2, command_2 )\n",
      "        shortcut_set.SetCommand( k_shortcut_3, command_2 )\n",
      "        shortcut_set.SetCommand( k_shortcut_4, command_3 )\n",
      "        \n",
      "        shortcut_set.SetCommand( m_shortcut_1, command_1 )\n",
      "        shortcut_set.SetCommand( m_shortcut_2, command_2 )\n",
      "        shortcut_set.SetCommand( m_shortcut_3, command_3 )\n",
      "        \n",
      "        self._dump_and_load_and_test( shortcut_set, test )\n",
      "        \n",
      "        self.assertEqual( shortcut_set.GetCommand( k_shortcut_1 ).GetData(), command_1.GetData() )\n",
      "        \n",
      "        shortcut_set.SetCommand( k_shortcut_1, command_3 )\n",
      "        \n",
      "        self.assertEqual( shortcut_set.GetCommand( k_shortcut_1 ).GetData(), command_3.GetData() )\n",
      "        \n",
      "    \n",
      "    def test_SERIALISABLE_TYPE_SUBSCRIPTION( self ):\n",
      "        \n",
      "        def test( obj, dupe_obj ):\n",
      "            \n",
      "            self.assertEqual( obj.GetName(), dupe_obj.GetName() )\n",
      "            \n",
      "            self.assertEqual( obj._gug_key_and_name, dupe_obj._gug_key_and_name )\n",
      "            self.assertEqual( len( obj._query_headers ), len( dupe_obj._query_headers ) )\n",
      "            self.assertEqual( obj._initial_file_limit, dupe_obj._initial_file_limit )\n",
      "            self.assertEqual( obj._periodic_file_limit, dupe_obj._periodic_file_limit )\n",
      "            self.assertEqual( obj._paused, dupe_obj._paused )\n",
      "            \n",
      "            self.assertEqual( obj._file_import_options.GetSerialisableTuple(), dupe_obj._file_import_options.GetSerialisableTuple() )\n",
      "            self.assertEqual( obj._tag_import_options.GetSerialisableTuple(), dupe_obj._tag_import_options.GetSerialisableTuple() )\n",
      "            \n",
      "            self.assertEqual( obj._no_work_until, dupe_obj._no_work_until )\n",
      "            \n",
      "        \n",
      "        sub = ClientImportSubscriptions.Subscription( 'test sub' )\n",
      "        \n",
      "        self._dump_and_load_and_test( sub, test )\n",
      "        \n",
      "        gug_key_and_name = ( HydrusData.GenerateKey(), 'muh test gug' )\n",
      "        \n",
      "        query_headers = []\n",
      "        \n",
      "        q = ClientImportSubscriptionQuery.SubscriptionQueryHeader()\n",
      "        q.SetQueryText( 'test query' )\n",
      "        query_headers.append( q )\n",
      "        \n",
      "        q = ClientImportSubscriptionQuery.SubscriptionQueryHeader()\n",
      "        q.SetQueryText( 'test query 2' )\n",
      "        query_headers.append( q )\n",
      "        \n",
      "        checker_options = ClientImportOptions.CheckerOptions()\n",
      "        initial_file_limit = 100\n",
      "        periodic_file_limit = 50\n",
      "        paused = False\n",
      "        \n",
      "        file_import_options = ClientImportOptions.FileImportOptions()\n",
      "        \n",
      "        service_tag_import_options = ClientImportOptions.ServiceTagImportOptions( get_tags = False, additional_tags = { 'test additional tag', 'and another' } )\n",
      "        \n",
      "        tag_import_options = ClientImportOptions.TagImportOptions( service_keys_to_service_tag_import_options = { HydrusData.GenerateKey() : service_tag_import_options } )\n",
      "        \n",
      "        no_work_until = HydrusData.GetNow() - 86400 * 20\n",
      "        \n",
      "        sub.SetTuple( gug_key_and_name, checker_options, initial_file_limit, periodic_file_limit, paused, file_import_options, tag_import_options, no_work_until )\n",
      "        \n",
      "        sub.SetQueryHeaders( query_headers )\n",
      "        \n",
      "        self.assertEqual( sub.GetGUGKeyAndName(), gug_key_and_name )\n",
      "        self.assertEqual( sub.GetTagImportOptions(), tag_import_options )\n",
      "        self.assertEqual( sub.GetQueryHeaders(), query_headers )\n",
      "        \n",
      "        self.assertEqual( sub._paused, False )\n",
      "        sub.PauseResume()\n",
      "        self.assertEqual( sub._paused, True )\n",
      "        sub.PauseResume()\n",
      "        self.assertEqual( sub._paused, False )\n",
      "        \n",
      "        self._dump_and_load_and_test( sub, test )\n",
      "        \n",
      "    \n",
      "    def test_SERIALISABLE_TYPE_TAG_FILTER( self ):\n",
      "        \n",
      "        def test( obj, dupe_obj ):\n",
      "            \n",
      "            self.assertEqual( obj._tag_slices_to_rules, dupe_obj._tag_slices_to_rules )\n",
      "            \n",
      "        \n",
      "        tags = set()\n",
      "        \n",
      "        tags.add( 'title:test title' )\n",
      "        tags.add( 'series:neon genesis evangelion' )\n",
      "        tags.add( 'series:kill la kill' )\n",
      "        tags.add( 'smile' )\n",
      "        tags.add( 'blue eyes' )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'smile', 'blue eyes', 'title:test title', 'series:neon genesis evangelion', 'series:kill la kill' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( '', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( ':', CC.FILTER_BLACKLIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), set() )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( '', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( ':', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'series:', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'series:neon genesis evangelion', 'series:kill la kill' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( '', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( ':', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'series:kill la kill', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'series:kill la kill' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( '', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( ':', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'smile', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'smile' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( ':', CC.FILTER_BLACKLIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'smile', 'blue eyes' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( ':', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'series:', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'smile', 'blue eyes', 'series:neon genesis evangelion', 'series:kill la kill' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( ':', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'series:kill la kill', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'smile', 'blue eyes', 'series:kill la kill' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( 'series:', CC.FILTER_BLACKLIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'smile', 'blue eyes', 'title:test title' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( 'series:', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'series:neon genesis evangelion', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'smile', 'blue eyes', 'title:test title', 'series:neon genesis evangelion' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( '', CC.FILTER_BLACKLIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'title:test title', 'series:neon genesis evangelion', 'series:kill la kill' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( '', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'blue eyes', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( tags ), { 'title:test title', 'series:neon genesis evangelion', 'series:kill la kill', 'blue eyes' } )\n",
      "        \n",
      "        # blacklist namespace test\n",
      "        \n",
      "        blacklist_tags = { 'nintendo', 'studio:nintendo' }\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( 'nintendo', CC.FILTER_BLACKLIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( blacklist_tags ), { 'studio:nintendo' } )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( 'nintendo', CC.FILTER_BLACKLIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( blacklist_tags, apply_unnamespaced_rules_to_namespaced_tags = True ), set() )\n",
      "        \n",
      "        #\n",
      "        \n",
      "        tag_filter = ClientTags.TagFilter()\n",
      "        \n",
      "        tag_filter.SetRule( 'nintendo', CC.FILTER_BLACKLIST )\n",
      "        tag_filter.SetRule( 'studio:nintendo', CC.FILTER_WHITELIST )\n",
      "        \n",
      "        self._dump_and_load_and_test( tag_filter, test )\n",
      "        \n",
      "        self.assertEqual( tag_filter.Filter( blacklist_tags, apply_unnamespaced_rules_to_namespaced_tags = True ), { 'studio:nintendo' } )\n",
      "        \n",
      "    \n",
      "\n",
      "import requests\n",
      "import json\n",
      "import time\n",
      "import random\n",
      "from . import conf, data, lang\n",
      "from inukit.timestamp import natural_date, natural_time, timestamp_now\n",
      "\n",
      "def is_same_day(ts1, ts2) -> bool:\n",
      "    def d(ts):\n",
      "        return natural_date(ts, '%Y-%m-%d')\n",
      "    return d(ts1) == d(ts2)\n",
      "\n",
      "def handle_morning(qq):\n",
      "    last_morning = data.get(qq, 'last_morning')\n",
      "    last_night = data.get(qq, 'last_night')\n",
      "    now = timestamp_now()\n",
      "    if last_morning > last_night:\n",
      "        msg = lang.no_sleep\n",
      "    else:\n",
      "        msg = lang.morning_success % (\n",
      "            natural_time(now - last_night)\n",
      "        )\n",
      "        data.set(qq, 'last_morning', now)\n",
      "    return msg\n",
      "\n",
      "def handle_night(qq):\n",
      "    last_morning = data.get(qq, 'last_morning')\n",
      "    last_night = data.get(qq, 'last_night')\n",
      "    now = timestamp_now()\n",
      "    if last_night > last_morning:\n",
      "        msg = lang.no_getup\n",
      "    else:\n",
      "        data.set(qq, 'last_night', now)\n",
      "        msg = lang.night_success % (\n",
      "            natural_time(now - last_morning)\n",
      "        )\n",
      "    return msg\n",
      "    \n",
      "def gen_sign_info():\n",
      "    rp = random.randint(1,100)\n",
      "    return {\n",
      "        \"rp\": rp\n",
      "    }\n",
      "\n",
      "def handle_sign(qq):\n",
      "    last_sign = data.get(qq, 'last_sign')\n",
      "    now = timestamp_now()\n",
      "    msg = ''\n",
      "    if is_same_day(last_sign, now):\n",
      "        info = data.get(qq, 'last_sign_info')\n",
      "        msg = lang.already_sign\n",
      "    else:\n",
      "        msg = lang.sign_success\n",
      "        info = gen_sign_info()\n",
      "        data.set(qq, 'last_sign', now)\n",
      "        data.set(qq, 'last_sign_info', info)\n",
      "    msg += lang.sign % (\n",
      "        natural_date(last_sign),\n",
      "        info['rp']\n",
      "    )\n",
      "    return msg\n",
      "\"\"\"\n",
      "Django settings for app project.\n",
      "\n",
      "Generated by 'django-admin startproject' using Django 2.1.15.\n",
      "\n",
      "For more information on this file, see\n",
      "https://docs.djangoproject.com/en/2.1/topics/settings/\n",
      "\n",
      "For the full list of settings and their values, see\n",
      "https://docs.djangoproject.com/en/2.1/ref/settings/\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "\n",
      "# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\n",
      "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
      "\n",
      "\n",
      "# Quick-start development settings - unsuitable for production\n",
      "# See https://docs.djangoproject.com/en/2.1/howto/deployment/checklist/\n",
      "\n",
      "# SECURITY WARNING: keep the secret key used in production secret!\n",
      "SECRET_KEY = 'lxb!(o00)qtw0p+6q_vs$01&wtsw(m*s!ol0_6^v*flo^!&ek&'\n",
      "\n",
      "# SECURITY WARNING: don't run with debug turned on in production!\n",
      "DEBUG = True\n",
      "\n",
      "ALLOWED_HOSTS = []\n",
      "\n",
      "\n",
      "# Application definition\n",
      "\n",
      "INSTALLED_APPS = [\n",
      "    'django.contrib.admin',\n",
      "    'django.contrib.auth',\n",
      "    'django.contrib.contenttypes',\n",
      "    'django.contrib.sessions',\n",
      "    'django.contrib.messages',\n",
      "    'django.contrib.staticfiles',\n",
      "    'rest_framework',\n",
      "    'rest_framework.authtoken',\n",
      "    'core',\n",
      "    'user',\n",
      "    'recipe',\n",
      "]\n",
      "\n",
      "MIDDLEWARE = [\n",
      "    'django.middleware.security.SecurityMiddleware',\n",
      "    'django.contrib.sessions.middleware.SessionMiddleware',\n",
      "    'django.middleware.common.CommonMiddleware',\n",
      "    'django.middleware.csrf.CsrfViewMiddleware',\n",
      "    'django.contrib.auth.middleware.AuthenticationMiddleware',\n",
      "    'django.contrib.messages.middleware.MessageMiddleware',\n",
      "    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n",
      "]\n",
      "\n",
      "ROOT_URLCONF = 'app.urls'\n",
      "\n",
      "TEMPLATES = [\n",
      "    {\n",
      "        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n",
      "        'DIRS': [],\n",
      "        'APP_DIRS': True,\n",
      "        'OPTIONS': {\n",
      "            'context_processors': [\n",
      "                'django.template.context_processors.debug',\n",
      "                'django.template.context_processors.request',\n",
      "                'django.contrib.auth.context_processors.auth',\n",
      "                'django.contrib.messages.context_processors.messages',\n",
      "            ],\n",
      "        },\n",
      "    },\n",
      "]\n",
      "\n",
      "WSGI_APPLICATION = 'app.wsgi.application'\n",
      "\n",
      "\n",
      "# Database\n",
      "# https://docs.djangoproject.com/en/2.1/ref/settings/#databases\n",
      "\n",
      "DATABASES = {\n",
      "    'default': {\n",
      "        'ENGINE': 'django.db.backends.postgresql',\n",
      "        'HOST': os.environ.get('DB_HOST'),\n",
      "        'NAME': os.environ.get('DB_NAME'),\n",
      "        'USER': os.environ.get('DB_USER'),\n",
      "        'PASSWORD': os.environ.get('DB_PASS'),\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "# Password validation\n",
      "# https://docs.djangoproject.com/en/2.1/ref/settings/#auth-password-validators\n",
      "\n",
      "AUTH_PASSWORD_VALIDATORS = [\n",
      "    {\n",
      "        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n",
      "    },\n",
      "    {\n",
      "        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n",
      "    },\n",
      "    {\n",
      "        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n",
      "    },\n",
      "    {\n",
      "        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n",
      "    },\n",
      "]\n",
      "\n",
      "\n",
      "# Internationalization\n",
      "# https://docs.djangoproject.com/en/2.1/topics/i18n/\n",
      "\n",
      "LANGUAGE_CODE = 'en-us'\n",
      "\n",
      "TIME_ZONE = 'UTC'\n",
      "\n",
      "USE_I18N = True\n",
      "\n",
      "USE_L10N = True\n",
      "\n",
      "USE_TZ = True\n",
      "\n",
      "\n",
      "# Static files (CSS, JavaScript, Images)\n",
      "# https://docs.djangoproject.com/en/2.1/howto/static-files/\n",
      "\n",
      "STATIC_URL = '/static/'\n",
      "MEDIA_URL = '/media/'\n",
      "\n",
      "MEDIA_ROOT = '/vol/web/media'\n",
      "STATIC_ROOT = '/vol/web/static'\n",
      "\n",
      "AUTH_USER_MODEL = 'core.User'\n",
      "\n",
      "#!/usr/bin/env python3\n",
      "'''Test config updates '''\n",
      "# ------------------------------------------------------------------------------\n",
      "# Imports\n",
      "# ------------------------------------------------------------------------------\n",
      "import subprocess\n",
      "import os\n",
      "import json\n",
      "import time\n",
      "import datetime\n",
      "import requests\n",
      "import pytest\n",
      "# ------------------------------------------------------------------------------\n",
      "# Constants\n",
      "# ------------------------------------------------------------------------------\n",
      "G_TEST_HOST = 'http://127.0.0.1:12345'\n",
      "# ------------------------------------------------------------------------------\n",
      "# run_command\n",
      "# ------------------------------------------------------------------------------\n",
      "def run_command(command):\n",
      "    p = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
      "    stdout, stderr = p.communicate()\n",
      "    return (p.returncode, stdout, stderr)\n",
      "# ------------------------------------------------------------------------------\n",
      "# setup scopez server in action mode\n",
      "# ------------------------------------------------------------------------------\n",
      "@pytest.fixture()\n",
      "def setup_scopez_server_action():\n",
      "    # ------------------------------------------------------\n",
      "    # setup\n",
      "    # ------------------------------------------------------\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_geoip2city_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/db/GeoLite2-City.mmdb'))\n",
      "    l_geoip2ISP_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/db/GeoLite2-ASN.mmdb'))\n",
      "    l_conf_dir = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf'))\n",
      "    l_ruleset_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/ruleset'))\n",
      "    l_scopez_dir = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf/scopes'))\n",
      "    l_an_list = os.path.realpath(os.path.join(l_file_path, '../../data/an/an-scopes.json'))\n",
      "    l_scopez_server_path = os.path.abspath(os.path.join(l_file_path, '../../../build/util/scopez_server/scopez_server'))\n",
      "    l_bot_challenge = os.path.realpath(os.path.join(l_file_path, '../../data/bot/bot-challenges.json'))\n",
      "    l_subproc = subprocess.Popen([l_scopez_server_path,\n",
      "                                  '-d', l_conf_dir,\n",
      "                                  '-S', l_scopez_dir,\n",
      "                                  '-l', l_an_list,\n",
      "                                  '-r', l_ruleset_path,\n",
      "                                  '-g', l_geoip2city_path,\n",
      "                                  '-i', l_geoip2ISP_path,\n",
      "                                  '-c', l_bot_challenge,\n",
      "                                  '-a'\n",
      "                                  ])\n",
      "    print('cmd: {}'.format(' '.join([l_scopez_server_path,\n",
      "                                  '-d', l_conf_dir,\n",
      "                                  '-S', l_scopez_dir,\n",
      "                                  '-l', l_an_list,\n",
      "                                  '-r', l_ruleset_path,\n",
      "                                  '-g', l_geoip2city_path,\n",
      "                                  '-i', l_geoip2ISP_path,\n",
      "                                  '-c', l_bot_challenge,\n",
      "                                  '-a'])))\n",
      "                                  # '-b'])))\n",
      "    time.sleep(1)\n",
      "    # ------------------------------------------------------\n",
      "    # yield...\n",
      "    # ------------------------------------------------------\n",
      "    yield setup_scopez_server_action\n",
      "    # ------------------------------------------------------\n",
      "    # tear down\n",
      "    # ------------------------------------------------------\n",
      "    _, _, _ = run_command('kill -9 %d'%(l_subproc.pid))\n",
      "    time.sleep(0.5)\n",
      "\n",
      "def test_acl_config_update(setup_scopez_server_action):\n",
      "    '''\n",
      "    update acl config 0050-ZrLf2KkQ - remove gizoogle from\n",
      "    user agent black list and test if request returns 200\n",
      "    '''\n",
      "    # ------------------------------------------------------\n",
      "    # test an 0050 with user-agent acl 'gizoogle' in the \n",
      "    # request\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'user-agent': 'gizoogle',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is acl custom response\\n'\n",
      "    #-------------------------------------------------------\n",
      "    # load acl config and remove gizoogle from blacklist\n",
      "    # ------------------------------------------------------\n",
      "    l_conf = {}\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_acl_conf_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf/acl/0050-ZrLf2KkQ.acl.json'))\n",
      "    try:\n",
      "        with open(l_acl_conf_path) as l_f:\n",
      "            l_conf = json.load(l_f)\n",
      "    except Exception as l_e:\n",
      "        print('error opening config file: %s.  Reason: %s error: %s, doc: %s' % (\n",
      "            l_acl_conf_path, type(l_e), l_e, l_e.__doc__))\n",
      "        assert False\n",
      "    l_conf['user_agent']['blacklist'] = []\n",
      "    l_conf['last_modified_date'] = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
      "    # ------------------------------------------------------\n",
      "    # post/update acl conf\n",
      "    # ------------------------------------------------------\n",
      "    l_url = '%s/update_acl'%(G_TEST_HOST)\n",
      "    l_headers = {'Content-Type': 'application/json',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.post(l_url,\n",
      "                        headers=l_headers,\n",
      "                        data=json.dumps(l_conf))\n",
      "    assert l_r.status_code == 200\n",
      "    # ------------------------------------------------------\n",
      "    # blacklist should have been updated and should get 200\n",
      "    #-------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'user-agent': 'gizoogle',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 200\n",
      "\n",
      "def test_rules_config_update(setup_scopez_server_action):\n",
      "    '''\n",
      "    update rules config 0050-ZrLf3KKq.rules.json - change \n",
      "    user agent to Donkeez from Monkeez\n",
      "    '''\n",
      "    # ------------------------------------------------------\n",
      "    # test an 0050 with user-agent 'Monkeez' in the \n",
      "    # request\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'user-agent': 'monkeez',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is rules custom response\\n'\n",
      "    #-------------------------------------------------------\n",
      "    # load rules config and changes monkeez to donkeez in \n",
      "    # custom rules\n",
      "    # ------------------------------------------------------\n",
      "    l_conf = {}\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_rules_conf_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf/rules/0050-ZrLf3KkQ.rules.json'))\n",
      "    try:\n",
      "        with open(l_rules_conf_path) as l_f:\n",
      "            l_conf = json.load(l_f)\n",
      "    except Exception as l_e:\n",
      "        print('error opening config file: %s.  Reason: %s error: %s, doc: %s' % (\n",
      "            l_file_path, type(l_e), l_e, l_e.__doc__))\n",
      "        assert False\n",
      "    l_conf['directive'][1]['sec_rule']['operator']['value'] = 'donkeez'\n",
      "    l_conf['last_modified_date'] = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
      "    # ------------------------------------------------------\n",
      "    # post/update rules conf\n",
      "    # ------------------------------------------------------\n",
      "    l_url = '%s/update_rules'%(G_TEST_HOST)\n",
      "    l_headers = {'Content-Type': 'application/json',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.post(l_url,\n",
      "                        headers=l_headers,\n",
      "                        data=json.dumps(l_conf))\n",
      "    assert l_r.status_code == 200\n",
      "    # ------------------------------------------------------\n",
      "    # test again with user-agent 'Monkeez' in the \n",
      "    # request. It should pass\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'user-agent': 'monkeez',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 200\n",
      "    # ------------------------------------------------------\n",
      "    # test with user-agent 'donkeez' in the \n",
      "    # request. should be blocked\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'user-agent': 'donkeez',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is rules custom response\\n'\n",
      "\n",
      "def test_profile_config_update(setup_scopez_server_action):\n",
      "    '''\n",
      "    update profile config 0050-YrLf3KkQ.wafprof.json - change\n",
      "    ignore_query_args to test from ignore\n",
      "    '''\n",
      "    # ------------------------------------------------------\n",
      "    # test an 0050 with sql injection\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/profile.html?a=%27select%20*%20from%20testing%27'\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is profile custom response\\n'\n",
      "    # ------------------------------------------------------\n",
      "    # test an 0050 with sql injection and query_args \"ignore\"\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/profile.html?ignore=%27select%20*%20from%20testing%27'\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 200\n",
      "    #-------------------------------------------------------\n",
      "    # load profile config and change \"ignore_query_args\"\n",
      "    # to \"test\"\n",
      "    # ------------------------------------------------------\n",
      "    l_conf = {}\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_profile_conf_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf/profile/0050-YrLf3KkQ.wafprof.json'))\n",
      "    try:\n",
      "        with open(l_profile_conf_path) as l_f:\n",
      "            l_conf = json.load(l_f)\n",
      "    except Exception as l_e:\n",
      "        print('error opening config file: %s.  Reason: %s error: %s, doc: %s' % (\n",
      "            l_profile_conf_path, type(l_e), l_e, l_e.__doc__))\n",
      "        assert False\n",
      "    l_conf[\"general_settings\"][\"ignore_query_args\"] = [\"test\"]\n",
      "    l_conf['last_modified_date'] = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
      "    # ------------------------------------------------------\n",
      "    # post/update profile conf\n",
      "    # ------------------------------------------------------\n",
      "    l_url = '%s/update_profile'%(G_TEST_HOST)\n",
      "    l_headers = {'Content-Type': 'application/json',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.post(l_url,\n",
      "                        headers=l_headers,\n",
      "                        data=json.dumps(l_conf))\n",
      "    assert l_r.status_code == 200\n",
      "    # ------------------------------------------------------\n",
      "    # test an 0050 with sql injection and query_args \"ignore\"\n",
      "    # should get 403\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/profile.html?ignore=%27select%20*%20from%20testing%27'\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is profile custom response\\n'\n",
      "    # ------------------------------------------------------\n",
      "    # test an 0050 with sql injection and query_args \"test\"\n",
      "    # sql injection should be ignored and get 200\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/profile.html?test=%27select%20*%20from%20testing%27'\n",
      "    l_headers = {'host': 'monkeez.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 200\n",
      "\n",
      "def test_limit_config_update(setup_scopez_server_action):\n",
      "    # ------------------------------------------------------\n",
      "    # Make 3 request in 2 sec for 3rd and\n",
      "    # 4th scope. Third request should get rate limited\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'limit.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    for _ in range(2):\n",
      "        l_r = requests.get(l_uri, headers=l_headers)\n",
      "        assert l_r.status_code == 200\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is ddos custom response\\n'\n",
      "\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'test.limit.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    for _ in range(2):\n",
      "        l_r = requests.get(l_uri, headers=l_headers)\n",
      "        assert l_r.status_code == 200\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'custom response for limits from limit_id_2\\n'\n",
      "    # ------------------------------------------------------\n",
      "    # sleep for 2 seconds. Enforcements should expire\n",
      "    # ------------------------------------------------------\n",
      "    time.sleep(2)\n",
      "    #-------------------------------------------------------\n",
      "    # load limit config and change duration_sec to 3\n",
      "    # ------------------------------------------------------\n",
      "    l_conf = {}\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_limit_conf_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf/limit/0050-MjMhNXMR.limit.json'))\n",
      "    try:\n",
      "        with open(l_limit_conf_path) as l_f:\n",
      "            l_conf = json.load(l_f)\n",
      "    except Exception as l_e:\n",
      "        print('error opening config file: %s.  Reason: %s error: %s, doc: %s' % (\n",
      "            l_limit_conf_path, type(l_e), l_e, l_e.__doc__))\n",
      "        assert False\n",
      "    l_conf[\"num\"] = 3\n",
      "    l_conf['last_modified_date'] = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
      "    #-------------------------------------------------------\n",
      "    # POST conf\n",
      "    # ------------------------------------------------------\n",
      "    l_url = '%s/update_limit'%(G_TEST_HOST)\n",
      "    l_headers = {'Content-Type': 'application/json',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    l_r = requests.post(l_url,\n",
      "                        headers=l_headers,\n",
      "                        data=json.dumps(l_conf))\n",
      "    assert l_r.status_code == 200\n",
      "    # ------------------------------------------------------\n",
      "    # Make 4 request in 2 sec. fourth request should get\n",
      "    # rate limited. Third request shouldn't be blocked\n",
      "    # because of the update\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'limit.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    for _ in range(3):\n",
      "        l_r = requests.get(l_uri, headers=l_headers)\n",
      "        assert l_r.status_code == 200\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is ddos custom response\\n'\n",
      "    # ------------------------------------------------------\n",
      "    # Make 4 request in 2 sec for fourth scope.\n",
      "    # verify if 4th scope was also updated\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'test.limit.com',\n",
      "                 'waf-scopes-id': '0050'}\n",
      "    for _ in range(3):\n",
      "        l_r = requests.get(l_uri, headers=l_headers)\n",
      "        assert l_r.status_code == 200\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'custom response for limits from limit_id_2\\n'\n",
      "\n",
      "def test_scopes_update(setup_scopez_server_action):\n",
      "    #-------------------------------------------------------\n",
      "    #  check second scope for AN 0051 working correctly\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/path.html'\n",
      "    l_headers = {'host': 'www.regexhost.com',\n",
      "                 'waf-scopes-id':'0051',\n",
      "                 'User-Agent': 'bananas'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is from RX scope\\n'\n",
      "    #-------------------------------------------------------\n",
      "    #  change the 'path' value for scope and update.\n",
      "    #  check if update was successful\n",
      "    # ------------------------------------------------------\n",
      "    l_conf = {}\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_scopes_conf_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf/scopes/0051.scopes.json'))\n",
      "    try:\n",
      "        with open(l_scopes_conf_path) as l_f:\n",
      "            l_conf = json.load(l_f)\n",
      "    except Exception as l_e:\n",
      "        print('error opening config file: %s.  Reason: %s error: %s, doc: %s' % (\n",
      "            l_scopes_conf_path, type(l_e), l_e, l_e.__doc__))\n",
      "        assert False \n",
      "    l_conf['scopes'][1]['path']['value'] = \".*/test.html\"\n",
      "    l_conf['last_modified_date'] = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
      "    #-------------------------------------------------------\n",
      "    # POST conf\n",
      "    # ------------------------------------------------------\n",
      "    l_url = '%s/update_scopes'%(G_TEST_HOST)\n",
      "    l_headers = {'Content-Type': 'application/json'}\n",
      "    l_r = requests.post(l_url,\n",
      "                        headers=l_headers,\n",
      "                        data=json.dumps(l_conf))\n",
      "    assert l_r.status_code == 200\n",
      "    #-------------------------------------------------------\n",
      "    # make a request with same path '/path.html',\n",
      "    # should match GLOB scope\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/path.html'\n",
      "    l_headers = {'host': 'www.regexhost.com',\n",
      "                 'waf-scopes-id':'0051',\n",
      "                 'User-Agent': 'bananas'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is from GLOB scope\\n'\n",
      "    #-------------------------------------------------------\n",
      "    # make a request with updated path '/test.html',\n",
      "    # should get 403 with custom response\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'www.regexhost.com',\n",
      "                 'waf-scopes-id':'0051',\n",
      "                 'User-Agent': 'bananas'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is from RX scope\\n'\n",
      "\n",
      "def test_scopes_linkage_update(setup_scopez_server_action):\n",
      "    \"\"\"\n",
      "    Test linkage update. Update rules config in second scope\n",
      "    (0050-scopes.json) to 0050-0gG8osWJ.rules.json from\n",
      "    0050-ZrLf3KkQ.rules.json check if update worked\n",
      "    \"\"\"\n",
      "    #-------------------------------------------------------\n",
      "    #  check second scope for AN 0050 working correctly\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/path.html'\n",
      "    l_headers = {'host': 'test.com',\n",
      "                 'waf-scopes-id':'0050',\n",
      "                 'User-Agent': 'monkeez'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is rules custom response\\n'\n",
      "    #-------------------------------------------------------\n",
      "    #  change the 'rules_prod_id' value for second scope \n",
      "    #  and update.\n",
      "    #  check if update was successful\n",
      "    # ------------------------------------------------------\n",
      "    l_conf = {}\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_scopes_conf_path = os.path.realpath(os.path.join(l_file_path, '../../data/waf/conf/scopes/0050.scopes.json'))\n",
      "    try:\n",
      "        with open(l_scopes_conf_path) as l_f:\n",
      "            l_conf = json.load(l_f)\n",
      "    except Exception as l_e:\n",
      "        print('error opening config file: %s.  Reason: %s error: %s, doc: %s' % (\n",
      "            l_scopes_conf_path, type(l_e), l_e, l_e.__doc__))\n",
      "        assert False \n",
      "    l_conf['scopes'][1]['rules_prod_id'] = \"0gG8osWJ\"\n",
      "    l_conf['last_modified_date'] = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
      "    #-------------------------------------------------------\n",
      "    # POST conf\n",
      "    # ------------------------------------------------------\n",
      "    l_url = '%s/update_scopes'%(G_TEST_HOST)\n",
      "    l_headers = {'Content-Type': 'application/json'}\n",
      "    l_r = requests.post(l_url,\n",
      "                        headers=l_headers,\n",
      "                        data=json.dumps(l_conf))\n",
      "    assert l_r.status_code == 200\n",
      "    #-------------------------------------------------------\n",
      "    # make the same request. should get 200\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/path.html'\n",
      "    l_headers = {'host': 'test.com',\n",
      "                 'waf-scopes-id':'0050',\n",
      "                 'User-Agent': 'monkeez'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 200\n",
      "    #assert l_r.text == 'This is from GLOB scope\\n'\n",
      "    #-------------------------------------------------------\n",
      "    # make a request with user-agent bananas\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/path.html'\n",
      "    l_headers = {'host': 'test.com',\n",
      "                 'waf-scopes-id':'0050',\n",
      "                 'User-Agent': 'bananas'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 403\n",
      "    assert l_r.text == 'This is rules custom response\\n'\n",
      "# ------------------------------------------------------------------------------\n",
      "# test /update_bots endpoint\n",
      "# ------------------------------------------------------------------------------\n",
      "def test_update_bots_endpoint(setup_scopez_server_action):\n",
      "    l_url = G_TEST_HOST + '/update_bots'\n",
      "    l_file_path = os.path.dirname(os.path.abspath(__file__))\n",
      "    l_test_file = os.path.realpath(os.path.join(l_file_path,\n",
      "                                                '../../data/waf/conf/bots/0052-wHyMHxV7.bots.json'))\n",
      "    l_test_payload = ''\n",
      "    # ------------------------------------------------------\n",
      "    # check setup\n",
      "    # ------------------------------------------------------\n",
      "    assert os.path.exists(l_test_file), 'test file not found!'\n",
      "    # ------------------------------------------------------\n",
      "    # slurp test file\n",
      "    # ------------------------------------------------------\n",
      "    with open(l_test_file) as l_tf:\n",
      "        l_test_payload = l_tf.read()\n",
      "    # ------------------------------------------------------\n",
      "    # check setup\n",
      "    # ------------------------------------------------------\n",
      "    assert l_test_payload, 'payload is empty!'\n",
      "    l_json_payload = json.loads(l_test_payload)\n",
      "    # ------------------------------------------------------\n",
      "    # Check that challenge works\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'mybot.com',\n",
      "                 'user-agent': 'bot-testing',\n",
      "                 'waf-scopes-id': '0052'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 401\n",
      "    # ------------------------------------------------------\n",
      "    # Update the bot config\n",
      "    # ------------------------------------------------------\n",
      "    l_json_payload['directive'][0]['sec_rule']['operator']['value'] = 'chowdah'\n",
      "    # ------------------------------------------------------\n",
      "    # update the timestamp, else it will silently do nothing and return 200\n",
      "    # ref: scopes.cc:load_bots (compare time)\n",
      "    # ------------------------------------------------------\n",
      "    l_json_payload['last_modified_date'] = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
      "    l_result = requests.post(l_url, timeout=3, json=l_json_payload)\n",
      "    assert l_result.status_code == 200\n",
      "    assert l_result.json()['status'] == 'success'\n",
      "    # ------------------------------------------------------\n",
      "    # Expect 200\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'mybot.com',\n",
      "                 'user-agent': 'bot-testing',\n",
      "                 'waf-scopes-id': '0052'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 200,\\\n",
      "        \"expecting 200, got {resp_code} since user-agent changed to chowdah\".format(resp_code=l_r.status_code)\n",
      "    # ------------------------------------------------------\n",
      "    # Expect 401 due to new UA\n",
      "    # ------------------------------------------------------\n",
      "    l_uri = G_TEST_HOST+'/test.html'\n",
      "    l_headers = {'host': 'mybot.com',\n",
      "                 'user-agent': 'chowdah',\n",
      "                 'waf-scopes-id': '0052'}\n",
      "    l_r = requests.get(l_uri, headers=l_headers)\n",
      "    assert l_r.status_code == 401,\\\n",
      "        \"expecting 401, got {resp_code} since user-agent changed to chowdah\".format(resp_code=l_r.status_code)\n",
      "    # ------------------------------------------------------\n",
      "    # check negative test - missing customer_id field\n",
      "    # ------------------------------------------------------\n",
      "    l_cust_id = l_json_payload.pop('customer_id')\n",
      "    l_n2_result = requests.post(l_url, json=l_json_payload)\n",
      "    assert l_n2_result.status_code == 500,\\\n",
      "        'expected 500 since customer_id {} is removed'.format(l_cust_id)\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "\"\"\"The setup script.\"\"\"\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "\n",
      "test_requirements = [\n",
      "    \"black>=19.10b0\",\n",
      "    \"flake8>=3.8.3\",\n",
      "    \"flake8-debugger>=3.2.1\",\n",
      "]\n",
      "\n",
      "dev_requirements = [\n",
      "    *test_requirements,\n",
      "    \"wheel>=0.34.2\",\n",
      "]\n",
      "\n",
      "requirements = [\n",
      "    \"cdp-backend[pipeline]==3.0.2\",\n",
      "    \"cdp-scrapers[king_county]>=0.3.2\",\n",
      "]\n",
      "\n",
      "extra_requirements = {\n",
      "    \"test\": test_requirements,\n",
      "    \"dev\": dev_requirements,\n",
      "    \"all\": [\n",
      "        *requirements,\n",
      "        *dev_requirements,\n",
      "    ],\n",
      "}\n",
      "\n",
      "setup(\n",
      "    author=\"JacksonMaxfield\",\n",
      "    classifiers=[\n",
      "        \"Development Status :: 2 - Pre-Alpha\",\n",
      "        \"Intended Audience :: Developers\",\n",
      "        \"License :: OSI Approved :: MIT License\",\n",
      "        \"Natural Language :: English\",\n",
      "        \"Programming Language :: Python :: 3.9\",\n",
      "    ],\n",
      "    description=\"Package containing the gather functions for Example.\",\n",
      "    install_requires=requirements,\n",
      "    license=\"MIT license\",\n",
      "    long_description_content_type=\"text/markdown\",\n",
      "    include_package_data=True,\n",
      "    keywords=\"civic technology, open government\",\n",
      "    name=\"cdp-king_county-backend\",\n",
      "    packages=find_packages(exclude=[\"tests\", \"*.tests\", \"*.tests.*\"]),\n",
      "    python_requires=\">=3.9\",\n",
      "    tests_require=test_requirements,\n",
      "    extras_require=extra_requirements,\n",
      "    url=\"https://github.com/CouncilDataProject/king-county\",\n",
      "    version=\"1.0.0\",\n",
      "    zip_safe=False,\n",
      ")\n",
      "\n",
      "from scrapy.spider import BaseSpider\n",
      "from scrapy.http import Request\n",
      "from scrapy.selector import XmlXPathSelector\n",
      "from openrecipes.spiders.elanaspantry_spider import ElanaspantryMixin\n",
      "\n",
      "\n",
      "class ElanaspantryfeedSpider(BaseSpider, ElanaspantryMixin):\n",
      "    name = \"elanaspantry.feed\"\n",
      "    allowed_domains = [\n",
      "        \"www.elanaspantry.com\",\n",
      "        \"feeds.feedburner.com\",\n",
      "        \"feedproxy.google.com\",\n",
      "    ]\n",
      "    start_urls = [\n",
      "        \"http://feeds.feedburner.com/elanaspantry\",\n",
      "    ]\n",
      "\n",
      "    def parse(self, response):\n",
      "        xxs = XmlXPathSelector(response)\n",
      "        links = xxs.select(\"//item/*[local-name()='origLink']/text()\").extract()\n",
      "\n",
      "        return [Request(x, callback=self.parse_item) for x in links]\n",
      "\n",
      "#!/usr/bin/env python\n",
      "#-*- encoding: UTF-8 -*-\n",
      "\n",
      "###############################################\n",
      "# Todos los derechos reservados a:            #\n",
      "# CreceLibre Consultores en Tecnologías Ltda. #\n",
      "#                                             #\n",
      "# ©Milton Inostroza Aguilera                  #\n",
      "# minostro@minostro.com                       #\n",
      "# 2009                                        #\n",
      "###############################################\n",
      "from django.db import models\n",
      "from AlyMoly.mantenedor.models import Producto, Promocion, Trabajador\n",
      "\n",
      "\n",
      "class Turno(models.Model):\n",
      "    \"\"\"\n",
      "        estado:\n",
      "            1 --> abierto\n",
      "            2 --> cerrado\n",
      "    \"\"\"\n",
      "    fecha_apertura_sistema = models.DateTimeField()\n",
      "    fecha_cierre_sistema = models.DateTimeField(null=True, blank=True)\n",
      "    estado = models.IntegerField(default=1, blank=True)\n",
      "    trabajador = models.ForeignKey(Trabajador, blank=True)\n",
      "    monto_apertura_caja = models.IntegerField(default=0)\n",
      "    monto_cierre_calculado = models.IntegerField(default=0, blank=True)\n",
      "    monto_afecto = models.IntegerField(default=0, blank=True)\n",
      "    monto_exento = models.IntegerField(default=0, blank=True)\n",
      "\n",
      "    def monto_cierre_informado(self):\n",
      "        return self.boletadeposito.total\n",
      "\n",
      "    def estado_turno(self):\n",
      "        if self.estado == 1:\n",
      "            return \"Abierto\"\n",
      "        else:\n",
      "            return \"Cerrado\"\n",
      "\n",
      "    def save(self, force_insert=False, force_update=False):\n",
      "        \"\"\"\n",
      "            Al guardar un turno abierto se verifica que el trabajador ya no cuente con un\n",
      "            turno abierto anteriormente.\n",
      "        \"\"\"\n",
      "        if self.estado == 1 and len(Turno.objects.exclude(id=self.id).filter(trabajador__id=self.trabajador.id).filter(estado=1)) > 0:\n",
      "            raise Exception(u\"Usted ya cuenta con un turno abierto.\")\n",
      "        super(Turno, self).save(force_insert, force_update)\n",
      "\n",
      "\n",
      "class BoletaDeposito(models.Model):\n",
      "    turno = models.OneToOneField(Turno, blank=True)\n",
      "    veintemil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    diezmil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    cincomil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    dosmil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    mil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    quinientos = models.PositiveIntegerField(default=0, blank=True)\n",
      "    cien = models.PositiveIntegerField(default=0, blank=True)\n",
      "    cincuenta = models.PositiveIntegerField(default=0, blank=True)\n",
      "    diez = models.PositiveIntegerField(default=0, blank=True)\n",
      "    tarjetas = models.PositiveIntegerField(default=0, blank=True)\n",
      "    otros = models.PositiveIntegerField(default=0, blank=True)\n",
      "    total = models.PositiveIntegerField(default=0, blank=True)\n",
      "\n",
      "\n",
      "class Venta(models.Model):\n",
      "    \"\"\"\n",
      "        medio_pago:\n",
      "            1 --> efectivo\n",
      "            2 --> otro\n",
      "    \"\"\"\n",
      "    fecha_venta = models.DateTimeField()\n",
      "    folio_boleta = models.PositiveIntegerField(null=True, blank=True)\n",
      "    monto_total = models.PositiveIntegerField()\n",
      "    monto_afecto = models.PositiveIntegerField()\n",
      "    monto_exento = models.PositiveIntegerField()\n",
      "    cantidad_productos = models.PositiveIntegerField()\n",
      "    medio_pago = models.PositiveIntegerField()\n",
      "    monto_pago = models.PositiveIntegerField(null=True)\n",
      "    turno = models.ForeignKey('Turno')\n",
      "\n",
      "    def __unicode__(self):\n",
      "        return u\"%s-%s\" % (self.id, self.folio_boleta)\n",
      "\n",
      "\n",
      "class LineaDetalle(models.Model):\n",
      "    cantidad = models.IntegerField()\n",
      "    precio_venta = models.IntegerField()\n",
      "    precio_venta_total = models.IntegerField()\n",
      "    producto = models.ForeignKey(Producto, null=True, blank=True)\n",
      "    promocion = models.ForeignKey(Promocion, null=True, blank=True)\n",
      "    venta = models.ForeignKey('Venta')\n",
      "\n",
      "\"\"\"Train (basic) densely-connected oracle.\"\"\"\n",
      "\n",
      "import os\n",
      "import time\n",
      "import multiprocessing as mp\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "import torch\n",
      "from torch import optim\n",
      "from torch.utils.data import DataLoader, Subset, TensorDataset, WeightedRandomSampler\n",
      "\n",
      "from profit.dataset.splitters import split_method_dict\n",
      "from profit.models.torch import SequenceOracle\n",
      "from profit.utils.data_utils.tokenizers import AminoAcidTokenizer\n",
      "from profit.utils.training_utils.torch import losses as L\n",
      "from profit.utils.training_utils.torch.callbacks import ModelCheckpoint\n",
      "from profit.utils.training_utils.torch.callbacks import EarlyStopping\n",
      "\n",
      "from examples.gb1.data import load_dataset\n",
      "\n",
      "\n",
      "timestep = time.strftime(\"%Y-%b-%d-%H:%M:%S\", time.gmtime())\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
      "splits = [\"train\", \"valid\"]\n",
      "\n",
      "# Preprocess + load the dataset\n",
      "dataset = load_dataset(\"lstm\", \"primary\", labels=\"Fitness\", num_data=-1,\n",
      "                       filetype=\"mdb\", as_numpy=False, vocab=\"aa20\")\n",
      "# Stratify train/val/test sets s.t. the target labels are equally represented in\n",
      "# each subset. Each subset will have the same ratio of low/mid/high variants in\n",
      "# each batch as the full dataset. See: https://discuss.pytorch.org/t/29907/2\n",
      "_dataset = dataset[:][\"arr_0\"]\n",
      "_labels = dataset[:][\"arr_1\"].view(-1)\n",
      "# # Remove samples below a certain threshold\n",
      "# high_idx = torch.where(_labels > _labels.mean())\n",
      "# dataset = Subset(dataset, sorted(high_idx))\n",
      "# _dataset = _dataset[high_idx]\n",
      "# _labels = _labels[high_idx]\n",
      "\n",
      "# Compute sample weights (each sample should get its own weight)\n",
      "def sampler(labels: torch.Tensor,\n",
      "            nbins: int = 10,\n",
      "            stratify: bool = False) -> WeightedRandomSampler:\n",
      "    discretize = pd.qcut if stratify else pd.cut\n",
      "    bin_labels = torch.LongTensor(discretize(labels.tolist(), nbins,\n",
      "                                             labels=False, duplicates=\"drop\"))\n",
      "    class_sample_count = torch.LongTensor(\n",
      "        [(bin_labels == t).sum() for t in torch.arange(nbins)])\n",
      "    weight = 1. / class_sample_count.float()\n",
      "    sample_weights = torch.zeros_like(labels)\n",
      "    for t in torch.unique(bin_labels):\n",
      "        sample_weights[bin_labels == t] = weight[t]\n",
      "    return WeightedRandomSampler(sample_weights, len(sample_weights))\n",
      "\n",
      "# Compute sample weights and add to original dataset\n",
      "weights = sampler(_labels, nbins=10, stratify=False).weights.type(torch.float)\n",
      "dataset = TensorDataset(*dataset[:].values(), weights)\n",
      "\n",
      "# Create subset indicies\n",
      "subset_idx = split_method_dict[\"stratified\"]().train_valid_test_split(\n",
      "    dataset=_dataset, labels=_labels.tolist(), frac_train=0.9,\n",
      "    frac_valid=0.1, frac_test=0.0, return_idxs=True, n_bins=10)\n",
      "stratified = {split: Subset(dataset, sorted(idx))\n",
      "              for split, idx in zip(splits, subset_idx)}\n",
      "\n",
      "# Create stratified sampler (only needed for training)\n",
      "train_sampler = sampler(stratified[\"train\"][:][1].view(-1), stratify=True)\n",
      "\n",
      "# Initialize model\n",
      "tokenizer = AminoAcidTokenizer(\"aa20\")\n",
      "vocab_size = tokenizer.vocab_size\n",
      "seqlen = stratified[\"train\"][0][0].size(0)\n",
      "model = SequenceOracle(seqlen, vocab_size, hidden_size=50, out_size=2)\n",
      "\n",
      "# Initialize callbacks\n",
      "# NOTE: Must set model (within save_clbk) to ensure weights get saved\n",
      "stop_clbk = EarlyStopping(patience=5, verbose=1)\n",
      "save_clbk = ModelCheckpoint(os.path.join(\"bin/3gb1/oracle\", timestep),\n",
      "                            monitor=\"val_loss\",\n",
      "                            verbose=1,\n",
      "                            save_weights_only=True)\n",
      "save_clbk.set_model(model)\n",
      "\n",
      "# Initialize callbacks\n",
      "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
      "\n",
      "epochs = 50\n",
      "for epoch in range(1, epochs+1):\n",
      "    for split in splits:\n",
      "        summed_loss = 0\n",
      "        data_loader = DataLoader(\n",
      "            dataset=stratified[split],\n",
      "            batch_size=32,\n",
      "            sampler=train_sampler if split == \"train\" else None,\n",
      "            num_workers=mp.cpu_count(),\n",
      "            pin_memory=torch.cuda.is_available()\n",
      "        )\n",
      "\n",
      "        # Enable/disable dropout\n",
      "        model.train() if split == \"train\" else model.eval()\n",
      "\n",
      "        for it, batch in enumerate(data_loader):\n",
      "            data = batch[0].long().to(device)\n",
      "            target = batch[1].to(device)\n",
      "            sample_weight = batch[2].to(device)\n",
      "            # One-hot encode (see: https://discuss.pytorch.org/t/507/34)\n",
      "            batch_size, seqlen = data.size()\n",
      "            onehot = torch.zeros(batch_size, seqlen, vocab_size)\n",
      "            onehot.scatter_(2, torch.unsqueeze(data, 2), 1)\n",
      "\n",
      "            # Forward pass\n",
      "            pred = model(onehot)\n",
      "            # Loss calculation\n",
      "            nll_loss = L.gaussian_nll_loss(pred, target, reduction=\"none\")\n",
      "            # Reweight nll_loss w/ sample weights\n",
      "            nll_loss = (nll_loss * sample_weight).sum()\n",
      "            summed_loss += nll_loss.item()\n",
      "            loss = nll_loss / batch_size\n",
      "            # Compute gradients and update params/weights\n",
      "            if split == \"train\":\n",
      "                optimizer.zero_grad()\n",
      "                loss.backward()\n",
      "                optimizer.step()\n",
      "\n",
      "            # Bookkeeping (batch)\n",
      "            if it % 5 == 0 or it+1 == len(data_loader):\n",
      "                print(\"{} Batch {:04d}/{:d} ({:.2f}%)\\tLoss: {:.4f}\".format(\n",
      "                    split.upper(), it+1, len(data_loader),\n",
      "                    100. * ((it+1)/len(data_loader)), loss.item()))\n",
      "\n",
      "        # Bookkeeping (epoch)\n",
      "        avg_loss = summed_loss / len(data_loader.dataset)\n",
      "        print(\"{} Epoch {}/{}, Average NLL loss: {:.4f}\".format(\n",
      "            split.upper(), epoch, epochs, avg_loss))\n",
      "\n",
      "        # Stop training (based off val loss) and save (top k) ckpts\n",
      "        if split == \"valid\":\n",
      "            save_clbk.on_epoch_end(epoch, logs={\"val_loss\": avg_loss})\n",
      "            should_stop = stop_clbk.on_epoch_end(epoch, logs={\"val_loss\": avg_loss})\n",
      "            if should_stop:\n",
      "                break\n",
      "    else:\n",
      "        continue\n",
      "    break\n",
      "\n",
      "# -*- coding: utf-8 -*-\n",
      "# Copyright 2020 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "import os\n",
      "import mock\n",
      "\n",
      "import grpc\n",
      "from grpc.experimental import aio\n",
      "import math\n",
      "import pytest\n",
      "from proto.marshal.rules.dates import DurationRule, TimestampRule\n",
      "\n",
      "\n",
      "from google.api_core import client_options\n",
      "from google.api_core import exceptions as core_exceptions\n",
      "from google.api_core import future\n",
      "from google.api_core import gapic_v1\n",
      "from google.api_core import grpc_helpers\n",
      "from google.api_core import grpc_helpers_async\n",
      "from google.api_core import operation_async  # type: ignore\n",
      "from google.api_core import operations_v1\n",
      "from google.api_core import path_template\n",
      "from google.auth import credentials as ga_credentials\n",
      "from google.auth.exceptions import MutualTLSChannelError\n",
      "from google.cloud.deploy_v1.services.cloud_deploy import CloudDeployAsyncClient\n",
      "from google.cloud.deploy_v1.services.cloud_deploy import CloudDeployClient\n",
      "from google.cloud.deploy_v1.services.cloud_deploy import pagers\n",
      "from google.cloud.deploy_v1.services.cloud_deploy import transports\n",
      "from google.cloud.deploy_v1.types import cloud_deploy\n",
      "from google.longrunning import operations_pb2\n",
      "from google.oauth2 import service_account\n",
      "from google.protobuf import field_mask_pb2  # type: ignore\n",
      "from google.protobuf import timestamp_pb2  # type: ignore\n",
      "import google.auth\n",
      "\n",
      "\n",
      "def client_cert_source_callback():\n",
      "    return b\"cert bytes\", b\"key bytes\"\n",
      "\n",
      "\n",
      "# If default endpoint is localhost, then default mtls endpoint will be the same.\n",
      "# This method modifies the default endpoint so the client can produce a different\n",
      "# mtls endpoint for endpoint testing purposes.\n",
      "def modify_default_endpoint(client):\n",
      "    return (\n",
      "        \"foo.googleapis.com\"\n",
      "        if (\"localhost\" in client.DEFAULT_ENDPOINT)\n",
      "        else client.DEFAULT_ENDPOINT\n",
      "    )\n",
      "\n",
      "\n",
      "def test__get_default_mtls_endpoint():\n",
      "    api_endpoint = \"example.googleapis.com\"\n",
      "    api_mtls_endpoint = \"example.mtls.googleapis.com\"\n",
      "    sandbox_endpoint = \"example.sandbox.googleapis.com\"\n",
      "    sandbox_mtls_endpoint = \"example.mtls.sandbox.googleapis.com\"\n",
      "    non_googleapi = \"api.example.com\"\n",
      "\n",
      "    assert CloudDeployClient._get_default_mtls_endpoint(None) is None\n",
      "    assert (\n",
      "        CloudDeployClient._get_default_mtls_endpoint(api_endpoint) == api_mtls_endpoint\n",
      "    )\n",
      "    assert (\n",
      "        CloudDeployClient._get_default_mtls_endpoint(api_mtls_endpoint)\n",
      "        == api_mtls_endpoint\n",
      "    )\n",
      "    assert (\n",
      "        CloudDeployClient._get_default_mtls_endpoint(sandbox_endpoint)\n",
      "        == sandbox_mtls_endpoint\n",
      "    )\n",
      "    assert (\n",
      "        CloudDeployClient._get_default_mtls_endpoint(sandbox_mtls_endpoint)\n",
      "        == sandbox_mtls_endpoint\n",
      "    )\n",
      "    assert CloudDeployClient._get_default_mtls_endpoint(non_googleapi) == non_googleapi\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"client_class\", [CloudDeployClient, CloudDeployAsyncClient,])\n",
      "def test_cloud_deploy_client_from_service_account_info(client_class):\n",
      "    creds = ga_credentials.AnonymousCredentials()\n",
      "    with mock.patch.object(\n",
      "        service_account.Credentials, \"from_service_account_info\"\n",
      "    ) as factory:\n",
      "        factory.return_value = creds\n",
      "        info = {\"valid\": True}\n",
      "        client = client_class.from_service_account_info(info)\n",
      "        assert client.transport._credentials == creds\n",
      "        assert isinstance(client, client_class)\n",
      "\n",
      "        assert client.transport._host == \"clouddeploy.googleapis.com:443\"\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"transport_class,transport_name\",\n",
      "    [\n",
      "        (transports.CloudDeployGrpcTransport, \"grpc\"),\n",
      "        (transports.CloudDeployGrpcAsyncIOTransport, \"grpc_asyncio\"),\n",
      "    ],\n",
      ")\n",
      "def test_cloud_deploy_client_service_account_always_use_jwt(\n",
      "    transport_class, transport_name\n",
      "):\n",
      "    with mock.patch.object(\n",
      "        service_account.Credentials, \"with_always_use_jwt_access\", create=True\n",
      "    ) as use_jwt:\n",
      "        creds = service_account.Credentials(None, None, None)\n",
      "        transport = transport_class(credentials=creds, always_use_jwt_access=True)\n",
      "        use_jwt.assert_called_once_with(True)\n",
      "\n",
      "    with mock.patch.object(\n",
      "        service_account.Credentials, \"with_always_use_jwt_access\", create=True\n",
      "    ) as use_jwt:\n",
      "        creds = service_account.Credentials(None, None, None)\n",
      "        transport = transport_class(credentials=creds, always_use_jwt_access=False)\n",
      "        use_jwt.assert_not_called()\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"client_class\", [CloudDeployClient, CloudDeployAsyncClient,])\n",
      "def test_cloud_deploy_client_from_service_account_file(client_class):\n",
      "    creds = ga_credentials.AnonymousCredentials()\n",
      "    with mock.patch.object(\n",
      "        service_account.Credentials, \"from_service_account_file\"\n",
      "    ) as factory:\n",
      "        factory.return_value = creds\n",
      "        client = client_class.from_service_account_file(\"dummy/file/path.json\")\n",
      "        assert client.transport._credentials == creds\n",
      "        assert isinstance(client, client_class)\n",
      "\n",
      "        client = client_class.from_service_account_json(\"dummy/file/path.json\")\n",
      "        assert client.transport._credentials == creds\n",
      "        assert isinstance(client, client_class)\n",
      "\n",
      "        assert client.transport._host == \"clouddeploy.googleapis.com:443\"\n",
      "\n",
      "\n",
      "def test_cloud_deploy_client_get_transport_class():\n",
      "    transport = CloudDeployClient.get_transport_class()\n",
      "    available_transports = [\n",
      "        transports.CloudDeployGrpcTransport,\n",
      "    ]\n",
      "    assert transport in available_transports\n",
      "\n",
      "    transport = CloudDeployClient.get_transport_class(\"grpc\")\n",
      "    assert transport == transports.CloudDeployGrpcTransport\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"client_class,transport_class,transport_name\",\n",
      "    [\n",
      "        (CloudDeployClient, transports.CloudDeployGrpcTransport, \"grpc\"),\n",
      "        (\n",
      "            CloudDeployAsyncClient,\n",
      "            transports.CloudDeployGrpcAsyncIOTransport,\n",
      "            \"grpc_asyncio\",\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "@mock.patch.object(\n",
      "    CloudDeployClient, \"DEFAULT_ENDPOINT\", modify_default_endpoint(CloudDeployClient)\n",
      ")\n",
      "@mock.patch.object(\n",
      "    CloudDeployAsyncClient,\n",
      "    \"DEFAULT_ENDPOINT\",\n",
      "    modify_default_endpoint(CloudDeployAsyncClient),\n",
      ")\n",
      "def test_cloud_deploy_client_client_options(\n",
      "    client_class, transport_class, transport_name\n",
      "):\n",
      "    # Check that if channel is provided we won't create a new one.\n",
      "    with mock.patch.object(CloudDeployClient, \"get_transport_class\") as gtc:\n",
      "        transport = transport_class(credentials=ga_credentials.AnonymousCredentials())\n",
      "        client = client_class(transport=transport)\n",
      "        gtc.assert_not_called()\n",
      "\n",
      "    # Check that if channel is provided via str we will create a new one.\n",
      "    with mock.patch.object(CloudDeployClient, \"get_transport_class\") as gtc:\n",
      "        client = client_class(transport=transport_name)\n",
      "        gtc.assert_called()\n",
      "\n",
      "    # Check the case api_endpoint is provided.\n",
      "    options = client_options.ClientOptions(api_endpoint=\"squid.clam.whelk\")\n",
      "    with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "        patched.return_value = None\n",
      "        client = client_class(transport=transport_name, client_options=options)\n",
      "        patched.assert_called_once_with(\n",
      "            credentials=None,\n",
      "            credentials_file=None,\n",
      "            host=\"squid.clam.whelk\",\n",
      "            scopes=None,\n",
      "            client_cert_source_for_mtls=None,\n",
      "            quota_project_id=None,\n",
      "            client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "            always_use_jwt_access=True,\n",
      "        )\n",
      "\n",
      "    # Check the case api_endpoint is not provided and GOOGLE_API_USE_MTLS_ENDPOINT is\n",
      "    # \"never\".\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_MTLS_ENDPOINT\": \"never\"}):\n",
      "        with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "            patched.return_value = None\n",
      "            client = client_class(transport=transport_name)\n",
      "            patched.assert_called_once_with(\n",
      "                credentials=None,\n",
      "                credentials_file=None,\n",
      "                host=client.DEFAULT_ENDPOINT,\n",
      "                scopes=None,\n",
      "                client_cert_source_for_mtls=None,\n",
      "                quota_project_id=None,\n",
      "                client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "                always_use_jwt_access=True,\n",
      "            )\n",
      "\n",
      "    # Check the case api_endpoint is not provided and GOOGLE_API_USE_MTLS_ENDPOINT is\n",
      "    # \"always\".\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_MTLS_ENDPOINT\": \"always\"}):\n",
      "        with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "            patched.return_value = None\n",
      "            client = client_class(transport=transport_name)\n",
      "            patched.assert_called_once_with(\n",
      "                credentials=None,\n",
      "                credentials_file=None,\n",
      "                host=client.DEFAULT_MTLS_ENDPOINT,\n",
      "                scopes=None,\n",
      "                client_cert_source_for_mtls=None,\n",
      "                quota_project_id=None,\n",
      "                client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "                always_use_jwt_access=True,\n",
      "            )\n",
      "\n",
      "    # Check the case api_endpoint is not provided and GOOGLE_API_USE_MTLS_ENDPOINT has\n",
      "    # unsupported value.\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_MTLS_ENDPOINT\": \"Unsupported\"}):\n",
      "        with pytest.raises(MutualTLSChannelError):\n",
      "            client = client_class(transport=transport_name)\n",
      "\n",
      "    # Check the case GOOGLE_API_USE_CLIENT_CERTIFICATE has unsupported value.\n",
      "    with mock.patch.dict(\n",
      "        os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": \"Unsupported\"}\n",
      "    ):\n",
      "        with pytest.raises(ValueError):\n",
      "            client = client_class(transport=transport_name)\n",
      "\n",
      "    # Check the case quota_project_id is provided\n",
      "    options = client_options.ClientOptions(quota_project_id=\"octopus\")\n",
      "    with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "        patched.return_value = None\n",
      "        client = client_class(client_options=options, transport=transport_name)\n",
      "        patched.assert_called_once_with(\n",
      "            credentials=None,\n",
      "            credentials_file=None,\n",
      "            host=client.DEFAULT_ENDPOINT,\n",
      "            scopes=None,\n",
      "            client_cert_source_for_mtls=None,\n",
      "            quota_project_id=\"octopus\",\n",
      "            client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "            always_use_jwt_access=True,\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"client_class,transport_class,transport_name,use_client_cert_env\",\n",
      "    [\n",
      "        (CloudDeployClient, transports.CloudDeployGrpcTransport, \"grpc\", \"true\"),\n",
      "        (\n",
      "            CloudDeployAsyncClient,\n",
      "            transports.CloudDeployGrpcAsyncIOTransport,\n",
      "            \"grpc_asyncio\",\n",
      "            \"true\",\n",
      "        ),\n",
      "        (CloudDeployClient, transports.CloudDeployGrpcTransport, \"grpc\", \"false\"),\n",
      "        (\n",
      "            CloudDeployAsyncClient,\n",
      "            transports.CloudDeployGrpcAsyncIOTransport,\n",
      "            \"grpc_asyncio\",\n",
      "            \"false\",\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "@mock.patch.object(\n",
      "    CloudDeployClient, \"DEFAULT_ENDPOINT\", modify_default_endpoint(CloudDeployClient)\n",
      ")\n",
      "@mock.patch.object(\n",
      "    CloudDeployAsyncClient,\n",
      "    \"DEFAULT_ENDPOINT\",\n",
      "    modify_default_endpoint(CloudDeployAsyncClient),\n",
      ")\n",
      "@mock.patch.dict(os.environ, {\"GOOGLE_API_USE_MTLS_ENDPOINT\": \"auto\"})\n",
      "def test_cloud_deploy_client_mtls_env_auto(\n",
      "    client_class, transport_class, transport_name, use_client_cert_env\n",
      "):\n",
      "    # This tests the endpoint autoswitch behavior. Endpoint is autoswitched to the default\n",
      "    # mtls endpoint, if GOOGLE_API_USE_CLIENT_CERTIFICATE is \"true\" and client cert exists.\n",
      "\n",
      "    # Check the case client_cert_source is provided. Whether client cert is used depends on\n",
      "    # GOOGLE_API_USE_CLIENT_CERTIFICATE value.\n",
      "    with mock.patch.dict(\n",
      "        os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": use_client_cert_env}\n",
      "    ):\n",
      "        options = client_options.ClientOptions(\n",
      "            client_cert_source=client_cert_source_callback\n",
      "        )\n",
      "        with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "            patched.return_value = None\n",
      "            client = client_class(client_options=options, transport=transport_name)\n",
      "\n",
      "            if use_client_cert_env == \"false\":\n",
      "                expected_client_cert_source = None\n",
      "                expected_host = client.DEFAULT_ENDPOINT\n",
      "            else:\n",
      "                expected_client_cert_source = client_cert_source_callback\n",
      "                expected_host = client.DEFAULT_MTLS_ENDPOINT\n",
      "\n",
      "            patched.assert_called_once_with(\n",
      "                credentials=None,\n",
      "                credentials_file=None,\n",
      "                host=expected_host,\n",
      "                scopes=None,\n",
      "                client_cert_source_for_mtls=expected_client_cert_source,\n",
      "                quota_project_id=None,\n",
      "                client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "                always_use_jwt_access=True,\n",
      "            )\n",
      "\n",
      "    # Check the case ADC client cert is provided. Whether client cert is used depends on\n",
      "    # GOOGLE_API_USE_CLIENT_CERTIFICATE value.\n",
      "    with mock.patch.dict(\n",
      "        os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": use_client_cert_env}\n",
      "    ):\n",
      "        with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "            with mock.patch(\n",
      "                \"google.auth.transport.mtls.has_default_client_cert_source\",\n",
      "                return_value=True,\n",
      "            ):\n",
      "                with mock.patch(\n",
      "                    \"google.auth.transport.mtls.default_client_cert_source\",\n",
      "                    return_value=client_cert_source_callback,\n",
      "                ):\n",
      "                    if use_client_cert_env == \"false\":\n",
      "                        expected_host = client.DEFAULT_ENDPOINT\n",
      "                        expected_client_cert_source = None\n",
      "                    else:\n",
      "                        expected_host = client.DEFAULT_MTLS_ENDPOINT\n",
      "                        expected_client_cert_source = client_cert_source_callback\n",
      "\n",
      "                    patched.return_value = None\n",
      "                    client = client_class(transport=transport_name)\n",
      "                    patched.assert_called_once_with(\n",
      "                        credentials=None,\n",
      "                        credentials_file=None,\n",
      "                        host=expected_host,\n",
      "                        scopes=None,\n",
      "                        client_cert_source_for_mtls=expected_client_cert_source,\n",
      "                        quota_project_id=None,\n",
      "                        client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "                        always_use_jwt_access=True,\n",
      "                    )\n",
      "\n",
      "    # Check the case client_cert_source and ADC client cert are not provided.\n",
      "    with mock.patch.dict(\n",
      "        os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": use_client_cert_env}\n",
      "    ):\n",
      "        with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "            with mock.patch(\n",
      "                \"google.auth.transport.mtls.has_default_client_cert_source\",\n",
      "                return_value=False,\n",
      "            ):\n",
      "                patched.return_value = None\n",
      "                client = client_class(transport=transport_name)\n",
      "                patched.assert_called_once_with(\n",
      "                    credentials=None,\n",
      "                    credentials_file=None,\n",
      "                    host=client.DEFAULT_ENDPOINT,\n",
      "                    scopes=None,\n",
      "                    client_cert_source_for_mtls=None,\n",
      "                    quota_project_id=None,\n",
      "                    client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "                    always_use_jwt_access=True,\n",
      "                )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"client_class\", [CloudDeployClient, CloudDeployAsyncClient])\n",
      "@mock.patch.object(\n",
      "    CloudDeployClient, \"DEFAULT_ENDPOINT\", modify_default_endpoint(CloudDeployClient)\n",
      ")\n",
      "@mock.patch.object(\n",
      "    CloudDeployAsyncClient,\n",
      "    \"DEFAULT_ENDPOINT\",\n",
      "    modify_default_endpoint(CloudDeployAsyncClient),\n",
      ")\n",
      "def test_cloud_deploy_client_get_mtls_endpoint_and_cert_source(client_class):\n",
      "    mock_client_cert_source = mock.Mock()\n",
      "\n",
      "    # Test the case GOOGLE_API_USE_CLIENT_CERTIFICATE is \"true\".\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": \"true\"}):\n",
      "        mock_api_endpoint = \"foo\"\n",
      "        options = client_options.ClientOptions(\n",
      "            client_cert_source=mock_client_cert_source, api_endpoint=mock_api_endpoint\n",
      "        )\n",
      "        api_endpoint, cert_source = client_class.get_mtls_endpoint_and_cert_source(\n",
      "            options\n",
      "        )\n",
      "        assert api_endpoint == mock_api_endpoint\n",
      "        assert cert_source == mock_client_cert_source\n",
      "\n",
      "    # Test the case GOOGLE_API_USE_CLIENT_CERTIFICATE is \"false\".\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": \"false\"}):\n",
      "        mock_client_cert_source = mock.Mock()\n",
      "        mock_api_endpoint = \"foo\"\n",
      "        options = client_options.ClientOptions(\n",
      "            client_cert_source=mock_client_cert_source, api_endpoint=mock_api_endpoint\n",
      "        )\n",
      "        api_endpoint, cert_source = client_class.get_mtls_endpoint_and_cert_source(\n",
      "            options\n",
      "        )\n",
      "        assert api_endpoint == mock_api_endpoint\n",
      "        assert cert_source is None\n",
      "\n",
      "    # Test the case GOOGLE_API_USE_MTLS_ENDPOINT is \"never\".\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_MTLS_ENDPOINT\": \"never\"}):\n",
      "        api_endpoint, cert_source = client_class.get_mtls_endpoint_and_cert_source()\n",
      "        assert api_endpoint == client_class.DEFAULT_ENDPOINT\n",
      "        assert cert_source is None\n",
      "\n",
      "    # Test the case GOOGLE_API_USE_MTLS_ENDPOINT is \"always\".\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_MTLS_ENDPOINT\": \"always\"}):\n",
      "        api_endpoint, cert_source = client_class.get_mtls_endpoint_and_cert_source()\n",
      "        assert api_endpoint == client_class.DEFAULT_MTLS_ENDPOINT\n",
      "        assert cert_source is None\n",
      "\n",
      "    # Test the case GOOGLE_API_USE_MTLS_ENDPOINT is \"auto\" and default cert doesn't exist.\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": \"true\"}):\n",
      "        with mock.patch(\n",
      "            \"google.auth.transport.mtls.has_default_client_cert_source\",\n",
      "            return_value=False,\n",
      "        ):\n",
      "            api_endpoint, cert_source = client_class.get_mtls_endpoint_and_cert_source()\n",
      "            assert api_endpoint == client_class.DEFAULT_ENDPOINT\n",
      "            assert cert_source is None\n",
      "\n",
      "    # Test the case GOOGLE_API_USE_MTLS_ENDPOINT is \"auto\" and default cert exists.\n",
      "    with mock.patch.dict(os.environ, {\"GOOGLE_API_USE_CLIENT_CERTIFICATE\": \"true\"}):\n",
      "        with mock.patch(\n",
      "            \"google.auth.transport.mtls.has_default_client_cert_source\",\n",
      "            return_value=True,\n",
      "        ):\n",
      "            with mock.patch(\n",
      "                \"google.auth.transport.mtls.default_client_cert_source\",\n",
      "                return_value=mock_client_cert_source,\n",
      "            ):\n",
      "                (\n",
      "                    api_endpoint,\n",
      "                    cert_source,\n",
      "                ) = client_class.get_mtls_endpoint_and_cert_source()\n",
      "                assert api_endpoint == client_class.DEFAULT_MTLS_ENDPOINT\n",
      "                assert cert_source == mock_client_cert_source\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"client_class,transport_class,transport_name\",\n",
      "    [\n",
      "        (CloudDeployClient, transports.CloudDeployGrpcTransport, \"grpc\"),\n",
      "        (\n",
      "            CloudDeployAsyncClient,\n",
      "            transports.CloudDeployGrpcAsyncIOTransport,\n",
      "            \"grpc_asyncio\",\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "def test_cloud_deploy_client_client_options_scopes(\n",
      "    client_class, transport_class, transport_name\n",
      "):\n",
      "    # Check the case scopes are provided.\n",
      "    options = client_options.ClientOptions(scopes=[\"1\", \"2\"],)\n",
      "    with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "        patched.return_value = None\n",
      "        client = client_class(client_options=options, transport=transport_name)\n",
      "        patched.assert_called_once_with(\n",
      "            credentials=None,\n",
      "            credentials_file=None,\n",
      "            host=client.DEFAULT_ENDPOINT,\n",
      "            scopes=[\"1\", \"2\"],\n",
      "            client_cert_source_for_mtls=None,\n",
      "            quota_project_id=None,\n",
      "            client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "            always_use_jwt_access=True,\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"client_class,transport_class,transport_name\",\n",
      "    [\n",
      "        (CloudDeployClient, transports.CloudDeployGrpcTransport, \"grpc\"),\n",
      "        (\n",
      "            CloudDeployAsyncClient,\n",
      "            transports.CloudDeployGrpcAsyncIOTransport,\n",
      "            \"grpc_asyncio\",\n",
      "        ),\n",
      "    ],\n",
      ")\n",
      "def test_cloud_deploy_client_client_options_credentials_file(\n",
      "    client_class, transport_class, transport_name\n",
      "):\n",
      "    # Check the case credentials file is provided.\n",
      "    options = client_options.ClientOptions(credentials_file=\"credentials.json\")\n",
      "    with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "        patched.return_value = None\n",
      "        client = client_class(client_options=options, transport=transport_name)\n",
      "        patched.assert_called_once_with(\n",
      "            credentials=None,\n",
      "            credentials_file=\"credentials.json\",\n",
      "            host=client.DEFAULT_ENDPOINT,\n",
      "            scopes=None,\n",
      "            client_cert_source_for_mtls=None,\n",
      "            quota_project_id=None,\n",
      "            client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "            always_use_jwt_access=True,\n",
      "        )\n",
      "\n",
      "\n",
      "def test_cloud_deploy_client_client_options_from_dict():\n",
      "    with mock.patch(\n",
      "        \"google.cloud.deploy_v1.services.cloud_deploy.transports.CloudDeployGrpcTransport.__init__\"\n",
      "    ) as grpc_transport:\n",
      "        grpc_transport.return_value = None\n",
      "        client = CloudDeployClient(client_options={\"api_endpoint\": \"squid.clam.whelk\"})\n",
      "        grpc_transport.assert_called_once_with(\n",
      "            credentials=None,\n",
      "            credentials_file=None,\n",
      "            host=\"squid.clam.whelk\",\n",
      "            scopes=None,\n",
      "            client_cert_source_for_mtls=None,\n",
      "            quota_project_id=None,\n",
      "            client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "            always_use_jwt_access=True,\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"request_type\", [cloud_deploy.ListDeliveryPipelinesRequest, dict,]\n",
      ")\n",
      "def test_list_delivery_pipelines(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "            next_page_token=\"next_page_token_value\", unreachable=[\"unreachable_value\"],\n",
      "        )\n",
      "        response = client.list_delivery_pipelines(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListDeliveryPipelinesRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListDeliveryPipelinesPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "def test_list_delivery_pipelines_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        client.list_delivery_pipelines()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListDeliveryPipelinesRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_delivery_pipelines_async(\n",
      "    transport: str = \"grpc_asyncio\",\n",
      "    request_type=cloud_deploy.ListDeliveryPipelinesRequest,\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                next_page_token=\"next_page_token_value\",\n",
      "                unreachable=[\"unreachable_value\"],\n",
      "            )\n",
      "        )\n",
      "        response = await client.list_delivery_pipelines(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListDeliveryPipelinesRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListDeliveryPipelinesAsyncPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_delivery_pipelines_async_from_dict():\n",
      "    await test_list_delivery_pipelines_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_list_delivery_pipelines_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListDeliveryPipelinesRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = cloud_deploy.ListDeliveryPipelinesResponse()\n",
      "        client.list_delivery_pipelines(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_delivery_pipelines_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListDeliveryPipelinesRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse()\n",
      "        )\n",
      "        await client.list_delivery_pipelines(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_list_delivery_pipelines_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListDeliveryPipelinesResponse()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.list_delivery_pipelines(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_list_delivery_pipelines_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.list_delivery_pipelines(\n",
      "            cloud_deploy.ListDeliveryPipelinesRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_delivery_pipelines_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListDeliveryPipelinesResponse()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.list_delivery_pipelines(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_delivery_pipelines_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.list_delivery_pipelines(\n",
      "            cloud_deploy.ListDeliveryPipelinesRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "def test_list_delivery_pipelines_pager(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[], next_page_token=\"def\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[cloud_deploy.DeliveryPipeline(),],\n",
      "                next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "\n",
      "        metadata = ()\n",
      "        metadata = tuple(metadata) + (\n",
      "            gapic_v1.routing_header.to_grpc_metadata(((\"parent\", \"\"),)),\n",
      "        )\n",
      "        pager = client.list_delivery_pipelines(request={})\n",
      "\n",
      "        assert pager._metadata == metadata\n",
      "\n",
      "        results = [i for i in pager]\n",
      "        assert len(results) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.DeliveryPipeline) for i in results)\n",
      "\n",
      "\n",
      "def test_list_delivery_pipelines_pages(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines), \"__call__\"\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[], next_page_token=\"def\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[cloud_deploy.DeliveryPipeline(),],\n",
      "                next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = list(client.list_delivery_pipelines(request={}).pages)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_delivery_pipelines_async_pager():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines),\n",
      "        \"__call__\",\n",
      "        new_callable=mock.AsyncMock,\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[], next_page_token=\"def\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[cloud_deploy.DeliveryPipeline(),],\n",
      "                next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        async_pager = await client.list_delivery_pipelines(request={},)\n",
      "        assert async_pager.next_page_token == \"abc\"\n",
      "        responses = []\n",
      "        async for response in async_pager:\n",
      "            responses.append(response)\n",
      "\n",
      "        assert len(responses) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.DeliveryPipeline) for i in responses)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_delivery_pipelines_async_pages():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_delivery_pipelines),\n",
      "        \"__call__\",\n",
      "        new_callable=mock.AsyncMock,\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[], next_page_token=\"def\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[cloud_deploy.DeliveryPipeline(),],\n",
      "                next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListDeliveryPipelinesResponse(\n",
      "                delivery_pipelines=[\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                    cloud_deploy.DeliveryPipeline(),\n",
      "                ],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = []\n",
      "        async for page_ in (await client.list_delivery_pipelines(request={})).pages:\n",
      "            pages.append(page_)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"request_type\", [cloud_deploy.GetDeliveryPipelineRequest, dict,]\n",
      ")\n",
      "def test_get_delivery_pipeline(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.get_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.DeliveryPipeline(\n",
      "            name=\"name_value\",\n",
      "            uid=\"uid_value\",\n",
      "            description=\"description_value\",\n",
      "            etag=\"etag_value\",\n",
      "            serial_pipeline=cloud_deploy.SerialPipeline(\n",
      "                stages=[cloud_deploy.Stage(target_id=\"target_id_value\")]\n",
      "            ),\n",
      "        )\n",
      "        response = client.get_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.DeliveryPipeline)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.etag == \"etag_value\"\n",
      "\n",
      "\n",
      "def test_get_delivery_pipeline_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.get_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        client.get_delivery_pipeline()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetDeliveryPipelineRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_delivery_pipeline_async(\n",
      "    transport: str = \"grpc_asyncio\",\n",
      "    request_type=cloud_deploy.GetDeliveryPipelineRequest,\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.get_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.DeliveryPipeline(\n",
      "                name=\"name_value\",\n",
      "                uid=\"uid_value\",\n",
      "                description=\"description_value\",\n",
      "                etag=\"etag_value\",\n",
      "            )\n",
      "        )\n",
      "        response = await client.get_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.DeliveryPipeline)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.etag == \"etag_value\"\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_delivery_pipeline_async_from_dict():\n",
      "    await test_get_delivery_pipeline_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_get_delivery_pipeline_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetDeliveryPipelineRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.get_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = cloud_deploy.DeliveryPipeline()\n",
      "        client.get_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_delivery_pipeline_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetDeliveryPipelineRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.get_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.DeliveryPipeline()\n",
      "        )\n",
      "        await client.get_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_get_delivery_pipeline_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.get_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.DeliveryPipeline()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.get_delivery_pipeline(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_get_delivery_pipeline_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.get_delivery_pipeline(\n",
      "            cloud_deploy.GetDeliveryPipelineRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_delivery_pipeline_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.get_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.DeliveryPipeline()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.DeliveryPipeline()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.get_delivery_pipeline(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_delivery_pipeline_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.get_delivery_pipeline(\n",
      "            cloud_deploy.GetDeliveryPipelineRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"request_type\", [cloud_deploy.CreateDeliveryPipelineRequest, dict,]\n",
      ")\n",
      "def test_create_delivery_pipeline(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.create_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.create_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_create_delivery_pipeline_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.create_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        client.create_delivery_pipeline()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateDeliveryPipelineRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_delivery_pipeline_async(\n",
      "    transport: str = \"grpc_asyncio\",\n",
      "    request_type=cloud_deploy.CreateDeliveryPipelineRequest,\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.create_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.create_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_delivery_pipeline_async_from_dict():\n",
      "    await test_create_delivery_pipeline_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_create_delivery_pipeline_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateDeliveryPipelineRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.create_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.create_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_delivery_pipeline_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateDeliveryPipelineRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.create_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.create_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_create_delivery_pipeline_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.create_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.create_delivery_pipeline(\n",
      "            parent=\"parent_value\",\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            delivery_pipeline_id=\"delivery_pipeline_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].delivery_pipeline\n",
      "        mock_val = cloud_deploy.DeliveryPipeline(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].delivery_pipeline_id\n",
      "        mock_val = \"delivery_pipeline_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_create_delivery_pipeline_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.create_delivery_pipeline(\n",
      "            cloud_deploy.CreateDeliveryPipelineRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            delivery_pipeline_id=\"delivery_pipeline_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_delivery_pipeline_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.create_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.create_delivery_pipeline(\n",
      "            parent=\"parent_value\",\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            delivery_pipeline_id=\"delivery_pipeline_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].delivery_pipeline\n",
      "        mock_val = cloud_deploy.DeliveryPipeline(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].delivery_pipeline_id\n",
      "        mock_val = \"delivery_pipeline_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_delivery_pipeline_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.create_delivery_pipeline(\n",
      "            cloud_deploy.CreateDeliveryPipelineRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            delivery_pipeline_id=\"delivery_pipeline_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"request_type\", [cloud_deploy.UpdateDeliveryPipelineRequest, dict,]\n",
      ")\n",
      "def test_update_delivery_pipeline(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.update_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.update_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.UpdateDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_update_delivery_pipeline_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.update_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        client.update_delivery_pipeline()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.UpdateDeliveryPipelineRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_delivery_pipeline_async(\n",
      "    transport: str = \"grpc_asyncio\",\n",
      "    request_type=cloud_deploy.UpdateDeliveryPipelineRequest,\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.update_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.update_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.UpdateDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_delivery_pipeline_async_from_dict():\n",
      "    await test_update_delivery_pipeline_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_update_delivery_pipeline_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.UpdateDeliveryPipelineRequest()\n",
      "\n",
      "    request.delivery_pipeline.name = \"delivery_pipeline.name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.update_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.update_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\n",
      "        \"x-goog-request-params\",\n",
      "        \"delivery_pipeline.name=delivery_pipeline.name/value\",\n",
      "    ) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_delivery_pipeline_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.UpdateDeliveryPipelineRequest()\n",
      "\n",
      "    request.delivery_pipeline.name = \"delivery_pipeline.name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.update_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.update_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\n",
      "        \"x-goog-request-params\",\n",
      "        \"delivery_pipeline.name=delivery_pipeline.name/value\",\n",
      "    ) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_update_delivery_pipeline_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.update_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.update_delivery_pipeline(\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].delivery_pipeline\n",
      "        mock_val = cloud_deploy.DeliveryPipeline(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].update_mask\n",
      "        mock_val = field_mask_pb2.FieldMask(paths=[\"paths_value\"])\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_update_delivery_pipeline_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.update_delivery_pipeline(\n",
      "            cloud_deploy.UpdateDeliveryPipelineRequest(),\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_delivery_pipeline_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.update_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.update_delivery_pipeline(\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].delivery_pipeline\n",
      "        mock_val = cloud_deploy.DeliveryPipeline(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].update_mask\n",
      "        mock_val = field_mask_pb2.FieldMask(paths=[\"paths_value\"])\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_delivery_pipeline_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.update_delivery_pipeline(\n",
      "            cloud_deploy.UpdateDeliveryPipelineRequest(),\n",
      "            delivery_pipeline=cloud_deploy.DeliveryPipeline(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"request_type\", [cloud_deploy.DeleteDeliveryPipelineRequest, dict,]\n",
      ")\n",
      "def test_delete_delivery_pipeline(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.delete_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.delete_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.DeleteDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_delete_delivery_pipeline_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.delete_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        client.delete_delivery_pipeline()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.DeleteDeliveryPipelineRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_delivery_pipeline_async(\n",
      "    transport: str = \"grpc_asyncio\",\n",
      "    request_type=cloud_deploy.DeleteDeliveryPipelineRequest,\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.delete_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.delete_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.DeleteDeliveryPipelineRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_delivery_pipeline_async_from_dict():\n",
      "    await test_delete_delivery_pipeline_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_delete_delivery_pipeline_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.DeleteDeliveryPipelineRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.delete_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.delete_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_delivery_pipeline_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.DeleteDeliveryPipelineRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.delete_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.delete_delivery_pipeline(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_delete_delivery_pipeline_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.delete_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.delete_delivery_pipeline(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_delete_delivery_pipeline_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.delete_delivery_pipeline(\n",
      "            cloud_deploy.DeleteDeliveryPipelineRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_delivery_pipeline_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.delete_delivery_pipeline), \"__call__\"\n",
      "    ) as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.delete_delivery_pipeline(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_delivery_pipeline_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.delete_delivery_pipeline(\n",
      "            cloud_deploy.DeleteDeliveryPipelineRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.ListTargetsRequest, dict,])\n",
      "def test_list_targets(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListTargetsResponse(\n",
      "            next_page_token=\"next_page_token_value\", unreachable=[\"unreachable_value\"],\n",
      "        )\n",
      "        response = client.list_targets(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListTargetsRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListTargetsPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "def test_list_targets_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        client.list_targets()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListTargetsRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_targets_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.ListTargetsRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                next_page_token=\"next_page_token_value\",\n",
      "                unreachable=[\"unreachable_value\"],\n",
      "            )\n",
      "        )\n",
      "        response = await client.list_targets(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListTargetsRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListTargetsAsyncPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_targets_async_from_dict():\n",
      "    await test_list_targets_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_list_targets_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListTargetsRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.ListTargetsResponse()\n",
      "        client.list_targets(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_targets_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListTargetsRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListTargetsResponse()\n",
      "        )\n",
      "        await client.list_targets(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_list_targets_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListTargetsResponse()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.list_targets(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_list_targets_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.list_targets(\n",
      "            cloud_deploy.ListTargetsRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_targets_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListTargetsResponse()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListTargetsResponse()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.list_targets(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_targets_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.list_targets(\n",
      "            cloud_deploy.ListTargetsRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "def test_list_targets_pager(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(targets=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(), cloud_deploy.Target(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "\n",
      "        metadata = ()\n",
      "        metadata = tuple(metadata) + (\n",
      "            gapic_v1.routing_header.to_grpc_metadata(((\"parent\", \"\"),)),\n",
      "        )\n",
      "        pager = client.list_targets(request={})\n",
      "\n",
      "        assert pager._metadata == metadata\n",
      "\n",
      "        results = [i for i in pager]\n",
      "        assert len(results) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.Target) for i in results)\n",
      "\n",
      "\n",
      "def test_list_targets_pages(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_targets), \"__call__\") as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(targets=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(), cloud_deploy.Target(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = list(client.list_targets(request={}).pages)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_targets_async_pager():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_targets), \"__call__\", new_callable=mock.AsyncMock\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(targets=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(), cloud_deploy.Target(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        async_pager = await client.list_targets(request={},)\n",
      "        assert async_pager.next_page_token == \"abc\"\n",
      "        responses = []\n",
      "        async for response in async_pager:\n",
      "            responses.append(response)\n",
      "\n",
      "        assert len(responses) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.Target) for i in responses)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_targets_async_pages():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_targets), \"__call__\", new_callable=mock.AsyncMock\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                    cloud_deploy.Target(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(targets=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListTargetsResponse(\n",
      "                targets=[cloud_deploy.Target(), cloud_deploy.Target(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = []\n",
      "        async for page_ in (await client.list_targets(request={})).pages:\n",
      "            pages.append(page_)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.GetTargetRequest, dict,])\n",
      "def test_get_target(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Target(\n",
      "            name=\"name_value\",\n",
      "            target_id=\"target_id_value\",\n",
      "            uid=\"uid_value\",\n",
      "            description=\"description_value\",\n",
      "            require_approval=True,\n",
      "            etag=\"etag_value\",\n",
      "            gke=cloud_deploy.GkeCluster(cluster=\"cluster_value\"),\n",
      "        )\n",
      "        response = client.get_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Target)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.target_id == \"target_id_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.require_approval is True\n",
      "    assert response.etag == \"etag_value\"\n",
      "\n",
      "\n",
      "def test_get_target_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_target), \"__call__\") as call:\n",
      "        client.get_target()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetTargetRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_target_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.GetTargetRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Target(\n",
      "                name=\"name_value\",\n",
      "                target_id=\"target_id_value\",\n",
      "                uid=\"uid_value\",\n",
      "                description=\"description_value\",\n",
      "                require_approval=True,\n",
      "                etag=\"etag_value\",\n",
      "            )\n",
      "        )\n",
      "        response = await client.get_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Target)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.target_id == \"target_id_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.require_approval is True\n",
      "    assert response.etag == \"etag_value\"\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_target_async_from_dict():\n",
      "    await test_get_target_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_get_target_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetTargetRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_target), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.Target()\n",
      "        client.get_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_target_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetTargetRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_target), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(cloud_deploy.Target())\n",
      "        await client.get_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_get_target_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Target()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.get_target(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_get_target_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.get_target(\n",
      "            cloud_deploy.GetTargetRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_target_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Target()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(cloud_deploy.Target())\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.get_target(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_target_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.get_target(\n",
      "            cloud_deploy.GetTargetRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.CreateTargetRequest, dict,])\n",
      "def test_create_target(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.create_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_create_target_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_target), \"__call__\") as call:\n",
      "        client.create_target()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateTargetRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_target_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.CreateTargetRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.create_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_target_async_from_dict():\n",
      "    await test_create_target_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_create_target_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateTargetRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_target), \"__call__\") as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.create_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_target_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateTargetRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_target), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.create_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_create_target_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.create_target(\n",
      "            parent=\"parent_value\",\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            target_id=\"target_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].target\n",
      "        mock_val = cloud_deploy.Target(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].target_id\n",
      "        mock_val = \"target_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_create_target_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.create_target(\n",
      "            cloud_deploy.CreateTargetRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            target_id=\"target_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_target_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.create_target(\n",
      "            parent=\"parent_value\",\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            target_id=\"target_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].target\n",
      "        mock_val = cloud_deploy.Target(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].target_id\n",
      "        mock_val = \"target_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_target_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.create_target(\n",
      "            cloud_deploy.CreateTargetRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            target_id=\"target_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.UpdateTargetRequest, dict,])\n",
      "def test_update_target(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.update_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.update_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.UpdateTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_update_target_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.update_target), \"__call__\") as call:\n",
      "        client.update_target()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.UpdateTargetRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_target_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.UpdateTargetRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.update_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.update_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.UpdateTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_target_async_from_dict():\n",
      "    await test_update_target_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_update_target_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.UpdateTargetRequest()\n",
      "\n",
      "    request.target.name = \"target.name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.update_target), \"__call__\") as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.update_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"target.name=target.name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_target_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.UpdateTargetRequest()\n",
      "\n",
      "    request.target.name = \"target.name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.update_target), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.update_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"target.name=target.name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_update_target_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.update_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.update_target(\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].target\n",
      "        mock_val = cloud_deploy.Target(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].update_mask\n",
      "        mock_val = field_mask_pb2.FieldMask(paths=[\"paths_value\"])\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_update_target_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.update_target(\n",
      "            cloud_deploy.UpdateTargetRequest(),\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_target_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.update_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.update_target(\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].target\n",
      "        mock_val = cloud_deploy.Target(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].update_mask\n",
      "        mock_val = field_mask_pb2.FieldMask(paths=[\"paths_value\"])\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_update_target_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.update_target(\n",
      "            cloud_deploy.UpdateTargetRequest(),\n",
      "            target=cloud_deploy.Target(name=\"name_value\"),\n",
      "            update_mask=field_mask_pb2.FieldMask(paths=[\"paths_value\"]),\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.DeleteTargetRequest, dict,])\n",
      "def test_delete_target(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.delete_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.delete_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.DeleteTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_delete_target_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.delete_target), \"__call__\") as call:\n",
      "        client.delete_target()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.DeleteTargetRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_target_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.DeleteTargetRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.delete_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.delete_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.DeleteTargetRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_target_async_from_dict():\n",
      "    await test_delete_target_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_delete_target_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.DeleteTargetRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.delete_target), \"__call__\") as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.delete_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_target_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.DeleteTargetRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.delete_target), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.delete_target(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_delete_target_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.delete_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.delete_target(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_delete_target_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.delete_target(\n",
      "            cloud_deploy.DeleteTargetRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_target_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.delete_target), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.delete_target(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_delete_target_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.delete_target(\n",
      "            cloud_deploy.DeleteTargetRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.ListReleasesRequest, dict,])\n",
      "def test_list_releases(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListReleasesResponse(\n",
      "            next_page_token=\"next_page_token_value\", unreachable=[\"unreachable_value\"],\n",
      "        )\n",
      "        response = client.list_releases(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListReleasesRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListReleasesPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "def test_list_releases_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        client.list_releases()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListReleasesRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_releases_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.ListReleasesRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                next_page_token=\"next_page_token_value\",\n",
      "                unreachable=[\"unreachable_value\"],\n",
      "            )\n",
      "        )\n",
      "        response = await client.list_releases(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListReleasesRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListReleasesAsyncPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_releases_async_from_dict():\n",
      "    await test_list_releases_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_list_releases_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListReleasesRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.ListReleasesResponse()\n",
      "        client.list_releases(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_releases_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListReleasesRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListReleasesResponse()\n",
      "        )\n",
      "        await client.list_releases(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_list_releases_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListReleasesResponse()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.list_releases(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_list_releases_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.list_releases(\n",
      "            cloud_deploy.ListReleasesRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_releases_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListReleasesResponse()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListReleasesResponse()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.list_releases(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_releases_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.list_releases(\n",
      "            cloud_deploy.ListReleasesRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "def test_list_releases_pager(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(releases=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(), cloud_deploy.Release(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "\n",
      "        metadata = ()\n",
      "        metadata = tuple(metadata) + (\n",
      "            gapic_v1.routing_header.to_grpc_metadata(((\"parent\", \"\"),)),\n",
      "        )\n",
      "        pager = client.list_releases(request={})\n",
      "\n",
      "        assert pager._metadata == metadata\n",
      "\n",
      "        results = [i for i in pager]\n",
      "        assert len(results) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.Release) for i in results)\n",
      "\n",
      "\n",
      "def test_list_releases_pages(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_releases), \"__call__\") as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(releases=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(), cloud_deploy.Release(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = list(client.list_releases(request={}).pages)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_releases_async_pager():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_releases), \"__call__\", new_callable=mock.AsyncMock\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(releases=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(), cloud_deploy.Release(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        async_pager = await client.list_releases(request={},)\n",
      "        assert async_pager.next_page_token == \"abc\"\n",
      "        responses = []\n",
      "        async for response in async_pager:\n",
      "            responses.append(response)\n",
      "\n",
      "        assert len(responses) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.Release) for i in responses)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_releases_async_pages():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_releases), \"__call__\", new_callable=mock.AsyncMock\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                    cloud_deploy.Release(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(releases=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListReleasesResponse(\n",
      "                releases=[cloud_deploy.Release(), cloud_deploy.Release(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = []\n",
      "        async for page_ in (await client.list_releases(request={})).pages:\n",
      "            pages.append(page_)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.GetReleaseRequest, dict,])\n",
      "def test_get_release(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Release(\n",
      "            name=\"name_value\",\n",
      "            uid=\"uid_value\",\n",
      "            description=\"description_value\",\n",
      "            skaffold_config_uri=\"skaffold_config_uri_value\",\n",
      "            skaffold_config_path=\"skaffold_config_path_value\",\n",
      "            render_state=cloud_deploy.Release.RenderState.SUCCEEDED,\n",
      "            etag=\"etag_value\",\n",
      "            skaffold_version=\"skaffold_version_value\",\n",
      "        )\n",
      "        response = client.get_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetReleaseRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Release)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.skaffold_config_uri == \"skaffold_config_uri_value\"\n",
      "    assert response.skaffold_config_path == \"skaffold_config_path_value\"\n",
      "    assert response.render_state == cloud_deploy.Release.RenderState.SUCCEEDED\n",
      "    assert response.etag == \"etag_value\"\n",
      "    assert response.skaffold_version == \"skaffold_version_value\"\n",
      "\n",
      "\n",
      "def test_get_release_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_release), \"__call__\") as call:\n",
      "        client.get_release()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetReleaseRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_release_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.GetReleaseRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Release(\n",
      "                name=\"name_value\",\n",
      "                uid=\"uid_value\",\n",
      "                description=\"description_value\",\n",
      "                skaffold_config_uri=\"skaffold_config_uri_value\",\n",
      "                skaffold_config_path=\"skaffold_config_path_value\",\n",
      "                render_state=cloud_deploy.Release.RenderState.SUCCEEDED,\n",
      "                etag=\"etag_value\",\n",
      "                skaffold_version=\"skaffold_version_value\",\n",
      "            )\n",
      "        )\n",
      "        response = await client.get_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetReleaseRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Release)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.skaffold_config_uri == \"skaffold_config_uri_value\"\n",
      "    assert response.skaffold_config_path == \"skaffold_config_path_value\"\n",
      "    assert response.render_state == cloud_deploy.Release.RenderState.SUCCEEDED\n",
      "    assert response.etag == \"etag_value\"\n",
      "    assert response.skaffold_version == \"skaffold_version_value\"\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_release_async_from_dict():\n",
      "    await test_get_release_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_get_release_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetReleaseRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_release), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.Release()\n",
      "        client.get_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_release_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetReleaseRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_release), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Release()\n",
      "        )\n",
      "        await client.get_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_get_release_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Release()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.get_release(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_get_release_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.get_release(\n",
      "            cloud_deploy.GetReleaseRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_release_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Release()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Release()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.get_release(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_release_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.get_release(\n",
      "            cloud_deploy.GetReleaseRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.CreateReleaseRequest, dict,])\n",
      "def test_create_release(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.create_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateReleaseRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_create_release_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_release), \"__call__\") as call:\n",
      "        client.create_release()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateReleaseRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_release_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.CreateReleaseRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.create_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateReleaseRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_release_async_from_dict():\n",
      "    await test_create_release_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_create_release_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateReleaseRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_release), \"__call__\") as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.create_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_release_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateReleaseRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_release), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.create_release(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_create_release_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.create_release(\n",
      "            parent=\"parent_value\",\n",
      "            release=cloud_deploy.Release(name=\"name_value\"),\n",
      "            release_id=\"release_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].release\n",
      "        mock_val = cloud_deploy.Release(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].release_id\n",
      "        mock_val = \"release_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_create_release_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.create_release(\n",
      "            cloud_deploy.CreateReleaseRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            release=cloud_deploy.Release(name=\"name_value\"),\n",
      "            release_id=\"release_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_release_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_release), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.create_release(\n",
      "            parent=\"parent_value\",\n",
      "            release=cloud_deploy.Release(name=\"name_value\"),\n",
      "            release_id=\"release_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].release\n",
      "        mock_val = cloud_deploy.Release(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].release_id\n",
      "        mock_val = \"release_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_release_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.create_release(\n",
      "            cloud_deploy.CreateReleaseRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            release=cloud_deploy.Release(name=\"name_value\"),\n",
      "            release_id=\"release_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.ApproveRolloutRequest, dict,])\n",
      "def test_approve_rollout(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.approve_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ApproveRolloutResponse()\n",
      "        response = client.approve_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ApproveRolloutRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.ApproveRolloutResponse)\n",
      "\n",
      "\n",
      "def test_approve_rollout_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.approve_rollout), \"__call__\") as call:\n",
      "        client.approve_rollout()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ApproveRolloutRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_approve_rollout_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.ApproveRolloutRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.approve_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ApproveRolloutResponse()\n",
      "        )\n",
      "        response = await client.approve_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ApproveRolloutRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.ApproveRolloutResponse)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_approve_rollout_async_from_dict():\n",
      "    await test_approve_rollout_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_approve_rollout_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ApproveRolloutRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.approve_rollout), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.ApproveRolloutResponse()\n",
      "        client.approve_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_approve_rollout_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ApproveRolloutRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.approve_rollout), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ApproveRolloutResponse()\n",
      "        )\n",
      "        await client.approve_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_approve_rollout_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.approve_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ApproveRolloutResponse()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.approve_rollout(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_approve_rollout_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.approve_rollout(\n",
      "            cloud_deploy.ApproveRolloutRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_approve_rollout_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.approve_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ApproveRolloutResponse()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ApproveRolloutResponse()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.approve_rollout(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_approve_rollout_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.approve_rollout(\n",
      "            cloud_deploy.ApproveRolloutRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.ListRolloutsRequest, dict,])\n",
      "def test_list_rollouts(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListRolloutsResponse(\n",
      "            next_page_token=\"next_page_token_value\", unreachable=[\"unreachable_value\"],\n",
      "        )\n",
      "        response = client.list_rollouts(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListRolloutsRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListRolloutsPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "def test_list_rollouts_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        client.list_rollouts()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListRolloutsRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_rollouts_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.ListRolloutsRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                next_page_token=\"next_page_token_value\",\n",
      "                unreachable=[\"unreachable_value\"],\n",
      "            )\n",
      "        )\n",
      "        response = await client.list_rollouts(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.ListRolloutsRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, pagers.ListRolloutsAsyncPager)\n",
      "    assert response.next_page_token == \"next_page_token_value\"\n",
      "    assert response.unreachable == [\"unreachable_value\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_rollouts_async_from_dict():\n",
      "    await test_list_rollouts_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_list_rollouts_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListRolloutsRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.ListRolloutsResponse()\n",
      "        client.list_rollouts(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_rollouts_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.ListRolloutsRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListRolloutsResponse()\n",
      "        )\n",
      "        await client.list_rollouts(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_list_rollouts_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListRolloutsResponse()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.list_rollouts(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_list_rollouts_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.list_rollouts(\n",
      "            cloud_deploy.ListRolloutsRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_rollouts_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.ListRolloutsResponse()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.ListRolloutsResponse()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.list_rollouts(parent=\"parent_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_rollouts_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.list_rollouts(\n",
      "            cloud_deploy.ListRolloutsRequest(), parent=\"parent_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "def test_list_rollouts_pager(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(rollouts=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(), cloud_deploy.Rollout(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "\n",
      "        metadata = ()\n",
      "        metadata = tuple(metadata) + (\n",
      "            gapic_v1.routing_header.to_grpc_metadata(((\"parent\", \"\"),)),\n",
      "        )\n",
      "        pager = client.list_rollouts(request={})\n",
      "\n",
      "        assert pager._metadata == metadata\n",
      "\n",
      "        results = [i for i in pager]\n",
      "        assert len(results) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.Rollout) for i in results)\n",
      "\n",
      "\n",
      "def test_list_rollouts_pages(transport_name: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials, transport=transport_name,\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.list_rollouts), \"__call__\") as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(rollouts=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(), cloud_deploy.Rollout(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = list(client.list_rollouts(request={}).pages)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_rollouts_async_pager():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_rollouts), \"__call__\", new_callable=mock.AsyncMock\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(rollouts=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(), cloud_deploy.Rollout(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        async_pager = await client.list_rollouts(request={},)\n",
      "        assert async_pager.next_page_token == \"abc\"\n",
      "        responses = []\n",
      "        async for response in async_pager:\n",
      "            responses.append(response)\n",
      "\n",
      "        assert len(responses) == 6\n",
      "        assert all(isinstance(i, cloud_deploy.Rollout) for i in responses)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_list_rollouts_async_pages():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials,)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(\n",
      "        type(client.transport.list_rollouts), \"__call__\", new_callable=mock.AsyncMock\n",
      "    ) as call:\n",
      "        # Set the response to a series of pages.\n",
      "        call.side_effect = (\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                    cloud_deploy.Rollout(),\n",
      "                ],\n",
      "                next_page_token=\"abc\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(rollouts=[], next_page_token=\"def\",),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(),], next_page_token=\"ghi\",\n",
      "            ),\n",
      "            cloud_deploy.ListRolloutsResponse(\n",
      "                rollouts=[cloud_deploy.Rollout(), cloud_deploy.Rollout(),],\n",
      "            ),\n",
      "            RuntimeError,\n",
      "        )\n",
      "        pages = []\n",
      "        async for page_ in (await client.list_rollouts(request={})).pages:\n",
      "            pages.append(page_)\n",
      "        for page_, token in zip(pages, [\"abc\", \"def\", \"ghi\", \"\"]):\n",
      "            assert page_.raw_page.next_page_token == token\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.GetRolloutRequest, dict,])\n",
      "def test_get_rollout(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Rollout(\n",
      "            name=\"name_value\",\n",
      "            uid=\"uid_value\",\n",
      "            description=\"description_value\",\n",
      "            target_id=\"target_id_value\",\n",
      "            approval_state=cloud_deploy.Rollout.ApprovalState.NEEDS_APPROVAL,\n",
      "            state=cloud_deploy.Rollout.State.SUCCEEDED,\n",
      "            failure_reason=\"failure_reason_value\",\n",
      "            deploying_build=\"deploying_build_value\",\n",
      "            etag=\"etag_value\",\n",
      "        )\n",
      "        response = client.get_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetRolloutRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Rollout)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.target_id == \"target_id_value\"\n",
      "    assert response.approval_state == cloud_deploy.Rollout.ApprovalState.NEEDS_APPROVAL\n",
      "    assert response.state == cloud_deploy.Rollout.State.SUCCEEDED\n",
      "    assert response.failure_reason == \"failure_reason_value\"\n",
      "    assert response.deploying_build == \"deploying_build_value\"\n",
      "    assert response.etag == \"etag_value\"\n",
      "\n",
      "\n",
      "def test_get_rollout_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_rollout), \"__call__\") as call:\n",
      "        client.get_rollout()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetRolloutRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_rollout_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.GetRolloutRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Rollout(\n",
      "                name=\"name_value\",\n",
      "                uid=\"uid_value\",\n",
      "                description=\"description_value\",\n",
      "                target_id=\"target_id_value\",\n",
      "                approval_state=cloud_deploy.Rollout.ApprovalState.NEEDS_APPROVAL,\n",
      "                state=cloud_deploy.Rollout.State.SUCCEEDED,\n",
      "                failure_reason=\"failure_reason_value\",\n",
      "                deploying_build=\"deploying_build_value\",\n",
      "                etag=\"etag_value\",\n",
      "            )\n",
      "        )\n",
      "        response = await client.get_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetRolloutRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Rollout)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.uid == \"uid_value\"\n",
      "    assert response.description == \"description_value\"\n",
      "    assert response.target_id == \"target_id_value\"\n",
      "    assert response.approval_state == cloud_deploy.Rollout.ApprovalState.NEEDS_APPROVAL\n",
      "    assert response.state == cloud_deploy.Rollout.State.SUCCEEDED\n",
      "    assert response.failure_reason == \"failure_reason_value\"\n",
      "    assert response.deploying_build == \"deploying_build_value\"\n",
      "    assert response.etag == \"etag_value\"\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_rollout_async_from_dict():\n",
      "    await test_get_rollout_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_get_rollout_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetRolloutRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_rollout), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.Rollout()\n",
      "        client.get_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_rollout_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetRolloutRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_rollout), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Rollout()\n",
      "        )\n",
      "        await client.get_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_get_rollout_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Rollout()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.get_rollout(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_get_rollout_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.get_rollout(\n",
      "            cloud_deploy.GetRolloutRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_rollout_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Rollout()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Rollout()\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.get_rollout(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_rollout_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.get_rollout(\n",
      "            cloud_deploy.GetRolloutRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.CreateRolloutRequest, dict,])\n",
      "def test_create_rollout(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/spam\")\n",
      "        response = client.create_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateRolloutRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "def test_create_rollout_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_rollout), \"__call__\") as call:\n",
      "        client.create_rollout()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateRolloutRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_rollout_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.CreateRolloutRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        response = await client.create_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.CreateRolloutRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, future.Future)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_rollout_async_from_dict():\n",
      "    await test_create_rollout_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_create_rollout_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateRolloutRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_rollout), \"__call__\") as call:\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        client.create_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_rollout_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.CreateRolloutRequest()\n",
      "\n",
      "    request.parent = \"parent/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_rollout), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/op\")\n",
      "        )\n",
      "        await client.create_rollout(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"parent=parent/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_create_rollout_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.create_rollout(\n",
      "            parent=\"parent_value\",\n",
      "            rollout=cloud_deploy.Rollout(name=\"name_value\"),\n",
      "            rollout_id=\"rollout_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].rollout\n",
      "        mock_val = cloud_deploy.Rollout(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].rollout_id\n",
      "        mock_val = \"rollout_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_create_rollout_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.create_rollout(\n",
      "            cloud_deploy.CreateRolloutRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            rollout=cloud_deploy.Rollout(name=\"name_value\"),\n",
      "            rollout_id=\"rollout_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_rollout_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.create_rollout), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = operations_pb2.Operation(name=\"operations/op\")\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            operations_pb2.Operation(name=\"operations/spam\")\n",
      "        )\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.create_rollout(\n",
      "            parent=\"parent_value\",\n",
      "            rollout=cloud_deploy.Rollout(name=\"name_value\"),\n",
      "            rollout_id=\"rollout_id_value\",\n",
      "        )\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].parent\n",
      "        mock_val = \"parent_value\"\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].rollout\n",
      "        mock_val = cloud_deploy.Rollout(name=\"name_value\")\n",
      "        assert arg == mock_val\n",
      "        arg = args[0].rollout_id\n",
      "        mock_val = \"rollout_id_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_create_rollout_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.create_rollout(\n",
      "            cloud_deploy.CreateRolloutRequest(),\n",
      "            parent=\"parent_value\",\n",
      "            rollout=cloud_deploy.Rollout(name=\"name_value\"),\n",
      "            rollout_id=\"rollout_id_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"request_type\", [cloud_deploy.GetConfigRequest, dict,])\n",
      "def test_get_config(request_type, transport: str = \"grpc\"):\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_config), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Config(\n",
      "            name=\"name_value\",\n",
      "            default_skaffold_version=\"default_skaffold_version_value\",\n",
      "        )\n",
      "        response = client.get_config(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetConfigRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Config)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.default_skaffold_version == \"default_skaffold_version_value\"\n",
      "\n",
      "\n",
      "def test_get_config_empty_call():\n",
      "    # This test is a coverage failsafe to make sure that totally empty calls,\n",
      "    # i.e. request == None and no flattened fields passed, work.\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_config), \"__call__\") as call:\n",
      "        client.get_config()\n",
      "        call.assert_called()\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetConfigRequest()\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_config_async(\n",
      "    transport: str = \"grpc_asyncio\", request_type=cloud_deploy.GetConfigRequest\n",
      "):\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "    )\n",
      "\n",
      "    # Everything is optional in proto3 as far as the runtime is concerned,\n",
      "    # and we are mocking out the actual API, so just send an empty request.\n",
      "    request = request_type()\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_config), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(\n",
      "            cloud_deploy.Config(\n",
      "                name=\"name_value\",\n",
      "                default_skaffold_version=\"default_skaffold_version_value\",\n",
      "            )\n",
      "        )\n",
      "        response = await client.get_config(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == cloud_deploy.GetConfigRequest()\n",
      "\n",
      "    # Establish that the response is the type that we expect.\n",
      "    assert isinstance(response, cloud_deploy.Config)\n",
      "    assert response.name == \"name_value\"\n",
      "    assert response.default_skaffold_version == \"default_skaffold_version_value\"\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_config_async_from_dict():\n",
      "    await test_get_config_async(request_type=dict)\n",
      "\n",
      "\n",
      "def test_get_config_field_headers():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetConfigRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_config), \"__call__\") as call:\n",
      "        call.return_value = cloud_deploy.Config()\n",
      "        client.get_config(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_config_field_headers_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Any value that is part of the HTTP/1.1 URI should be sent as\n",
      "    # a field header. Set these to a non-empty value.\n",
      "    request = cloud_deploy.GetConfigRequest()\n",
      "\n",
      "    request.name = \"name/value\"\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_config), \"__call__\") as call:\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(cloud_deploy.Config())\n",
      "        await client.get_config(request)\n",
      "\n",
      "        # Establish that the underlying gRPC stub method was called.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        assert args[0] == request\n",
      "\n",
      "    # Establish that the field header was sent.\n",
      "    _, _, kw = call.mock_calls[0]\n",
      "    assert (\"x-goog-request-params\", \"name=name/value\",) in kw[\"metadata\"]\n",
      "\n",
      "\n",
      "def test_get_config_flattened():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_config), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Config()\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        client.get_config(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls) == 1\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "def test_get_config_flattened_error():\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        client.get_config(\n",
      "            cloud_deploy.GetConfigRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_config_flattened_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Mock the actual call within the gRPC stub, and fake the request.\n",
      "    with mock.patch.object(type(client.transport.get_config), \"__call__\") as call:\n",
      "        # Designate an appropriate return value for the call.\n",
      "        call.return_value = cloud_deploy.Config()\n",
      "\n",
      "        call.return_value = grpc_helpers_async.FakeUnaryUnaryCall(cloud_deploy.Config())\n",
      "        # Call the method with a truthy value for each flattened field,\n",
      "        # using the keyword arguments to the method.\n",
      "        response = await client.get_config(name=\"name_value\",)\n",
      "\n",
      "        # Establish that the underlying call was made with the expected\n",
      "        # request object values.\n",
      "        assert len(call.mock_calls)\n",
      "        _, args, _ = call.mock_calls[0]\n",
      "        arg = args[0].name\n",
      "        mock_val = \"name_value\"\n",
      "        assert arg == mock_val\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_get_config_flattened_error_async():\n",
      "    client = CloudDeployAsyncClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "\n",
      "    # Attempting to call a method with both a request object and flattened\n",
      "    # fields is an error.\n",
      "    with pytest.raises(ValueError):\n",
      "        await client.get_config(\n",
      "            cloud_deploy.GetConfigRequest(), name=\"name_value\",\n",
      "        )\n",
      "\n",
      "\n",
      "def test_credentials_transport_error():\n",
      "    # It is an error to provide credentials and a transport instance.\n",
      "    transport = transports.CloudDeployGrpcTransport(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "    )\n",
      "    with pytest.raises(ValueError):\n",
      "        client = CloudDeployClient(\n",
      "            credentials=ga_credentials.AnonymousCredentials(), transport=transport,\n",
      "        )\n",
      "\n",
      "    # It is an error to provide a credentials file and a transport instance.\n",
      "    transport = transports.CloudDeployGrpcTransport(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "    )\n",
      "    with pytest.raises(ValueError):\n",
      "        client = CloudDeployClient(\n",
      "            client_options={\"credentials_file\": \"credentials.json\"},\n",
      "            transport=transport,\n",
      "        )\n",
      "\n",
      "    # It is an error to provide an api_key and a transport instance.\n",
      "    transport = transports.CloudDeployGrpcTransport(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "    )\n",
      "    options = client_options.ClientOptions()\n",
      "    options.api_key = \"api_key\"\n",
      "    with pytest.raises(ValueError):\n",
      "        client = CloudDeployClient(client_options=options, transport=transport,)\n",
      "\n",
      "    # It is an error to provide an api_key and a credential.\n",
      "    options = mock.Mock()\n",
      "    options.api_key = \"api_key\"\n",
      "    with pytest.raises(ValueError):\n",
      "        client = CloudDeployClient(\n",
      "            client_options=options, credentials=ga_credentials.AnonymousCredentials()\n",
      "        )\n",
      "\n",
      "    # It is an error to provide scopes and a transport instance.\n",
      "    transport = transports.CloudDeployGrpcTransport(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "    )\n",
      "    with pytest.raises(ValueError):\n",
      "        client = CloudDeployClient(\n",
      "            client_options={\"scopes\": [\"1\", \"2\"]}, transport=transport,\n",
      "        )\n",
      "\n",
      "\n",
      "def test_transport_instance():\n",
      "    # A client may be instantiated with a custom transport instance.\n",
      "    transport = transports.CloudDeployGrpcTransport(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "    )\n",
      "    client = CloudDeployClient(transport=transport)\n",
      "    assert client.transport is transport\n",
      "\n",
      "\n",
      "def test_transport_get_channel():\n",
      "    # A client may be instantiated with a custom transport instance.\n",
      "    transport = transports.CloudDeployGrpcTransport(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "    )\n",
      "    channel = transport.grpc_channel\n",
      "    assert channel\n",
      "\n",
      "    transport = transports.CloudDeployGrpcAsyncIOTransport(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "    )\n",
      "    channel = transport.grpc_channel\n",
      "    assert channel\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"transport_class\",\n",
      "    [transports.CloudDeployGrpcTransport, transports.CloudDeployGrpcAsyncIOTransport,],\n",
      ")\n",
      "def test_transport_adc(transport_class):\n",
      "    # Test default credentials are used if not provided.\n",
      "    with mock.patch.object(google.auth, \"default\") as adc:\n",
      "        adc.return_value = (ga_credentials.AnonymousCredentials(), None)\n",
      "        transport_class()\n",
      "        adc.assert_called_once()\n",
      "\n",
      "\n",
      "def test_transport_grpc_default():\n",
      "    # A client should use the gRPC transport by default.\n",
      "    client = CloudDeployClient(credentials=ga_credentials.AnonymousCredentials(),)\n",
      "    assert isinstance(client.transport, transports.CloudDeployGrpcTransport,)\n",
      "\n",
      "\n",
      "def test_cloud_deploy_base_transport_error():\n",
      "    # Passing both a credentials object and credentials_file should raise an error\n",
      "    with pytest.raises(core_exceptions.DuplicateCredentialArgs):\n",
      "        transport = transports.CloudDeployTransport(\n",
      "            credentials=ga_credentials.AnonymousCredentials(),\n",
      "            credentials_file=\"credentials.json\",\n",
      "        )\n",
      "\n",
      "\n",
      "def test_cloud_deploy_base_transport():\n",
      "    # Instantiate the base transport.\n",
      "    with mock.patch(\n",
      "        \"google.cloud.deploy_v1.services.cloud_deploy.transports.CloudDeployTransport.__init__\"\n",
      "    ) as Transport:\n",
      "        Transport.return_value = None\n",
      "        transport = transports.CloudDeployTransport(\n",
      "            credentials=ga_credentials.AnonymousCredentials(),\n",
      "        )\n",
      "\n",
      "    # Every method on the transport should just blindly\n",
      "    # raise NotImplementedError.\n",
      "    methods = (\n",
      "        \"list_delivery_pipelines\",\n",
      "        \"get_delivery_pipeline\",\n",
      "        \"create_delivery_pipeline\",\n",
      "        \"update_delivery_pipeline\",\n",
      "        \"delete_delivery_pipeline\",\n",
      "        \"list_targets\",\n",
      "        \"get_target\",\n",
      "        \"create_target\",\n",
      "        \"update_target\",\n",
      "        \"delete_target\",\n",
      "        \"list_releases\",\n",
      "        \"get_release\",\n",
      "        \"create_release\",\n",
      "        \"approve_rollout\",\n",
      "        \"list_rollouts\",\n",
      "        \"get_rollout\",\n",
      "        \"create_rollout\",\n",
      "        \"get_config\",\n",
      "    )\n",
      "    for method in methods:\n",
      "        with pytest.raises(NotImplementedError):\n",
      "            getattr(transport, method)(request=object())\n",
      "\n",
      "    with pytest.raises(NotImplementedError):\n",
      "        transport.close()\n",
      "\n",
      "    # Additionally, the LRO client (a property) should\n",
      "    # also raise NotImplementedError\n",
      "    with pytest.raises(NotImplementedError):\n",
      "        transport.operations_client\n",
      "\n",
      "\n",
      "def test_cloud_deploy_base_transport_with_credentials_file():\n",
      "    # Instantiate the base transport with a credentials file\n",
      "    with mock.patch.object(\n",
      "        google.auth, \"load_credentials_from_file\", autospec=True\n",
      "    ) as load_creds, mock.patch(\n",
      "        \"google.cloud.deploy_v1.services.cloud_deploy.transports.CloudDeployTransport._prep_wrapped_messages\"\n",
      "    ) as Transport:\n",
      "        Transport.return_value = None\n",
      "        load_creds.return_value = (ga_credentials.AnonymousCredentials(), None)\n",
      "        transport = transports.CloudDeployTransport(\n",
      "            credentials_file=\"credentials.json\", quota_project_id=\"octopus\",\n",
      "        )\n",
      "        load_creds.assert_called_once_with(\n",
      "            \"credentials.json\",\n",
      "            scopes=None,\n",
      "            default_scopes=(\"https://www.googleapis.com/auth/cloud-platform\",),\n",
      "            quota_project_id=\"octopus\",\n",
      "        )\n",
      "\n",
      "\n",
      "def test_cloud_deploy_base_transport_with_adc():\n",
      "    # Test the default credentials are used if credentials and credentials_file are None.\n",
      "    with mock.patch.object(google.auth, \"default\", autospec=True) as adc, mock.patch(\n",
      "        \"google.cloud.deploy_v1.services.cloud_deploy.transports.CloudDeployTransport._prep_wrapped_messages\"\n",
      "    ) as Transport:\n",
      "        Transport.return_value = None\n",
      "        adc.return_value = (ga_credentials.AnonymousCredentials(), None)\n",
      "        transport = transports.CloudDeployTransport()\n",
      "        adc.assert_called_once()\n",
      "\n",
      "\n",
      "def test_cloud_deploy_auth_adc():\n",
      "    # If no credentials are provided, we should use ADC credentials.\n",
      "    with mock.patch.object(google.auth, \"default\", autospec=True) as adc:\n",
      "        adc.return_value = (ga_credentials.AnonymousCredentials(), None)\n",
      "        CloudDeployClient()\n",
      "        adc.assert_called_once_with(\n",
      "            scopes=None,\n",
      "            default_scopes=(\"https://www.googleapis.com/auth/cloud-platform\",),\n",
      "            quota_project_id=None,\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"transport_class\",\n",
      "    [transports.CloudDeployGrpcTransport, transports.CloudDeployGrpcAsyncIOTransport,],\n",
      ")\n",
      "def test_cloud_deploy_transport_auth_adc(transport_class):\n",
      "    # If credentials and host are not provided, the transport class should use\n",
      "    # ADC credentials.\n",
      "    with mock.patch.object(google.auth, \"default\", autospec=True) as adc:\n",
      "        adc.return_value = (ga_credentials.AnonymousCredentials(), None)\n",
      "        transport_class(quota_project_id=\"octopus\", scopes=[\"1\", \"2\"])\n",
      "        adc.assert_called_once_with(\n",
      "            scopes=[\"1\", \"2\"],\n",
      "            default_scopes=(\"https://www.googleapis.com/auth/cloud-platform\",),\n",
      "            quota_project_id=\"octopus\",\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"transport_class,grpc_helpers\",\n",
      "    [\n",
      "        (transports.CloudDeployGrpcTransport, grpc_helpers),\n",
      "        (transports.CloudDeployGrpcAsyncIOTransport, grpc_helpers_async),\n",
      "    ],\n",
      ")\n",
      "def test_cloud_deploy_transport_create_channel(transport_class, grpc_helpers):\n",
      "    # If credentials and host are not provided, the transport class should use\n",
      "    # ADC credentials.\n",
      "    with mock.patch.object(\n",
      "        google.auth, \"default\", autospec=True\n",
      "    ) as adc, mock.patch.object(\n",
      "        grpc_helpers, \"create_channel\", autospec=True\n",
      "    ) as create_channel:\n",
      "        creds = ga_credentials.AnonymousCredentials()\n",
      "        adc.return_value = (creds, None)\n",
      "        transport_class(quota_project_id=\"octopus\", scopes=[\"1\", \"2\"])\n",
      "\n",
      "        create_channel.assert_called_with(\n",
      "            \"clouddeploy.googleapis.com:443\",\n",
      "            credentials=creds,\n",
      "            credentials_file=None,\n",
      "            quota_project_id=\"octopus\",\n",
      "            default_scopes=(\"https://www.googleapis.com/auth/cloud-platform\",),\n",
      "            scopes=[\"1\", \"2\"],\n",
      "            default_host=\"clouddeploy.googleapis.com\",\n",
      "            ssl_credentials=None,\n",
      "            options=[\n",
      "                (\"grpc.max_send_message_length\", -1),\n",
      "                (\"grpc.max_receive_message_length\", -1),\n",
      "            ],\n",
      "        )\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"transport_class\",\n",
      "    [transports.CloudDeployGrpcTransport, transports.CloudDeployGrpcAsyncIOTransport],\n",
      ")\n",
      "def test_cloud_deploy_grpc_transport_client_cert_source_for_mtls(transport_class):\n",
      "    cred = ga_credentials.AnonymousCredentials()\n",
      "\n",
      "    # Check ssl_channel_credentials is used if provided.\n",
      "    with mock.patch.object(transport_class, \"create_channel\") as mock_create_channel:\n",
      "        mock_ssl_channel_creds = mock.Mock()\n",
      "        transport_class(\n",
      "            host=\"squid.clam.whelk\",\n",
      "            credentials=cred,\n",
      "            ssl_channel_credentials=mock_ssl_channel_creds,\n",
      "        )\n",
      "        mock_create_channel.assert_called_once_with(\n",
      "            \"squid.clam.whelk:443\",\n",
      "            credentials=cred,\n",
      "            credentials_file=None,\n",
      "            scopes=None,\n",
      "            ssl_credentials=mock_ssl_channel_creds,\n",
      "            quota_project_id=None,\n",
      "            options=[\n",
      "                (\"grpc.max_send_message_length\", -1),\n",
      "                (\"grpc.max_receive_message_length\", -1),\n",
      "            ],\n",
      "        )\n",
      "\n",
      "    # Check if ssl_channel_credentials is not provided, then client_cert_source_for_mtls\n",
      "    # is used.\n",
      "    with mock.patch.object(transport_class, \"create_channel\", return_value=mock.Mock()):\n",
      "        with mock.patch(\"grpc.ssl_channel_credentials\") as mock_ssl_cred:\n",
      "            transport_class(\n",
      "                credentials=cred,\n",
      "                client_cert_source_for_mtls=client_cert_source_callback,\n",
      "            )\n",
      "            expected_cert, expected_key = client_cert_source_callback()\n",
      "            mock_ssl_cred.assert_called_once_with(\n",
      "                certificate_chain=expected_cert, private_key=expected_key\n",
      "            )\n",
      "\n",
      "\n",
      "def test_cloud_deploy_host_no_port():\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "        client_options=client_options.ClientOptions(\n",
      "            api_endpoint=\"clouddeploy.googleapis.com\"\n",
      "        ),\n",
      "    )\n",
      "    assert client.transport._host == \"clouddeploy.googleapis.com:443\"\n",
      "\n",
      "\n",
      "def test_cloud_deploy_host_with_port():\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(),\n",
      "        client_options=client_options.ClientOptions(\n",
      "            api_endpoint=\"clouddeploy.googleapis.com:8000\"\n",
      "        ),\n",
      "    )\n",
      "    assert client.transport._host == \"clouddeploy.googleapis.com:8000\"\n",
      "\n",
      "\n",
      "def test_cloud_deploy_grpc_transport_channel():\n",
      "    channel = grpc.secure_channel(\"http://localhost/\", grpc.local_channel_credentials())\n",
      "\n",
      "    # Check that channel is used if provided.\n",
      "    transport = transports.CloudDeployGrpcTransport(\n",
      "        host=\"squid.clam.whelk\", channel=channel,\n",
      "    )\n",
      "    assert transport.grpc_channel == channel\n",
      "    assert transport._host == \"squid.clam.whelk:443\"\n",
      "    assert transport._ssl_channel_credentials == None\n",
      "\n",
      "\n",
      "def test_cloud_deploy_grpc_asyncio_transport_channel():\n",
      "    channel = aio.secure_channel(\"http://localhost/\", grpc.local_channel_credentials())\n",
      "\n",
      "    # Check that channel is used if provided.\n",
      "    transport = transports.CloudDeployGrpcAsyncIOTransport(\n",
      "        host=\"squid.clam.whelk\", channel=channel,\n",
      "    )\n",
      "    assert transport.grpc_channel == channel\n",
      "    assert transport._host == \"squid.clam.whelk:443\"\n",
      "    assert transport._ssl_channel_credentials == None\n",
      "\n",
      "\n",
      "# Remove this test when deprecated arguments (api_mtls_endpoint, client_cert_source) are\n",
      "# removed from grpc/grpc_asyncio transport constructor.\n",
      "@pytest.mark.parametrize(\n",
      "    \"transport_class\",\n",
      "    [transports.CloudDeployGrpcTransport, transports.CloudDeployGrpcAsyncIOTransport],\n",
      ")\n",
      "def test_cloud_deploy_transport_channel_mtls_with_client_cert_source(transport_class):\n",
      "    with mock.patch(\n",
      "        \"grpc.ssl_channel_credentials\", autospec=True\n",
      "    ) as grpc_ssl_channel_cred:\n",
      "        with mock.patch.object(\n",
      "            transport_class, \"create_channel\"\n",
      "        ) as grpc_create_channel:\n",
      "            mock_ssl_cred = mock.Mock()\n",
      "            grpc_ssl_channel_cred.return_value = mock_ssl_cred\n",
      "\n",
      "            mock_grpc_channel = mock.Mock()\n",
      "            grpc_create_channel.return_value = mock_grpc_channel\n",
      "\n",
      "            cred = ga_credentials.AnonymousCredentials()\n",
      "            with pytest.warns(DeprecationWarning):\n",
      "                with mock.patch.object(google.auth, \"default\") as adc:\n",
      "                    adc.return_value = (cred, None)\n",
      "                    transport = transport_class(\n",
      "                        host=\"squid.clam.whelk\",\n",
      "                        api_mtls_endpoint=\"mtls.squid.clam.whelk\",\n",
      "                        client_cert_source=client_cert_source_callback,\n",
      "                    )\n",
      "                    adc.assert_called_once()\n",
      "\n",
      "            grpc_ssl_channel_cred.assert_called_once_with(\n",
      "                certificate_chain=b\"cert bytes\", private_key=b\"key bytes\"\n",
      "            )\n",
      "            grpc_create_channel.assert_called_once_with(\n",
      "                \"mtls.squid.clam.whelk:443\",\n",
      "                credentials=cred,\n",
      "                credentials_file=None,\n",
      "                scopes=None,\n",
      "                ssl_credentials=mock_ssl_cred,\n",
      "                quota_project_id=None,\n",
      "                options=[\n",
      "                    (\"grpc.max_send_message_length\", -1),\n",
      "                    (\"grpc.max_receive_message_length\", -1),\n",
      "                ],\n",
      "            )\n",
      "            assert transport.grpc_channel == mock_grpc_channel\n",
      "            assert transport._ssl_channel_credentials == mock_ssl_cred\n",
      "\n",
      "\n",
      "# Remove this test when deprecated arguments (api_mtls_endpoint, client_cert_source) are\n",
      "# removed from grpc/grpc_asyncio transport constructor.\n",
      "@pytest.mark.parametrize(\n",
      "    \"transport_class\",\n",
      "    [transports.CloudDeployGrpcTransport, transports.CloudDeployGrpcAsyncIOTransport],\n",
      ")\n",
      "def test_cloud_deploy_transport_channel_mtls_with_adc(transport_class):\n",
      "    mock_ssl_cred = mock.Mock()\n",
      "    with mock.patch.multiple(\n",
      "        \"google.auth.transport.grpc.SslCredentials\",\n",
      "        __init__=mock.Mock(return_value=None),\n",
      "        ssl_credentials=mock.PropertyMock(return_value=mock_ssl_cred),\n",
      "    ):\n",
      "        with mock.patch.object(\n",
      "            transport_class, \"create_channel\"\n",
      "        ) as grpc_create_channel:\n",
      "            mock_grpc_channel = mock.Mock()\n",
      "            grpc_create_channel.return_value = mock_grpc_channel\n",
      "            mock_cred = mock.Mock()\n",
      "\n",
      "            with pytest.warns(DeprecationWarning):\n",
      "                transport = transport_class(\n",
      "                    host=\"squid.clam.whelk\",\n",
      "                    credentials=mock_cred,\n",
      "                    api_mtls_endpoint=\"mtls.squid.clam.whelk\",\n",
      "                    client_cert_source=None,\n",
      "                )\n",
      "\n",
      "            grpc_create_channel.assert_called_once_with(\n",
      "                \"mtls.squid.clam.whelk:443\",\n",
      "                credentials=mock_cred,\n",
      "                credentials_file=None,\n",
      "                scopes=None,\n",
      "                ssl_credentials=mock_ssl_cred,\n",
      "                quota_project_id=None,\n",
      "                options=[\n",
      "                    (\"grpc.max_send_message_length\", -1),\n",
      "                    (\"grpc.max_receive_message_length\", -1),\n",
      "                ],\n",
      "            )\n",
      "            assert transport.grpc_channel == mock_grpc_channel\n",
      "\n",
      "\n",
      "def test_cloud_deploy_grpc_lro_client():\n",
      "    client = CloudDeployClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc\",\n",
      "    )\n",
      "    transport = client.transport\n",
      "\n",
      "    # Ensure that we have a api-core operations client.\n",
      "    assert isinstance(transport.operations_client, operations_v1.OperationsClient,)\n",
      "\n",
      "    # Ensure that subsequent calls to the property send the exact same object.\n",
      "    assert transport.operations_client is transport.operations_client\n",
      "\n",
      "\n",
      "def test_cloud_deploy_grpc_lro_async_client():\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc_asyncio\",\n",
      "    )\n",
      "    transport = client.transport\n",
      "\n",
      "    # Ensure that we have a api-core operations client.\n",
      "    assert isinstance(transport.operations_client, operations_v1.OperationsAsyncClient,)\n",
      "\n",
      "    # Ensure that subsequent calls to the property send the exact same object.\n",
      "    assert transport.operations_client is transport.operations_client\n",
      "\n",
      "\n",
      "def test_build_path():\n",
      "    project = \"squid\"\n",
      "    location = \"clam\"\n",
      "    build = \"whelk\"\n",
      "    expected = \"projects/{project}/locations/{location}/builds/{build}\".format(\n",
      "        project=project, location=location, build=build,\n",
      "    )\n",
      "    actual = CloudDeployClient.build_path(project, location, build)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_build_path():\n",
      "    expected = {\n",
      "        \"project\": \"octopus\",\n",
      "        \"location\": \"oyster\",\n",
      "        \"build\": \"nudibranch\",\n",
      "    }\n",
      "    path = CloudDeployClient.build_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_build_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_cluster_path():\n",
      "    project = \"cuttlefish\"\n",
      "    location = \"mussel\"\n",
      "    cluster = \"winkle\"\n",
      "    expected = \"projects/{project}/locations/{location}/clusters/{cluster}\".format(\n",
      "        project=project, location=location, cluster=cluster,\n",
      "    )\n",
      "    actual = CloudDeployClient.cluster_path(project, location, cluster)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_cluster_path():\n",
      "    expected = {\n",
      "        \"project\": \"nautilus\",\n",
      "        \"location\": \"scallop\",\n",
      "        \"cluster\": \"abalone\",\n",
      "    }\n",
      "    path = CloudDeployClient.cluster_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_cluster_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_config_path():\n",
      "    project = \"squid\"\n",
      "    location = \"clam\"\n",
      "    expected = \"projects/{project}/locations/{location}/config\".format(\n",
      "        project=project, location=location,\n",
      "    )\n",
      "    actual = CloudDeployClient.config_path(project, location)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_config_path():\n",
      "    expected = {\n",
      "        \"project\": \"whelk\",\n",
      "        \"location\": \"octopus\",\n",
      "    }\n",
      "    path = CloudDeployClient.config_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_config_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_delivery_pipeline_path():\n",
      "    project = \"oyster\"\n",
      "    location = \"nudibranch\"\n",
      "    delivery_pipeline = \"cuttlefish\"\n",
      "    expected = \"projects/{project}/locations/{location}/deliveryPipelines/{delivery_pipeline}\".format(\n",
      "        project=project, location=location, delivery_pipeline=delivery_pipeline,\n",
      "    )\n",
      "    actual = CloudDeployClient.delivery_pipeline_path(\n",
      "        project, location, delivery_pipeline\n",
      "    )\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_delivery_pipeline_path():\n",
      "    expected = {\n",
      "        \"project\": \"mussel\",\n",
      "        \"location\": \"winkle\",\n",
      "        \"delivery_pipeline\": \"nautilus\",\n",
      "    }\n",
      "    path = CloudDeployClient.delivery_pipeline_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_delivery_pipeline_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_release_path():\n",
      "    project = \"scallop\"\n",
      "    location = \"abalone\"\n",
      "    delivery_pipeline = \"squid\"\n",
      "    release = \"clam\"\n",
      "    expected = \"projects/{project}/locations/{location}/deliveryPipelines/{delivery_pipeline}/releases/{release}\".format(\n",
      "        project=project,\n",
      "        location=location,\n",
      "        delivery_pipeline=delivery_pipeline,\n",
      "        release=release,\n",
      "    )\n",
      "    actual = CloudDeployClient.release_path(\n",
      "        project, location, delivery_pipeline, release\n",
      "    )\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_release_path():\n",
      "    expected = {\n",
      "        \"project\": \"whelk\",\n",
      "        \"location\": \"octopus\",\n",
      "        \"delivery_pipeline\": \"oyster\",\n",
      "        \"release\": \"nudibranch\",\n",
      "    }\n",
      "    path = CloudDeployClient.release_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_release_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_rollout_path():\n",
      "    project = \"cuttlefish\"\n",
      "    location = \"mussel\"\n",
      "    delivery_pipeline = \"winkle\"\n",
      "    release = \"nautilus\"\n",
      "    rollout = \"scallop\"\n",
      "    expected = \"projects/{project}/locations/{location}/deliveryPipelines/{delivery_pipeline}/releases/{release}/rollouts/{rollout}\".format(\n",
      "        project=project,\n",
      "        location=location,\n",
      "        delivery_pipeline=delivery_pipeline,\n",
      "        release=release,\n",
      "        rollout=rollout,\n",
      "    )\n",
      "    actual = CloudDeployClient.rollout_path(\n",
      "        project, location, delivery_pipeline, release, rollout\n",
      "    )\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_rollout_path():\n",
      "    expected = {\n",
      "        \"project\": \"abalone\",\n",
      "        \"location\": \"squid\",\n",
      "        \"delivery_pipeline\": \"clam\",\n",
      "        \"release\": \"whelk\",\n",
      "        \"rollout\": \"octopus\",\n",
      "    }\n",
      "    path = CloudDeployClient.rollout_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_rollout_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_target_path():\n",
      "    project = \"oyster\"\n",
      "    location = \"nudibranch\"\n",
      "    target = \"cuttlefish\"\n",
      "    expected = \"projects/{project}/locations/{location}/targets/{target}\".format(\n",
      "        project=project, location=location, target=target,\n",
      "    )\n",
      "    actual = CloudDeployClient.target_path(project, location, target)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_target_path():\n",
      "    expected = {\n",
      "        \"project\": \"mussel\",\n",
      "        \"location\": \"winkle\",\n",
      "        \"target\": \"nautilus\",\n",
      "    }\n",
      "    path = CloudDeployClient.target_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_target_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_worker_pool_path():\n",
      "    project = \"scallop\"\n",
      "    location = \"abalone\"\n",
      "    worker_pool = \"squid\"\n",
      "    expected = \"projects/{project}/locations/{location}/workerPools/{worker_pool}\".format(\n",
      "        project=project, location=location, worker_pool=worker_pool,\n",
      "    )\n",
      "    actual = CloudDeployClient.worker_pool_path(project, location, worker_pool)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_worker_pool_path():\n",
      "    expected = {\n",
      "        \"project\": \"clam\",\n",
      "        \"location\": \"whelk\",\n",
      "        \"worker_pool\": \"octopus\",\n",
      "    }\n",
      "    path = CloudDeployClient.worker_pool_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_worker_pool_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_common_billing_account_path():\n",
      "    billing_account = \"oyster\"\n",
      "    expected = \"billingAccounts/{billing_account}\".format(\n",
      "        billing_account=billing_account,\n",
      "    )\n",
      "    actual = CloudDeployClient.common_billing_account_path(billing_account)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_common_billing_account_path():\n",
      "    expected = {\n",
      "        \"billing_account\": \"nudibranch\",\n",
      "    }\n",
      "    path = CloudDeployClient.common_billing_account_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_common_billing_account_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_common_folder_path():\n",
      "    folder = \"cuttlefish\"\n",
      "    expected = \"folders/{folder}\".format(folder=folder,)\n",
      "    actual = CloudDeployClient.common_folder_path(folder)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_common_folder_path():\n",
      "    expected = {\n",
      "        \"folder\": \"mussel\",\n",
      "    }\n",
      "    path = CloudDeployClient.common_folder_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_common_folder_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_common_organization_path():\n",
      "    organization = \"winkle\"\n",
      "    expected = \"organizations/{organization}\".format(organization=organization,)\n",
      "    actual = CloudDeployClient.common_organization_path(organization)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_common_organization_path():\n",
      "    expected = {\n",
      "        \"organization\": \"nautilus\",\n",
      "    }\n",
      "    path = CloudDeployClient.common_organization_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_common_organization_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_common_project_path():\n",
      "    project = \"scallop\"\n",
      "    expected = \"projects/{project}\".format(project=project,)\n",
      "    actual = CloudDeployClient.common_project_path(project)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_common_project_path():\n",
      "    expected = {\n",
      "        \"project\": \"abalone\",\n",
      "    }\n",
      "    path = CloudDeployClient.common_project_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_common_project_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_common_location_path():\n",
      "    project = \"squid\"\n",
      "    location = \"clam\"\n",
      "    expected = \"projects/{project}/locations/{location}\".format(\n",
      "        project=project, location=location,\n",
      "    )\n",
      "    actual = CloudDeployClient.common_location_path(project, location)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_parse_common_location_path():\n",
      "    expected = {\n",
      "        \"project\": \"whelk\",\n",
      "        \"location\": \"octopus\",\n",
      "    }\n",
      "    path = CloudDeployClient.common_location_path(**expected)\n",
      "\n",
      "    # Check that the path construction is reversible.\n",
      "    actual = CloudDeployClient.parse_common_location_path(path)\n",
      "    assert expected == actual\n",
      "\n",
      "\n",
      "def test_client_with_default_client_info():\n",
      "    client_info = gapic_v1.client_info.ClientInfo()\n",
      "\n",
      "    with mock.patch.object(\n",
      "        transports.CloudDeployTransport, \"_prep_wrapped_messages\"\n",
      "    ) as prep:\n",
      "        client = CloudDeployClient(\n",
      "            credentials=ga_credentials.AnonymousCredentials(), client_info=client_info,\n",
      "        )\n",
      "        prep.assert_called_once_with(client_info)\n",
      "\n",
      "    with mock.patch.object(\n",
      "        transports.CloudDeployTransport, \"_prep_wrapped_messages\"\n",
      "    ) as prep:\n",
      "        transport_class = CloudDeployClient.get_transport_class()\n",
      "        transport = transport_class(\n",
      "            credentials=ga_credentials.AnonymousCredentials(), client_info=client_info,\n",
      "        )\n",
      "        prep.assert_called_once_with(client_info)\n",
      "\n",
      "\n",
      "@pytest.mark.asyncio\n",
      "async def test_transport_close_async():\n",
      "    client = CloudDeployAsyncClient(\n",
      "        credentials=ga_credentials.AnonymousCredentials(), transport=\"grpc_asyncio\",\n",
      "    )\n",
      "    with mock.patch.object(\n",
      "        type(getattr(client.transport, \"grpc_channel\")), \"close\"\n",
      "    ) as close:\n",
      "        async with client:\n",
      "            close.assert_not_called()\n",
      "        close.assert_called_once()\n",
      "\n",
      "\n",
      "def test_transport_close():\n",
      "    transports = {\n",
      "        \"grpc\": \"_grpc_channel\",\n",
      "    }\n",
      "\n",
      "    for transport, close_name in transports.items():\n",
      "        client = CloudDeployClient(\n",
      "            credentials=ga_credentials.AnonymousCredentials(), transport=transport\n",
      "        )\n",
      "        with mock.patch.object(\n",
      "            type(getattr(client.transport, close_name)), \"close\"\n",
      "        ) as close:\n",
      "            with client:\n",
      "                close.assert_not_called()\n",
      "            close.assert_called_once()\n",
      "\n",
      "\n",
      "def test_client_ctx():\n",
      "    transports = [\n",
      "        \"grpc\",\n",
      "    ]\n",
      "    for transport in transports:\n",
      "        client = CloudDeployClient(\n",
      "            credentials=ga_credentials.AnonymousCredentials(), transport=transport\n",
      "        )\n",
      "        # Test client calls underlying transport.\n",
      "        with mock.patch.object(type(client.transport), \"close\") as close:\n",
      "            close.assert_not_called()\n",
      "            with client:\n",
      "                pass\n",
      "            close.assert_called()\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    \"client_class,transport_class\",\n",
      "    [\n",
      "        (CloudDeployClient, transports.CloudDeployGrpcTransport),\n",
      "        (CloudDeployAsyncClient, transports.CloudDeployGrpcAsyncIOTransport),\n",
      "    ],\n",
      ")\n",
      "def test_api_key_credentials(client_class, transport_class):\n",
      "    with mock.patch.object(\n",
      "        google.auth._default, \"get_api_key_credentials\", create=True\n",
      "    ) as get_api_key_credentials:\n",
      "        mock_cred = mock.Mock()\n",
      "        get_api_key_credentials.return_value = mock_cred\n",
      "        options = client_options.ClientOptions()\n",
      "        options.api_key = \"api_key\"\n",
      "        with mock.patch.object(transport_class, \"__init__\") as patched:\n",
      "            patched.return_value = None\n",
      "            client = client_class(client_options=options)\n",
      "            patched.assert_called_once_with(\n",
      "                credentials=mock_cred,\n",
      "                credentials_file=None,\n",
      "                host=client.DEFAULT_ENDPOINT,\n",
      "                scopes=None,\n",
      "                client_cert_source_for_mtls=None,\n",
      "                quota_project_id=None,\n",
      "                client_info=transports.base.DEFAULT_CLIENT_INFO,\n",
      "                always_use_jwt_access=True,\n",
      "            )\n",
      "\n",
      "import numpy as np\n",
      "from skimage.morphology import max_tree, area_closing, area_opening\n",
      "from skimage.morphology import max_tree_local_maxima, diameter_opening\n",
      "from skimage.morphology import diameter_closing\n",
      "from skimage.util import invert\n",
      "\n",
      "from skimage._shared.testing import assert_array_equal, TestCase\n",
      "\n",
      "eps = 1e-12\n",
      "\n",
      "\n",
      "def _full_type_test(img, param, expected, func, param_scale=False,\n",
      "                    **keywords):\n",
      "\n",
      "    # images as they are\n",
      "    out = func(img, param, **keywords)\n",
      "    assert_array_equal(out, expected)\n",
      "\n",
      "    # unsigned int\n",
      "    for dt in [np.uint32, np.uint64]:\n",
      "        img_cast = img.astype(dt)\n",
      "        out = func(img_cast, param, **keywords)\n",
      "        exp_cast = expected.astype(dt)\n",
      "        assert_array_equal(out, exp_cast)\n",
      "\n",
      "    # float\n",
      "    data_float = img.astype(np.float64)\n",
      "    data_float = data_float / 255.0\n",
      "    expected_float = expected.astype(np.float64)\n",
      "    expected_float = expected_float / 255.0\n",
      "    if param_scale:\n",
      "        param_cast = param / 255.0\n",
      "    else:\n",
      "        param_cast = param\n",
      "    for dt in [np.float32, np.float64]:\n",
      "        data_cast = data_float.astype(dt)\n",
      "        out = func(data_cast, param_cast, **keywords)\n",
      "        exp_cast = expected_float.astype(dt)\n",
      "        error_img = 255.0 * exp_cast - 255.0 * out\n",
      "        error = (error_img >= 1.0).sum()\n",
      "        assert error < eps\n",
      "\n",
      "    # signed images\n",
      "    img_signed = img.astype(np.int16)\n",
      "    img_signed = img_signed - 128\n",
      "    exp_signed = expected.astype(np.int16)\n",
      "    exp_signed = exp_signed - 128\n",
      "    for dt in [np.int8, np.int16, np.int32, np.int64]:\n",
      "        img_s = img_signed.astype(dt)\n",
      "        out = func(img_s, param, **keywords)\n",
      "        exp_s = exp_signed.astype(dt)\n",
      "        assert_array_equal(out, exp_s)\n",
      "\n",
      "\n",
      "class TestMaxtree(TestCase):\n",
      "\n",
      "    def test_max_tree(self):\n",
      "        \"Test for max tree\"\n",
      "        img_type = np.uint8\n",
      "        img = np.array([[10, 8, 8, 9],\n",
      "                        [7, 7, 9, 9],\n",
      "                        [8, 7, 10, 10],\n",
      "                        [9, 9, 10, 10]], dtype=img_type)\n",
      "\n",
      "        P_exp = np.array([[1, 4, 1, 1],\n",
      "                          [4, 4, 3, 3],\n",
      "                          [1, 4, 3, 10],\n",
      "                          [3, 3, 10, 10]], dtype=np.int64)\n",
      "\n",
      "        S_exp = np.array([4, 5, 9, 1, 2, 8, 3, 6, 7,\n",
      "                          12, 13, 0, 10, 11, 14, 15],\n",
      "                         dtype=np.int64)\n",
      "\n",
      "        for img_type in [np.uint8, np.uint16, np.uint32, np.uint64]:\n",
      "            img = img.astype(img_type)\n",
      "            P, S = max_tree(img, connectivity=2)\n",
      "            assert_array_equal(P, P_exp)\n",
      "            assert_array_equal(S, S_exp)\n",
      "\n",
      "        for img_type in [np.int8, np.int16, np.int32, np.int64]:\n",
      "            img = img.astype(img_type)\n",
      "            img_shifted = img - 9\n",
      "            P, S = max_tree(img_shifted, connectivity=2)\n",
      "            assert_array_equal(P, P_exp)\n",
      "            assert_array_equal(S, S_exp)\n",
      "\n",
      "        img_float = img.astype(float)\n",
      "        img_float = (img_float - 8) / 2.0\n",
      "        for img_type in [np.float32, np.float64]:\n",
      "            img_float = img_float.astype(img_type)\n",
      "            P, S = max_tree(img_float, connectivity=2)\n",
      "            assert_array_equal(P, P_exp)\n",
      "            assert_array_equal(S, S_exp)\n",
      "\n",
      "        return\n",
      "\n",
      "    def test_area_closing(self):\n",
      "        \"Test for Area Closing (2 thresholds, all types)\"\n",
      "\n",
      "        # original image\n",
      "        img = np.array(\n",
      "            [[240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [240, 200, 200, 240, 200, 240, 200, 200, 240, 240, 200, 240],\n",
      "             [240, 200, 40, 240, 240, 240, 240, 240, 240, 240, 40, 240],\n",
      "             [240, 240, 240, 240, 100, 240, 100, 100, 240, 240, 200, 240],\n",
      "             [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 240, 200, 200, 255, 255],\n",
      "             [200, 255, 200, 200, 200, 255, 200, 240, 255, 255, 255, 40],\n",
      "             [200, 200, 200, 100, 200, 200, 200, 240, 255, 255, 255, 255],\n",
      "             [200, 200, 200, 100, 200, 200, 200, 240, 200, 200, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 40, 200, 240, 240, 100, 255, 255],\n",
      "             [200, 40, 255, 255, 255, 40, 200, 255, 200, 200, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 255, 255, 255, 255, 255]],\n",
      "            dtype=np.uint8)\n",
      "\n",
      "        # expected area closing with area 2\n",
      "        expected_2 = np.array(\n",
      "            [[240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [240, 200, 200, 240, 240, 240, 200, 200, 240, 240, 200, 240],\n",
      "             [240, 200, 200, 240, 240, 240, 240, 240, 240, 240, 200, 240],\n",
      "             [240, 240, 240, 240, 240, 240, 100, 100, 240, 240, 200, 240],\n",
      "             [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 240, 200, 200, 255, 255],\n",
      "             [200, 255, 200, 200, 200, 255, 200, 240, 255, 255, 255, 255],\n",
      "             [200, 200, 200, 100, 200, 200, 200, 240, 255, 255, 255, 255],\n",
      "             [200, 200, 200, 100, 200, 200, 200, 240, 200, 200, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 40, 200, 240, 240, 200, 255, 255],\n",
      "             [200, 200, 255, 255, 255, 40, 200, 255, 200, 200, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 255, 255, 255, 255, 255]],\n",
      "            dtype=np.uint8)\n",
      "\n",
      "        # expected diameter closing with diameter 4\n",
      "        expected_4 = np.array(\n",
      "            [[240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [240, 200, 200, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [240, 200, 200, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 240, 240, 240, 255, 255],\n",
      "             [200, 255, 200, 200, 200, 255, 200, 240, 255, 255, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 240, 255, 255, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 240, 200, 200, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 240, 240, 200, 255, 255],\n",
      "             [200, 200, 255, 255, 255, 200, 200, 255, 200, 200, 255, 255],\n",
      "             [200, 200, 200, 200, 200, 200, 200, 255, 255, 255, 255, 255]],\n",
      "            dtype=np.uint8)\n",
      "\n",
      "        # _full_type_test makes a test with many image types.\n",
      "        _full_type_test(img, 2, expected_2, area_closing, connectivity=2)\n",
      "        _full_type_test(img, 4, expected_4, area_closing, connectivity=2)\n",
      "\n",
      "        P, S = max_tree(invert(img), connectivity=2)\n",
      "        _full_type_test(img, 4, expected_4, area_closing,\n",
      "                        parent=P, tree_traverser=S)\n",
      "\n",
      "    def test_area_opening(self):\n",
      "        \"Test for Area Opening (2 thresholds, all types)\"\n",
      "\n",
      "        # original image\n",
      "        img = np.array([[15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "                        [15, 55, 55, 15, 55, 15, 55, 55, 15, 15, 55, 15],\n",
      "                        [15, 55, 215, 15, 15, 15, 15, 15, 15, 15, 215, 15],\n",
      "                        [15, 15, 15, 15, 155, 15, 155, 155, 15, 15, 55, 15],\n",
      "                        [15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "                        [55, 55, 55, 55, 55, 55, 55, 15, 55, 55, 0, 0],\n",
      "                        [55, 0, 55, 55, 55, 0, 55, 15, 0, 0, 0, 215],\n",
      "                        [55, 55, 55, 155, 55, 55, 55, 15, 0, 0, 0, 0],\n",
      "                        [55, 55, 55, 155, 55, 55, 55, 15, 55, 55, 0, 0],\n",
      "                        [55, 55, 55, 55, 55, 215, 55, 15, 15, 155, 0, 0],\n",
      "                        [55, 215, 0, 0, 0, 215, 55, 0, 55, 55, 0, 0],\n",
      "                        [55, 55, 55, 55, 55, 55, 55, 0, 0, 0, 0, 0]],\n",
      "                       dtype=np.uint8)\n",
      "\n",
      "        # expected area closing with area 2\n",
      "        expected_2 = np.array([[15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 15, 15],\n",
      "                               [15, 55, 55, 15, 15, 15, 55, 55, 15,\n",
      "                                15, 55, 15],\n",
      "                               [15, 55, 55, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 55, 15],\n",
      "                               [15, 15, 15, 15, 15, 15, 155, 155, 15,\n",
      "                                15, 55, 15],\n",
      "                               [15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 15, 15],\n",
      "                               [55, 55, 55, 55, 55, 55, 55, 15, 55,\n",
      "                                55, 0, 0],\n",
      "                               [55, 0, 55, 55, 55, 0, 55, 15, 0,\n",
      "                                0, 0, 0],\n",
      "                               [55, 55, 55, 155, 55, 55, 55, 15, 0,\n",
      "                                0, 0, 0],\n",
      "                               [55, 55, 55, 155, 55, 55, 55, 15, 55,\n",
      "                                55, 0, 0],\n",
      "                               [55, 55, 55, 55, 55, 215, 55, 15, 15,\n",
      "                                55, 0, 0],\n",
      "                               [55, 55, 0, 0, 0, 215, 55, 0, 55,\n",
      "                                55, 0, 0],\n",
      "                               [55, 55, 55, 55, 55, 55, 55, 0, 0,\n",
      "                                0, 0, 0]],\n",
      "                              dtype=np.uint8)\n",
      "\n",
      "        # expected diameter closing with diameter 4\n",
      "        expected_4 = np.array([[15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 15, 15],\n",
      "                               [15, 55, 55, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 15, 15],\n",
      "                               [15, 55, 55, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 15, 15],\n",
      "                               [15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 15, 15],\n",
      "                               [15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "                                15, 15, 15],\n",
      "                               [55, 55, 55, 55, 55, 55, 55, 15, 15,\n",
      "                                15, 0, 0],\n",
      "                               [55, 0, 55, 55, 55, 0, 55, 15, 0,\n",
      "                                0, 0, 0],\n",
      "                               [55, 55, 55, 55, 55, 55, 55, 15, 0,\n",
      "                                0, 0, 0],\n",
      "                               [55, 55, 55, 55, 55, 55, 55, 15, 55,\n",
      "                                55, 0, 0],\n",
      "                               [55, 55, 55, 55, 55, 55, 55, 15, 15,\n",
      "                                55, 0, 0],\n",
      "                               [55, 55, 0, 0, 0, 55, 55, 0, 55,\n",
      "                                55, 0, 0],\n",
      "                               [55, 55, 55, 55, 55, 55, 55, 0, 0,\n",
      "                                0, 0, 0]],\n",
      "                              dtype=np.uint8)\n",
      "\n",
      "        # _full_type_test makes a test with many image types.\n",
      "        _full_type_test(img, 2, expected_2, area_opening, connectivity=2)\n",
      "        _full_type_test(img, 4, expected_4, area_opening, connectivity=2)\n",
      "\n",
      "        P, S = max_tree(img, connectivity=2)\n",
      "        _full_type_test(img, 4, expected_4, area_opening,\n",
      "                        parent=P, tree_traverser=S)\n",
      "\n",
      "    def test_diameter_closing(self):\n",
      "        \"Test for Diameter Opening (2 thresholds, all types)\"\n",
      "        img = np.array([[97, 95, 93, 92, 91, 90, 90, 90, 91, 92, 93, 95],\n",
      "                        [95, 93, 91, 89, 88, 88, 88, 88, 88, 89, 91, 93],\n",
      "                        [93, 63, 63, 63, 63, 86, 86, 86, 87, 43, 43, 91],\n",
      "                        [92, 89, 88, 86, 85, 85, 84, 85, 85, 43, 43, 89],\n",
      "                        [91, 88, 87, 85, 84, 84, 83, 84, 84, 85, 87, 88],\n",
      "                        [90, 88, 86, 85, 84, 83, 83, 83, 84, 85, 86, 88],\n",
      "                        [90, 88, 86, 84, 83, 83, 82, 83, 83, 84, 86, 88],\n",
      "                        [90, 88, 86, 85, 84, 83, 83, 83, 84, 85, 86, 88],\n",
      "                        [91, 88, 87, 85, 84, 84, 83, 84, 84, 85, 87, 88],\n",
      "                        [92, 89, 23, 23, 85, 85, 84, 85, 85, 3, 3, 89],\n",
      "                        [93, 91, 23, 23, 87, 86, 86, 86, 87, 88, 3, 91],\n",
      "                        [95, 93, 91, 89, 88, 88, 88, 88, 88, 89, 91, 93]],\n",
      "                       dtype=np.uint8)\n",
      "\n",
      "        ex2 = np.array([[97, 95, 93, 92, 91, 90, 90, 90, 91, 92, 93, 95],\n",
      "                        [95, 93, 91, 89, 88, 88, 88, 88, 88, 89, 91, 93],\n",
      "                        [93, 63, 63, 63, 63, 86, 86, 86, 87, 43, 43, 91],\n",
      "                        [92, 89, 88, 86, 85, 85, 84, 85, 85, 43, 43, 89],\n",
      "                        [91, 88, 87, 85, 84, 84, 83, 84, 84, 85, 87, 88],\n",
      "                        [90, 88, 86, 85, 84, 83, 83, 83, 84, 85, 86, 88],\n",
      "                        [90, 88, 86, 84, 83, 83, 83, 83, 83, 84, 86, 88],\n",
      "                        [90, 88, 86, 85, 84, 83, 83, 83, 84, 85, 86, 88],\n",
      "                        [91, 88, 87, 85, 84, 84, 83, 84, 84, 85, 87, 88],\n",
      "                        [92, 89, 23, 23, 85, 85, 84, 85, 85, 3, 3, 89],\n",
      "                        [93, 91, 23, 23, 87, 86, 86, 86, 87, 88, 3, 91],\n",
      "                        [95, 93, 91, 89, 88, 88, 88, 88, 88, 89, 91, 93]],\n",
      "                       dtype=np.uint8)\n",
      "\n",
      "        ex4 = np.array([[97, 95, 93, 92, 91, 90, 90, 90, 91, 92, 93, 95],\n",
      "                        [95, 93, 91, 89, 88, 88, 88, 88, 88, 89, 91, 93],\n",
      "                        [93, 63, 63, 63, 63, 86, 86, 86, 87, 84, 84, 91],\n",
      "                        [92, 89, 88, 86, 85, 85, 84, 85, 85, 84, 84, 89],\n",
      "                        [91, 88, 87, 85, 84, 84, 83, 84, 84, 85, 87, 88],\n",
      "                        [90, 88, 86, 85, 84, 83, 83, 83, 84, 85, 86, 88],\n",
      "                        [90, 88, 86, 84, 83, 83, 83, 83, 83, 84, 86, 88],\n",
      "                        [90, 88, 86, 85, 84, 83, 83, 83, 84, 85, 86, 88],\n",
      "                        [91, 88, 87, 85, 84, 84, 83, 84, 84, 85, 87, 88],\n",
      "                        [92, 89, 84, 84, 85, 85, 84, 85, 85, 84, 84, 89],\n",
      "                        [93, 91, 84, 84, 87, 86, 86, 86, 87, 88, 84, 91],\n",
      "                        [95, 93, 91, 89, 88, 88, 88, 88, 88, 89, 91, 93]],\n",
      "                       dtype=np.uint8)\n",
      "\n",
      "        # _full_type_test makes a test with many image types.\n",
      "        _full_type_test(img, 2, ex2, diameter_closing, connectivity=2)\n",
      "        _full_type_test(img, 4, ex4, diameter_closing, connectivity=2)\n",
      "\n",
      "        P, S = max_tree(invert(img), connectivity=2)\n",
      "        _full_type_test(img, 4, ex4, diameter_opening,\n",
      "                        parent=P, tree_traverser=S)\n",
      "\n",
      "    def test_diameter_opening(self):\n",
      "        \"Test for Diameter Opening (2 thresholds, all types)\"\n",
      "        img = np.array([[5, 7, 9, 11, 12, 12, 12, 12, 12, 11, 9, 7],\n",
      "                        [7, 10, 11, 13, 14, 14, 15, 14, 14, 13, 11, 10],\n",
      "                        [9, 40, 40, 40, 40, 16, 16, 16, 16, 60, 60, 11],\n",
      "                        [11, 13, 15, 16, 17, 18, 18, 18, 17, 60, 60, 13],\n",
      "                        [12, 14, 16, 17, 18, 19, 19, 19, 18, 17, 16, 14],\n",
      "                        [12, 14, 16, 18, 19, 19, 19, 19, 19, 18, 16, 14],\n",
      "                        [12, 15, 16, 18, 19, 19, 20, 19, 19, 18, 16, 15],\n",
      "                        [12, 14, 16, 18, 19, 19, 19, 19, 19, 18, 16, 14],\n",
      "                        [12, 14, 16, 17, 18, 19, 19, 19, 18, 17, 16, 14],\n",
      "                        [11, 13, 80, 80, 17, 18, 18, 18, 17, 100, 100, 13],\n",
      "                        [9, 11, 80, 80, 16, 16, 16, 16, 16, 15, 100, 11],\n",
      "                        [7, 10, 11, 13, 14, 14, 15, 14, 14, 13, 11, 10]])\n",
      "\n",
      "        ex2 = np.array([[5, 7, 9, 11, 12, 12, 12, 12, 12, 11, 9, 7],\n",
      "                        [7, 10, 11, 13, 14, 14, 15, 14, 14, 13, 11, 10],\n",
      "                        [9, 40, 40, 40, 40, 16, 16, 16, 16, 60, 60, 11],\n",
      "                        [11, 13, 15, 16, 17, 18, 18, 18, 17, 60, 60, 13],\n",
      "                        [12, 14, 16, 17, 18, 19, 19, 19, 18, 17, 16, 14],\n",
      "                        [12, 14, 16, 18, 19, 19, 19, 19, 19, 18, 16, 14],\n",
      "                        [12, 15, 16, 18, 19, 19, 19, 19, 19, 18, 16, 15],\n",
      "                        [12, 14, 16, 18, 19, 19, 19, 19, 19, 18, 16, 14],\n",
      "                        [12, 14, 16, 17, 18, 19, 19, 19, 18, 17, 16, 14],\n",
      "                        [11, 13, 80, 80, 17, 18, 18, 18, 17, 100, 100, 13],\n",
      "                        [9, 11, 80, 80, 16, 16, 16, 16, 16, 15, 100, 11],\n",
      "                        [7, 10, 11, 13, 14, 14, 15, 14, 14, 13, 11, 10]])\n",
      "\n",
      "        ex4 = np.array([[5, 7, 9, 11, 12, 12, 12, 12, 12, 11, 9, 7],\n",
      "                        [7, 10, 11, 13, 14, 14, 15, 14, 14, 13, 11, 10],\n",
      "                        [9, 40, 40, 40, 40, 16, 16, 16, 16, 18, 18, 11],\n",
      "                        [11, 13, 15, 16, 17, 18, 18, 18, 17, 18, 18, 13],\n",
      "                        [12, 14, 16, 17, 18, 19, 19, 19, 18, 17, 16, 14],\n",
      "                        [12, 14, 16, 18, 19, 19, 19, 19, 19, 18, 16, 14],\n",
      "                        [12, 15, 16, 18, 19, 19, 19, 19, 19, 18, 16, 15],\n",
      "                        [12, 14, 16, 18, 19, 19, 19, 19, 19, 18, 16, 14],\n",
      "                        [12, 14, 16, 17, 18, 19, 19, 19, 18, 17, 16, 14],\n",
      "                        [11, 13, 18, 18, 17, 18, 18, 18, 17, 18, 18, 13],\n",
      "                        [9, 11, 18, 18, 16, 16, 16, 16, 16, 15, 18, 11],\n",
      "                        [7, 10, 11, 13, 14, 14, 15, 14, 14, 13, 11, 10]])\n",
      "\n",
      "        # _full_type_test makes a test with many image types.\n",
      "        _full_type_test(img, 2, ex2, diameter_opening, connectivity=2)\n",
      "        _full_type_test(img, 4, ex4, diameter_opening, connectivity=2)\n",
      "\n",
      "        P, S = max_tree(img, connectivity=2)\n",
      "        _full_type_test(img, 4, ex4, diameter_opening,\n",
      "                        parent=P, tree_traverser=S)\n",
      "\n",
      "    def test_local_maxima(self):\n",
      "        \"local maxima for various data types\"\n",
      "        data = np.array([[10, 11, 13, 14, 14, 15, 14, 14, 13, 11],\n",
      "                         [11, 13, 15, 16, 16, 16, 16, 16, 15, 13],\n",
      "                         [13, 15, 40, 40, 18, 18, 18, 60, 60, 15],\n",
      "                         [14, 16, 40, 40, 19, 19, 19, 60, 60, 16],\n",
      "                         [14, 16, 18, 19, 19, 19, 19, 19, 18, 16],\n",
      "                         [15, 16, 18, 19, 19, 20, 19, 19, 18, 16],\n",
      "                         [14, 16, 18, 19, 19, 19, 19, 19, 18, 16],\n",
      "                         [14, 16, 80, 80, 19, 19, 19, 100, 100, 16],\n",
      "                         [13, 15, 80, 80, 18, 18, 18, 100, 100, 15],\n",
      "                         [11, 13, 15, 16, 16, 16, 16, 16, 15, 13]],\n",
      "                        dtype=np.uint8)\n",
      "        expected_result = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "                                   dtype=np.uint64)\n",
      "        for dtype in [np.uint8, np.uint64, np.int8, np.int64]:\n",
      "\n",
      "            test_data = data.astype(dtype)\n",
      "            out = max_tree_local_maxima(test_data, connectivity=1)\n",
      "            out_bin = out > 0\n",
      "            assert_array_equal(expected_result, out_bin)\n",
      "            assert out.dtype == expected_result.dtype\n",
      "            assert np.max(out) == 5\n",
      "\n",
      "            P, S = max_tree(test_data)\n",
      "            out = max_tree_local_maxima(test_data,\n",
      "                                        parent=P,\n",
      "                                        tree_traverser=S)\n",
      "\n",
      "            assert_array_equal(expected_result, out_bin)\n",
      "\n",
      "            assert out.dtype == expected_result.dtype\n",
      "            assert np.max(out) == 5\n",
      "\n",
      "    def test_extrema_float(self):\n",
      "        \"specific tests for float type\"\n",
      "        data = np.array([[0.10, 0.11, 0.13, 0.14, 0.14, 0.15, 0.14,\n",
      "                          0.14, 0.13, 0.11],\n",
      "                         [0.11, 0.13, 0.15, 0.16, 0.16, 0.16, 0.16,\n",
      "                          0.16, 0.15, 0.13],\n",
      "                         [0.13, 0.15, 0.40, 0.40, 0.18, 0.18, 0.18,\n",
      "                          0.60, 0.60, 0.15],\n",
      "                         [0.14, 0.16, 0.40, 0.40, 0.19, 0.19, 0.19,\n",
      "                          0.60, 0.60, 0.16],\n",
      "                         [0.14, 0.16, 0.18, 0.19, 0.19, 0.19, 0.19,\n",
      "                          0.19, 0.18, 0.16],\n",
      "                         [0.15, 0.182, 0.18, 0.19, 0.204, 0.20, 0.19,\n",
      "                          0.19, 0.18, 0.16],\n",
      "                         [0.14, 0.16, 0.18, 0.19, 0.19, 0.19, 0.19,\n",
      "                          0.19, 0.18, 0.16],\n",
      "                         [0.14, 0.16, 0.80, 0.80, 0.19, 0.19, 0.19,\n",
      "                          4.0, 1.0, 0.16],\n",
      "                         [0.13, 0.15, 0.80, 0.80, 0.18, 0.18, 0.18,\n",
      "                          1.0, 1.0, 0.15],\n",
      "                         [0.11, 0.13, 0.15, 0.16, 0.16, 0.16, 0.16,\n",
      "                          0.16, 0.15, 0.13]],\n",
      "                        dtype=np.float32)\n",
      "\n",
      "        expected_result = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 1, 0, 0],\n",
      "                                    [0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "                                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "                                   dtype=np.uint8)\n",
      "\n",
      "        # test for local maxima\n",
      "        out = max_tree_local_maxima(data, connectivity=1)\n",
      "        out_bin = out > 0\n",
      "        assert_array_equal(expected_result, out_bin)\n",
      "        assert np.max(out) == 6\n",
      "\n",
      "    def test_3d(self):\n",
      "        \"\"\"tests the detection of maxima in 3D.\"\"\"\n",
      "        img = np.zeros((8, 8, 8), dtype=np.uint8)\n",
      "        local_maxima = np.zeros((8, 8, 8), dtype=np.uint64)\n",
      "\n",
      "        # first maximum: only one pixel\n",
      "        img[1, 1:3, 1:3] = 100\n",
      "        img[2, 2, 2] = 200\n",
      "        img[3, 1:3, 1:3] = 100\n",
      "        local_maxima[2, 2, 2] = 1\n",
      "\n",
      "        # second maximum: three pixels in z-direction\n",
      "        img[5:8, 1, 1] = 200\n",
      "        local_maxima[5:8, 1, 1] = 1\n",
      "\n",
      "        # third: two maxima in 0 and 3.\n",
      "        img[0, 5:8, 5:8] = 200\n",
      "        img[1, 6, 6] = 100\n",
      "        img[2, 5:7, 5:7] = 200\n",
      "        img[0:3, 5:8, 5:8] += 50\n",
      "        local_maxima[0, 5:8, 5:8] = 1\n",
      "        local_maxima[2, 5:7, 5:7] = 1\n",
      "\n",
      "        # four : one maximum in the corner of the square\n",
      "        img[6:8, 6:8, 6:8] = 200\n",
      "        img[7, 7, 7] = 255\n",
      "        local_maxima[7, 7, 7] = 1\n",
      "\n",
      "        out = max_tree_local_maxima(img)\n",
      "        out_bin = out > 0\n",
      "        assert_array_equal(local_maxima, out_bin)\n",
      "        assert np.max(out) == 5\n",
      "\n",
      "import logging\n",
      "import os\n",
      "from pathlib import Path\n",
      "from typing import Any, Callable, Optional\n",
      "\n",
      "from torch.utils.data import Dataset\n",
      "from torchvision import transforms\n",
      "from PIL import Image\n",
      "import cv2\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class URISC(Dataset):\n",
      "    def __init__(\n",
      "        self, \n",
      "        dir: str, \n",
      "        mode: str = 'train',\n",
      "        transform: Optional[Callable] = None, \n",
      "        data_rank: str = 'simple',\n",
      "    ):\n",
      "        super(URISC, self).__init__()\n",
      "        self.dir = dir\n",
      "        self.mode = mode\n",
      "        self.transform = transform\n",
      "        self.data_rank = data_rank\n",
      "\n",
      "        if data_rank == 'simple':\n",
      "            self.transform_normalize = transforms.Normalize(mean=0.520, std=0.185)\n",
      "        elif data_rank == 'complex':\n",
      "            self.transform_normalize = transforms.Normalize(mean=0.518, std=0.190)\n",
      "        self.transform_totensor = transforms.ToTensor()\n",
      "\n",
      "        self.ids = [os.path.join(dir, data_rank, mode, filename) for filename in os.listdir(os.path.join(dir, data_rank, mode))]\n",
      "        if not self.ids:\n",
      "            raise RuntimeError(f'No input file found in {os.path.join(dir, data_rank, mode)}, make sure you put your images there')\n",
      "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.ids)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        image = cv2.imread(self.ids[idx])\n",
      "        # print(image.shape)\n",
      "        \n",
      "        if self.mode == 'test':\n",
      "            if self.transform is not None:\n",
      "                image = self.transform(image=image)\n",
      "            return image.float().contiguous(), self.ids[idx]\n",
      "        \n",
      "        mask_path = self.ids[idx].replace(self.mode, \"label/\"+self.mode)\n",
      "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
      "        # print(mask)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            transformed = self.transform(image=image, mask=mask)\n",
      "            transformed_image = transformed['image']\n",
      "            transformed_mask = transformed['mask']\n",
      "        else:\n",
      "            transformed_image = image\n",
      "            transformed_mask = mask\n",
      "\n",
      "        transformed_image = self.transform_totensor(transformed_image)\n",
      "        transformed_image = self.transform_normalize(transformed_image)\n",
      "        transformed_mask = self.transform_totensor(transformed_mask)\n",
      "\n",
      "        # transformed_image = np.transpose(transformed_image, (2, 0, 1))\n",
      "        # transformed_mask = np.expand_dims(transformed_mask, axis=0)\n",
      "\n",
      "        return transformed_image, transformed_mask\n",
      "# Copyright (C) 2021 NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "#\n",
      "# This work is made available under the Nvidia Source Code License-NC.\n",
      "# To view a copy of this license, check out LICENSE.md\n",
      "import torch.nn as nn\n",
      "\n",
      "\n",
      "class FeatureMatchingLoss(nn.Module):\n",
      "    r\"\"\"Compute feature matching loss\"\"\"\n",
      "    def __init__(self, criterion='l1'):\n",
      "        super(FeatureMatchingLoss, self).__init__()\n",
      "        if criterion == 'l1':\n",
      "            self.criterion = nn.L1Loss()\n",
      "        elif criterion == 'l2' or criterion == 'mse':\n",
      "            self.criterion = nn.MSELoss()\n",
      "        else:\n",
      "            raise ValueError('Criterion %s is not recognized' % criterion)\n",
      "\n",
      "    def forward(self, fake_features, real_features):\n",
      "        r\"\"\"Return the target vector for the binary cross entropy loss\n",
      "        computation.\n",
      "\n",
      "        Args:\n",
      "           fake_features (list of lists): Discriminator features of fake images.\n",
      "           real_features (list of lists): Discriminator features of real images.\n",
      "\n",
      "        Returns:\n",
      "           (tensor): Loss value.\n",
      "        \"\"\"\n",
      "        num_d = len(fake_features)\n",
      "        dis_weight = 1.0 / num_d\n",
      "        loss = fake_features[0][0].new_tensor(0)\n",
      "        for i in range(num_d):\n",
      "            for j in range(len(fake_features[i])):\n",
      "                tmp_loss = self.criterion(fake_features[i][j],\n",
      "                                          real_features[i][j].detach())\n",
      "                loss += dis_weight * tmp_loss\n",
      "        return loss\n",
      "\n",
      "# Copyright 2017 Datera\n",
      "# All Rights Reserved.\n",
      "#\n",
      "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
      "#    not use this file except in compliance with the License. You may obtain\n",
      "#    a copy of the License at\n",
      "#\n",
      "#         http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "#    Unless required by applicable law or agreed to in writing, software\n",
      "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
      "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
      "#    License for the specific language governing permissions and limitations\n",
      "#    under the License.\n",
      "\n",
      "import time\n",
      "import uuid\n",
      "\n",
      "from eventlet.green import threading\n",
      "from oslo_config import cfg\n",
      "from oslo_log import log as logging\n",
      "import six\n",
      "\n",
      "from cinder import exception\n",
      "from cinder.i18n import _\n",
      "from cinder import utils\n",
      "from cinder.volume import configuration\n",
      "from cinder.volume.drivers.san import san\n",
      "\n",
      "import cinder.volume.drivers.datera.datera_api2 as api2\n",
      "import cinder.volume.drivers.datera.datera_api21 as api21\n",
      "import cinder.volume.drivers.datera.datera_common as datc\n",
      "\n",
      "\n",
      "LOG = logging.getLogger(__name__)\n",
      "\n",
      "d_opts = [\n",
      "    cfg.StrOpt('datera_api_port',\n",
      "               default='7717',\n",
      "               help='Datera API port.'),\n",
      "    cfg.StrOpt('datera_api_version',\n",
      "               default='2',\n",
      "               deprecated_for_removal=True,\n",
      "               help='Datera API version.'),\n",
      "    cfg.IntOpt('datera_503_timeout',\n",
      "               default='120',\n",
      "               help='Timeout for HTTP 503 retry messages'),\n",
      "    cfg.IntOpt('datera_503_interval',\n",
      "               default='5',\n",
      "               help='Interval between 503 retries'),\n",
      "    cfg.BoolOpt('datera_debug',\n",
      "                default=False,\n",
      "                help=\"True to set function arg and return logging\"),\n",
      "    cfg.BoolOpt('datera_debug_replica_count_override',\n",
      "                default=False,\n",
      "                help=\"ONLY FOR DEBUG/TESTING PURPOSES\\n\"\n",
      "                     \"True to set replica_count to 1\"),\n",
      "    cfg.StrOpt('datera_tenant_id',\n",
      "               default=None,\n",
      "               help=\"If set to 'Map' --> OpenStack project ID will be mapped \"\n",
      "                    \"implicitly to Datera tenant ID\\n\"\n",
      "                    \"If set to 'None' --> Datera tenant ID will not be used \"\n",
      "                    \"during volume provisioning\\n\"\n",
      "                    \"If set to anything else --> Datera tenant ID will be the \"\n",
      "                    \"provided value\"),\n",
      "    cfg.BoolOpt('datera_disable_profiler',\n",
      "                default=False,\n",
      "                help=\"Set to True to disable profiling in the Datera driver\"),\n",
      "]\n",
      "\n",
      "\n",
      "CONF = cfg.CONF\n",
      "CONF.import_opt('driver_use_ssl', 'cinder.volume.driver')\n",
      "CONF.register_opts(d_opts, group=configuration.SHARED_CONF_GROUP)\n",
      "\n",
      "\n",
      "@six.add_metaclass(utils.TraceWrapperWithABCMetaclass)\n",
      "class DateraDriver(san.SanISCSIDriver, api2.DateraApi, api21.DateraApi):\n",
      "\n",
      "    \"\"\"The OpenStack Datera Driver\n",
      "\n",
      "    Version history:\n",
      "        * 1.0 - Initial driver\n",
      "        * 1.1 - Look for lun-0 instead of lun-1.\n",
      "        * 2.0 - Update For Datera API v2\n",
      "        * 2.1 - Multipath, ACL and reorg\n",
      "        * 2.2 - Capabilites List, Extended Volume-Type Support\n",
      "                Naming convention change,\n",
      "                Volume Manage/Unmanage support\n",
      "        * 2.3 - Templates, Tenants, Snapshot Polling,\n",
      "                2.1 Api Version Support, Restructure\n",
      "        * 2.3.1 - Scalability bugfixes\n",
      "        * 2.3.2 - Volume Placement, ACL multi-attach bugfix\n",
      "        * 2.4.0 - Fast Retype Support\n",
      "    \"\"\"\n",
      "    VERSION = '2.4.0'\n",
      "\n",
      "    CI_WIKI_NAME = \"datera-ci\"\n",
      "\n",
      "    HEADER_DATA = {'Datera-Driver': 'OpenStack-Cinder-{}'.format(VERSION)}\n",
      "\n",
      "    # TODO(jsbryant) Remove driver in the 'U' release if CI is not fixed.\n",
      "    SUPPORTED = False\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super(DateraDriver, self).__init__(*args, **kwargs)\n",
      "        self.configuration.append_config_values(d_opts)\n",
      "        self.username = self.configuration.san_login\n",
      "        self.password = self.configuration.san_password\n",
      "        self.cluster_stats = {}\n",
      "        self.datera_api_token = None\n",
      "        self.interval = self.configuration.datera_503_interval\n",
      "        self.retry_attempts = (self.configuration.datera_503_timeout /\n",
      "                               self.interval)\n",
      "        self.driver_prefix = str(uuid.uuid4())[:4]\n",
      "        self.datera_debug = self.configuration.datera_debug\n",
      "        self.datera_api_versions = []\n",
      "\n",
      "        if self.datera_debug:\n",
      "            utils.setup_tracing(['method'])\n",
      "        self.tenant_id = self.configuration.datera_tenant_id\n",
      "        if self.tenant_id and self.tenant_id.lower() == 'none':\n",
      "            self.tenant_id = None\n",
      "        self.api_check = time.time()\n",
      "        self.api_cache = []\n",
      "        self.api_timeout = 0\n",
      "        self.do_profile = not self.configuration.datera_disable_profiler\n",
      "        self.thread_local = threading.local()\n",
      "\n",
      "        backend_name = self.configuration.safe_get(\n",
      "            'volume_backend_name')\n",
      "        self.backend_name = backend_name or 'Datera'\n",
      "\n",
      "        datc.register_driver(self)\n",
      "\n",
      "    def do_setup(self, context):\n",
      "        # If we can't authenticate through the old and new method, just fail\n",
      "        # now.\n",
      "        if not all([self.username, self.password]):\n",
      "            msg = _(\"san_login and/or san_password is not set for Datera \"\n",
      "                    \"driver in the cinder.conf. Set this information and \"\n",
      "                    \"start the cinder-volume service again.\")\n",
      "            LOG.error(msg)\n",
      "            raise exception.InvalidInput(msg)\n",
      "\n",
      "        self.login()\n",
      "        self._create_tenant()\n",
      "\n",
      "    # =================\n",
      "\n",
      "    # =================\n",
      "    # = Create Volume =\n",
      "    # =================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def create_volume(self, volume):\n",
      "        \"\"\"Create a logical volume.\"\"\"\n",
      "        pass\n",
      "\n",
      "    # =================\n",
      "    # = Extend Volume =\n",
      "    # =================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def extend_volume(self, volume, new_size):\n",
      "        pass\n",
      "\n",
      "    # =================\n",
      "\n",
      "    # =================\n",
      "    # = Cloned Volume =\n",
      "    # =================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def create_cloned_volume(self, volume, src_vref):\n",
      "        pass\n",
      "\n",
      "    # =================\n",
      "    # = Delete Volume =\n",
      "    # =================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def delete_volume(self, volume):\n",
      "        pass\n",
      "\n",
      "    # =================\n",
      "    # = Ensure Export =\n",
      "    # =================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def ensure_export(self, context, volume, connector=None):\n",
      "        \"\"\"Gets the associated account, retrieves CHAP info and updates.\"\"\"\n",
      "\n",
      "    # =========================\n",
      "    # = Initialize Connection =\n",
      "    # =========================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def initialize_connection(self, volume, connector):\n",
      "        pass\n",
      "\n",
      "    # =================\n",
      "    # = Create Export =\n",
      "    # =================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def create_export(self, context, volume, connector):\n",
      "        pass\n",
      "\n",
      "    # =================\n",
      "    # = Detach Volume =\n",
      "    # =================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def detach_volume(self, context, volume, attachment=None):\n",
      "        pass\n",
      "\n",
      "    # ===================\n",
      "    # = Create Snapshot =\n",
      "    # ===================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def create_snapshot(self, snapshot):\n",
      "        pass\n",
      "\n",
      "    # ===================\n",
      "    # = Delete Snapshot =\n",
      "    # ===================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def delete_snapshot(self, snapshot):\n",
      "        pass\n",
      "\n",
      "    # ========================\n",
      "    # = Volume From Snapshot =\n",
      "    # ========================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def create_volume_from_snapshot(self, volume, snapshot):\n",
      "        pass\n",
      "\n",
      "    # ==========\n",
      "    # = Retype =\n",
      "    # ==========\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def retype(self, ctxt, volume, new_type, diff, host):\n",
      "        \"\"\"Convert the volume to be of the new type.\n",
      "\n",
      "        Returns a boolean indicating whether the retype occurred.\n",
      "\n",
      "        :param ctxt: Context\n",
      "        :param volume: A dictionary describing the volume to migrate\n",
      "        :param new_type: A dictionary describing the volume type to convert to\n",
      "        :param diff: A dictionary with the difference between the two types\n",
      "        :param host: A dictionary describing the host to migrate to, where\n",
      "                     host['host'] is its name, and host['capabilities'] is a\n",
      "                     dictionary of its reported capabilities (Not Used).\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    # ==========\n",
      "    # = Manage =\n",
      "    # ==========\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def manage_existing(self, volume, existing_ref):\n",
      "        \"\"\"Manage an existing volume on the Datera backend\n",
      "\n",
      "        The existing_ref must be either the current name or Datera UUID of\n",
      "        an app_instance on the Datera backend in a colon separated list with\n",
      "        the storage instance name and volume name.  This means only\n",
      "        single storage instances and single volumes are supported for\n",
      "        managing by cinder.\n",
      "\n",
      "        Eg.\n",
      "\n",
      "        (existing_ref['source-name'] ==\n",
      "             tenant:app_inst_name:storage_inst_name:vol_name)\n",
      "\n",
      "        if using Datera 2.1 API\n",
      "\n",
      "        or\n",
      "\n",
      "        (existing_ref['source-name'] ==\n",
      "             app_inst_name:storage_inst_name:vol_name)\n",
      "\n",
      "        if using 2.0 API\n",
      "\n",
      "        :param volume:       Cinder volume to manage\n",
      "        :param existing_ref: Driver-specific information used to identify a\n",
      "                             volume\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    # ===================\n",
      "    # = Manage Get Size =\n",
      "    # ===================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def manage_existing_get_size(self, volume, existing_ref):\n",
      "        \"\"\"Get the size of an unmanaged volume on the Datera backend\n",
      "\n",
      "        The existing_ref must be either the current name or Datera UUID of\n",
      "        an app_instance on the Datera backend in a colon separated list with\n",
      "        the storage instance name and volume name.  This means only\n",
      "        single storage instances and single volumes are supported for\n",
      "        managing by cinder.\n",
      "\n",
      "        Eg.\n",
      "\n",
      "        existing_ref == app_inst_name:storage_inst_name:vol_name\n",
      "\n",
      "        :param volume:       Cinder volume to manage\n",
      "        :param existing_ref: Driver-specific information used to identify a\n",
      "                             volume on the Datera backend\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    # =========================\n",
      "    # = Get Manageable Volume =\n",
      "    # =========================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def get_manageable_volumes(self, cinder_volumes, marker, limit, offset,\n",
      "                               sort_keys, sort_dirs):\n",
      "        \"\"\"List volumes on the backend available for management by Cinder.\n",
      "\n",
      "        Returns a list of dictionaries, each specifying a volume in the host,\n",
      "        with the following keys:\n",
      "\n",
      "        - reference (dictionary): The reference for a volume, which can be\n",
      "          passed to 'manage_existing'.\n",
      "        - size (int): The size of the volume according to the storage\n",
      "          backend, rounded up to the nearest GB.\n",
      "        - safe_to_manage (boolean): Whether or not this volume is safe to\n",
      "          manage according to the storage backend. For example, is the volume\n",
      "          in use or invalid for any reason.\n",
      "        - reason_not_safe (string): If safe_to_manage is False, the reason why.\n",
      "        - cinder_id (string): If already managed, provide the Cinder ID.\n",
      "        - extra_info (string): Any extra information to return to the user\n",
      "\n",
      "        :param cinder_volumes: A list of volumes in this host that Cinder\n",
      "                               currently manages, used to determine if\n",
      "                               a volume is manageable or not.\n",
      "        :param marker:    The last item of the previous page; we return the\n",
      "                          next results after this value (after sorting)\n",
      "        :param limit:     Maximum number of items to return\n",
      "        :param offset:    Number of items to skip after marker\n",
      "        :param sort_keys: List of keys to sort results by (valid keys are\n",
      "                          'identifier' and 'size')\n",
      "        :param sort_dirs: List of directions to sort by, corresponding to\n",
      "                          sort_keys (valid directions are 'asc' and 'desc')\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    # ============\n",
      "    # = Unmanage =\n",
      "    # ============\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def unmanage(self, volume):\n",
      "        \"\"\"Unmanage a currently managed volume in Cinder\n",
      "\n",
      "        :param volume:       Cinder volume to unmanage\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    # ================\n",
      "    # = Volume Stats =\n",
      "    # ================\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def get_volume_stats(self, refresh=False):\n",
      "        \"\"\"Get volume stats.\n",
      "\n",
      "        If 'refresh' is True, run update first.\n",
      "        The name is a bit misleading as\n",
      "        the majority of the data here is cluster\n",
      "        data.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    # =========\n",
      "    # = Login =\n",
      "    # =========\n",
      "\n",
      "    @datc._api_lookup\n",
      "    def login(self):\n",
      "        pass\n",
      "\n",
      "    # =======\n",
      "    # = QoS =\n",
      "    # =======\n",
      "\n",
      "    def _update_qos(self, resource, policies):\n",
      "        url = datc.URL_TEMPLATES['vol_inst'](\n",
      "            policies['default_storage_name'],\n",
      "            policies['default_volume_name']) + '/performance_policy'\n",
      "        url = url.format(datc._get_name(resource['id']))\n",
      "        type_id = resource.get('volume_type_id', None)\n",
      "        if type_id is not None:\n",
      "            # Filter for just QOS policies in result. All of their keys\n",
      "            # should end with \"max\"\n",
      "            fpolicies = {k: int(v) for k, v in\n",
      "                         policies.items() if k.endswith(\"max\")}\n",
      "            # Filter all 0 values from being passed\n",
      "            fpolicies = dict(filter(lambda _v: _v[1] > 0, fpolicies.items()))\n",
      "            if fpolicies:\n",
      "                self._issue_api_request(url, 'post', body=fpolicies,\n",
      "                                        api_version='2')\n",
      "\n",
      "    def _get_lunid(self):\n",
      "        return 0\n",
      "\n",
      "    # ============================\n",
      "    # = Volume-Types/Extra-Specs =\n",
      "    # ============================\n",
      "\n",
      "    def _init_vendor_properties(self):\n",
      "        \"\"\"Create a dictionary of vendor unique properties.\n",
      "\n",
      "        This method creates a dictionary of vendor unique properties\n",
      "        and returns both created dictionary and vendor name.\n",
      "        Returned vendor name is used to check for name of vendor\n",
      "        unique properties.\n",
      "\n",
      "        - Vendor name shouldn't include colon(:) because of the separator\n",
      "          and it is automatically replaced by underscore(_).\n",
      "          ex. abc:d -> abc_d\n",
      "        - Vendor prefix is equal to vendor name.\n",
      "          ex. abcd\n",
      "        - Vendor unique properties must start with vendor prefix + ':'.\n",
      "          ex. abcd:maxIOPS\n",
      "\n",
      "        Each backend driver needs to override this method to expose\n",
      "        its own properties using _set_property() like this:\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"vendorPrefix:specific_property\",\n",
      "            \"Title of property\",\n",
      "            _(\"Description of property\"),\n",
      "            \"type\")\n",
      "\n",
      "        : return dictionary of vendor unique properties\n",
      "        : return vendor name\n",
      "\n",
      "        prefix: DF --> Datera Fabric\n",
      "        \"\"\"\n",
      "\n",
      "        properties = {}\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:placement_mode\",\n",
      "            \"Datera Volume Placement\",\n",
      "            _(\"'single_flash' for single-flash-replica placement, \"\n",
      "              \"'all_flash' for all-flash-replica placement, \"\n",
      "              \"'hybrid' for hybrid placement\"),\n",
      "            \"string\",\n",
      "            default=\"hybrid\")\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:round_robin\",\n",
      "            \"Datera Round Robin Portals\",\n",
      "            _(\"True to round robin the provided portals for a target\"),\n",
      "            \"boolean\",\n",
      "            default=False)\n",
      "\n",
      "        if self.configuration.get('datera_debug_replica_count_override'):\n",
      "            replica_count = 1\n",
      "        else:\n",
      "            replica_count = 3\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:replica_count\",\n",
      "            \"Datera Volume Replica Count\",\n",
      "            _(\"Specifies number of replicas for each volume. Can only be \"\n",
      "              \"increased once volume is created\"),\n",
      "            \"integer\",\n",
      "            minimum=1,\n",
      "            default=replica_count)\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:acl_allow_all\",\n",
      "            \"Datera ACL Allow All\",\n",
      "            _(\"True to set acl 'allow_all' on volumes created.  Cannot be \"\n",
      "              \"changed on volume once set\"),\n",
      "            \"boolean\",\n",
      "            default=False)\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:ip_pool\",\n",
      "            \"Datera IP Pool\",\n",
      "            _(\"Specifies IP pool to use for volume\"),\n",
      "            \"string\",\n",
      "            default=\"default\")\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:template\",\n",
      "            \"Datera Template\",\n",
      "            _(\"Specifies Template to use for volume provisioning\"),\n",
      "            \"string\",\n",
      "            default=\"\")\n",
      "\n",
      "        # ###### QoS Settings ###### #\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:read_bandwidth_max\",\n",
      "            \"Datera QoS Max Bandwidth Read\",\n",
      "            _(\"Max read bandwidth setting for volume qos, \"\n",
      "              \"use 0 for unlimited\"),\n",
      "            \"integer\",\n",
      "            minimum=0,\n",
      "            default=0)\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:default_storage_name\",\n",
      "            \"Datera Default Storage Instance Name\",\n",
      "            _(\"The name to use for storage instances created\"),\n",
      "            \"string\",\n",
      "            default=\"storage-1\")\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:default_volume_name\",\n",
      "            \"Datera Default Volume Name\",\n",
      "            _(\"The name to use for volumes created\"),\n",
      "            \"string\",\n",
      "            default=\"volume-1\")\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:write_bandwidth_max\",\n",
      "            \"Datera QoS Max Bandwidth Write\",\n",
      "            _(\"Max write bandwidth setting for volume qos, \"\n",
      "              \"use 0 for unlimited\"),\n",
      "            \"integer\",\n",
      "            minimum=0,\n",
      "            default=0)\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:total_bandwidth_max\",\n",
      "            \"Datera QoS Max Bandwidth Total\",\n",
      "            _(\"Max total bandwidth setting for volume qos, \"\n",
      "              \"use 0 for unlimited\"),\n",
      "            \"integer\",\n",
      "            minimum=0,\n",
      "            default=0)\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:read_iops_max\",\n",
      "            \"Datera QoS Max iops Read\",\n",
      "            _(\"Max read iops setting for volume qos, \"\n",
      "              \"use 0 for unlimited\"),\n",
      "            \"integer\",\n",
      "            minimum=0,\n",
      "            default=0)\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:write_iops_max\",\n",
      "            \"Datera QoS Max IOPS Write\",\n",
      "            _(\"Max write iops setting for volume qos, \"\n",
      "              \"use 0 for unlimited\"),\n",
      "            \"integer\",\n",
      "            minimum=0,\n",
      "            default=0)\n",
      "\n",
      "        self._set_property(\n",
      "            properties,\n",
      "            \"DF:total_iops_max\",\n",
      "            \"Datera QoS Max IOPS Total\",\n",
      "            _(\"Max total iops setting for volume qos, \"\n",
      "              \"use 0 for unlimited\"),\n",
      "            \"integer\",\n",
      "            minimum=0,\n",
      "            default=0)\n",
      "        # ###### End QoS Settings ###### #\n",
      "\n",
      "        return properties, 'DF'\n",
      "\n",
      "#!/usr/bin/env python\n",
      "\"\"\"\n",
      "Author: Alexander David Leech\n",
      "Date:   30/09/2015\n",
      "Rev:    2\n",
      "Lang:   Python 2.7\n",
      "Deps:\tPyserial, Pymodbus, logging\n",
      "\"\"\"\n",
      "\n",
      "import time                                            # For sleep functionality\n",
      "import logging                                         # For detailed error output\n",
      "from pymodbus.client.sync import ModbusSerialClient \\\n",
      "as ModbusClient                                        # Import MODBUS support class\n",
      "\n",
      "comSettings = {    \n",
      "                \"method\"   : 'rtu',\n",
      "                \"port\"     : 'COM3',\n",
      "                \"stopbits\" : 1,                \n",
      "                \"bytesize\" : 8,                \n",
      "                \"parity\"   : 'N',\n",
      "                \"baudrate\" : 9600,\n",
      "                \"timeout\"  : 1\n",
      "              }\n",
      "\n",
      "logging.basicConfig()                                   # Setup error logging\n",
      "log = logging.getLogger()                               # Start logging\n",
      "\n",
      "client = ModbusClient(**comSettings)                    # Setup connection object\n",
      "client.connect()                                        # Open the MODBUS connection\n",
      "\n",
      "while(True):\n",
      "    client.write_register(3,1000,unit=0x01)             # Write valve to 100%\n",
      "    time.sleep(4)                                       # Sleep 4 seconds\n",
      "    client.write_register(3,0,unit=0x01)                # Write valve to 0%\n",
      "    time.sleep(4)                                       # Sleep 4 seconds\n",
      "\n",
      "client.close()                                          # Close the connection\n",
      "import os\n",
      "import subprocess\n",
      "from unittest import mock\n",
      "\n",
      "import pytest\n",
      "from pre_commit.constants import VERSION as PRE_COMMIT_VERSION\n",
      "\n",
      "import testing.git\n",
      "from all_repos import autofix_lib\n",
      "from all_repos import clone\n",
      "from all_repos import git\n",
      "from all_repos.config import load_config\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\n",
      "    ('cli_repos', 'expected'),\n",
      "    (\n",
      "        (None, ['found_repo']),\n",
      "        ([], []),\n",
      "        (['cli_repo'], ['cli_repo']),\n",
      "    ),\n",
      ")\n",
      "def test_filter_repos(file_config, cli_repos, expected):\n",
      "    ret = autofix_lib.filter_repos(\n",
      "        file_config, cli_repos, lambda _: ['found_repo'],\n",
      "    )\n",
      "    assert ret == expected\n",
      "\n",
      "\n",
      "def test_assert_importable_is_importable():\n",
      "    autofix_lib.assert_importable('pre_commit', install='pre-commit')\n",
      "\n",
      "\n",
      "def test_assert_importable_not_importable():\n",
      "    with pytest.raises(SystemExit) as excinfo:\n",
      "        autofix_lib.assert_importable('watmodule', install='wat')\n",
      "    msg, = excinfo.value.args\n",
      "    assert msg == (\n",
      "        'This tool requires the `watmodule` module to be installed.\\n'\n",
      "        'Try installing it via `pip install wat`.'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_require_version_new_enough():\n",
      "    autofix_lib.require_version_gte('pre-commit', '0.17.0')\n",
      "\n",
      "\n",
      "def test_require_version_not_new_enough():\n",
      "    with pytest.raises(SystemExit) as excinfo:\n",
      "        autofix_lib.require_version_gte('pre-commit', '999')\n",
      "    msg, = excinfo.value.args\n",
      "    assert msg == (\n",
      "        f'This tool requires the `pre-commit` package is at least version '\n",
      "        f'999.  The currently installed version is {PRE_COMMIT_VERSION}.\\n\\n'\n",
      "        f'Try `pip install --upgrade pre-commit`'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_run(capfd):\n",
      "    autofix_lib.run('echo', 'h\"i')\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == (\n",
      "        '$ echo \\'h\"i\\'\\n'\n",
      "        'h\"i\\n'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_cwd(tmpdir):\n",
      "    orig = os.getcwd()\n",
      "    with autofix_lib.cwd(tmpdir):\n",
      "        assert os.getcwd() == tmpdir\n",
      "    assert os.getcwd() == orig\n",
      "\n",
      "\n",
      "def test_repo_context_success(file_config_files, capsys):\n",
      "    expected_rev = testing.git.revparse(file_config_files.dir1)\n",
      "    with autofix_lib.repo_context(\n",
      "            str(file_config_files.output_dir.join('repo1')), use_color=False,\n",
      "    ):\n",
      "        assert testing.git.revparse('.') == expected_rev\n",
      "        assert git.remote('.') == file_config_files.dir1\n",
      "    out, err = capsys.readouterr()\n",
      "    assert err == ''\n",
      "    assert 'Errored' not in out\n",
      "\n",
      "\n",
      "def test_repo_context_errors(file_config_files, capsys):\n",
      "    with autofix_lib.repo_context(\n",
      "            str(file_config_files.output_dir.join('repo1')), use_color=False,\n",
      "    ):\n",
      "        assert False\n",
      "    out, err = capsys.readouterr()\n",
      "    assert 'Errored' in out\n",
      "    assert 'assert False' in err\n",
      "\n",
      "\n",
      "def test_interactive_control_c(mock_input, capfd):\n",
      "    mock_input.set_side_effect(KeyboardInterrupt)\n",
      "    with pytest.raises(SystemExit):\n",
      "        autofix_lib._interactive_check(use_color=False)\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == (\n",
      "        '***Looks good [y,n,s,q,?]? ^C\\n'\n",
      "        'Goodbye!\\n'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_interactive_eof(mock_input, capfd):\n",
      "    mock_input.set_side_effect(EOFError)\n",
      "    with pytest.raises(SystemExit):\n",
      "        autofix_lib._interactive_check(use_color=False)\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == (\n",
      "        '***Looks good [y,n,s,q,?]? ^D\\n'\n",
      "        'Goodbye!\\n'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_interactive_quit(mock_input, capfd):\n",
      "    mock_input.set_side_effect('q')\n",
      "    with pytest.raises(SystemExit):\n",
      "        autofix_lib._interactive_check(use_color=False)\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == (\n",
      "        '***Looks good [y,n,s,q,?]? <<q\\n'\n",
      "        'Goodbye!\\n'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_interactive_yes(mock_input, capfd):\n",
      "    mock_input.set_side_effect('y')\n",
      "    assert autofix_lib._interactive_check(use_color=False) is True\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == '***Looks good [y,n,s,q,?]? <<y\\n'\n",
      "\n",
      "\n",
      "def test_interactive_no(mock_input, capfd):\n",
      "    mock_input.set_side_effect('n')\n",
      "    assert autofix_lib._interactive_check(use_color=False) is False\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == '***Looks good [y,n,s,q,?]? <<n\\n'\n",
      "\n",
      "\n",
      "def test_interactive_shell(mock_input, capfd):\n",
      "    mock_input.set_side_effect('s', 'n')\n",
      "    with mock.patch.dict(os.environ, {'SHELL': 'echo'}):\n",
      "        assert autofix_lib._interactive_check(use_color=False) is False\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == (\n",
      "        '***Looks good [y,n,s,q,?]? <<s\\n'\n",
      "        'Opening an interactive shell, type `exit` to continue.\\n'\n",
      "        'Any modifications will be committed.\\n'\n",
      "        # A newline from echo\n",
      "        '\\n'\n",
      "        '***Looks good [y,n,s,q,?]? <<n\\n'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_interactive_help(mock_input, capfd):\n",
      "    mock_input.set_side_effect('?', 'n')\n",
      "    assert autofix_lib._interactive_check(use_color=False) is False\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == (\n",
      "        '***Looks good [y,n,s,q,?]? <<?\\n'\n",
      "        'y (yes): yes it looks good, commit and continue.\\n'\n",
      "        'n (no): no, do not commit this repository.\\n'\n",
      "        's (shell): open an interactive shell in the repo.\\n'\n",
      "        'q (quit, ^C): early exit from the autofixer.\\n'\n",
      "        '? (help): show this help message.\\n'\n",
      "        '***Looks good [y,n,s,q,?]? <<n\\n'\n",
      "    )\n",
      "\n",
      "\n",
      "def test_interactive_garbage(mock_input, capfd):\n",
      "    mock_input.set_side_effect('garbage', 'n')\n",
      "    assert autofix_lib._interactive_check(use_color=False) is False\n",
      "    out, _ = capfd.readouterr()\n",
      "    assert out == (\n",
      "        '***Looks good [y,n,s,q,?]? <<garbage\\n'\n",
      "        'Unexpected input: garbage\\n'\n",
      "        'y (yes): yes it looks good, commit and continue.\\n'\n",
      "        'n (no): no, do not commit this repository.\\n'\n",
      "        's (shell): open an interactive shell in the repo.\\n'\n",
      "        'q (quit, ^C): early exit from the autofixer.\\n'\n",
      "        '? (help): show this help message.\\n'\n",
      "        '***Looks good [y,n,s,q,?]? <<n\\n'\n",
      "    )\n",
      "\n",
      "\n",
      "def lower_case_f():\n",
      "    f_contents = open('f').read()\n",
      "    with open('f', 'w') as f:\n",
      "        f.write(f_contents.lower())\n",
      "\n",
      "\n",
      "def failing_check_fix():\n",
      "    raise AssertionError('nope!')\n",
      "\n",
      "\n",
      "def test_fix_dry_run_no_change(file_config_files, capfd):\n",
      "    autofix_lib.fix(\n",
      "        (\n",
      "            str(file_config_files.output_dir.join('repo1')),\n",
      "            str(file_config_files.output_dir.join('repo2')),\n",
      "        ),\n",
      "        apply_fix=lower_case_f,\n",
      "        config=load_config(file_config_files.cfg),\n",
      "        commit=autofix_lib.Commit('message!', 'test-branch', None),\n",
      "        autofix_settings=autofix_lib.AutofixSettings(\n",
      "            jobs=1, color=False, limit=None, dry_run=True, interactive=False,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    out, err = capfd.readouterr()\n",
      "    assert err == ''\n",
      "    assert 'Errored' not in out\n",
      "    # Showed the diff of what would have happened\n",
      "    assert '-OHAI\\n+ohai\\n' in out\n",
      "    assert '-OHELLO\\n+ohello\\n' in out\n",
      "\n",
      "    # Didn't actually perform any changes\n",
      "    assert file_config_files.dir1.join('f').read() == 'OHAI\\n'\n",
      "    assert file_config_files.dir2.join('f').read() == 'OHELLO\\n'\n",
      "\n",
      "\n",
      "def test_fix_with_limit(file_config_files, capfd):\n",
      "    autofix_lib.fix(\n",
      "        (\n",
      "            str(file_config_files.output_dir.join('repo1')),\n",
      "            str(file_config_files.output_dir.join('repo2')),\n",
      "        ),\n",
      "        apply_fix=lower_case_f,\n",
      "        config=load_config(file_config_files.cfg),\n",
      "        commit=autofix_lib.Commit('message!', 'test-branch', None),\n",
      "        autofix_settings=autofix_lib.AutofixSettings(\n",
      "            jobs=1, color=False, limit=1, dry_run=True, interactive=False,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    out, err = capfd.readouterr()\n",
      "    assert err == ''\n",
      "    assert 'Errored' not in out\n",
      "    # Should still see the diff from the first repository\n",
      "    assert '-OHAI\\n+ohai\\n' in out\n",
      "    assert '-OHELLO\\n+ohello\\n' not in out\n",
      "\n",
      "\n",
      "def test_fix_interactive(file_config_files, capfd, mock_input):\n",
      "    mock_input.set_side_effect('y', 'n')\n",
      "    autofix_lib.fix(\n",
      "        (\n",
      "            str(file_config_files.output_dir.join('repo1')),\n",
      "            str(file_config_files.output_dir.join('repo2')),\n",
      "        ),\n",
      "        apply_fix=lower_case_f,\n",
      "        config=load_config(file_config_files.cfg),\n",
      "        commit=autofix_lib.Commit('message!', 'test-branch', None),\n",
      "        autofix_settings=autofix_lib.AutofixSettings(\n",
      "            jobs=1, color=False, limit=None, dry_run=False, interactive=True,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    assert file_config_files.dir1.join('f').read() == 'ohai\\n'\n",
      "    assert file_config_files.dir2.join('f').read() == 'OHELLO\\n'\n",
      "\n",
      "\n",
      "def test_autofix_makes_commits(file_config_files, capfd):\n",
      "    autofix_lib.fix(\n",
      "        (\n",
      "            str(file_config_files.output_dir.join('repo1')),\n",
      "            str(file_config_files.output_dir.join('repo2')),\n",
      "        ),\n",
      "        apply_fix=lower_case_f,\n",
      "        config=load_config(file_config_files.cfg),\n",
      "        commit=autofix_lib.Commit('message!', 'test-branch', 'A B <a@a.a>'),\n",
      "        autofix_settings=autofix_lib.AutofixSettings(\n",
      "            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    out, err = capfd.readouterr()\n",
      "    assert err == ''\n",
      "    assert 'Errored' not in out\n",
      "\n",
      "    assert file_config_files.dir1.join('f').read() == 'ohai\\n'\n",
      "    assert file_config_files.dir2.join('f').read() == 'ohello\\n'\n",
      "\n",
      "    # The branch name should be what we specified\n",
      "    last_commit_msg = subprocess.check_output((\n",
      "        'git', '-C', file_config_files.dir1, 'log',\n",
      "        '--format=%s', '--first-parent', '-1',\n",
      "    )).decode()\n",
      "    assert last_commit_msg == \"Merge branch 'all-repos_autofix_test-branch'\\n\"\n",
      "\n",
      "    # We should see a commit from the autofix change we made\n",
      "    commit = subprocess.check_output((\n",
      "        'git', '-C', file_config_files.dir1, 'log',\n",
      "        '--patch', '--grep', 'message!', '--format=%an %ae\\n%B',\n",
      "    )).decode()\n",
      "    assert commit.startswith(\n",
      "        'A B a@a.a\\n'\n",
      "        'message!\\n'\n",
      "        '\\n'\n",
      "        'Committed via https://github.com/asottile/all-repos\\n',\n",
      "    )\n",
      "    assert commit.endswith('-OHAI\\n+ohai\\n')\n",
      "\n",
      "\n",
      "def test_fix_failing_check_no_changes(file_config_files, capfd):\n",
      "    autofix_lib.fix(\n",
      "        (\n",
      "            str(file_config_files.output_dir.join('repo1')),\n",
      "            str(file_config_files.output_dir.join('repo2')),\n",
      "        ),\n",
      "        apply_fix=lower_case_f,\n",
      "        check_fix=failing_check_fix,\n",
      "        config=load_config(file_config_files.cfg),\n",
      "        commit=autofix_lib.Commit('message!', 'test-branch', None),\n",
      "        autofix_settings=autofix_lib.AutofixSettings(\n",
      "            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    out, err = capfd.readouterr()\n",
      "    assert 'nope!' in err\n",
      "    assert out.count('Errored') == 2\n",
      "\n",
      "    # An error while checking should not allow the changes\n",
      "    assert file_config_files.dir1.join('f').read() == 'OHAI\\n'\n",
      "    assert file_config_files.dir2.join('f').read() == 'OHELLO\\n'\n",
      "\n",
      "\n",
      "def test_noop_does_not_commit(file_config_files):\n",
      "    rev_before1 = testing.git.revparse(file_config_files.dir1)\n",
      "    rev_before2 = testing.git.revparse(file_config_files.dir2)\n",
      "    autofix_lib.fix(\n",
      "        (\n",
      "            str(file_config_files.output_dir.join('repo1')),\n",
      "            str(file_config_files.output_dir.join('repo2')),\n",
      "        ),\n",
      "        apply_fix=lambda: None,\n",
      "        config=load_config(file_config_files.cfg),\n",
      "        commit=autofix_lib.Commit('message!', 'test-branch', None),\n",
      "        autofix_settings=autofix_lib.AutofixSettings(\n",
      "            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n",
      "        ),\n",
      "    )\n",
      "    rev_after1 = testing.git.revparse(file_config_files.dir1)\n",
      "    rev_after2 = testing.git.revparse(file_config_files.dir2)\n",
      "    assert (rev_before1, rev_before2) == (rev_after1, rev_after2)\n",
      "\n",
      "\n",
      "def test_fix_non_default_branch(file_config_non_default):\n",
      "    clone.main(('--config-filename', str(file_config_non_default.cfg)))\n",
      "\n",
      "    autofix_lib.fix(\n",
      "        (\n",
      "            str(file_config_non_default.output_dir.join('repo1')),\n",
      "        ),\n",
      "        apply_fix=lower_case_f,\n",
      "        config=load_config(file_config_non_default.cfg),\n",
      "        commit=autofix_lib.Commit('message!', 'test-branch', 'A B <a@a.a>'),\n",
      "        autofix_settings=autofix_lib.AutofixSettings(\n",
      "            jobs=1, color=False, limit=None, dry_run=False, interactive=False,\n",
      "        ),\n",
      "    )\n",
      "\n",
      "    assert file_config_non_default.dir1.join('f').read() == 'ohai\\n'\n",
      "\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "    Generate random usernames in\n",
      "\"\"\"\n",
      "\n",
      "import random\n",
      "\n",
      "from .names import names as default_names\n",
      "\n",
      "\n",
      "class NameGenerator(object):\n",
      "\n",
      "    def __init__(self, names=None):\n",
      "        self.names = names or default_names\n",
      "\n",
      "\n",
      "    def __call__(self):\n",
      "        return self.names.pop(random.randrange(len(self.names)))\n",
      "\n",
      "    def __iter__(self):\n",
      "        while self.names:\n",
      "            yield self()\n",
      "\n",
      "from django.utils import timezone\n",
      "from rest_framework.authtoken.models import Token\n",
      "\n",
      "\n",
      "class AuthTokenHandler:\n",
      "    \"\"\"\n",
      "    Handles variations in auth token\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def expired_token(auth_token):\n",
      "        \"\"\"\n",
      "        Checks expiry of auth token\n",
      "        \"\"\"\n",
      "        utc_now = timezone.now()\n",
      "        expired = auth_token.created < utc_now - \\\n",
      "            timezone.timedelta(hours=24)\n",
      "        return expired\n",
      "\n",
      "    @staticmethod\n",
      "    def create_auth_token(user):\n",
      "        \"\"\"\n",
      "        Creates an auth token for a user\n",
      "        \"\"\"\n",
      "        token, created = Token.objects.get_or_create(user=user)\n",
      "        if not created:\n",
      "            token.created = timezone.now()\n",
      "            token.save()\n",
      "        return token\n",
      "\n",
      "\n",
      "\n",
      "from .useful_functions import get_ngrams, words_to_ngrams_list, remove_hook_words, remove_words\n",
      "\n",
      "from .transformers import phrases_transform, phrases2lower, phrases_without_excess_symbols\n",
      "\n",
      "from .tokenizers import text2sentences, split_by_words, sentence_split\n",
      "\n",
      "from .stemlem_operators import create_stemmer_lemmer, create_stemmer, create_lemmatizer\n",
      "\n",
      "from .pipeline import StemLemPipeline\n",
      "\n",
      "from .simplifiers import sum_phrases, wordlist2set\n",
      "\n",
      "from .stopwords import stopwords\n",
      "\n",
      "from .metrics import Levenstein\n",
      "\n",
      "\n",
      "\n",
      "# internal imports\n",
      "import dependency_checker\n",
      "import dependency_installer\n",
      "import dependency_updater\n",
      "import logger\n",
      "from rendering import VortexWindow\n",
      "\n",
      "# external imports\n",
      "import pyglet\n",
      "import sys\n",
      "\n",
      "# check if python version is too old. If it is, exit.\n",
      "if sys.version_info < (3, 6):  # if python version is less than 3.6\n",
      "    logger.critical(\n",
      "        \"Vortex\", \"Python version is too old. Please use python 3.6 or higher.\")\n",
      "    sys.exit(1)\n",
      "\n",
      "# check all deps and update them if needed\n",
      "if not dependency_checker.check_deps():  # if any deps are missing\n",
      "    dependency_installer.install_deps()  # install them\n",
      "    if not dependency_checker.check_deps():  # if any deps are still missing\n",
      "        # warn user and exit\n",
      "        logger.warn(\n",
      "            \"Vortex\", \"Dependencies are not installed. Please install them manually.\")\n",
      "        sys.exit(1)\n",
      "else:\n",
      "    dependency_updater.update_deps()  # update deps\n",
      "\n",
      "window = VortexWindow()  # create the window\n",
      "pyglet.app.run()  # run the app\n",
      "\n",
      "\"\"\"\n",
      "Intergation of the pytorch_transformers openai and gpt2 modules.\n",
      "\n",
      "Note that these objects are only to be used to load\n",
      "pretrained models. The pytorch-transformers library\n",
      "wasn't designed to train these models from scratch.\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "import pytorch_transformers as pt\n",
      "\n",
      "from flambe.nlp.transformers.utils import TransformerTextField, TransformerEmbedder\n",
      "\n",
      "\n",
      "class GPTTextField(TransformerTextField):\n",
      "    \"\"\"Integrate the pytorch_transformers OpenAIGPTTokenizer.\n",
      "\n",
      "    Currently available aliases:\n",
      "        . `openai-gpt`\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    _cls = pt.OpenAIGPTTokenizer\n",
      "\n",
      "\n",
      "class GPTEmbedder(TransformerEmbedder):\n",
      "    \"\"\"Integrate the pytorch_transformers OpenAIGPTmodel.\n",
      "\n",
      "    Currently available aliases:\n",
      "        . `openai-gpt`\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    _cls = pt.OpenAIGPTModel\n",
      "\n",
      "\n",
      "class GPT2TextField(TransformerTextField):\n",
      "    \"\"\"Integrate the pytorch_transformers GPT2Tokenizer.\n",
      "\n",
      "    Currently available aliases:\n",
      "        . `gpt2`\n",
      "        . `gpt2-medium`\n",
      "        . `gpt2-large`\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    _cls = pt.GPT2Tokenizer\n",
      "\n",
      "\n",
      "class GPT2Embedder(TransformerEmbedder):\n",
      "    \"\"\"Integrate the pytorch_transformers GPT2Model.\n",
      "\n",
      "    Currently available aliases:\n",
      "        . `gpt2`\n",
      "        . `gpt2-medium`\n",
      "        . `gpt2-large`\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    _cls = pt.GPT2Model\n",
      "\n",
      "#\n",
      "# Copyright 2018 Analytics Zoo Authors.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "#\n",
      "\n",
      "from tensorflow.keras.models import Model\n",
      "from tensorflow.keras.layers import Input, LSTM, Dense\n",
      "import tensorflow.keras as keras\n",
      "\n",
      "from zoo.automl.model.abstract import BaseModel\n",
      "from zoo.automl.common.util import *\n",
      "from zoo.automl.common.metrics import Evaluator\n",
      "\n",
      "\n",
      "class LSTMSeq2Seq(BaseModel):\n",
      "\n",
      "    def __init__(self, check_optional_config=True, future_seq_len=2):\n",
      "        \"\"\"\n",
      "        Constructor of LSTM Seq2Seq model\n",
      "        \"\"\"\n",
      "        self.model = None\n",
      "        self.past_seq_len = None\n",
      "        self.future_seq_len = future_seq_len\n",
      "        self.feature_num = None\n",
      "        self.target_col_num = None\n",
      "        self.metric = None\n",
      "        self.latent_dim = None\n",
      "        self.batch_size = None\n",
      "        self.check_optional_config = check_optional_config\n",
      "\n",
      "    def _build_train(self, mc=False, **config):\n",
      "        \"\"\"\n",
      "        build LSTM Seq2Seq model\n",
      "        :param config:\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        super()._check_config(**config)\n",
      "        self.metric = config.get('metric', 'mean_squared_error')\n",
      "        self.latent_dim = config.get('latent_dim', 128)\n",
      "        self.dropout = config.get('dropout', 0.2)\n",
      "        self.lr = config.get('lr', 0.001)\n",
      "        # for restore in continuous training\n",
      "        self.batch_size = config.get('batch_size', 64)\n",
      "        training = True if mc else None\n",
      "\n",
      "        # Define an input sequence and process it.\n",
      "        self.encoder_inputs = Input(shape=(None, self.feature_num), name=\"encoder_inputs\")\n",
      "        encoder = LSTM(units=self.latent_dim,\n",
      "                       dropout=self.dropout,\n",
      "                       return_state=True,\n",
      "                       name=\"encoder_lstm\")\n",
      "        encoder_outputs, state_h, state_c = encoder(self.encoder_inputs, training=training)\n",
      "        # We discard `encoder_outputs` and only keep the states.\n",
      "        self.encoder_states = [state_h, state_c]\n",
      "\n",
      "        # Set up the decoder, using `encoder_states` as initial state.\n",
      "        self.decoder_inputs = Input(shape=(None, self.target_col_num), name=\"decoder_inputs\")\n",
      "        # We set up our decoder to return full output sequences,\n",
      "        # and to return internal states as well. We don't use the\n",
      "        # return states in the training model, but we will use them in inference.\n",
      "        self.decoder_lstm = LSTM(self.latent_dim,\n",
      "                                 dropout=self.dropout,\n",
      "                                 return_sequences=True,\n",
      "                                 return_state=True,\n",
      "                                 name=\"decoder_lstm\")\n",
      "        decoder_outputs, _, _ = self.decoder_lstm(self.decoder_inputs,\n",
      "                                                  training=training,\n",
      "                                                  initial_state=self.encoder_states)\n",
      "\n",
      "        self.decoder_dense = Dense(self.target_col_num, name=\"decoder_dense\")\n",
      "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
      "\n",
      "        # Define the model that will turn\n",
      "        # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
      "        self.model = Model([self.encoder_inputs, self.decoder_inputs], decoder_outputs)\n",
      "        self.model.compile(loss='mse',\n",
      "                           metrics=[self.metric],\n",
      "                           optimizer=keras.optimizers.RMSprop(lr=self.lr))\n",
      "        return self.model\n",
      "\n",
      "    def _restore_model(self):\n",
      "        self.encoder_inputs = self.model.input[0]  # input_1\n",
      "        encoder_outputs, state_h_enc, state_c_enc = self.model.layers[2].output  # lstm_1\n",
      "        self.encoder_states = [state_h_enc, state_c_enc]\n",
      "\n",
      "        self.decoder_inputs = self.model.input[1]  # input_2\n",
      "        self.decoder_lstm = self.model.layers[3]\n",
      "\n",
      "        self.decoder_dense = self.model.layers[4]\n",
      "\n",
      "    def _build_inference(self, mc=False):\n",
      "        training = True if mc else None\n",
      "        # from our previous model - mapping encoder sequence to state vectors\n",
      "        encoder_model = Model(self.encoder_inputs, self.encoder_states)\n",
      "\n",
      "        # A modified version of the decoding stage that takes in predicted target inputs\n",
      "        # and encoded state vectors, returning predicted target outputs and decoder state vectors.\n",
      "        # We need to hang onto these state vectors to run the next step of the inference loop.\n",
      "        decoder_state_input_h = Input(shape=(self.latent_dim,))\n",
      "        decoder_state_input_c = Input(shape=(self.latent_dim,))\n",
      "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
      "\n",
      "        decoder_outputs, state_h, state_c = self.decoder_lstm(self.decoder_inputs,\n",
      "                                                              training=training,\n",
      "                                                              initial_state=decoder_states_inputs)\n",
      "        decoder_states = [state_h, state_c]\n",
      "\n",
      "        decoder_outputs = self.decoder_dense(decoder_outputs)\n",
      "        decoder_model = Model([self.decoder_inputs] + decoder_states_inputs,\n",
      "                              [decoder_outputs] + decoder_states)\n",
      "        return encoder_model, decoder_model\n",
      "\n",
      "    def _decode_sequence(self, input_seq, mc=False):\n",
      "        encoder_model, decoder_model = self._build_inference(mc=mc)\n",
      "        # Encode the input as state vectors.\n",
      "        states_value = encoder_model.predict(input_seq)\n",
      "\n",
      "        # Generate empty target sequence of length 1.\n",
      "        target_seq = np.zeros((len(input_seq), 1, self.target_col_num))\n",
      "\n",
      "        # Populate the first target sequence with end of encoding series value\n",
      "        target_seq[:, 0] = input_seq[:, -1, :self.target_col_num]\n",
      "\n",
      "        # Sampling loop for a batch of sequences - we will fill decoded_seq with predictions\n",
      "        # (to simplify, here we assume a batch of size 1).\n",
      "\n",
      "        decoded_seq = np.zeros((len(input_seq), self.future_seq_len, self.target_col_num))\n",
      "\n",
      "        for i in range(self.future_seq_len):\n",
      "            output, h, c = decoder_model.predict([target_seq] + states_value)\n",
      "\n",
      "            decoded_seq[:, i] = output[:, 0]\n",
      "\n",
      "            # Update the target sequence (of length 1).\n",
      "            target_seq = np.zeros((len(input_seq), 1, self.target_col_num))\n",
      "            target_seq[:, 0] = output[:, 0]\n",
      "\n",
      "            # Update states\n",
      "            states_value = [h, c]\n",
      "\n",
      "        return decoded_seq\n",
      "\n",
      "    def _get_decoder_inputs(self, x, y):\n",
      "        \"\"\"\n",
      "        lagged target series for teacher forcing\n",
      "        decoder_input data is one timestamp ahead of y\n",
      "        :param x: 3-d array in format of (sample_num, past_sequence_len, feature_num)\n",
      "        :param y: 3-d array in format of (sample_num, future_sequence_len, target_col_num)\n",
      "                  Need to expand dimension if y is a 2-d array with one target col\n",
      "        :return: 3-d array of decoder inputs\n",
      "        \"\"\"\n",
      "        decoder_input_data = np.zeros(y.shape)\n",
      "        decoder_input_data[1:, ] = y[:-1, ]\n",
      "        decoder_input_data[0, 0] = x[-1, -1, :self.target_col_num]\n",
      "        decoder_input_data[0, 1:] = y[0, :-1]\n",
      "\n",
      "        return decoder_input_data\n",
      "\n",
      "    def _get_len(self, x, y):\n",
      "        self.past_seq_len = x.shape[1]\n",
      "        self.feature_num = x.shape[2]\n",
      "        # self.future_seq_len = y.shape[1]\n",
      "        self.target_col_num = y.shape[2]\n",
      "\n",
      "    def _expand_y(self, y):\n",
      "        \"\"\"\n",
      "        expand dims for y.\n",
      "        :param y:\n",
      "        :return:\n",
      "        \"\"\"\n",
      "        while len(y.shape) < 3:\n",
      "            y = np.expand_dims(y, axis=2)\n",
      "        return y\n",
      "\n",
      "    def _pre_processing(self, x, y, validation_data):\n",
      "        \"\"\"\n",
      "        pre_process input data.\n",
      "        1. expand dims for y and val_y\n",
      "        2. get decoder inputs for train data\n",
      "        3. get decoder inputs for validation data\n",
      "        :param x: train_x\n",
      "        :param y: train_y\n",
      "        :param validation_data:\n",
      "        :return: network input\n",
      "        \"\"\"\n",
      "        y = self._expand_y(y)\n",
      "        self._get_len(x, y)\n",
      "        decoder_input_data = self._get_decoder_inputs(x, y)\n",
      "        if validation_data is not None:\n",
      "            val_x, val_y = validation_data\n",
      "            val_y = self._expand_y(val_y)\n",
      "            val_decoder_input = self._get_decoder_inputs(val_x, val_y)\n",
      "            validation_data = ([val_x, val_decoder_input], val_y)\n",
      "        return x, y, decoder_input_data, validation_data\n",
      "\n",
      "    def fit_eval(self, data, validation_data=None, mc=False, verbose=0, **config):\n",
      "        \"\"\"\n",
      "        fit for one iteration\n",
      "        :param data: could be a tuple with numpy ndarray with form (x, y)\n",
      "        x: 3-d array in format (no. of samples, past sequence length, 2+feature length),\n",
      "        in the last dimension, the 1st col is the time index (data type needs to be numpy datetime\n",
      "        type, e.g. \"datetime64\"),\n",
      "        the 2nd col is the target value (data type should be numeric)\n",
      "        y: 2-d numpy array in format (no. of samples, future sequence length)\n",
      "        if future sequence length > 1,\n",
      "        or 1-d numpy array in format (no. of samples, ) if future sequence length = 1\n",
      "        :param validation_data: tuple in format (x_test,y_test), data used for validation.\n",
      "        If this is specified, validation result will be the optimization target for automl.\n",
      "        Otherwise, train metric will be the optimization target.\n",
      "        :param config: optimization hyper parameters\n",
      "        :return: the resulting metric\n",
      "        \"\"\"\n",
      "        x, y = data[0], data[1]\n",
      "        x, y, decoder_input_data, validation_data = self._pre_processing(x, y, validation_data)\n",
      "\n",
      "        # if model is not initialized, __build the model\n",
      "        if self.model is None:\n",
      "            self._build_train(mc=mc, **config)\n",
      "\n",
      "        # batch_size = config.get('batch_size', 64)\n",
      "        # lr = self.lr\n",
      "        # name = \"seq2seq-batch_size-{}-epochs-{}-lr-{}-time-{}\"\\\n",
      "        #     .format(batch_size, epochs, lr, time())\n",
      "        # tensorboard = TensorBoard(log_dir=\"logs/\" + name)\n",
      "\n",
      "        hist = self.model.fit([x, decoder_input_data], y,\n",
      "                              validation_data=validation_data,\n",
      "                              batch_size=self.batch_size,\n",
      "                              epochs=config.get(\"epochs\", 10),\n",
      "                              verbose=verbose,\n",
      "                              # callbacks=[tensorboard]\n",
      "                              )\n",
      "        # print(hist.history)\n",
      "\n",
      "        if validation_data is None:\n",
      "            # get train metrics\n",
      "            # results = self.model.evaluate(x, y)\n",
      "            result = hist.history.get(self.metric)[-1]\n",
      "        else:\n",
      "            result = hist.history.get('val_' + str(self.metric))[-1]\n",
      "        return result\n",
      "\n",
      "    def evaluate(self, x, y, metric=['mse']):\n",
      "        \"\"\"\n",
      "        Evaluate on x, y\n",
      "        :param x: input\n",
      "        :param y: target\n",
      "        :param metric: a list of metrics in string format\n",
      "        :return: a list of metric evaluation results\n",
      "        \"\"\"\n",
      "        y_pred = self.predict(x)\n",
      "        # y = np.squeeze(y, axis=2)\n",
      "        if self.target_col_num == 1:\n",
      "            return [Evaluator.evaluate(m, y, y_pred) for m in metric]\n",
      "        else:\n",
      "            return [np.array([Evaluator.evaluate(m, y[:, i, :], y_pred[:, i, :])\n",
      "                              for i in range(self.future_seq_len)])\n",
      "                    for m in metric]\n",
      "\n",
      "    def predict(self, x, mc=False):\n",
      "        \"\"\"\n",
      "        Prediction on x.\n",
      "        :param x: input\n",
      "        :return: predicted y (expected dimension = 2)\n",
      "        \"\"\"\n",
      "        y_pred = self._decode_sequence(x, mc=mc)\n",
      "        if self.target_col_num == 1:\n",
      "            y_pred = np.squeeze(y_pred, axis=2)\n",
      "        return y_pred\n",
      "\n",
      "    def predict_with_uncertainty(self, x, n_iter=100):\n",
      "        result = np.array([self.predict(x, mc=True) for i in range(n_iter)])\n",
      "        prediction = result.mean(axis=0)\n",
      "        uncertainty = result.var(axis=0)\n",
      "        return prediction, uncertainty\n",
      "\n",
      "    def save(self, model_path, config_path):\n",
      "        \"\"\"\n",
      "        save model to file.\n",
      "        :param model_path: the model file path to be saved to.\n",
      "        :param config_path: the config file path to be saved to.\n",
      "        :return:\n",
      "        \"\"\"\n",
      "\n",
      "        self.model.save(model_path)\n",
      "\n",
      "        config_to_save = {\"past_seq_len\": self.past_seq_len,\n",
      "                          \"feature_num\": self.feature_num,\n",
      "                          \"future_seq_len\": self.future_seq_len,\n",
      "                          \"target_col_num\": self.target_col_num,\n",
      "                          \"metric\": self.metric,\n",
      "                          \"latent_dim\": self.latent_dim,\n",
      "                          \"batch_size\": self.batch_size}\n",
      "        save_config(config_path, config_to_save)\n",
      "\n",
      "    def restore(self, model_path, **config):\n",
      "        \"\"\"\n",
      "        restore model from file\n",
      "        :param model_path: the model file\n",
      "        :param config: the trial config\n",
      "        :return: the restored model\n",
      "        \"\"\"\n",
      "\n",
      "        self.past_seq_len = config[\"past_seq_len\"]\n",
      "        self.feature_num = config[\"feature_num\"]\n",
      "        self.future_seq_len = config[\"future_seq_len\"]\n",
      "        self.target_col_num = config[\"target_col_num\"]\n",
      "        self.metric = config[\"metric\"]\n",
      "        self.latent_dim = config[\"latent_dim\"]\n",
      "        self.batch_size = config[\"batch_size\"]\n",
      "\n",
      "        self.model = keras.models.load_model(model_path)\n",
      "        self._restore_model()\n",
      "        # self.model.load_weights(file_path)\n",
      "\n",
      "    def _get_required_parameters(self):\n",
      "        return {\n",
      "            # 'input_shape_x',\n",
      "            # 'input_shape_y',\n",
      "            # 'out_units'\n",
      "        }\n",
      "\n",
      "    def _get_optional_parameters(self):\n",
      "        return {\n",
      "            'past_seq_len'\n",
      "            'latent_dim'\n",
      "            'dropout',\n",
      "            'metric',\n",
      "            'lr',\n",
      "            'epochs',\n",
      "            'batch_size'\n",
      "        }\n",
      "\n",
      "from src.layers.LayerHelper import *\n",
      "from settings import LayerSettings as layerSettings\n",
      "import tensorflow as tf\n",
      "import os\n",
      "CUDA_VISIBLE_DEVICES=0\n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # set gpu number\n",
      "\n",
      "def LSTM(name_, inputTensor_, numberOfOutputs_, isTraining_, dropoutProb_=None):\n",
      "\twith tf.name_scope(name_):\n",
      "\t\tcell = tf.nn.rnn_cell.LSTMCell(num_units=numberOfOutputs_,\n",
      "\t\t\t\t\t\t use_peepholes=True,\n",
      "\t\t\t\t\t\t initializer=layerSettings.LSTM_INITIALIZER,\n",
      "\t\t\t\t\t\t forget_bias=1.0,\n",
      "\t\t\t\t\t\t state_is_tuple=True,\n",
      "\t\t\t\t\t\t activation=tf.nn.tanh,\n",
      "\t\t\t\t\t\t name=name_+\"_cell\")\n",
      "\n",
      "\t\tif dropoutProb_ != None:\n",
      "\t\t\tdropoutProbTensor = tf.cond(isTraining_, lambda: 0.5, lambda: 1.0)\n",
      "\t\t\tcell = tf.nn.rnn_cell.DropoutWrapper(cell,\n",
      "\t\t\t\t\t\t\t     input_keep_prob=dropoutProbTensor,\n",
      "\t\t\t\t\t\t\t     output_keep_prob=dropoutProbTensor)\n",
      "\n",
      "\t\tstatePlaceHolder = tf.nn.rnn_cell.LSTMStateTuple( tf.placeholder(layerSettings.FLOAT_TYPE, [None, numberOfOutputs_]),\n",
      "\t\t\t\t\t\t\t\t  tf.placeholder(layerSettings.FLOAT_TYPE, [None, numberOfOutputs_]) )\n",
      "\n",
      "\t\toutputTensor, stateTensor = tf.nn.dynamic_rnn(\tcell=cell,\n",
      "\t\t\t\t\t\t\t\tinitial_state=statePlaceHolder,\n",
      "\t\t\t\t\t\t\t\tinputs=inputTensor_)\n",
      "\n",
      "\t\t# Add Regularization Loss\n",
      "\t\tfor eachVariable in tf.trainable_variables():\n",
      "\t\t\tif name_ in eachVariable.name:\n",
      "\t\t\t\tif ('bias' not in eachVariable.name)and(layerSettings.REGULARIZER_WEIGHTS_DECAY != None):\n",
      "\t\t\t\t\tregularizationLoss = L2_Regularizer(eachVariable)\n",
      "\t\t\t\t\ttf.losses.add_loss(regularizationLoss, loss_collection=tf.GraphKeys.REGULARIZATION_LOSSES)\n",
      "\t\t\t\t\t\n",
      "\n",
      "\treturn outputTensor, stateTensor, statePlaceHolder\n",
      "\n",
      "\n",
      "#------------------------------------------------------------------------------\n",
      "# Copyright (c) 2005, Enthought, Inc.\n",
      "# All rights reserved.\n",
      "#\n",
      "# This software is provided without warranty under the terms of the BSD\n",
      "# license included in enthought/LICENSE.txt and may be redistributed only\n",
      "# under the conditions described in the aforementioned license.  The license\n",
      "# is also available online at http://www.enthought.com/licenses/BSD.txt\n",
      "# Thanks for using Enthought open source!\n",
      "#\n",
      "# Author: David C. Morrill\n",
      "# Date: 02/02/2004\n",
      "# Description: Dynamically construct Tkinter Menus or MenuBars from a supplied\n",
      "#              string string description of the menu.\n",
      "#------------------------------------------------------------------------------\n",
      "#\n",
      "#  Menu Description Syntax:\n",
      "#\n",
      "#     submenu_label {help_string}\n",
      "#        menuitem_label | accelerator {help_string} [~/-name]: code\n",
      "#        -\n",
      "#\n",
      "#  where:\n",
      "#     submenu_label  = Label of a sub menu\n",
      "#     menuitem_label = Label of a menu item\n",
      "#     help_string    = Help string to display on the status line (optional)\n",
      "#     accelerator    = Accelerator key (e.g. Ctrl-C) (| and key are optional)\n",
      "#     [~]            = Menu item checkable, but not checked initially (optional)\n",
      "#     [/]            = Menu item checkable, and checked initially (optional)\n",
      "#     [-]            = Menu item disabled initially (optional)\n",
      "#     [name]         = Symbolic name used to refer to menu item (optional)\n",
      "#     code           = Python code invoked when menu item is selected\n",
      "#\n",
      "#-------------------------------------------------------------------------------\n",
      "\n",
      "#-------------------------------------------------------------------------------\n",
      "#  Imports:\n",
      "#-------------------------------------------------------------------------------\n",
      "\n",
      "import wx\n",
      "import re\n",
      "import string\n",
      "\n",
      "#-------------------------------------------------------------------------------\n",
      "#  Constants:\n",
      "#-------------------------------------------------------------------------------\n",
      "\n",
      "help_pat    = re.compile( r'(.*){(.*)}(.*)' )\n",
      "options_pat = re.compile( r'(.*)\\[(.*)\\](.*)' )\n",
      "\n",
      "key_map = {\n",
      "    'F1':  wx.WXK_F1,\n",
      "    'F2':  wx.WXK_F2,\n",
      "    'F3':  wx.WXK_F3,\n",
      "    'F4':  wx.WXK_F4,\n",
      "    'F5':  wx.WXK_F5,\n",
      "    'F6':  wx.WXK_F6,\n",
      "    'F7':  wx.WXK_F7,\n",
      "    'F8':  wx.WXK_F8,\n",
      "    'F9':  wx.WXK_F9,\n",
      "    'F10': wx.WXK_F10,\n",
      "    'F11': wx.WXK_F11,\n",
      "    'F12': wx.WXK_F12\n",
      "}\n",
      "\n",
      "#-------------------------------------------------------------------------------\n",
      "#  'MakeMenu' class:\n",
      "#-------------------------------------------------------------------------------\n",
      "\n",
      "class MakeMenu:\n",
      "\n",
      "    # Initialize the globally unique menu ID:\n",
      "    cur_id = 1000\n",
      "\n",
      "    #---------------------------------------------------------------------------\n",
      "    #  Initializes the object:\n",
      "    #---------------------------------------------------------------------------\n",
      "\n",
      "    def __init__ ( self, desc, owner, popup = False, window = None ):\n",
      "        \"\"\" Initializes the object.\n",
      "        \"\"\"\n",
      "        self.owner = owner\n",
      "        if window is None:\n",
      "            window = owner\n",
      "        self.window   = window\n",
      "        self.indirect = getattr( owner, 'call_menu', None )\n",
      "        self.names    = {}\n",
      "        self.desc     = desc.split( '\\n' )\n",
      "        self.index    = 0\n",
      "        self.keys     = []\n",
      "        if popup:\n",
      "            self.menu = menu = wx.Menu()\n",
      "            self.parse( menu, -1 )\n",
      "        else:\n",
      "            self.menu = menu = wx.MenuBar()\n",
      "            self.parse( menu, -1 )\n",
      "            window.SetMenuBar( menu )\n",
      "            if len( self.keys ) > 0:\n",
      "                 window.SetAcceleratorTable( wx.AcceleratorTable( self.keys ) )\n",
      "\n",
      "    #---------------------------------------------------------------------------\n",
      "    #  Recursively parses menu items from the description:\n",
      "    #---------------------------------------------------------------------------\n",
      "\n",
      "    def parse ( self, menu, indent ):\n",
      "        \"\"\" Recursively parses menu items from the description.\n",
      "        \"\"\"\n",
      "\n",
      "        while True:\n",
      "\n",
      "            # Make sure we have not reached the end of the menu description yet:\n",
      "            if self.index >= len( self.desc ):\n",
      "                return\n",
      "\n",
      "            # Get the next menu description line and check its indentation:\n",
      "            dline    = self.desc[ self.index ]\n",
      "            line     = dline.lstrip()\n",
      "            indented = len( dline ) - len( line )\n",
      "            if indented <= indent:\n",
      "                return\n",
      "\n",
      "            # Indicate that the current line has been processed:\n",
      "            self.index += 1\n",
      "\n",
      "            # Check for a blank or comment line:\n",
      "            if (line == '') or (line[0:1] == '#'):\n",
      "                continue\n",
      "\n",
      "            # Check for a menu separator:\n",
      "            if line[0:1] == '-':\n",
      "                menu.AppendSeparator()\n",
      "                continue\n",
      "\n",
      "            # Allocate a new menu ID:\n",
      "            MakeMenu.cur_id += 1\n",
      "            cur_id = MakeMenu.cur_id\n",
      "\n",
      "            # Extract the help string (if any):\n",
      "            help  = ''\n",
      "            match = help_pat.search( line )\n",
      "            if match:\n",
      "                help = ' ' + match.group(2).strip()\n",
      "                line = match.group(1) + match.group(3)\n",
      "\n",
      "            # Check for a menu item:\n",
      "            col = line.find( ':' )\n",
      "            if col >= 0:\n",
      "                handler = line[ col + 1: ].strip()\n",
      "                if handler != '':\n",
      "                    if self.indirect:\n",
      "                        self.indirect( cur_id, handler )\n",
      "                        handler = self.indirect\n",
      "                    else:\n",
      "                        try:\n",
      "                            exec ('def handler(event,self=self.owner):\\n %s\\n' %\n",
      "                                  handler)\n",
      "                        except:\n",
      "                            handler = null_handler\n",
      "                else:\n",
      "                    try:\n",
      "                        exec 'def handler(event,self=self.owner):\\n%s\\n' % (\n",
      "                            self.get_body( indented ), ) in globals()\n",
      "                    except:\n",
      "                        handler = null_handler\n",
      "                wx.EVT_MENU( self.window, cur_id, handler )\n",
      "                not_checked = checked = disabled = False\n",
      "                line        = line[ : col ]\n",
      "                match       = options_pat.search( line )\n",
      "                if match:\n",
      "                    line = match.group(1) + match.group(3)\n",
      "                    not_checked, checked, disabled, name = option_check( '~/-',\n",
      "                              match.group(2).strip() )\n",
      "                    if name != '':\n",
      "                        self.names[ name ] = cur_id\n",
      "                        setattr( self.owner, name, MakeMenuItem( self, cur_id ) )\n",
      "                label = line.strip()\n",
      "                col   = label.find( '|' )\n",
      "                if col >= 0:\n",
      "                    key   = label[ col + 1: ].strip()\n",
      "                    label = '%s%s%s' % ( label[ : col ].strip(), '\\t', key )\n",
      "                    key   = key.upper()\n",
      "                    flag  = wx.ACCEL_NORMAL\n",
      "                    col   = key.find( '-' )\n",
      "                    if col >= 0:\n",
      "                        flag = { 'CTRL':  wx.ACCEL_CTRL,\n",
      "                                 'SHIFT': wx.ACCEL_SHIFT,\n",
      "                                 'ALT':   wx.ACCEL_ALT\n",
      "                                 }.get( key[ : col ].strip(), wx.ACCEL_CTRL )\n",
      "                        key  = key[ col + 1: ].strip()\n",
      "                    code = key_map.get( key, None )\n",
      "                    try:\n",
      "                        if code is None:\n",
      "                            code = ord( key )\n",
      "                        self.keys.append(\n",
      "                            wx.AcceleratorEntry( flag, code, cur_id ) )\n",
      "                    except:\n",
      "                        pass\n",
      "                menu.Append( cur_id, label, help, not_checked or checked )\n",
      "                if checked:\n",
      "                    menu.Check( cur_id, True )\n",
      "                if disabled:\n",
      "                    menu.Enable( cur_id, False )\n",
      "                continue\n",
      "\n",
      "            # Else must be the start of a sub menu:\n",
      "            submenu = wx.Menu()\n",
      "            label   = line.strip()\n",
      "\n",
      "            # Recursively parse the sub-menu:\n",
      "            self.parse( submenu, indented )\n",
      "\n",
      "            # Add the menu to its parent:\n",
      "            try:\n",
      "                menu.AppendMenu( cur_id, label, submenu, help )\n",
      "            except:\n",
      "                # Handle the case where 'menu' is really a 'MenuBar' (which does\n",
      "                # not understand 'MenuAppend'):\n",
      "                menu.Append( submenu, label )\n",
      "\n",
      "    #---------------------------------------------------------------------------\n",
      "    #  Returns the body of an inline method:\n",
      "    #---------------------------------------------------------------------------\n",
      "\n",
      "    def get_body ( self, indent ):\n",
      "        \"\"\" Returns the body of an inline method.\n",
      "        \"\"\"\n",
      "        result = []\n",
      "        while self.index < len( self.desc ):\n",
      "            line = self.desc[ self.index ]\n",
      "            if (len( line ) - len( line.lstrip() )) <= indent:\n",
      "                break\n",
      "            result.append( line )\n",
      "            self.index += 1\n",
      "        result = '\\n'.join( result ).rstrip()\n",
      "        if result != '':\n",
      "            return result\n",
      "        return '  pass'\n",
      "\n",
      "    #---------------------------------------------------------------------------\n",
      "    #  Returns the id associated with a specified name:\n",
      "    #---------------------------------------------------------------------------\n",
      "\n",
      "    def get_id ( self, name ):\n",
      "        \"\"\" Returns the id associated with a specified name.\n",
      "        \"\"\"\n",
      "        if isinstance(name, basestring):\n",
      "            return self.names[ name ]\n",
      "        return name\n",
      "\n",
      "    #---------------------------------------------------------------------------\n",
      "    #  Checks (or unchecks) a menu item specified by name:\n",
      "    #---------------------------------------------------------------------------\n",
      "\n",
      "    def checked ( self, name, check = None ):\n",
      "        \"\"\" Checks (or unchecks) a menu item specified by name.\n",
      "        \"\"\"\n",
      "        if check is None:\n",
      "            return self.menu.IsChecked( self.get_id( name ) )\n",
      "        self.menu.Check( self.get_id( name ), check )\n",
      "\n",
      "    #---------------------------------------------------------------------------\n",
      "    #  Enables (or disables) a menu item specified by name:\n",
      "    #---------------------------------------------------------------------------\n",
      "\n",
      "    def enabled ( self, name, enable = None ):\n",
      "        \"\"\" Enables (or disables) a menu item specified by name.\n",
      "        \"\"\"\n",
      "        if enable is None:\n",
      "            return self.menu.IsEnabled( self.get_id( name ) )\n",
      "        self.menu.Enable( self.get_id( name ), enable )\n",
      "\n",
      "    #---------------------------------------------------------------------------\n",
      "    #  Gets/Sets the label for a menu item:\n",
      "    #---------------------------------------------------------------------------\n",
      "\n",
      "    def label ( self, name, label = None ):\n",
      "        \"\"\" Gets/Sets the label for a menu item.\n",
      "        \"\"\"\n",
      "        if label is None:\n",
      "            return self.menu.GetLabel( self.get_id( name ) )\n",
      "        self.menu.SetLabel( self.get_id( name ), label )\n",
      "\n",
      "#-------------------------------------------------------------------------------\n",
      "#  'MakeMenuItem' class:\n",
      "#-------------------------------------------------------------------------------\n",
      "\n",
      "class MakeMenuItem:\n",
      "\n",
      "    def __init__ ( self, menu, id ):\n",
      "        self.menu = menu\n",
      "        self.id   = id\n",
      "\n",
      "    def checked ( self, check = None ):\n",
      "        return self.menu.checked( self.id, check )\n",
      "\n",
      "    def toggle ( self ):\n",
      "        checked = not self.checked()\n",
      "        self.checked( checked )\n",
      "        return checked\n",
      "\n",
      "    def enabled ( self, enable = None ):\n",
      "        return self.menu.enabled( self.id, enable )\n",
      "\n",
      "    def label ( self, label = None ):\n",
      "        return self.menu.label( self.id, label )\n",
      "\n",
      "#-------------------------------------------------------------------------------\n",
      "#  Determine whether a string contains any specified option characters, and\n",
      "#  remove them if it does:\n",
      "#-------------------------------------------------------------------------------\n",
      "\n",
      "def option_check ( test, string ):\n",
      "    result = []\n",
      "    for char in test:\n",
      "        col = string.find( char )\n",
      "        result.append( col >= 0 )\n",
      "        if col >= 0:\n",
      "            string = string[ : col ] + string[ col + 1: ]\n",
      "    return result + [ string.strip() ]\n",
      "\n",
      "#-------------------------------------------------------------------------------\n",
      "#  Null menu option selection handler:\n",
      "#-------------------------------------------------------------------------------\n",
      "\n",
      "def null_handler ( event ):\n",
      "    print 'null_handler invoked'\n",
      "\n",
      "\n",
      "\"\"\"LCM type definitions\n",
      "This file automatically generated by lcm.\n",
      "DO NOT MODIFY BY HAND!!!!\n",
      "\"\"\"\n",
      "\n",
      "import cStringIO as StringIO\n",
      "import struct\n",
      "\n",
      "class request_t(object):\n",
      "    __slots__ = [\"utime\"]\n",
      "\n",
      "    def __init__(self):\n",
      "        self.utime = 0\n",
      "\n",
      "    def encode(self):\n",
      "        buf = StringIO.StringIO()\n",
      "        buf.write(request_t._get_packed_fingerprint())\n",
      "        self._encode_one(buf)\n",
      "        return buf.getvalue()\n",
      "\n",
      "    def _encode_one(self, buf):\n",
      "        buf.write(struct.pack(\">q\", self.utime))\n",
      "\n",
      "    def decode(data):\n",
      "        if hasattr(data, 'read'):\n",
      "            buf = data\n",
      "        else:\n",
      "            buf = StringIO.StringIO(data)\n",
      "        if buf.read(8) != request_t._get_packed_fingerprint():\n",
      "            raise ValueError(\"Decode error\")\n",
      "        return request_t._decode_one(buf)\n",
      "    decode = staticmethod(decode)\n",
      "\n",
      "    def _decode_one(buf):\n",
      "        self = request_t()\n",
      "        self.utime = struct.unpack(\">q\", buf.read(8))[0]\n",
      "        return self\n",
      "    _decode_one = staticmethod(_decode_one)\n",
      "\n",
      "    _hash = None\n",
      "    def _get_hash_recursive(parents):\n",
      "        if request_t in parents: return 0\n",
      "        tmphash = (0xa686a0e0f882d897) & 0xffffffffffffffff\n",
      "        tmphash  = (((tmphash<<1)&0xffffffffffffffff)  + (tmphash>>63)) & 0xffffffffffffffff\n",
      "        return tmphash\n",
      "    _get_hash_recursive = staticmethod(_get_hash_recursive)\n",
      "    _packed_fingerprint = None\n",
      "\n",
      "    def _get_packed_fingerprint():\n",
      "        if request_t._packed_fingerprint is None:\n",
      "            request_t._packed_fingerprint = struct.pack(\">Q\", request_t._get_hash_recursive([]))\n",
      "        return request_t._packed_fingerprint\n",
      "    _get_packed_fingerprint = staticmethod(_get_packed_fingerprint)\n",
      "\n",
      "\n",
      "from conans import ConanFile, CMake\n",
      "import os\n",
      "\n",
      "channel = os.getenv(\"CONAN_CHANNEL\", \"testing\")\n",
      "username = os.getenv(\"CONAN_USERNAME\", \"memsharded\")\n",
      "\n",
      "class EasyLoggingTestConan(ConanFile):\n",
      "    settings = \"os\", \"compiler\", \"build_type\", \"arch\"\n",
      "    requires = \"easyloggingpp/9.94.1@%s/%s\" % (username, channel)\n",
      "    generators = \"cmake\"\n",
      "\n",
      "    def build(self):\n",
      "        cmake = CMake(self.settings)\n",
      "        self.run('cmake \"%s\" %s' % (self.conanfile_directory, cmake.command_line))\n",
      "        self.run(\"cmake --build . %s\" % cmake.build_config)\n",
      "\n",
      "    def imports(self):\n",
      "        self.copy(\"*.cc\")\n",
      "\n",
      "    def test(self):\n",
      "        os.chdir(\"bin\")\n",
      "        self.run(\".%sexample\" % os.sep)\n",
      "\n",
      "# Author:  Guilherme Aldeia\n",
      "# Contact: guilherme.aldeia@ufabc.edu.br\n",
      "# Version: 1.0.1\n",
      "# Last modified: 06-07-2021 by Guilherme Aldeia\n",
      "\n",
      "\n",
      "\"\"\"Interaction Transformation expression's **Inspector**\n",
      "\n",
      "Sub-module containing three classes to help inspect and explain the\n",
      "results obtained with the itea.\n",
      "\n",
      "- ``ITExpr_explainer``: Implementations of feature importances methods specific\n",
      "  to the Interaction-Transformation representation, and several visualization\n",
      "  tools to help interpret the final expression;\n",
      "- ``ITExpr_inspector``: Based on a more statistical approach, this class \n",
      "  implements methods to measure the quality of the final expression by\n",
      "  calculating information between individual terms;\n",
      "- ``ITExpr_texifier``: Creation of latex representations of the final expression\n",
      "  and its derivatives. In cases where the final expression is simple enough,\n",
      "  the analysis of the expression can provide useful insights.\n",
      "\n",
      "All the modules are designed to work with `ITExpr`s. After the evolutionary\n",
      "process is performed (by calling `fit()` on the `ITEA_classifier` or\n",
      "`ITEA_regressor`), the best final expression can be accessed by\n",
      "`itea.bestsol_`, and those classes are specialized in different ways of\n",
      "inspecting the final model.\n",
      "\n",
      "Additionally, there is one class designed to work with the ´`itea``, instead\n",
      "of ``ITExpr`` expressions. The class ``ITEA_summarizer`` implements a method\n",
      "to automatically create a pdf file containing information generated with \n",
      "all the inspection classes, in an attempt to automate the task of generating\n",
      "an interpretability report. \n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from itea.inspection._ITExpr_explainer import ITExpr_explainer\n",
      "from itea.inspection._ITExpr_inspector import ITExpr_inspector\n",
      "from itea.inspection._ITExpr_texifier  import ITExpr_texifier\n",
      "from itea.inspection._ITEA_summarizer  import ITEA_summarizer\n",
      "\n",
      "import jax\n",
      "\n",
      "\n",
      "# Must be used at startup. We'll perform lightweight usage with jax \n",
      "jax.config.update('jax_platform_name', 'cpu')\n",
      "\n",
      "\n",
      "__all__ = [\n",
      "    'ITExpr_explainer',\n",
      "    'ITExpr_inspector',\n",
      "    'ITExpr_texifier',\n",
      "    'ITEA_summarizer'\n",
      "]\n",
      "# write your first unittest!\n",
      "import unittest\n",
      "from ovos_plugin_manager.skills import find_skill_plugins\n",
      "\n",
      "\n",
      "class TestPlugin(unittest.TestCase):\n",
      "    @classmethod\n",
      "    def setUpClass(self):\n",
      "        self.skill_id = \"ovos-skill-timer.OpenVoiceOS\"\n",
      "\n",
      "    def test_find_plugin(self):\n",
      "        plugins = find_skill_plugins()\n",
      "        self.assertIn(self.skill_id, list(plugins))\n",
      "\n",
      "\n",
      "from keras.preprocessing.image import ImageDataGenerator\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Convolution2D, MaxPooling2D\n",
      "from keras.layers import Activation, Dropout, Flatten, Dense, Lambda, ELU\n",
      "from keras.optimizers import Adam\n",
      "from sklearn.model_selection import train_test_split\n",
      "from keras.models import model_from_json\n",
      "from sklearn.preprocessing import normalize\n",
      "import cv2\n",
      "import numpy as np\n",
      "import glob\n",
      "import json\n",
      "from keras.layers import merge\n",
      "from keras.layers.core import Lambda\n",
      "from keras.models import Model\n",
      "\n",
      "import tensorflow as tf\n",
      "\n",
      "\n",
      "def make_parallel(model, gpu_count):\n",
      "    def get_slice(data, idx, parts):\n",
      "        shape = tf.shape(data)\n",
      "        size = tf.concat(0, [shape[:1] // parts, shape[1:]])\n",
      "        stride = tf.concat(0, [shape[:1] // parts, shape[1:] * 0])\n",
      "        start = stride * idx\n",
      "        return tf.slice(data, start, size)\n",
      "\n",
      "    outputs_all = []\n",
      "    for i in range(len(model.outputs)):\n",
      "        outputs_all.append([])\n",
      "\n",
      "    # Place a copy of the model on each GPU, each getting a slice of the batch\n",
      "    for i in range(gpu_count):\n",
      "        with tf.device('/gpu:%d' % i):\n",
      "            with tf.name_scope('tower_%d' % i) as scope:\n",
      "\n",
      "                inputs = []\n",
      "                # Slice each input into a piece for processing on this GPU\n",
      "                for x in model.inputs:\n",
      "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
      "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx': i, 'parts': gpu_count})(x)\n",
      "                    inputs.append(slice_n)\n",
      "\n",
      "                outputs = model(inputs)\n",
      "\n",
      "                if not isinstance(outputs, list):\n",
      "                    outputs = [outputs]\n",
      "\n",
      "                # Save all the outputs for merging back together later\n",
      "                for l in range(len(outputs)):\n",
      "                    outputs_all[l].append(outputs[l])\n",
      "\n",
      "    # merge outputs on CPU\n",
      "    with tf.device('/cpu:0'):\n",
      "        merged = []\n",
      "        for outputs in outputs_all:\n",
      "            merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
      "\n",
      "        return Model(input=model.inputs, output=merged)\n",
      "\n",
      "\n",
      "class CNNClassifier:\n",
      "    def __init__(self):\n",
      "        self.classifier = None\n",
      "\n",
      "    def get_model(self, parallel=False):\n",
      "        model = Sequential()\n",
      "        #model.add(Lambda(lambda x: x / 127.5 - 1., input_shape=(64, 64, 3)))\n",
      "        model.add(Convolution2D(8, 8, 8, subsample=(4, 4), border_mode=\"same\", activation='elu', name='Conv1'))\n",
      "        model.add(Convolution2D(16, 5, 5, subsample=(2, 2), border_mode=\"same\", activation='elu', name='Conv2'))\n",
      "        model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\", activation='elu', name='Conv3'))\n",
      "        model.add(Flatten())\n",
      "        model.add(ELU())\n",
      "        model.add(Dense(1024, activation='elu'))\n",
      "        model.add(Dropout(.5))\n",
      "        model.add(ELU())\n",
      "        model.add(Dense(512, activation='elu'))\n",
      "        model.add(Dropout(.5))\n",
      "        model.add(Dense(1, name='output'))\n",
      "        model.add(Activation('sigmoid'))\n",
      "        if parallel:\n",
      "            model = make_parallel(model, 2)\n",
      "        #model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
      "        self.model = model\n",
      "        return model\n",
      "\n",
      "    def _model(self):\n",
      "        img_width, img_height = 64, 64\n",
      "        model = Sequential()\n",
      "        model.add(Convolution2D(8, 3, 3, input_shape=(img_width, img_height, 3)))\n",
      "        model.add(Activation('elu'))\n",
      "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "        #model.add(Convolution2D(16, 3, 3))\n",
      "        #model.add(Activation('elu'))\n",
      "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "        #model.add(Convolution2D(32, 3, 3))\n",
      "        #model.add(Activation('elu'))\n",
      "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
      "\n",
      "        model.add(Flatten())\n",
      "        model.add(Dense(512))\n",
      "        model.add(Dropout(0.5))\n",
      "        model.add(Dense(1, activation='sigmoid'))\n",
      "        #model = make_parallel(model, 2)\n",
      "        self.model = model\n",
      "\n",
      "    def compile(self):\n",
      "        self.model.compile(loss='binary_crossentropy',\n",
      "                      optimizer='rmsprop', class_mode='binary',\n",
      "                      metrics=['accuracy'])\n",
      "\n",
      "    def save(self):\n",
      "        model_json = self.model.to_json()\n",
      "        with open(\"./model.json\", \"w\") as json_file:\n",
      "            json.dump(model_json, json_file)\n",
      "        self.model.save_weights(\"./model.h5\")\n",
      "        print(\"Saved model to disk\")\n",
      "\n",
      "    def load(self):\n",
      "        with open('./model.json', 'r') as jfile:\n",
      "            self.model = model_from_json(json.load(jfile))\n",
      "\n",
      "        self.compile()\n",
      "        self.model.load_weights('./model.h5')\n",
      "\n",
      "    def get_list(self):\n",
      "        vehicles = np.array(glob.glob('training_data/vehicles/*/*'))\n",
      "        y_vehicles = np.zeros(vehicles.shape) + 1\n",
      "        non_vehicles = np.array(glob.glob('training_data/non-vehicles/*/*'))\n",
      "        y_non_vehicles = np.zeros(non_vehicles.shape)\n",
      "        X_data = np.concatenate((vehicles, non_vehicles))\n",
      "        Y_data = np.concatenate((y_vehicles, y_non_vehicles))\n",
      "        return X_data, Y_data\n",
      "\n",
      "    def predict(self, image):\n",
      "        #img = np.copy(image)\n",
      "        #img = cv2.resize(img, (64, 64))\n",
      "        x = image[None, :, :, :]\n",
      "        result = self.model.predict(x, 1)\n",
      "        return result\n",
      "\n",
      "    def train(self, file_list, labels, test_size=0.2, nb_epoch=30, batch_size=128):\n",
      "        X_train, X_test, Y_train, Y_test = train_test_split(file_list, labels, test_size=test_size, random_state=100)\n",
      "\n",
      "        test_images = build_images(X_test)\n",
      "        train_images = build_images(X_train)\n",
      "\n",
      "        train_datagen = ImageDataGenerator(\n",
      "            rescale=1. / 255,\n",
      "            shear_range=0.05,\n",
      "            zoom_range=0.05,\n",
      "            width_shift_range=0.1,\n",
      "            height_shift_range=0.1,\n",
      "            rotation_range=5,\n",
      "            horizontal_flip=True)\n",
      "        test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
      "        train_generator = train_datagen.flow(train_images, Y_train, batch_size)\n",
      "        test_generator = test_datagen.flow(test_images, Y_test, batch_size)\n",
      "\n",
      "        nb_train_samples = (batch_size-1)*100\n",
      "        nb_validation_samples = (batch_size-1)*20\n",
      "\n",
      "        #self.get_model(parallel=False)\n",
      "        self._model()\n",
      "        self.compile()\n",
      "\n",
      "        self.model.fit_generator(\n",
      "            train_generator,\n",
      "            samples_per_epoch=nb_train_samples,\n",
      "            nb_epoch=nb_epoch, show_accuracy=True,\n",
      "            validation_data=test_generator,\n",
      "            nb_val_samples=nb_validation_samples)\n",
      "\n",
      "def build_images(x):\n",
      "    images = np.zeros((len(x), 64, 64, 3))\n",
      "    for idx, img_fname in enumerate(x):\n",
      "        im = cv2.imread(img_fname)\n",
      "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
      "        im = cv2.resize(im, (64, 64), interpolation=cv2.INTER_AREA)\n",
      "        images[idx] = im\n",
      "    return images\n",
      "\n",
      "def do_all(nb_epoch=30, batch_size=256):\n",
      "    clf = CNNClassifier()\n",
      "    x, y = clf.get_list()\n",
      "    clf.train(x, y, nb_epoch=nb_epoch, batch_size=batch_size)\n",
      "    clf.save()\n",
      "\n",
      "\n",
      "import pytest\n",
      "from brownie import interface\n",
      "\n",
      "\n",
      "def test_uniswap_add_two_tokens(\n",
      "    admin, alice, chain, bank, werc20, ufactory, urouter, simple_oracle, oracle, celo, cusd, ceur, UniswapV2SpellV1, UniswapV2Oracle, core_oracle\n",
      "):\n",
      "    spell = UniswapV2SpellV1.deploy(bank, werc20, urouter, celo, {'from': admin})\n",
      "    cusd.mint(admin, 10000000 * 10**6, {'from': admin})\n",
      "    ceur.mint(admin, 10000000 * 10**6, {'from': admin})\n",
      "    cusd.approve(urouter, 2**256-1, {'from': admin})\n",
      "    ceur.approve(urouter, 2**256-1, {'from': admin})\n",
      "    urouter.addLiquidity(\n",
      "        cusd,\n",
      "        ceur,\n",
      "        1000000 * 10**6,\n",
      "        1000000 * 10**6,\n",
      "        0,\n",
      "        0,\n",
      "        admin,\n",
      "        chain.time() + 60,\n",
      "        {'from': admin},\n",
      "    )\n",
      "\n",
      "    lp = ufactory.getPair(cusd, ceur)\n",
      "    print('admin lp bal', interface.IERC20(lp).balanceOf(admin))\n",
      "    uniswap_lp_oracle = UniswapV2Oracle.deploy(core_oracle, {'from': admin})\n",
      "\n",
      "    print('ceur Px', simple_oracle.getCELOPx(ceur))\n",
      "    print('cusd Px', simple_oracle.getCELOPx(cusd))\n",
      "\n",
      "    core_oracle.setRoute([cusd, ceur, lp], [simple_oracle, simple_oracle, uniswap_lp_oracle])\n",
      "    print('lp Px', uniswap_lp_oracle.getCELOPx(lp))\n",
      "\n",
      "    oracle.setTokenFactors(\n",
      "        [cusd, ceur, lp],\n",
      "        [\n",
      "            [10000, 10000, 10000],\n",
      "            [10000, 10000, 10000],\n",
      "            [10000, 10000, 10000],\n",
      "        ],\n",
      "        {'from': admin},\n",
      "    )\n",
      "    cusd.mint(alice, 10000000 * 10**6, {'from': admin})\n",
      "    ceur.mint(alice, 10000000 * 10**6, {'from': admin})\n",
      "    cusd.approve(bank, 2**256-1, {'from': alice})\n",
      "    ceur.approve(bank, 2**256-1, {'from': alice})\n",
      "    spell.getAndApprovePair(cusd, ceur, {'from': admin})\n",
      "    lp = ufactory.getPair(cusd, ceur)\n",
      "    spell.setWhitelistLPTokens([lp], [True], {'from': admin})\n",
      "    bank.setWhitelistSpells([spell], [True], {'from': admin})\n",
      "    bank.setWhitelistTokens([cusd, ceur], [True, True], {'from': admin})\n",
      "    tx = bank.execute(\n",
      "        0,\n",
      "        spell,\n",
      "        spell.addLiquidityWERC20.encode_input(\n",
      "            ceur,  # token 0\n",
      "            cusd,  # token 1\n",
      "            [\n",
      "                40000 * 10**6,  # 40000 ceur\n",
      "                50000 * 10**6,  # 50000 cusd\n",
      "                0,\n",
      "                1000 * 10**6,  # 1000 ceur\n",
      "                200 * 10**6,  # 200 cusd\n",
      "                0,  # borrow LP tokens\n",
      "                0,  # min ceur\n",
      "                0,  # min cusd\n",
      "            ],\n",
      "        ),\n",
      "        {'from': alice}\n",
      "    )\n",
      "\n",
      "    position_id = tx.return_value\n",
      "    print('tx gas used', tx.gas_used)\n",
      "    print('bank collateral size', bank.getPositionInfo(position_id))\n",
      "    print('bank collateral value', bank.getCollateralCELOValue(position_id))\n",
      "    print('bank borrow value', bank.getBorrowCELOValue(position_id))\n",
      "\n",
      "    print('bank ceur', bank.getBankInfo(ceur))\n",
      "    print('bank cusd', bank.getBankInfo(cusd))\n",
      "\n",
      "    print('ceur Px', simple_oracle.getCELOPx(ceur))\n",
      "    print('cusd Px', simple_oracle.getCELOPx(cusd))\n",
      "\n",
      "    print('lp Px', uniswap_lp_oracle.getCELOPx(lp))\n",
      "\n",
      "\"\"\"\n",
      "This module patches a few core functions to add compression capabilities,\n",
      "since gevent-websocket does not appear to be maintained anymore.\n",
      "\"\"\"\n",
      "from socket import error\n",
      "from zlib import (\n",
      "    decompressobj,\n",
      "    MAX_WBITS,\n",
      "    Z_FULL_FLUSH,\n",
      ")\n",
      "\n",
      "from geventwebsocket.exceptions import (\n",
      "    ProtocolError,\n",
      "    WebSocketError,\n",
      ")\n",
      "from geventwebsocket.websocket import (\n",
      "    MSG_SOCKET_DEAD,\n",
      "    Header,\n",
      "    WebSocket,\n",
      ")\n",
      "\n",
      "\n",
      "DECOMPRESSOR = decompressobj(-MAX_WBITS)\n",
      "\n",
      "\n",
      "def _encode_bytes(text):\n",
      "    if isinstance(text, str):\n",
      "        return text\n",
      "\n",
      "    if not isinstance(text, unicode):\n",
      "        text = unicode(text or '')\n",
      "\n",
      "    return text.encode('utf-8')\n",
      "\n",
      "\n",
      "def make_compressed_frame(message, compressor):\n",
      "    \"\"\"\n",
      "    Make a compressed websocket frame from a message and compressor.\n",
      "\n",
      "    Generates header and a compressed message which can then be used on any\n",
      "    websocket connection where `no_context_takeover` has been negotiated.\n",
      "    This prevents the need to re-compress a broadcast-style message for every\n",
      "    websocket connection.\n",
      "\n",
      "    `compressor` is a zlib compressor object.\n",
      "    \"\"\"\n",
      "    binary = not isinstance(message, (str, unicode))\n",
      "    opcode = WebSocket.OPCODE_BINARY if binary else WebSocket.OPCODE_TEXT\n",
      "    if binary:\n",
      "        message = str(message)\n",
      "    else:\n",
      "        message = _encode_bytes(message)\n",
      "    message = compressor.compress(message)\n",
      "    # We use Z_FULL_FLUSH (rather than Z_SYNC_FLUSH) here when\n",
      "    # server_no_context_takeover has been passed, to reset the context at\n",
      "    # the end of every frame.  Patches to the actual gevent-websocket\n",
      "    # library should probably be able to support both.\n",
      "    message += compressor.flush(Z_FULL_FLUSH)\n",
      "    # See https://tools.ietf.org/html/rfc7692#page-19\n",
      "    if message.endswith('\\x00\\x00\\xff\\xff'):\n",
      "        message = message[:-4]\n",
      "\n",
      "    # Generate header.  The RSV0 bit indicates the payload is compressed.\n",
      "    flags = Header.RSV0_MASK\n",
      "    header = Header.encode_header(\n",
      "        fin=True, opcode=opcode, mask='', length=len(message), flags=flags)\n",
      "\n",
      "    return header + message\n",
      "\n",
      "\n",
      "def send_raw_frame(websocket, raw_message):\n",
      "    \"\"\"\n",
      "    `raw_message` includes both the header and the encoded message.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        websocket.raw_write(raw_message)\n",
      "    except error:\n",
      "        websocket.current_app.on_close(MSG_SOCKET_DEAD)\n",
      "        raise WebSocketError(MSG_SOCKET_DEAD)\n",
      "\n",
      "\n",
      "def read_frame(websocket):\n",
      "    # Patched `read_frame` method that supports decompression\n",
      "\n",
      "    header = Header.decode_header(websocket.stream)\n",
      "\n",
      "    # Start patched lines\n",
      "    compressed = header.flags & header.RSV0_MASK\n",
      "    if compressed:\n",
      "        header.flags &= ~header.RSV0_MASK\n",
      "    # End patched lines\n",
      "\n",
      "    if header.flags:\n",
      "        raise ProtocolError\n",
      "\n",
      "    if not header.length:\n",
      "        return header, ''\n",
      "\n",
      "    try:\n",
      "        payload = websocket.raw_read(header.length)\n",
      "    except error:\n",
      "        payload = ''\n",
      "    except Exception:\n",
      "\n",
      "        # Start patched lines\n",
      "        raise WebSocketError('Could not read payload')\n",
      "        # End patched lines\n",
      "\n",
      "    if len(payload) != header.length:\n",
      "        raise WebSocketError('Unexpected EOF reading frame payload')\n",
      "\n",
      "    if header.mask:\n",
      "        payload = header.unmask_payload(payload)\n",
      "\n",
      "    # Start patched lines\n",
      "    if compressed:\n",
      "        payload = ''.join((\n",
      "            DECOMPRESSOR.decompress(payload),\n",
      "            DECOMPRESSOR.decompress('\\0\\0\\xff\\xff'),\n",
      "            DECOMPRESSOR.flush(),\n",
      "        ))\n",
      "    # End patched lines\n",
      "\n",
      "    return header, payload\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "ds = load_dataset(\"bigcode/the-stack\", data_dir=\"data/python\", streaming=True, split=\"train\")\n",
    "\n",
    "# Initialize a counter\n",
    "counter = 0\n",
    "\n",
    "# Iterate over the dataset\n",
    "dataset = {\n",
    "    \"sample\": [],\n",
    "    \"label\": [],\n",
    "    \"logprob\": []\n",
    "}\n",
    "\n",
    "for sample in ds:\n",
    "    dataset[\"sample\"].append(sample[\"content\"])\n",
    "    print(sample[\"content\"])\n",
    "    counter += 1\n",
    "    if counter >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "\"\"\"The setup script.\"\"\"\n",
      "\n",
      "from setuptools import find_packages, setup\n",
      "\n",
      "test_requirements = [\n",
      "    \"black>=19.10b0\",\n",
      "    \"flake8>=3.8.3\",\n",
      "    \"flake8-debugger>=3.2.1\",\n",
      "]\n",
      "\n",
      "dev_requirements = [\n",
      "    *test_requirements,\n",
      "    \"wheel>=0.34.2\",\n",
      "]\n",
      "\n",
      "requirements = [\n",
      "    \"cdp-backend[pipeline]==3.0.2\",\n",
      "    \"cdp-scrapers[king_county]>=0.3.2\",\n",
      "]\n",
      "\n",
      "extra_requirements = {\n",
      "    \"test\": test_requirements,\n",
      "    \"dev\": dev_requirements,\n",
      "    \"all\": [\n",
      "        *requirements,\n",
      "        *dev_requirements,\n",
      "    ],\n",
      "}\n",
      "\n",
      "setup(\n",
      "    author=\"JacksonMaxfield\",\n",
      "    classifiers=[\n",
      "        \"Development Status :: 2 - Pre-Alpha\",\n",
      "        \"Intended Audience :: Developers\",\n",
      "        \"License :: OSI Approved :: MIT License\",\n",
      "        \"Natural Language :: English\",\n",
      "        \"Programming Language :: Python :: 3.9\",\n",
      "    ],\n",
      "    description=\"Package containing the gather functions for Example.\",\n",
      "    install_requires=requirements,\n",
      "    license=\"MIT license\",\n",
      "    long_description_content_type=\"text/markdown\",\n",
      "    include_package_data=True,\n",
      "    keywords=\"civic technology, open government\",\n",
      "    name=\"cdp-king_county-backend\",\n",
      "    packages=find_packages(exclude=[\"tests\", \"*.tests\", \"*.tests.*\"]),\n",
      "    python_requires=\">=3.9\",\n",
      "    tests_require=test_requirements,\n",
      "    extras_require=extra_requirements,\n",
      "    url=\"https://github.com/CouncilDataProject/king-county\",\n",
      "    version=\"1.0.0\",\n",
      "    zip_safe=False,\n",
      ")\n",
      "\n",
      "-------\n",
      "from django.utils import timezone\n",
      "from rest_framework.authtoken.models import Token\n",
      "\n",
      "\n",
      "class AuthTokenHandler:\n",
      "    \"\"\"\n",
      "    Handles variations in auth token\n",
      "    \"\"\"\n",
      "    @staticmethod\n",
      "    def expired_token(auth_token):\n",
      "        \"\"\"\n",
      "        Checks expiry of auth token\n",
      "        \"\"\"\n",
      "        utc_now = timezone.now()\n",
      "        expired = auth_token.created < utc_now - \\\n",
      "            timezone.timedelta(hours=24)\n",
      "        return expired\n",
      "\n",
      "    @staticmethod\n",
      "    def create_auth_token(user):\n",
      "        \"\"\"\n",
      "        Creates an auth token for a user\n",
      "        \"\"\"\n",
      "        token, created = Token.objects.get_or_create(user=user)\n",
      "        if not created:\n",
      "            token.created = timezone.now()\n",
      "            token.save()\n",
      "        return token\n",
      "\n",
      "-------\n",
      "#!/usr/bin/env python\n",
      "#-*- encoding: UTF-8 -*-\n",
      "\n",
      "###############################################\n",
      "# Todos los derechos reservados a:            #\n",
      "# CreceLibre Consultores en Tecnologías Ltda. #\n",
      "#                                             #\n",
      "# ©Milton Inostroza Aguilera                  #\n",
      "# minostro@minostro.com                       #\n",
      "# 2009                                        #\n",
      "###############################################\n",
      "from django.db import models\n",
      "from AlyMoly.mantenedor.models import Producto, Promocion, Trabajador\n",
      "\n",
      "\n",
      "class Turno(models.Model):\n",
      "    \"\"\"\n",
      "        estado:\n",
      "            1 --> abierto\n",
      "            2 --> cerrado\n",
      "    \"\"\"\n",
      "    fecha_apertura_sistema = models.DateTimeField()\n",
      "    fecha_cierre_sistema = models.DateTimeField(null=True, blank=True)\n",
      "    estado = models.IntegerField(default=1, blank=True)\n",
      "    trabajador = models.ForeignKey(Trabajador, blank=True)\n",
      "    monto_apertura_caja = models.IntegerField(default=0)\n",
      "    monto_cierre_calculado = models.IntegerField(default=0, blank=True)\n",
      "    monto_afecto = models.IntegerField(default=0, blank=True)\n",
      "    monto_exento = models.IntegerField(default=0, blank=True)\n",
      "\n",
      "    def monto_cierre_informado(self):\n",
      "        return self.boletadeposito.total\n",
      "\n",
      "    def estado_turno(self):\n",
      "        if self.estado == 1:\n",
      "            return \"Abierto\"\n",
      "        else:\n",
      "            return \"Cerrado\"\n",
      "\n",
      "    def save(self, force_insert=False, force_update=False):\n",
      "        \"\"\"\n",
      "            Al guardar un turno abierto se verifica que el trabajador ya no cuente con un\n",
      "            turno abierto anteriormente.\n",
      "        \"\"\"\n",
      "        if self.estado == 1 and len(Turno.objects.exclude(id=self.id).filter(trabajador__id=self.trabajador.id).filter(estado=1)) > 0:\n",
      "            raise Exception(u\"Usted ya cuenta con un turno abierto.\")\n",
      "        super(Turno, self).save(force_insert, force_update)\n",
      "\n",
      "\n",
      "class BoletaDeposito(models.Model):\n",
      "    turno = models.OneToOneField(Turno, blank=True)\n",
      "    veintemil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    diezmil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    cincomil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    dosmil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    mil = models.PositiveIntegerField(default=0, blank=True)\n",
      "    quinientos = models.PositiveIntegerField(default=0, blank=True)\n",
      "    cien = models.PositiveIntegerField(default=0, blank=True)\n",
      "    cincuenta = models.PositiveIntegerField(default=0, blank=True)\n",
      "    diez = models.PositiveIntegerField(default=0, blank=True)\n",
      "    tarjetas = models.PositiveIntegerField(default=0, blank=True)\n",
      "    otros = models.PositiveIntegerField(default=0, blank=True)\n",
      "    total = models.PositiveIntegerField(default=0, blank=True)\n",
      "\n",
      "\n",
      "class Venta(models.Model):\n",
      "    \"\"\"\n",
      "        medio_pago:\n",
      "            1 --> efectivo\n",
      "            2 --> otro\n",
      "    \"\"\"\n",
      "    fecha_venta = models.DateTimeField()\n",
      "    folio_boleta = models.PositiveIntegerField(null=True, blank=True)\n",
      "    monto_total = models.PositiveIntegerField()\n",
      "    monto_afecto = models.PositiveIntegerField()\n",
      "    monto_exento = models.PositiveIntegerField()\n",
      "    cantidad_productos = models.PositiveIntegerField()\n",
      "    medio_pago = models.PositiveIntegerField()\n",
      "    monto_pago = models.PositiveIntegerField(null=True)\n",
      "    turno = models.ForeignKey('Turno')\n",
      "\n",
      "    def __unicode__(self):\n",
      "        return u\"%s-%s\" % (self.id, self.folio_boleta)\n",
      "\n",
      "\n",
      "class LineaDetalle(models.Model):\n",
      "    cantidad = models.IntegerField()\n",
      "    precio_venta = models.IntegerField()\n",
      "    precio_venta_total = models.IntegerField()\n",
      "    producto = models.ForeignKey(Producto, null=True, blank=True)\n",
      "    promocion = models.ForeignKey(Promocion, null=True, blank=True)\n",
      "    venta = models.ForeignKey('Venta')\n",
      "\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "# Randomly check some examples from the dataset\n",
    "import random \n",
    "\n",
    "random_samples = random.choices(dataset[\"sample\"], k=3)\n",
    "for i in random_samples: \n",
    "    print(i)\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label data with GPT-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_labeling(sample: str, model_type: str=\"gpt-3.5-turbo-0125\"): \n",
    "    response = client.chat.completions.create(\n",
    "        model=model_type,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"\n",
    "                You are an AI assistant and your job is to classify. \n",
    "                Your job is to determine its educational value for a student whose goal is to learn basic coding concepts. \n",
    "            \n",
    "                Here are the main points if an example is of a bad quality: \n",
    "                - Many samples are not self-contained, meaning that they depend on other modules or files that are\n",
    "                external to the snippet, making them hard to understand without additional context.\n",
    "                - Typical examples do not involve any meaningful computation, but rather consist of trivial or boil-\n",
    "                erplate code, such as defining constants, setting parameters, or configuring GUI elements.\n",
    "                - Samples that do contain algorithmic logic are often buried inside complex or poorly documented\n",
    "                functions, making them difficult to follow or learn from.\n",
    "                - The examples are skewed towards certain topics or use cases, resulting in an unbalanced distribution\n",
    "                of coding concepts and skills across the dataset.\n",
    "            \n",
    "                If the educational value is high, return a 1. If the educational value is low, return a 0. \n",
    "                Return ONLY a number and nothing else. Otherwise I will NOT process your output!\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Code example: {sample[:10000]}\"},\n",
    "            {\"role\": \"user\", \"content\": \"Classification: \"}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        logprobs=True,\n",
    "        logit_bias={15: 1, 16: 1},\n",
    "        max_tokens=1, \n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fd58bc39cd47ffa6e03e48f59b8872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(len(dataset[\"sample\"]))): \n",
    "    # Label data with GPT-3.5\n",
    "    response = gpt_labeling(sample=dataset[\"sample\"][i])\n",
    "\n",
    "    # Get the label from the response\n",
    "    label = response.choices[0].message.content\n",
    "    logprobs = response.choices[0].logprobs.content[0].logprob\n",
    "\n",
    "    # Add the label and prob to the dataset\n",
    "    dataset[\"label\"].append(int(label))\n",
    "    dataset[\"logprob\"].append(float(logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"sample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"logprob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/training-subset-labeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_value = df[df[\"label\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the synthetic textbook dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the synthetic excercise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
